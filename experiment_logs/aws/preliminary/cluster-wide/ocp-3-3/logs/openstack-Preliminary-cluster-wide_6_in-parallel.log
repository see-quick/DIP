[INFO] Scanning for projects...
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Build Order:
[INFO] 
[INFO] Strimzi - Apache Kafka on Kubernetes and OpenShift                 [pom]
[INFO] test                                                               [jar]
[INFO] crd-annotations                                                    [jar]
[INFO] crd-generator                                                      [jar]
[INFO] api                                                                [jar]
[INFO] mockkube                                                           [jar]
[INFO] config-model                                                       [jar]
[INFO] certificate-manager                                                [jar]
[INFO] operator-common                                                    [jar]
[INFO] systemtest                                                         [jar]
[INFO] 
[INFO] -------------------------< io.strimzi:strimzi >-------------------------
[INFO] Building Strimzi - Apache Kafka on Kubernetes and OpenShift 0.29.0-SNAPSHOT [1/10]
[INFO] --------------------------------[ pom ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ strimzi ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ strimzi ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ strimzi ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/target/jacoco.exec
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:report (report) @ strimzi ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] >>> maven-source-plugin:3.0.1:jar (attach-sources) > generate-sources @ strimzi >>>
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ strimzi ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ strimzi ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ strimzi ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/target/jacoco.exec
[INFO] 
[INFO] <<< maven-source-plugin:3.0.1:jar (attach-sources) < generate-sources @ strimzi <<<
[INFO] 
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar (attach-sources) @ strimzi ---
[INFO] 
[INFO] --- maven-javadoc-plugin:3.1.0:jar (attach-javadocs) @ strimzi ---
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:integration-test (default) @ strimzi ---
[INFO] No tests to run.
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:verify (default) @ strimzi ---
[INFO] 
[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (analyze) @ strimzi ---
[INFO] Skipping pom project
[INFO] 
[INFO] --------------------------< io.strimzi:test >---------------------------
[INFO] Building test 0.29.0-SNAPSHOT                                     [2/10]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ test ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ test ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ test ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/test/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ test ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/test/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ test ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ test ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/test/src/test/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ test ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M5:test (default-test) @ test ---
[INFO] No tests to run.
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ test ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:report (report) @ test ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] >>> maven-source-plugin:3.0.1:jar (attach-sources) > generate-sources @ test >>>
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ test ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ test ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ test ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/test/target/jacoco.exec
[INFO] 
[INFO] <<< maven-source-plugin:3.0.1:jar (attach-sources) < generate-sources @ test <<<
[INFO] 
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar (attach-sources) @ test ---
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/test/target/test-0.29.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] --- maven-javadoc-plugin:3.1.0:jar (attach-javadocs) @ test ---
[INFO] 
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/EmbeddedZooKeeper.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/WaitException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/executor/ExecResult.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/executor/Exec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/cluster/OpenShift.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/cluster/KubeCluster.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/cluster/Kubernetes.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/cluster/Minikube.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/cmdClient/Kubectl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/cmdClient/Oc.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/cmdClient/BaseCmdKubeClient.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/cmdClient/KubeCmdClient.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/exceptions/KubeClusterException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/exceptions/NoClusterException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/HelmClient.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/KubeClient.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/KubeClusterResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/annotations/IsolatedTest.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/annotations/ParallelSuite.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/annotations/ParallelTest.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/annotations/IsolatedSuite.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/interfaces/ExtensionContextParameterResolver.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/interfaces/TestSeparator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/TestUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/logs/CollectorElement.java...
Constructing Javadoc information...
Standard Doclet version 11.0.11
Building tree for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/IsolatedSuite.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/IsolatedTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/ParallelSuite.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/ParallelTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/KubeCluster.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/Kubernetes.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/Minikube.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/OpenShift.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/BaseCmdKubeClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/KubeCmdClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/Kubectl.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/Oc.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/KubeClusterException.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/KubeClusterException.AlreadyExists.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/KubeClusterException.InvalidResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/KubeClusterException.NotFound.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/NoClusterException.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/executor/Exec.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/executor/ExecResult.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/interfaces/ExtensionContextParameterResolver.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/interfaces/TestSeparator.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/HelmClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/KubeClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/KubeClusterResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/logs/CollectorElement.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/EmbeddedZooKeeper.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/TestUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/WaitException.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/executor/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/executor/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/interfaces/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/interfaces/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/logs/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/logs/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/constant-values.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/serialized-form.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/class-use/EmbeddedZooKeeper.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/class-use/WaitException.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/executor/class-use/ExecResult.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/executor/class-use/Exec.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/class-use/OpenShift.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/class-use/KubeCluster.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/class-use/Kubernetes.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/class-use/Minikube.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/class-use/Kubectl.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/class-use/Oc.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/class-use/BaseCmdKubeClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/class-use/KubeCmdClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/class-use/KubeClusterException.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/class-use/KubeClusterException.InvalidResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/class-use/KubeClusterException.AlreadyExists.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/class-use/KubeClusterException.NotFound.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/class-use/NoClusterException.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/class-use/HelmClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/class-use/KubeClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/class-use/KubeClusterResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/class-use/IsolatedTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/class-use/ParallelSuite.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/class-use/ParallelTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/class-use/IsolatedSuite.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/interfaces/class-use/ExtensionContextParameterResolver.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/interfaces/class-use/TestSeparator.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/class-use/TestUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/logs/class-use/CollectorElement.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/executor/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/interfaces/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/logs/package-use.html...
Building index for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/overview-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/index-all.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/allclasses-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/allpackages-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/deprecated-list.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/index.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/overview-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/help-doc.html...
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/test/target/test-0.29.0-SNAPSHOT-javadoc.jar
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:integration-test (default) @ test ---
[INFO] No tests to run.
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:verify (default) @ test ---
[INFO] 
[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (analyze) @ test ---
[INFO] No dependency problems found
[INFO] 
[INFO] ---------------------< io.strimzi:crd-annotations >---------------------
[INFO] Building crd-annotations 0.29.0-SNAPSHOT                          [3/10]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ crd-annotations ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ crd-annotations ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ crd-annotations ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/crd-annotations/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ crd-annotations ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/crd-annotations/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ crd-annotations ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ crd-annotations ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/crd-annotations/src/test/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ crd-annotations ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M5:test (default-test) @ crd-annotations ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ crd-annotations ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:report (report) @ crd-annotations ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] >>> maven-source-plugin:3.0.1:jar (attach-sources) > generate-sources @ crd-annotations >>>
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ crd-annotations ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ crd-annotations ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ crd-annotations ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/crd-annotations/target/jacoco.exec
[INFO] 
[INFO] <<< maven-source-plugin:3.0.1:jar (attach-sources) < generate-sources @ crd-annotations <<<
[INFO] 
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar (attach-sources) @ crd-annotations ---
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/crd-annotations-0.29.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] --- maven-javadoc-plugin:3.1.0:jar (attach-javadocs) @ crd-annotations ---
[INFO] 
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-annotations/src/main/java/io/strimzi/api/annotations/DeprecatedProperty.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-annotations/src/main/java/io/strimzi/api/annotations/ApiVersion.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-annotations/src/main/java/io/strimzi/api/annotations/DeprecatedType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-annotations/src/main/java/io/strimzi/api/annotations/KubeVersion.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-annotations/src/main/java/io/strimzi/api/annotations/VersionRange.java...
Constructing Javadoc information...
Standard Doclet version 11.0.11
Building tree for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/ApiVersion.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/DeprecatedProperty.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/DeprecatedType.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/KubeVersion.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/VersionRange.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/constant-values.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/class-use/DeprecatedProperty.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/class-use/ApiVersion.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/class-use/DeprecatedType.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/class-use/KubeVersion.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/class-use/VersionRange.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/package-use.html...
Building index for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/overview-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/index-all.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/allclasses-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/allpackages-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/deprecated-list.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/index.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/help-doc.html...
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/crd-annotations-0.29.0-SNAPSHOT-javadoc.jar
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:integration-test (default) @ crd-annotations ---
[WARNING] useSystemClassLoader setting has no effect when not forking
[WARNING] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:verify (default) @ crd-annotations ---
[INFO] 
[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (analyze) @ crd-annotations ---
[INFO] No dependency problems found
[INFO] 
[INFO] ----------------------< io.strimzi:crd-generator >----------------------
[INFO] Building crd-generator 0.29.0-SNAPSHOT                            [4/10]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ crd-generator ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ crd-generator ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ crd-generator ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/crd-generator/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ crd-generator ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ crd-generator ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ crd-generator ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 7 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ crd-generator ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M5:test (default-test) @ crd-generator ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ crd-generator ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:report (report) @ crd-generator ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] >>> maven-source-plugin:3.0.1:jar (attach-sources) > generate-sources @ crd-generator >>>
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ crd-generator ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ crd-generator ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ crd-generator ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/crd-generator/target/jacoco.exec
[INFO] 
[INFO] <<< maven-source-plugin:3.0.1:jar (attach-sources) < generate-sources @ crd-generator <<<
[INFO] 
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar (attach-sources) @ crd-generator ---
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/crd-generator/target/crd-generator-0.29.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] --- maven-javadoc-plugin:3.1.0:jar (attach-javadocs) @ crd-generator ---
[INFO] 
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/KubeLinker.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/Linker.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/OpenShiftOriginLinker.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/PropertyType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/Schema.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/Alternation.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/DescriptionFile.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/Example.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/KubeLink.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/OneOf.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/Type.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/Alternative.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/Description.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/Maximum.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/Minimum.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/MinimumItems.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/PresentInVersions.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/Crd.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/Pattern.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/Property.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/CrdGenerator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/DocGenerator.java...
Constructing Javadoc information...
Standard Doclet version 11.0.11
Building tree for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Alternation.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Alternative.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Crd.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Crd.Spec.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Crd.Spec.AdditionalPrinterColumn.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Crd.Spec.Names.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Crd.Spec.Subresources.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Crd.Spec.Subresources.Scale.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Crd.Spec.Subresources.Status.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Crd.Spec.Version.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Description.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Description.List.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/DescriptionFile.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Example.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/KubeLink.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Maximum.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Maximum.List.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Minimum.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Minimum.List.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/MinimumItems.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/MinimumItems.List.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/OneOf.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/OneOf.Alternative.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/OneOf.Alternative.Property.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Pattern.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Pattern.List.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/PresentInVersions.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Type.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/CrdGenerator.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/CrdGenerator.ConversionStrategy.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/CrdGenerator.DefaultReporter.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/CrdGenerator.NoneConversionStrategy.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/CrdGenerator.Reporter.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/CrdGenerator.WebhookConversionStrategy.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/DocGenerator.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/KubeLinker.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/Linker.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/OpenShiftOriginLinker.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/PropertyType.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/constant-values.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/KubeLinker.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/Linker.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/OpenShiftOriginLinker.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/PropertyType.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Alternation.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/DescriptionFile.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Example.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/KubeLink.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/OneOf.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/OneOf.Alternative.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/OneOf.Alternative.Property.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Type.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Alternative.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Description.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Description.List.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Maximum.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Maximum.List.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Minimum.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Minimum.List.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/MinimumItems.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/MinimumItems.List.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/PresentInVersions.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Crd.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Crd.Spec.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Crd.Spec.AdditionalPrinterColumn.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Crd.Spec.Subresources.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Crd.Spec.Subresources.Scale.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Crd.Spec.Subresources.Status.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Crd.Spec.Version.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Crd.Spec.Names.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Pattern.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Pattern.List.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/CrdGenerator.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/CrdGenerator.WebhookConversionStrategy.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/CrdGenerator.NoneConversionStrategy.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/CrdGenerator.ConversionStrategy.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/CrdGenerator.DefaultReporter.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/CrdGenerator.Reporter.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/DocGenerator.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/package-use.html...
Building index for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/overview-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/index-all.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/allclasses-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/allpackages-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/deprecated-list.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/index.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/overview-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/help-doc.html...
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/crd-generator/target/crd-generator-0.29.0-SNAPSHOT-javadoc.jar
[INFO] 
[INFO] --- maven-shade-plugin:3.1.0:shade (default) @ crd-generator ---
[INFO] Including io.strimzi:crd-annotations:jar:0.29.0-SNAPSHOT in the shaded jar.
[INFO] Including com.fasterxml.jackson.core:jackson-core:jar:2.12.6 in the shaded jar.
[INFO] Including com.fasterxml.jackson.core:jackson-databind:jar:2.12.6 in the shaded jar.
[INFO] Including com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:jar:2.12.6 in the shaded jar.
[INFO] Including org.yaml:snakeyaml:jar:1.27 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-client:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-rbac:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-admissionregistration:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-apps:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-autoscaling:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-apiextensions:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-batch:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-certificates:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-coordination:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-discovery:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-events:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-extensions:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-flowcontrol:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-networking:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-metrics:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-policy:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-scheduling:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-storageclass:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-node:jar:5.12.0 in the shaded jar.
[INFO] Including com.squareup.okhttp3:okhttp:jar:3.12.12 in the shaded jar.
[INFO] Including com.squareup.okio:okio:jar:1.15.0 in the shaded jar.
[INFO] Including com.squareup.okhttp3:logging-interceptor:jar:3.12.12 in the shaded jar.
[INFO] Including org.slf4j:slf4j-api:jar:1.7.36 in the shaded jar.
[INFO] Including com.fasterxml.jackson.datatype:jackson-datatype-jsr310:jar:2.13.1 in the shaded jar.
[INFO] Including io.fabric8:zjsonpatch:jar:0.3.0 in the shaded jar.
[INFO] Including com.github.mifmif:generex:jar:1.0.2 in the shaded jar.
[INFO] Including dk.brics.automaton:automaton:jar:1.11-8 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-core:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-common:jar:5.12.0 in the shaded jar.
[INFO] Including com.fasterxml.jackson.core:jackson-annotations:jar:2.12.6 in the shaded jar.
[WARNING] Discovered module-info.class. Shading will break its strong encapsulation.
[WARNING] Discovered module-info.class. Shading will break its strong encapsulation.
[WARNING] Discovered module-info.class. Shading will break its strong encapsulation.
[WARNING] Discovered module-info.class. Shading will break its strong encapsulation.
[WARNING] kubernetes-model-coordination-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 18 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseListFluentImpl$ItemsNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseFluentImpl$SpecNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseSpec
[WARNING]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseFluent$SpecNested
[WARNING]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseListFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseSpecBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseSpecFluent
[WARNING]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseFluentImpl$MetadataNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseFluent
[WARNING]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseListFluent
[WARNING]   - 8 more...
[WARNING] logging-interceptor-3.12.12.jar, crd-generator-0.29.0-SNAPSHOT.jar define 8 overlapping classes: 
[WARNING]   - okhttp3.logging.HttpLoggingInterceptor$Logger$1
[WARNING]   - okhttp3.logging.LoggingEventListener$Factory
[WARNING]   - okhttp3.logging.HttpLoggingInterceptor$Level
[WARNING]   - okhttp3.logging.HttpLoggingInterceptor
[WARNING]   - okhttp3.logging.package-info
[WARNING]   - okhttp3.logging.LoggingEventListener
[WARNING]   - okhttp3.logging.LoggingEventListener$1
[WARNING]   - okhttp3.logging.HttpLoggingInterceptor$Logger
[WARNING] kubernetes-client-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 536 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.client.internal.CertUtils
[WARNING]   - io.fabric8.kubernetes.client.CustomResource
[WARNING]   - io.fabric8.kubernetes.client.osgi.ManagedKubernetesClient
[WARNING]   - io.fabric8.kubernetes.client.V1beta1ApiextensionAPIGroupDSL
[WARNING]   - io.fabric8.kubernetes.client.internal.PatchUtils$SingletonHolder
[WARNING]   - io.fabric8.kubernetes.client.VersionInfo$1
[WARNING]   - io.fabric8.kubernetes.client.utils.ReplaceValueStream
[WARNING]   - io.fabric8.kubernetes.client.dsl.CreateFromServerGettable
[WARNING]   - io.fabric8.kubernetes.client.dsl.ApiextensionsAPIGroupDSL
[WARNING]   - io.fabric8.kubernetes.client.dsl.Containerable
[WARNING]   - 526 more...
[WARNING] kubernetes-model-events-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 44 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventFluent$MetadataNested
[WARNING]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventFluent$SeriesNested
[WARNING]   - io.fabric8.kubernetes.api.model.events.v1.EventList
[WARNING]   - io.fabric8.kubernetes.api.model.events.v1.EventListBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventFluent$RegardingNested
[WARNING]   - io.fabric8.kubernetes.api.model.events.v1.EventSeriesFluent
[WARNING]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventFluent
[WARNING]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventListFluent$ItemsNested
[WARNING]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventListFluentImpl$ItemsNestedImpl
[WARNING]   - 34 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, automaton-1.11-8.jar define 25 overlapping classes: 
[WARNING]   - dk.brics.automaton.AutomatonMatcher
[WARNING]   - dk.brics.automaton.ShuffleOperations$ShuffleConfiguration
[WARNING]   - dk.brics.automaton.RegExp$Kind
[WARNING]   - dk.brics.automaton.RunAutomaton
[WARNING]   - dk.brics.automaton.Automaton
[WARNING]   - dk.brics.automaton.RegExp
[WARNING]   - dk.brics.automaton.AutomatonProvider
[WARNING]   - dk.brics.automaton.RegExp$1
[WARNING]   - dk.brics.automaton.MinimizationOperations$StateListNode
[WARNING]   - dk.brics.automaton.State
[WARNING]   - 15 more...
[WARNING] kubernetes-model-metrics-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 30 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.PodMetricsFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetricsListFluentImpl$ItemsNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.ContainerMetricsFluent
[WARNING]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetrics
[WARNING]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetricsListFluent
[WARNING]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetricsListFluent$ItemsNested
[WARNING]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetricsFluent$MetadataNested
[WARNING]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.PodMetricsFluentImpl$ContainersNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.PodMetricsListFluentImpl$ItemsNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.PodMetricsListBuilder
[WARNING]   - 20 more...
[WARNING] kubernetes-model-networking-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 234 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.networking.v1beta1.IngressClassListFluent$ItemsNested
[WARNING]   - io.fabric8.kubernetes.api.model.networking.v1beta1.IngressClassListFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.networking.v1.IngressServiceBackend
[WARNING]   - io.fabric8.kubernetes.api.model.networking.v1.NetworkPolicyPort
[WARNING]   - io.fabric8.kubernetes.api.model.networking.v1beta1.IngressFluentImpl$StatusNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.networking.v1beta1.IngressClassSpec
[WARNING]   - io.fabric8.kubernetes.api.model.networking.v1.IngressStatus
[WARNING]   - io.fabric8.kubernetes.api.model.networking.v1.IngressClassFluent$SpecNested
[WARNING]   - io.fabric8.kubernetes.api.model.networking.v1.NetworkPolicyBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.networking.v1.IngressRuleFluentImpl$HttpNestedImpl
[WARNING]   - 224 more...
[WARNING] kubernetes-model-apiextensions-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 350 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.CustomResourceDefinitionVersionBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.JSONSchemaPropsOrBoolBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.CustomResourceDefinitionSpecFluent$ValidationNested
[WARNING]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.CustomResourceDefinitionVersionFluentImpl$SchemaNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.JSONSchemaPropsOrStringArraySerDe$Deserializer$1
[WARNING]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.CustomResourceValidationFluentImpl$OpenAPIV3SchemaNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.JSONSchemaPropsFluentImpl$NotNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.WebhookClientConfigFluentImpl$ServiceNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.JSONSchemaPropsOrArrayFluent$SchemaNested
[WARNING]   - io.fabric8.kubernetes.api.model.apiextensions.v1.JSONSchemaPropsOrBoolSerDe
[WARNING]   - 340 more...
[WARNING] generex-1.0.2.jar, crd-generator-0.29.0-SNAPSHOT.jar define 7 overlapping classes: 
[WARNING]   - com.mifmif.common.regex.GenerexIterator
[WARNING]   - com.mifmif.common.regex.Generex
[WARNING]   - com.mifmif.common.regex.GenerexIterator$Step
[WARNING]   - com.mifmif.common.regex.Node
[WARNING]   - com.mifmif.common.regex.Main
[WARNING]   - com.mifmif.common.regex.util.Iterable
[WARNING]   - com.mifmif.common.regex.util.Iterator
[WARNING] kubernetes-model-autoscaling-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 350 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.MetricSpecFluentImpl$ObjectNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta2.HorizontalPodAutoscalerSpec
[WARNING]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.CrossVersionObjectReferenceBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.HorizontalPodAutoscalerStatusFluent
[WARNING]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta2.HorizontalPodAutoscalerFluent$SpecNested
[WARNING]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta2.ContainerResourceMetricStatusFluent
[WARNING]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.HorizontalPodAutoscalerStatus
[WARNING]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.MetricStatusFluent$ObjectNested
[WARNING]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta2.HorizontalPodAutoscalerSpecFluent$ScaleTargetRefNested
[WARNING]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.HorizontalPodAutoscalerFluent$SpecNested
[WARNING]   - 340 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, zjsonpatch-0.3.0.jar define 24 overlapping classes: 
[WARNING]   - io.fabric8.zjsonpatch.internal.collections4.sequence.InsertCommand
[WARNING]   - io.fabric8.zjsonpatch.Operation
[WARNING]   - io.fabric8.zjsonpatch.internal.collections4.sequence.CommandVisitor
[WARNING]   - io.fabric8.zjsonpatch.internal.guava.Strings
[WARNING]   - io.fabric8.zjsonpatch.internal.collections4.sequence.EditCommand
[WARNING]   - io.fabric8.zjsonpatch.JsonDiff$EncodePathFunction
[WARNING]   - io.fabric8.zjsonpatch.internal.collections4.sequence.SequencesComparator
[WARNING]   - io.fabric8.zjsonpatch.Diff
[WARNING]   - io.fabric8.zjsonpatch.internal.collections4.ListUtils
[WARNING]   - io.fabric8.zjsonpatch.JsonPatch
[WARNING]   - 14 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-admissionregistration-5.12.0.jar define 362 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.admissionregistration.v1beta1.MutatingWebhookFluent$ObjectSelectorNested
[WARNING]   - io.fabric8.kubernetes.api.model.authorization.v1.SubjectAccessReviewSpecFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.authorization.v1beta1.SubjectRulesReviewStatusBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.admissionregistration.v1.ValidatingWebhookConfigurationBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.authentication.TokenReviewFluentImpl$MetadataNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.authorization.v1beta1.SelfSubjectRulesReviewSpec
[WARNING]   - io.fabric8.kubernetes.api.model.admissionregistration.v1beta1.MutatingWebhookFluentImpl$NamespaceSelectorNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.admissionregistration.v1beta1.MutatingWebhookConfigurationFluentImpl$WebhooksNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.authorization.v1beta1.SelfSubjectAccessReviewFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.admissionregistration.v1.MutatingWebhookFluent$ClientConfigNested
[WARNING]   - 352 more...
[WARNING] kubernetes-model-rbac-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 80 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.rbac.SubjectBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.rbac.ClusterRoleBindingFluent
[WARNING]   - io.fabric8.kubernetes.api.model.rbac.RoleListFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.rbac.ClusterRoleBindingFluent$MetadataNested
[WARNING]   - io.fabric8.kubernetes.api.model.rbac.RoleBindingBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.rbac.ClusterRoleBindingBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.rbac.AggregationRuleFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.rbac.SubjectFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.rbac.ClusterRoleListFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.rbac.PolicyRuleFluent
[WARNING]   - 70 more...
[WARNING] kubernetes-model-certificates-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 60 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestStatusFluent$ConditionsNested
[WARNING]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestFluent$SpecNested
[WARNING]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestFluentImpl$StatusNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestListFluent$ItemsNested
[WARNING]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestStatusFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestFluent$MetadataNested
[WARNING]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestConditionFluent
[WARNING]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestStatusBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestStatusFluent$ConditionsNested
[WARNING]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestFluentImpl
[WARNING]   - 50 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-policy-5.12.0.jar define 162 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.policy.v1.PodDisruptionBudgetList
[WARNING]   - io.fabric8.kubernetes.api.model.policy.v1beta1.HostPortRangeBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.policy.v1beta1.EvictionFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.policy.v1beta1.PodSecurityPolicyFluent
[WARNING]   - io.fabric8.kubernetes.api.model.policy.v1beta1.PodSecurityPolicySpecFluentImpl$AllowedCSIDriversNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.policy.v1beta1.PodSecurityPolicyListFluentImpl$ItemsNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.policy.v1beta1.AllowedFlexVolumeBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.policy.v1beta1.IDRangeFluent
[WARNING]   - io.fabric8.kubernetes.api.model.policy.v1beta1.SELinuxStrategyOptionsFluent
[WARNING]   - io.fabric8.kubernetes.api.model.policy.v1beta1.PodSecurityPolicySpecFluentImpl$FsGroupNestedImpl
[WARNING]   - 152 more...
[WARNING] jackson-datatype-jsr310-2.13.1.jar, crd-generator-0.29.0-SNAPSHOT.jar define 59 overlapping classes: 
[WARNING]   - com.fasterxml.jackson.datatype.jsr310.deser.LocalDateDeserializer
[WARNING]   - com.fasterxml.jackson.datatype.jsr310.deser.key.Jsr310KeyDeserializer
[WARNING]   - com.fasterxml.jackson.datatype.jsr310.PackageVersion
[WARNING]   - com.fasterxml.jackson.datatype.jsr310.deser.YearDeserializer
[WARNING]   - com.fasterxml.jackson.datatype.jsr310.ser.key.Jsr310NullKeySerializer
[WARNING]   - com.fasterxml.jackson.datatype.jsr310.deser.key.LocalDateTimeKeyDeserializer
[WARNING]   - com.fasterxml.jackson.datatype.jsr310.util.DurationUnitConverter
[WARNING]   - com.fasterxml.jackson.datatype.jsr310.ser.InstantSerializerBase
[WARNING]   - com.fasterxml.jackson.datatype.jsr310.ser.LocalDateTimeSerializer
[WARNING]   - com.fasterxml.jackson.datatype.jsr310.ser.OffsetDateTimeSerializer
[WARNING]   - 49 more...
[WARNING] kubernetes-model-flowcontrol-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 132 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfigurationFluentImpl$SpecNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.FlowSchemaConditionFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.FlowDistinguisherMethodBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfigurationReferenceBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.QueuingConfiguration
[WARNING]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.QueuingConfigurationFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfiguration
[WARNING]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfigurationReference
[WARNING]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PolicyRulesWithSubjects
[WARNING]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfigurationListFluent$ItemsNested
[WARNING]   - 122 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, crd-annotations-0.29.0-SNAPSHOT.jar define 8 overlapping classes: 
[WARNING]   - io.strimzi.api.annotations.VersionRange
[WARNING]   - io.strimzi.api.annotations.ApiVersion
[WARNING]   - io.strimzi.api.annotations.ApiVersion$Stability
[WARNING]   - io.strimzi.api.annotations.ApiVersion$1
[WARNING]   - io.strimzi.api.annotations.DeprecatedType
[WARNING]   - io.strimzi.api.annotations.DeprecatedProperty
[WARNING]   - io.strimzi.api.annotations.VersionRange$VersionParser
[WARNING]   - io.strimzi.api.annotations.KubeVersion
[WARNING] jackson-core-2.12.6.jar, crd-generator-0.29.0-SNAPSHOT.jar define 124 overlapping classes: 
[WARNING]   - com.fasterxml.jackson.core.JsonGenerator$Feature
[WARNING]   - com.fasterxml.jackson.core.json.JsonReadFeature
[WARNING]   - com.fasterxml.jackson.core.util.ThreadLocalBufferManager$ThreadLocalBufferManagerHolder
[WARNING]   - com.fasterxml.jackson.core.util.Separators
[WARNING]   - com.fasterxml.jackson.core.io.SegmentedStringWriter
[WARNING]   - com.fasterxml.jackson.core.TreeNode
[WARNING]   - com.fasterxml.jackson.core.sym.Name
[WARNING]   - com.fasterxml.jackson.core.util.RequestPayload
[WARNING]   - com.fasterxml.jackson.core.util.JsonGeneratorDelegate
[WARNING]   - com.fasterxml.jackson.core.async.NonBlockingInputFeeder
[WARNING]   - 114 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, okio-1.15.0.jar define 44 overlapping classes: 
[WARNING]   - okio.ByteString
[WARNING]   - okio.Source
[WARNING]   - okio.ForwardingSink
[WARNING]   - okio.BufferedSource
[WARNING]   - okio.Util
[WARNING]   - okio.AsyncTimeout$1
[WARNING]   - okio.HashingSource
[WARNING]   - okio.GzipSink
[WARNING]   - okio.Okio$1
[WARNING]   - okio.Pipe$PipeSink
[WARNING]   - 34 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, jackson-databind-2.12.6.jar define 700 overlapping classes: 
[WARNING]   - com.fasterxml.jackson.databind.introspect.AnnotationCollector$NoAnnotations
[WARNING]   - com.fasterxml.jackson.databind.jsontype.BasicPolymorphicTypeValidator$Builder
[WARNING]   - com.fasterxml.jackson.databind.BeanDescription
[WARNING]   - com.fasterxml.jackson.databind.deser.impl.BeanAsArrayBuilderDeserializer
[WARNING]   - com.fasterxml.jackson.databind.introspect.AnnotatedMethodMap
[WARNING]   - com.fasterxml.jackson.databind.SerializerProvider
[WARNING]   - com.fasterxml.jackson.databind.introspect.AnnotationCollector$OneAnnotation
[WARNING]   - com.fasterxml.jackson.databind.ser.std.StaticListSerializerBase
[WARNING]   - com.fasterxml.jackson.databind.ser.std.NumberSerializers$ShortSerializer
[WARNING]   - com.fasterxml.jackson.databind.ser.BeanSerializerFactory
[WARNING]   - 690 more...
[WARNING] kubernetes-model-discovery-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 88 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointSliceFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointFluent
[WARNING]   - io.fabric8.kubernetes.api.model.discovery.v1.EndpointSliceBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointPort
[WARNING]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.ForZoneBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.discovery.v1.EndpointFluent$TargetRefNested
[WARNING]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointFluentImpl$ConditionsNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.discovery.v1.EndpointSliceListFluentImpl$ItemsNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointConditionsFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointSliceListFluentImpl$ItemsNestedImpl
[WARNING]   - 78 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-extensions-5.12.0.jar define 264 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.extensions.DaemonSetConditionBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.extensions.NetworkPolicyListFluent$ItemsNested
[WARNING]   - io.fabric8.kubernetes.api.model.extensions.DeploymentStrategyFluent
[WARNING]   - io.fabric8.kubernetes.api.model.extensions.IngressListFluentImpl$ItemsNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.extensions.NetworkPolicySpecFluent$IngressNested
[WARNING]   - io.fabric8.kubernetes.api.model.extensions.IngressStatus
[WARNING]   - io.fabric8.kubernetes.api.model.extensions.DaemonSetFluentImpl$SpecNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.extensions.IngressSpecFluent$RulesNested
[WARNING]   - io.fabric8.kubernetes.api.model.extensions.DaemonSetSpecFluent$UpdateStrategyNested
[WARNING]   - io.fabric8.kubernetes.api.model.extensions.NetworkPolicyPeerBuilder
[WARNING]   - 254 more...
[WARNING] snakeyaml-1.27.jar, crd-generator-0.29.0-SNAPSHOT.jar define 216 overlapping classes: 
[WARNING]   - org.yaml.snakeyaml.emitter.Emitter$ExpectBlockMappingValue
[WARNING]   - org.yaml.snakeyaml.parser.ParserImpl$ParseBlockNode
[WARNING]   - org.yaml.snakeyaml.emitter.Emitter$ExpectBlockMappingSimpleValue
[WARNING]   - org.yaml.snakeyaml.emitter.Emitter$ExpectDocumentEnd
[WARNING]   - org.yaml.snakeyaml.Yaml$3
[WARNING]   - org.yaml.snakeyaml.emitter.Emitter$ExpectBlockSequenceItem
[WARNING]   - org.yaml.snakeyaml.parser.ParserImpl$ParseBlockSequenceEntry
[WARNING]   - org.yaml.snakeyaml.util.ArrayUtils
[WARNING]   - org.yaml.snakeyaml.tokens.Token$ID
[WARNING]   - org.yaml.snakeyaml.reader.StreamReader
[WARNING]   - 206 more...
[WARNING] kubernetes-model-node-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 78 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.node.v1beta1.OverheadBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.node.v1.RuntimeClassListFluent$ItemsNested
[WARNING]   - io.fabric8.kubernetes.api.model.node.v1alpha1.Scheduling
[WARNING]   - io.fabric8.kubernetes.api.model.node.v1beta1.RuntimeClassFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.node.v1beta1.RuntimeClassListBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.node.v1.RuntimeClassFluent
[WARNING]   - io.fabric8.kubernetes.api.model.node.v1alpha1.SchedulingFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.node.v1alpha1.RuntimeClassSpecFluent$OverheadNested
[WARNING]   - io.fabric8.kubernetes.api.model.node.v1.RuntimeClassBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.node.v1.RuntimeClassFluentImpl
[WARNING]   - 68 more...
[WARNING] kubernetes-model-core-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 2394 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.BaseKubernetesListFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.StatusBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.KubeSchemaFluentImpl$APIResourceNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.NodeListBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.ResourceQuotaListFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.WatchEventFluentImpl$APIServiceStatusObjectNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.WatchEventFluent$VsphereVirtualDiskVolumeSourceObjectNested
[WARNING]   - io.fabric8.kubernetes.api.model.ProbeFluentImpl$HttpGetNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.PatchOptionsFluent
[WARNING]   - io.fabric8.kubernetes.api.model.ServerAddressByClientCIDRFluentImpl
[WARNING]   - 2384 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, jackson-annotations-2.12.6.jar define 71 overlapping classes: 
[WARNING]   - com.fasterxml.jackson.annotation.JsonAutoDetect
[WARNING]   - com.fasterxml.jackson.annotation.JsonInclude
[WARNING]   - com.fasterxml.jackson.annotation.ObjectIdGenerators
[WARNING]   - com.fasterxml.jackson.annotation.JsonFormat$Features
[WARNING]   - com.fasterxml.jackson.annotation.JsonFormat$Feature
[WARNING]   - com.fasterxml.jackson.annotation.JsonIgnore
[WARNING]   - com.fasterxml.jackson.annotation.JsonSetter
[WARNING]   - com.fasterxml.jackson.annotation.JsonTypeInfo$None
[WARNING]   - com.fasterxml.jackson.annotation.JsonFormat$Shape
[WARNING]   - com.fasterxml.jackson.annotation.JsonSubTypes
[WARNING]   - 61 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, slf4j-api-1.7.36.jar define 34 overlapping classes: 
[WARNING]   - org.slf4j.helpers.SubstituteLogger
[WARNING]   - org.slf4j.helpers.NamedLoggerBase
[WARNING]   - org.slf4j.helpers.NOPMDCAdapter
[WARNING]   - org.slf4j.MarkerFactory
[WARNING]   - org.slf4j.helpers.BasicMarker
[WARNING]   - org.slf4j.spi.LoggerFactoryBinder
[WARNING]   - org.slf4j.MDC$MDCCloseable
[WARNING]   - org.slf4j.spi.LocationAwareLogger
[WARNING]   - org.slf4j.helpers.MessageFormatter
[WARNING]   - org.slf4j.helpers.Util$ClassContextSecurityManager
[WARNING]   - 24 more...
[WARNING] jackson-dataformat-yaml-2.12.6.jar, crd-generator-0.29.0-SNAPSHOT.jar define 17 overlapping classes: 
[WARNING]   - com.fasterxml.jackson.dataformat.yaml.YAMLMapper$Builder
[WARNING]   - com.fasterxml.jackson.dataformat.yaml.snakeyaml.error.Mark
[WARNING]   - com.fasterxml.jackson.dataformat.yaml.YAMLGenerator
[WARNING]   - com.fasterxml.jackson.dataformat.yaml.UTF8Reader
[WARNING]   - com.fasterxml.jackson.dataformat.yaml.YAMLParser
[WARNING]   - com.fasterxml.jackson.dataformat.yaml.util.StringQuotingChecker
[WARNING]   - com.fasterxml.jackson.dataformat.yaml.YAMLGenerator$Feature
[WARNING]   - com.fasterxml.jackson.dataformat.yaml.JacksonYAMLParseException
[WARNING]   - com.fasterxml.jackson.dataformat.yaml.YAMLParser$Feature
[WARNING]   - com.fasterxml.jackson.dataformat.yaml.util.StringQuotingChecker$Default
[WARNING]   - 7 more...
[WARNING] kubernetes-model-apps-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 212 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.apps.ReplicaSetSpec
[WARNING]   - io.fabric8.kubernetes.api.model.apps.StatefulSetConditionFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.apps.DeploymentStrategyFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.apps.ControllerRevisionFluent$DeploymentDataNested
[WARNING]   - io.fabric8.kubernetes.api.model.apps.ReplicaSetSpecFluent
[WARNING]   - io.fabric8.kubernetes.api.model.apps.DeploymentFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.apps.ReplicaSetStatusFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.apps.ControllerRevisionFluentImpl$PersistentVolumeClaimDataNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.apps.ReplicaSetCondition
[WARNING]   - io.fabric8.kubernetes.api.model.apps.StatefulSetSpecFluent$UpdateStrategyNested
[WARNING]   - 202 more...
[WARNING] kubernetes-model-scheduling-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 24 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassFluent
[WARNING]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassFluentImpl$MetadataNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassListFluent$ItemsNested
[WARNING]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassList
[WARNING]   - io.fabric8.kubernetes.api.model.scheduling.v1beta1.PriorityClassListFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassListFluent
[WARNING]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClass
[WARNING]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassListFluentImpl$ItemsNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.scheduling.v1beta1.PriorityClassBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.scheduling.v1beta1.PriorityClassFluent$MetadataNested
[WARNING]   - 14 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-common-5.12.0.jar define 16 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.model.annotation.Plural
[WARNING]   - io.fabric8.kubernetes.model.annotation.Group
[WARNING]   - io.fabric8.kubernetes.model.jackson.JsonUnwrappedDeserializer$CancelUnwrapped
[WARNING]   - io.fabric8.kubernetes.model.jackson.JsonUnwrappedDeserializer
[WARNING]   - io.fabric8.kubernetes.model.annotation.PrinterColumn
[WARNING]   - io.fabric8.kubernetes.model.jackson.UnwrappedTypeResolverBuilder
[WARNING]   - io.fabric8.kubernetes.model.annotation.Singular
[WARNING]   - io.fabric8.kubernetes.model.annotation.StatusReplicas
[WARNING]   - io.fabric8.kubernetes.model.annotation.SpecReplicas
[WARNING]   - io.fabric8.kubernetes.model.annotation.Version
[WARNING]   - 6 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-batch-5.12.0.jar define 112 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.batch.v1.JobFluentImpl$StatusNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.batch.v1.CronJobStatusFluentImpl$ActiveNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.batch.v1.JobSpecFluent$TemplateNested
[WARNING]   - io.fabric8.kubernetes.api.model.batch.v1beta1.CronJobListFluentImpl$ItemsNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.batch.v1beta1.CronJobSpecFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.batch.v1.JobSpecFluentImpl$TemplateNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.batch.v1.Job
[WARNING]   - io.fabric8.kubernetes.api.model.batch.v1beta1.CronJobFluent
[WARNING]   - io.fabric8.kubernetes.api.model.batch.v1beta1.CronJobListFluent
[WARNING]   - io.fabric8.kubernetes.api.model.batch.v1.CronJobListFluent
[WARNING]   - 102 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-storageclass-5.12.0.jar define 172 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.storage.v1beta1.CSIStorageCapacityListFluentImpl$ItemsNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.storage.CSINodeFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.storage.CSINodeDriverFluentImpl$AllocatableNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.storage.StorageClass
[WARNING]   - io.fabric8.kubernetes.api.model.storage.VolumeAttachmentSourceBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.storage.VolumeAttachmentSourceFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.storage.TokenRequestFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.storage.v1beta1.CSINodeDriverFluent$AllocatableNested
[WARNING]   - io.fabric8.kubernetes.api.model.storage.v1beta1.CSIDriverSpecBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.storage.VolumeAttachmentSpecFluent
[WARNING]   - 162 more...
[WARNING] okhttp-3.12.12.jar, crd-generator-0.29.0-SNAPSHOT.jar define 208 overlapping classes: 
[WARNING]   - okhttp3.WebSocket
[WARNING]   - okhttp3.Cookie$Builder
[WARNING]   - okhttp3.internal.http.HttpHeaders
[WARNING]   - okhttp3.internal.http2.Http2Connection$ReaderRunnable
[WARNING]   - okhttp3.internal.http2.Http2Reader$ContinuationSource
[WARNING]   - okhttp3.internal.tls.OkHostnameVerifier
[WARNING]   - okhttp3.Cache$Entry
[WARNING]   - okhttp3.internal.http2.Http2Connection$3
[WARNING]   - okhttp3.internal.ws.RealWebSocket$Streams
[WARNING]   - okhttp3.CacheControl$Builder
[WARNING]   - 198 more...
[WARNING] maven-shade-plugin has detected that some class files are
[WARNING] present in two or more JARs. When this happens, only one
[WARNING] single version of the class is copied to the uber jar.
[WARNING] Usually this is not harmful and you can skip these warnings,
[WARNING] otherwise try to manually exclude artifacts based on
[WARNING] mvn dependency:tree -Ddetail=true and the above output.
[WARNING] See http://maven.apache.org/plugins/maven-shade-plugin/
[INFO] Replacing original artifact with shaded artifact.
[INFO] Replacing /home/cloud-user/strimzi-kafka-operator/crd-generator/target/crd-generator-0.29.0-SNAPSHOT.jar with /home/cloud-user/strimzi-kafka-operator/crd-generator/target/crd-generator-0.29.0-SNAPSHOT-shaded.jar
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:integration-test (default) @ crd-generator ---
[WARNING] useSystemClassLoader setting has no effect when not forking
[WARNING] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:verify (default) @ crd-generator ---
[INFO] 
[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (analyze) @ crd-generator ---
[INFO] No dependency problems found
[INFO] 
[INFO] ---------------------------< io.strimzi:api >---------------------------
[INFO] Building api 0.29.0-SNAPSHOT                                      [5/10]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ api ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ api ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ api ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/api/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ api ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/api/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ api ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- exec-maven-plugin:1.6.0:exec (generate-crd-co-install-v1) @ api ---
[INFO] 
[INFO] --- exec-maven-plugin:1.6.0:exec (generate-crd-co-install-v1-eo) @ api ---
[INFO] 
[INFO] --- exec-maven-plugin:1.6.0:exec (generate-doc) @ api ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ api ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 99 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ api ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-test-compile) @ api ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M5:test (default-test) @ api ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ api ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:report (report) @ api ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] >>> maven-source-plugin:3.0.1:jar (attach-sources) > generate-sources @ api >>>
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ api ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ api ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ api ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/api/target/jacoco.exec
[INFO] 
[INFO] <<< maven-source-plugin:3.0.1:jar (attach-sources) < generate-sources @ api <<<
[INFO] 
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar (attach-sources) @ api ---
[INFO] 
[INFO] --- maven-javadoc-plugin:3.1.0:jar (attach-javadocs) @ api ---
[INFO] 
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/KafkaBridgeList.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/KafkaConnectList.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/KafkaConnectorList.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/KafkaList.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/KafkaMirrorMaker2List.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/KafkaMirrorMakerList.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/KafkaRebalanceList.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/KafkaTopicList.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/KafkaUserList.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AclRuleClusterResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/CruiseControlResources.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AclOperation.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AclResourcePatternType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AclRuleResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AclRuleGroupResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AclRuleTopicResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AclRuleTransactionalIdResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/ExternalLogging.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AclRuleType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/CertAndKeySecretSource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/CertSecretSource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/CruiseControlSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/CertificateExpirationPolicy.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaConnect.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/ContainerEnvVar.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/EntityOperatorSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/EntityTopicOperatorSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/EntityUserOperatorSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/HasConfigurableMetrics.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/InlineLogging.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/JvmOptions.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/GenericSecretSource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/ClientTls.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/JmxTransResources.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/JmxTransSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaAuthorization.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaAuthorizationKeycloak.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaAuthorizationOpa.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaAuthorizationSimple.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaBridge.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/Probe.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaBridgeClientSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaBridgeConsumerSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaBridgeHttpConfig.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaBridgeHttpCors.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaBridgeProducerSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaBridgeResources.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaClusterSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaConnector.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/Constants.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaConnectResources.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/Spec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AbstractKafkaConnectSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AclRule.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/CertificateAuthority.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaConnectSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaExporterSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaJmxAuthentication.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaConnectorSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaExporterResources.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaJmxAuthenticationPassword.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMaker.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaJmxOptions.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMaker2.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMaker2Spec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMakerSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMaker2ConnectorSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMaker2Resources.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaRebalance.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/HasSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMakerConsumerSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMakerProducerSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMakerResources.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaResources.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaBridgeSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaRebalanceSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaTopic.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaUser.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaTopicSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaUserAuthentication.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaUserAuthorization.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaUserAuthorizationSimple.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaUserQuotas.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaUserScramSha512ClientAuthentication.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaUserSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaUserTlsClientAuthentication.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/MetricsConfig.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/Password.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/Logging.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/PasswordSecretSource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/Rack.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/Sidecar.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/SystemProperty.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/TlsClientAuthentication.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/TlsSidecar.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/TlsSidecarLogLevel.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/Kafka.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/UnknownPropertyPreserving.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/ZookeeperClusterSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/authentication/KafkaClientAuthentication.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuth.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationPlain.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScram.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha256.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha512.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationTls.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/balancing/KafkaRebalanceAnnotation.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/balancing/KafkaRebalanceState.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/balancing/BrokerCapacity.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/ConnectorPlugin.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnv.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/ExternalConfigurationVolumeSource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvVarSource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/Build.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/Plugin.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/Artifact.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/DockerOutput.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/DownloadableArtifact.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/ImageStreamOutput.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/JarArtifact.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/MavenArtifact.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/OtherArtifact.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/Output.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/TgzArtifact.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/ZipArtifact.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/ExternalConfiguration.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/NodeAddressType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/arraylistener/KafkaListenerType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListener.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBootstrap.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBroker.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfiguration.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/KafkaListenerAuthentication.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationCustom.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuth.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationScramSha512.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationTls.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/Condition.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/KafkaBridgeStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/KafkaConnectStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/KafkaMirrorMaker2Status.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/KafkaMirrorMakerStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/KafkaRebalanceStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/KafkaUserStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/ListenerAddress.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/HasStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/KafkaConnectorStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/KafkaStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/KafkaTopicStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/Status.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/ListenerStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/StrimziPodSetStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/storage/PersistentClaimStorageOverride.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/storage/PersistentClaimStorage.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/storage/EphemeralStorage.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/storage/JbodStorage.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/storage/SingleVolumeStorage.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/storage/Storage.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/ContainerTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/ExternalTrafficPolicy.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/JmxTransOutputDefinitionTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/KafkaUserTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/PodManagementPolicy.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/StatefulSetTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/DeploymentStrategy.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/DeploymentTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/InternalServiceTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/IpFamily.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/IpFamilyPolicy.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/MetadataTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/PodDisruptionBudgetTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/PodTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/ResourceTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/BuildConfigTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/CruiseControlTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/EntityOperatorTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/JmxTransQueryTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/JmxTransTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/KafkaBridgeTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/KafkaClusterTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/KafkaConnectTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/KafkaExporterTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2Template.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/tracing/JaegerTracing.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/tracing/Tracing.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AbstractConnectorSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/ExternalConfigurationReference.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/JmxPrometheusExporterMetrics.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaAuthorizationCustom.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaBridgeAdminClientSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaUserTlsExternalClientAuthentication.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/PasswordSource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/StrimziPodSet.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/StrimziPodSetSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/Crds.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/StrimziPodSetList.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaStatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaStatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaStatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/ListenerStatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/ListenerStatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/ListenerStatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/ConditionFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/ConditionFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/ConditionBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaBridgeStatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaBridgeStatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaBridgeStatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaMirrorMaker2StatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaMirrorMaker2StatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaMirrorMaker2StatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaRebalanceStatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaRebalanceStatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaRebalanceStatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaUserStatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaUserStatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaUserStatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaConnectorStatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaConnectorStatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaConnectorStatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/StatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/StatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaTopicStatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaTopicStatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaTopicStatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaMirrorMakerStatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaMirrorMakerStatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaMirrorMakerStatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/ListenerAddressFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/ListenerAddressFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/ListenerAddressBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaConnectStatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaConnectStatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaConnectStatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/StrimziPodSetStatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/StrimziPodSetStatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/StrimziPodSetStatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ProbeFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ProbeFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ProbeBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2SpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2SpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2SpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/TgzArtifactFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/TgzArtifactFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/TgzArtifactBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/BuildFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/BuildFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/BuildBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/DockerOutputFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/DockerOutputFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/DockerOutputBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/MavenArtifactFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/MavenArtifactFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/MavenArtifactBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/DownloadableArtifactFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/DownloadableArtifactFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/JarArtifactFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/JarArtifactFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/JarArtifactBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/OtherArtifactFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/OtherArtifactFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/OtherArtifactBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/ZipArtifactFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/ZipArtifactFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/ZipArtifactBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/ImageStreamOutputFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/ImageStreamOutputFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/ImageStreamOutputBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/PluginFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/PluginFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/PluginBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationVolumeSourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationVolumeSourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationVolumeSourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ConnectorPluginFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ConnectorPluginFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ConnectorPluginBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvVarSourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvVarSourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvVarSourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationScramSha512Fluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationScramSha512FluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationScramSha512Builder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuthFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuthFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuthBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBootstrapFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBootstrapFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBootstrapBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBrokerFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBrokerFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBrokerBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationCustomFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationCustomFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationCustomBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationTlsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationTlsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationTlsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserAuthorizationSimpleFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserAuthorizationSimpleFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserAuthorizationSimpleBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/StrimziPodSetSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/StrimziPodSetSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/StrimziPodSetSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/EntityOperatorTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/PodTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/PodTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/PodTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/PodDisruptionBudgetTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/PodDisruptionBudgetTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/PodDisruptionBudgetTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/JmxTransOutputDefinitionTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/JmxTransOutputDefinitionTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/JmxTransOutputDefinitionTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/StatefulSetTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/StatefulSetTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/StatefulSetTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/CruiseControlTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/MetadataTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/MetadataTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/MetadataTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaExporterTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaClusterTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/ContainerTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/ContainerTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/ContainerTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaConnectTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaUserTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaUserTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaUserTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/JmxTransQueryTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/JmxTransQueryTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/JmxTransQueryTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/InternalServiceTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/InternalServiceTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/InternalServiceTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/ResourceTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/ResourceTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/ResourceTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/JmxTransTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/JmxTransTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/JmxTransTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/DeploymentTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/DeploymentTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/DeploymentTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/BuildConfigTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/BuildConfigTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/BuildConfigTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaJmxOptionsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaJmxOptionsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaJmxOptionsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/JbodStorageFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/JbodStorageFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/JbodStorageBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/PersistentClaimStorageOverrideFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/PersistentClaimStorageOverrideFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/PersistentClaimStorageOverrideBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/EphemeralStorageFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/EphemeralStorageFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/EphemeralStorageBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/PersistentClaimStorageFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/PersistentClaimStorageFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/PersistentClaimStorageBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/RackFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/RackFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/RackBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/TlsClientAuthenticationFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/TlsClientAuthenticationFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/TlsClientAuthenticationBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2Fluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2FluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2Builder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ClientTlsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ClientTlsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ClientTlsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectorSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectorSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectorSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/EntityUserOperatorSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaTopicFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaTopicFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaTopicBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserTlsClientAuthenticationFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserTlsClientAuthenticationFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserTlsClientAuthenticationBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeHttpConfigFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeHttpConfigFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeHttpConfigBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/JmxPrometheusExporterMetricsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/JmxPrometheusExporterMetricsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/JmxPrometheusExporterMetricsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/SystemPropertyFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/SystemPropertyFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/SystemPropertyBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaTopicSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaTopicSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaTopicSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CruiseControlSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CruiseControlSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserTlsExternalClientAuthenticationFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserTlsExternalClientAuthenticationFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserTlsExternalClientAuthenticationBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ZookeeperClusterSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/StrimziPodSetFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/StrimziPodSetFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/StrimziPodSetBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2ConnectorSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2ConnectorSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2ConnectorSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/balancing/BrokerCapacityFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/balancing/BrokerCapacityFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/balancing/BrokerCapacityBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/PasswordSourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/tracing/JaegerTracingFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/tracing/JaegerTracingFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/tracing/JaegerTracingBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/PasswordSourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/PasswordSourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaRebalanceSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaRebalanceSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaRebalanceSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaJmxAuthenticationPasswordFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaJmxAuthenticationPasswordFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaJmxAuthenticationPasswordBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha256Fluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha256FluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha256Builder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha512Fluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha512FluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha512Builder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationPlainFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationPlainFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationPlainBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationTlsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationTlsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationTlsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/TlsSidecarFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/TlsSidecarFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/TlsSidecarBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CertificateAuthorityFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CertificateAuthorityFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CertificateAuthorityBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeConsumerSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeConsumerSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeConsumerSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleTopicResourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleTopicResourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleTopicResourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleGroupResourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleGroupResourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleGroupResourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AbstractConnectorSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AbstractConnectorSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/EntityOperatorSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/EntityOperatorSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/EntityOperatorSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeHttpCorsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeHttpCorsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeHttpCorsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/EntityTopicOperatorSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/SidecarFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/SidecarFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/SidecarBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/JvmOptionsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/JvmOptionsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/JvmOptionsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaRebalanceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaRebalanceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaRebalanceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationOpaFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationOpaFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationOpaBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeProducerSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeProducerSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeProducerSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ExternalLoggingFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ExternalLoggingFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ExternalLoggingBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/PasswordSecretSourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/PasswordSecretSourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/PasswordSecretSourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationCustomFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationCustomFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationCustomBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/PasswordFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/PasswordFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/PasswordBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserScramSha512ClientAuthenticationFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserScramSha512ClientAuthenticationFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserScramSha512ClientAuthenticationBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CertAndKeySecretSourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CertAndKeySecretSourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CertAndKeySecretSourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerConsumerSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerConsumerSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerConsumerSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CertSecretSourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CertSecretSourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CertSecretSourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/InlineLoggingFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/InlineLoggingFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/InlineLoggingBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserQuotasFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserQuotasFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserQuotasBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ExternalConfigurationReferenceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ExternalConfigurationReferenceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ExternalConfigurationReferenceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationSimpleFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationSimpleFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationSimpleBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleTransactionalIdResourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleTransactionalIdResourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleTransactionalIdResourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerProducerSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerProducerSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerProducerSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/GenericSecretSourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/GenericSecretSourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/GenericSecretSourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/JmxTransSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/JmxTransSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/JmxTransSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaClusterSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectorFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectorFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectorBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/SpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/SpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeClientSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeClientSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ContainerEnvVarFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ContainerEnvVarFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ContainerEnvVarBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationKeycloakFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationKeycloakFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationKeycloakBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeAdminClientSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeAdminClientSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeAdminClientSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaExporterSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaExporterSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaExporterSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleClusterResourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleClusterResourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleClusterResourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectBuilder.java...
Constructing Javadoc information...
Standard Doclet version 11.0.11
Building tree for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListener.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfiguration.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBootstrap.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBootstrapBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBootstrapFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBootstrapFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBroker.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBrokerBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBrokerFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBrokerFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationFluent.BootstrapNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationFluent.BrokerCertChainAndKeyNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationFluent.BrokersNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationFluentImpl.BootstrapNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationFluentImpl.BrokerCertChainAndKeyNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationFluentImpl.BrokersNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluent.ConfigurationNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluent.KafkaListenerAuthenticationCustomAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluent.KafkaListenerAuthenticationOAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluent.KafkaListenerAuthenticationScramSha512AuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluent.KafkaListenerAuthenticationTlsAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluentImpl.ConfigurationNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluentImpl.KafkaListenerAuthenticationCustomAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluentImpl.KafkaListenerAuthenticationOAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluentImpl.KafkaListenerAuthenticationScramSha512AuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluentImpl.KafkaListenerAuthenticationTlsAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/KafkaListenerType.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuth.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluent.AccessTokenNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluent.ClientSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluent.RefreshTokenNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluent.TlsTrustedCertificatesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluentImpl.AccessTokenNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluentImpl.ClientSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluentImpl.RefreshTokenNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluentImpl.TlsTrustedCertificatesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationPlain.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationPlainBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationPlainFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationPlainFluent.PasswordSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationPlainFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationPlainFluentImpl.PasswordSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScram.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha256.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha256Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha256Fluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha256Fluent.PasswordSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha256FluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha256FluentImpl.PasswordSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha512.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha512Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha512Fluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha512Fluent.PasswordSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha512FluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha512FluentImpl.PasswordSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationTls.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationTlsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationTlsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationTlsFluent.CertificateAndKeyNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationTlsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationTlsFluentImpl.CertificateAndKeyNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/BrokerCapacity.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/BrokerCapacityBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/BrokerCapacityFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/BrokerCapacityFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/KafkaRebalanceAnnotation.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/KafkaRebalanceState.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/Artifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/Build.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/BuildBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/BuildFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/BuildFluent.DockerOutputNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/BuildFluent.ImageStreamOutputNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/BuildFluent.PluginsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/BuildFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/BuildFluentImpl.DockerOutputNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/BuildFluentImpl.ImageStreamOutputNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/BuildFluentImpl.PluginsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/DockerOutput.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/DockerOutputBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/DockerOutputFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/DockerOutputFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/DownloadableArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/DownloadableArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/DownloadableArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/ImageStreamOutput.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/ImageStreamOutputBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/ImageStreamOutputFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/ImageStreamOutputFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/JarArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/JarArtifactBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/JarArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/JarArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/MavenArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/MavenArtifactBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/MavenArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/MavenArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/OtherArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/OtherArtifactBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/OtherArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/OtherArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/Output.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/Plugin.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluent.JarArtifactArtifactsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluent.MavenArtifactArtifactsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluent.OtherArtifactArtifactsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluent.TgzArtifactArtifactsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluent.ZipArtifactArtifactsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluentImpl.JarArtifactArtifactsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluentImpl.MavenArtifactArtifactsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluentImpl.OtherArtifactArtifactsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluentImpl.TgzArtifactArtifactsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluentImpl.ZipArtifactArtifactsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/TgzArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/TgzArtifactBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/TgzArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/TgzArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/ZipArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/ZipArtifactBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/ZipArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/ZipArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ConnectorPlugin.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ConnectorPluginBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ConnectorPluginFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ConnectorPluginFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfiguration.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnv.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvFluent.ValueFromNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvFluentImpl.ValueFromNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvVarSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvVarSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvVarSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvVarSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationFluent.EnvNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationFluent.VolumesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationFluentImpl.EnvNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationFluentImpl.VolumesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationVolumeSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationVolumeSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationVolumeSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationVolumeSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/Crds.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/KafkaBridgeList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/KafkaConnectList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/KafkaConnectorList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/KafkaList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/KafkaMirrorMaker2List.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/KafkaMirrorMakerList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/KafkaRebalanceList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/KafkaTopicList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/KafkaUserList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/StrimziPodSetList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationCustom.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationCustomBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationCustomFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationCustomFluent.SecretsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationCustomFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationCustomFluentImpl.SecretsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuth.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuthBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuthFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuthFluent.ClientSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuthFluent.TlsTrustedCertificatesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuthFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuthFluentImpl.ClientSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuthFluentImpl.TlsTrustedCertificatesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationScramSha512.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationScramSha512Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationScramSha512Fluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationScramSha512FluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationTls.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationTlsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationTlsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationTlsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/NodeAddressType.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractConnectorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractConnectorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractConnectorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.ExternalConfigurationNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.JaegerTracingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.JmxOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.JmxPrometheusExporterMetricsConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.ExternalConfigurationNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.JaegerTracingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.JmxOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.JmxPrometheusExporterMetricsConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclOperation.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclResourcePatternType.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRule.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleClusterResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleClusterResourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleClusterResourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleClusterResourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleFluent.AclRuleClusterResourceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleFluent.AclRuleGroupResourceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleFluent.AclRuleTopicResourceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleFluent.AclRuleTransactionalIdResourceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleFluentImpl.AclRuleClusterResourceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleFluentImpl.AclRuleGroupResourceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleFluentImpl.AclRuleTopicResourceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleFluentImpl.AclRuleTransactionalIdResourceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleGroupResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleGroupResourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleGroupResourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleGroupResourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleTopicResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleTopicResourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleTopicResourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleTopicResourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleTransactionalIdResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleTransactionalIdResourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleTransactionalIdResourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleTransactionalIdResourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleType.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertAndKeySecretSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertAndKeySecretSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertAndKeySecretSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertAndKeySecretSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertificateAuthority.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertificateAuthorityBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertificateAuthorityFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertificateAuthorityFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertificateExpirationPolicy.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertSecretSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertSecretSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertSecretSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertSecretSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ClientTls.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ClientTlsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ClientTlsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ClientTlsFluent.TrustedCertificatesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ClientTlsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ClientTlsFluentImpl.TrustedCertificatesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/Constants.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ContainerEnvVar.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ContainerEnvVarBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ContainerEnvVarFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ContainerEnvVarFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluent.BrokerCapacityNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluent.JmxPrometheusExporterMetricsConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluent.TlsSidecarNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.BrokerCapacityNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.JmxPrometheusExporterMetricsConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.TlsSidecarNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecFluent.TlsSidecarNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecFluent.TopicOperatorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecFluent.UserOperatorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecFluentImpl.TlsSidecarNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecFluentImpl.TopicOperatorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecFluentImpl.UserOperatorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluent.StartupProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluentImpl.StartupProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ExternalConfigurationReference.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ExternalConfigurationReferenceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ExternalConfigurationReferenceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ExternalConfigurationReferenceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ExternalLogging.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ExternalLoggingBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ExternalLoggingFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ExternalLoggingFluent.ValueFromNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ExternalLoggingFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ExternalLoggingFluentImpl.ValueFromNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/GenericSecretSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/GenericSecretSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/GenericSecretSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/GenericSecretSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/HasConfigurableMetrics.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/HasSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/InlineLogging.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/InlineLoggingBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/InlineLoggingFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/InlineLoggingFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxPrometheusExporterMetrics.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxPrometheusExporterMetricsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxPrometheusExporterMetricsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxPrometheusExporterMetricsFluent.ValueFromNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxPrometheusExporterMetricsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxPrometheusExporterMetricsFluentImpl.ValueFromNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransSpecFluent.KafkaQueriesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransSpecFluent.OutputDefinitionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransSpecFluentImpl.KafkaQueriesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransSpecFluentImpl.OutputDefinitionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JvmOptions.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JvmOptionsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JvmOptionsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JvmOptionsFluent.JavaSystemPropertiesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JvmOptionsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JvmOptionsFluentImpl.JavaSystemPropertiesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/Kafka.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorization.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationCustom.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationCustomBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationCustomFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationCustomFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationKeycloak.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationKeycloakBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationKeycloakFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationKeycloakFluent.TlsTrustedCertificatesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationKeycloakFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationKeycloakFluentImpl.TlsTrustedCertificatesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationOpa.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationOpaBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationOpaFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationOpaFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationSimple.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationSimpleBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationSimpleFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationSimpleFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridge.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeAdminClientSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeAdminClientSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeAdminClientSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeAdminClientSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeClientSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeClientSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeClientSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeConsumerSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeConsumerSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeConsumerSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeConsumerSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeHttpConfig.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeHttpConfigBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeHttpConfigFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeHttpConfigFluent.CorsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeHttpConfigFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeHttpConfigFluentImpl.CorsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeHttpCors.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeHttpCorsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeHttpCorsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeHttpCorsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeProducerSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeProducerSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeProducerSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeProducerSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.AdminClientNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.ConsumerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.HttpNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.JaegerTracingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.KafkaClientAuthenticationOAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.KafkaClientAuthenticationPlainNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.KafkaClientAuthenticationScramSha256Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.KafkaClientAuthenticationScramSha512Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.KafkaClientAuthenticationTlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.ProducerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.TlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.AdminClientNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.ConsumerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.HttpNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.JaegerTracingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.KafkaClientAuthenticationOAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.KafkaClientAuthenticationPlainNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.KafkaClientAuthenticationScramSha256NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.KafkaClientAuthenticationScramSha512NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.KafkaClientAuthenticationTlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.ProducerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.TlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.EphemeralStorageNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.JbodStorageNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.JmxOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.JmxPrometheusExporterMetricsConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.KafkaAuthorizationCustomNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.KafkaAuthorizationKeycloakNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.KafkaAuthorizationOpaNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.KafkaAuthorizationSimpleNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.ListenersNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.PersistentClaimStorageNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.RackNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.EphemeralStorageNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.JbodStorageNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.JmxOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.JmxPrometheusExporterMetricsConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.KafkaAuthorizationCustomNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.KafkaAuthorizationKeycloakNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.KafkaAuthorizationOpaNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.KafkaAuthorizationSimpleNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.ListenersNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.PersistentClaimStorageNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.RackNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnect.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnector.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluent.BuildNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluent.KafkaClientAuthenticationOAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluent.KafkaClientAuthenticationPlainNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluent.KafkaClientAuthenticationScramSha256Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluent.KafkaClientAuthenticationScramSha512Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluent.KafkaClientAuthenticationTlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluent.RackNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluent.TlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluentImpl.BuildNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluentImpl.KafkaClientAuthenticationOAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluentImpl.KafkaClientAuthenticationPlainNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluentImpl.KafkaClientAuthenticationScramSha256NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluentImpl.KafkaClientAuthenticationScramSha512NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluentImpl.KafkaClientAuthenticationTlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluentImpl.RackNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluentImpl.TlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxAuthenticationPassword.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxAuthenticationPasswordBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxAuthenticationPasswordFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxAuthenticationPasswordFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxOptions.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxOptionsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxOptionsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxOptionsFluent.KafkaJmxAuthenticationPasswordNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxOptionsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxOptionsFluentImpl.KafkaJmxAuthenticationPasswordNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluent.KafkaClientAuthenticationOAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluent.KafkaClientAuthenticationPlainNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluent.KafkaClientAuthenticationScramSha256Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluent.KafkaClientAuthenticationScramSha512Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluent.KafkaClientAuthenticationTlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluent.TlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluentImpl.KafkaClientAuthenticationOAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluentImpl.KafkaClientAuthenticationPlainNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluentImpl.KafkaClientAuthenticationScramSha256NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluentImpl.KafkaClientAuthenticationScramSha512NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluentImpl.KafkaClientAuthenticationTlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluentImpl.TlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ConnectorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ConnectorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ConnectorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ConnectorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2Fluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2Fluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2Fluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2Fluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2FluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2FluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2FluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2FluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecFluent.CheckpointConnectorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecFluent.HeartbeatConnectorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecFluent.SourceConnectorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecFluentImpl.CheckpointConnectorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecFluentImpl.HeartbeatConnectorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecFluentImpl.SourceConnectorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2Resources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2Spec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2SpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2SpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2SpecFluent.ClustersNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2SpecFluent.MirrorsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2SpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2SpecFluentImpl.ClustersNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2SpecFluentImpl.MirrorsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluent.KafkaClientAuthenticationOAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluent.KafkaClientAuthenticationPlainNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluent.KafkaClientAuthenticationScramSha256Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluent.KafkaClientAuthenticationScramSha512Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluent.KafkaClientAuthenticationTlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluent.TlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluentImpl.KafkaClientAuthenticationOAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluentImpl.KafkaClientAuthenticationPlainNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluentImpl.KafkaClientAuthenticationScramSha256NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluentImpl.KafkaClientAuthenticationScramSha512NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluentImpl.KafkaClientAuthenticationTlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluentImpl.TlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerConsumerSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerConsumerSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerConsumerSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerConsumerSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerProducerSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerProducerSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerProducerSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerProducerSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.ConsumerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.JaegerTracingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.JmxPrometheusExporterMetricsConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.ProducerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.ConsumerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.JaegerTracingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.JmxPrometheusExporterMetricsConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.ProducerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalance.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluent.ClientsCaNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluent.ClusterCaNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluent.CruiseControlNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluent.EntityOperatorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluent.JmxTransNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluent.KafkaExporterNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluent.KafkaNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluent.ZookeeperNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluentImpl.ClientsCaNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluentImpl.ClusterCaNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluentImpl.CruiseControlNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluentImpl.EntityOperatorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluentImpl.JmxTransNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluentImpl.KafkaExporterNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluentImpl.KafkaNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluentImpl.ZookeeperNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopic.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUser.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserAuthorization.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserAuthorizationSimple.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserAuthorizationSimpleBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserAuthorizationSimpleFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserAuthorizationSimpleFluent.AclsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserAuthorizationSimpleFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserAuthorizationSimpleFluentImpl.AclsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserQuotas.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserQuotasBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserQuotasFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserQuotasFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserScramSha512ClientAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserScramSha512ClientAuthenticationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserScramSha512ClientAuthenticationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserScramSha512ClientAuthenticationFluent.PasswordNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserScramSha512ClientAuthenticationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserScramSha512ClientAuthenticationFluentImpl.PasswordNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluent.KafkaUserAuthorizationSimpleNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluent.KafkaUserScramSha512ClientAuthenticationNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluent.KafkaUserTlsClientAuthenticationNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluent.KafkaUserTlsExternalClientAuthenticationNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluent.QuotasNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluentImpl.KafkaUserAuthorizationSimpleNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluentImpl.KafkaUserScramSha512ClientAuthenticationNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluentImpl.KafkaUserTlsClientAuthenticationNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluentImpl.KafkaUserTlsExternalClientAuthenticationNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluentImpl.QuotasNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserTlsClientAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserTlsClientAuthenticationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserTlsClientAuthenticationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserTlsClientAuthenticationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserTlsExternalClientAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserTlsExternalClientAuthenticationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserTlsExternalClientAuthenticationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserTlsExternalClientAuthenticationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/Logging.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/MetricsConfig.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/Password.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordFluent.ValueFromNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordFluentImpl.ValueFromNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordSecretSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordSecretSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordSecretSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordSecretSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/Probe.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ProbeBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ProbeFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ProbeFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/Rack.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/RackBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/RackFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/RackFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/Sidecar.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/SidecarBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/SidecarFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/SidecarFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/Spec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/SpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/SpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSet.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/SystemProperty.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/SystemPropertyBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/SystemPropertyFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/SystemPropertyFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsClientAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsClientAuthenticationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsClientAuthenticationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsClientAuthenticationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsSidecar.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsSidecarBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsSidecarFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsSidecarFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsSidecarFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsSidecarFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsSidecarFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsSidecarFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsSidecarLogLevel.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/UnknownPropertyPreserving.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.EphemeralStorageNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.JmxOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.JmxPrometheusExporterMetricsConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.PersistentClaimStorageNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.EphemeralStorageNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.JmxOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.JmxPrometheusExporterMetricsConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.PersistentClaimStorageNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/Condition.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ConditionBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ConditionFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ConditionFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/HasStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaBridgeStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaBridgeStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaBridgeStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaBridgeStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaConnectorStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaConnectorStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaConnectorStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaConnectorStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaConnectStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaConnectStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaConnectStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaConnectStatusFluent.ConnectorPluginsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaConnectStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaConnectStatusFluentImpl.ConnectorPluginsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaMirrorMaker2Status.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaMirrorMaker2StatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaMirrorMaker2StatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaMirrorMaker2StatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaMirrorMakerStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaMirrorMakerStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaMirrorMakerStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaMirrorMakerStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaRebalanceStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaRebalanceStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaRebalanceStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaRebalanceStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaStatusFluent.ListenersNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaStatusFluentImpl.ListenersNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaTopicStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaTopicStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaTopicStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaTopicStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaUserStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaUserStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaUserStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaUserStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ListenerAddress.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ListenerAddressBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ListenerAddressFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ListenerAddressFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ListenerStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ListenerStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ListenerStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ListenerStatusFluent.AddressesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ListenerStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ListenerStatusFluentImpl.AddressesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/Status.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/StatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/StatusFluent.ConditionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/StatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/StatusFluentImpl.ConditionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/StrimziPodSetStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/StrimziPodSetStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/StrimziPodSetStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/StrimziPodSetStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/EphemeralStorage.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/EphemeralStorageBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/EphemeralStorageFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/EphemeralStorageFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/JbodStorage.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/JbodStorageBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/JbodStorageFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/JbodStorageFluent.EphemeralStorageVolumesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/JbodStorageFluent.PersistentClaimStorageVolumesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/JbodStorageFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/JbodStorageFluentImpl.EphemeralStorageVolumesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/JbodStorageFluentImpl.PersistentClaimStorageVolumesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/PersistentClaimStorage.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/PersistentClaimStorageBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/PersistentClaimStorageFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/PersistentClaimStorageFluent.OverridesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/PersistentClaimStorageFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/PersistentClaimStorageFluentImpl.OverridesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/PersistentClaimStorageOverride.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/PersistentClaimStorageOverrideBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/PersistentClaimStorageOverrideFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/PersistentClaimStorageOverrideFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/SingleVolumeStorage.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/Storage.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/BuildConfigTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/BuildConfigTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/BuildConfigTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/BuildConfigTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/BuildConfigTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/BuildConfigTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ContainerTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ContainerTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ContainerTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ContainerTemplateFluent.EnvNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ContainerTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ContainerTemplateFluentImpl.EnvNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluent.ApiServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluent.CruiseControlContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluent.TlsSidecarContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluentImpl.ApiServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluentImpl.CruiseControlContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluentImpl.TlsSidecarContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/DeploymentStrategy.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/DeploymentTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/DeploymentTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/DeploymentTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/DeploymentTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/DeploymentTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/DeploymentTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluent.TlsSidecarContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluent.TopicOperatorContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluent.UserOperatorContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluentImpl.TlsSidecarContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluentImpl.TopicOperatorContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluentImpl.UserOperatorContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ExternalTrafficPolicy.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/InternalServiceTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/InternalServiceTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/InternalServiceTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/InternalServiceTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/InternalServiceTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/InternalServiceTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/IpFamily.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/IpFamilyPolicy.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransOutputDefinitionTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransOutputDefinitionTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransOutputDefinitionTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransOutputDefinitionTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransQueryTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransQueryTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransQueryTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransQueryTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateFluent.ContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateFluentImpl.ContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluent.ApiServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluent.BridgeContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluentImpl.ApiServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluentImpl.BridgeContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.BootstrapServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.BrokersServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.ClusterCaCertNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.ClusterRoleBindingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.ExternalBootstrapIngressNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.ExternalBootstrapRouteNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.ExternalBootstrapServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.InitContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.JmxSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.KafkaContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.PerPodIngressNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.PerPodRouteNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.PerPodServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.PersistentVolumeClaimNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.PodSetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.StatefulsetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.BootstrapServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.BrokersServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.ClusterCaCertNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.ClusterRoleBindingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.ExternalBootstrapIngressNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.ExternalBootstrapRouteNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.ExternalBootstrapServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.InitContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.JmxSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.KafkaContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.PerPodIngressNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.PerPodRouteNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.PerPodServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.PersistentVolumeClaimNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.PodSetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.StatefulsetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.ApiServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.BuildConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.BuildContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.BuildPodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.BuildServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.ClusterRoleBindingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.ConnectContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.InitContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.JmxSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.ApiServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.BuildConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.BuildContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.BuildPodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.BuildServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.ClusterRoleBindingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.ConnectContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.InitContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.JmxSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluent.ContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluent.ServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluentImpl.ContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluentImpl.ServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2Template.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluent.ApiServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluent.JmxSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluent.MirrorMaker2ContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluentImpl.ApiServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluentImpl.JmxSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluentImpl.MirrorMaker2ContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluent.MirrorMakerContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluentImpl.MirrorMakerContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaUserTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaUserTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaUserTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaUserTemplateFluent.SecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaUserTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaUserTemplateFluentImpl.SecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/MetadataTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/MetadataTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/MetadataTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/MetadataTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodDisruptionBudgetTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodDisruptionBudgetTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodDisruptionBudgetTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodDisruptionBudgetTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodDisruptionBudgetTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodDisruptionBudgetTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodManagementPolicy.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ResourceTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ResourceTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ResourceTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ResourceTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ResourceTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ResourceTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/StatefulSetTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/StatefulSetTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/StatefulSetTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/StatefulSetTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/StatefulSetTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/StatefulSetTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.ClientServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.JmxSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.NodesServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.PersistentVolumeClaimNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.PodSetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.StatefulsetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.ZookeeperContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.ClientServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.JmxSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.NodesServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.PersistentVolumeClaimNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.PodSetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.StatefulsetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.ZookeeperContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/JaegerTracing.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/JaegerTracingBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/JaegerTracingFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/JaegerTracingFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/Tracing.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/constant-values.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/serialized-form.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/KafkaBridgeList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/KafkaConnectList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/KafkaConnectorList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/KafkaList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/KafkaMirrorMaker2List.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/KafkaMirrorMakerList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/KafkaRebalanceList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/KafkaTopicList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/KafkaUserList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleClusterResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclOperation.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclResourcePatternType.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleGroupResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleTopicResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleTransactionalIdResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ExternalLogging.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleType.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertAndKeySecretSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertSecretSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertificateExpirationPolicy.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnect.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ContainerEnvVar.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/HasConfigurableMetrics.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/InlineLogging.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JvmOptions.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/GenericSecretSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ClientTls.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorization.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationKeycloak.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationOpa.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationSimple.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridge.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/Probe.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeClientSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeConsumerSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeHttpConfig.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeHttpCors.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeProducerSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnector.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/Constants.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/Spec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRule.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertificateAuthority.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxAuthenticationPassword.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxOptions.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2Spec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ConnectorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2MirrorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2Resources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalance.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/HasSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerConsumerSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerProducerSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopic.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUser.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserAuthorization.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserAuthorizationSimple.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserQuotas.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserScramSha512ClientAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserTlsClientAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/MetricsConfig.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/Password.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/Logging.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordSecretSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/Rack.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/Sidecar.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/SystemProperty.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsClientAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsSidecar.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsSidecarLogLevel.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/Kafka.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/UnknownPropertyPreserving.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuth.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationPlain.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScram.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha256.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha512.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationTls.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/class-use/KafkaRebalanceAnnotation.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/class-use/KafkaRebalanceState.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/class-use/BrokerCapacity.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ConnectorPlugin.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationEnv.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationVolumeSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationEnvVarSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/Build.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/Plugin.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/Artifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/DockerOutput.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/DownloadableArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/ImageStreamOutput.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/JarArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/MavenArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/OtherArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/Output.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/TgzArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/ZipArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfiguration.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/NodeAddressType.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/KafkaListenerType.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListener.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationBootstrap.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationBroker.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfiguration.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationCustom.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationOAuth.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationScramSha512.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationTls.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/Condition.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaBridgeStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaConnectStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaMirrorMaker2Status.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaMirrorMakerStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaRebalanceStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaUserStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ListenerAddress.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/HasStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaConnectorStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaTopicStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/Status.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ListenerStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/StrimziPodSetStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/PersistentClaimStorageOverride.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/PersistentClaimStorage.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/EphemeralStorage.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/JbodStorage.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/SingleVolumeStorage.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/Storage.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ContainerTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ExternalTrafficPolicy.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransOutputDefinitionTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaUserTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodManagementPolicy.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/StatefulSetTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/DeploymentStrategy.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/DeploymentTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/InternalServiceTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/IpFamily.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/IpFamilyPolicy.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/MetadataTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodDisruptionBudgetTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ResourceTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/BuildConfigTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransQueryTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2Template.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/class-use/JaegerTracing.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/class-use/Tracing.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractConnectorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ExternalConfigurationReference.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxPrometheusExporterMetrics.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationCustom.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeAdminClientSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserTlsExternalClientAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSet.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/Crds.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/StrimziPodSetList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaStatusFluent.ListenersNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaStatusFluentImpl.ListenersNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ListenerStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ListenerStatusFluent.AddressesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ListenerStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ListenerStatusFluentImpl.AddressesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ListenerStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ConditionFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ConditionFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ConditionBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaBridgeStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaBridgeStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaBridgeStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaMirrorMaker2StatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaMirrorMaker2StatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaMirrorMaker2StatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaRebalanceStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaRebalanceStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaRebalanceStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaUserStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaUserStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaUserStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaConnectorStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaConnectorStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaConnectorStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/StatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/StatusFluent.ConditionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/StatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/StatusFluentImpl.ConditionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaTopicStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaTopicStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaTopicStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaMirrorMakerStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaMirrorMakerStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaMirrorMakerStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ListenerAddressFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ListenerAddressFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ListenerAddressBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaConnectStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaConnectStatusFluent.ConnectorPluginsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaConnectStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaConnectStatusFluentImpl.ConnectorPluginsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaConnectStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/StrimziPodSetStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/StrimziPodSetStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/StrimziPodSetStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ProbeFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ProbeFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ProbeBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2SpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2SpecFluent.MirrorsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2SpecFluent.ClustersNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2SpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2SpecFluentImpl.MirrorsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2SpecFluentImpl.ClustersNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2SpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/TgzArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/TgzArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/TgzArtifactBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/BuildFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/BuildFluent.PluginsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/BuildFluent.DockerOutputNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/BuildFluent.ImageStreamOutputNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/BuildFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/BuildFluentImpl.PluginsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/BuildFluentImpl.DockerOutputNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/BuildFluentImpl.ImageStreamOutputNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/BuildBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/DockerOutputFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/DockerOutputFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/DockerOutputBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/MavenArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/MavenArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/MavenArtifactBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/DownloadableArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/DownloadableArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/JarArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/JarArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/JarArtifactBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/OtherArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/OtherArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/OtherArtifactBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/ZipArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/ZipArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/ZipArtifactBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/ImageStreamOutputFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/ImageStreamOutputFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/ImageStreamOutputBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluent.TgzArtifactArtifactsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluent.JarArtifactArtifactsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluent.OtherArtifactArtifactsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluent.ZipArtifactArtifactsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluent.MavenArtifactArtifactsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluentImpl.TgzArtifactArtifactsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluentImpl.JarArtifactArtifactsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluentImpl.OtherArtifactArtifactsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluentImpl.ZipArtifactArtifactsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluentImpl.MavenArtifactArtifactsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationVolumeSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationVolumeSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationVolumeSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ConnectorPluginFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ConnectorPluginFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ConnectorPluginBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationEnvVarSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationEnvVarSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationEnvVarSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationFluent.VolumesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationFluent.EnvNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationFluentImpl.VolumesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationFluentImpl.EnvNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationEnvFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationEnvFluent.ValueFromNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationEnvFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationEnvFluentImpl.ValueFromNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationEnvBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationScramSha512Fluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationScramSha512FluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationScramSha512Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationOAuthFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationOAuthFluent.TlsTrustedCertificatesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationOAuthFluent.ClientSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationOAuthFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationOAuthFluentImpl.TlsTrustedCertificatesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationOAuthFluentImpl.ClientSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationOAuthBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationFluent.BrokersNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationFluent.BootstrapNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationFluent.BrokerCertChainAndKeyNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationFluentImpl.BrokersNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationFluentImpl.BootstrapNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationFluentImpl.BrokerCertChainAndKeyNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationBootstrapFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationBootstrapFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationBootstrapBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationBrokerFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationBrokerFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationBrokerBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluent.ConfigurationNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluent.KafkaListenerAuthenticationCustomAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluent.KafkaListenerAuthenticationScramSha512AuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluent.KafkaListenerAuthenticationTlsAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluent.KafkaListenerAuthenticationOAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluentImpl.ConfigurationNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluentImpl.KafkaListenerAuthenticationCustomAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluentImpl.KafkaListenerAuthenticationScramSha512AuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluentImpl.KafkaListenerAuthenticationTlsAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluentImpl.KafkaListenerAuthenticationOAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationCustomFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationCustomFluent.SecretsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationCustomFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationCustomFluentImpl.SecretsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationCustomBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationTlsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationTlsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationTlsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.JaegerTracingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.AdminClientNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.ProducerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.ConsumerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.KafkaClientAuthenticationTlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.KafkaClientAuthenticationOAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.KafkaClientAuthenticationPlainNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.KafkaClientAuthenticationScramSha256Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.KafkaClientAuthenticationScramSha512Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.TlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.HttpNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.JaegerTracingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.AdminClientNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.ProducerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.ConsumerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.KafkaClientAuthenticationTlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.KafkaClientAuthenticationOAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.KafkaClientAuthenticationPlainNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.KafkaClientAuthenticationScramSha256NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.KafkaClientAuthenticationScramSha512NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.TlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.HttpNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserAuthorizationSimpleFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserAuthorizationSimpleFluent.AclsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserAuthorizationSimpleFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserAuthorizationSimpleFluentImpl.AclsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserAuthorizationSimpleBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluent.TlsSidecarContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluent.UserOperatorContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluent.TopicOperatorContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluentImpl.TlsSidecarContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluentImpl.UserOperatorContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluentImpl.TopicOperatorContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodDisruptionBudgetTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodDisruptionBudgetTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodDisruptionBudgetTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodDisruptionBudgetTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodDisruptionBudgetTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransOutputDefinitionTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransOutputDefinitionTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransOutputDefinitionTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/StatefulSetTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/StatefulSetTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/StatefulSetTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/StatefulSetTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/StatefulSetTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluent.TlsSidecarContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluent.CruiseControlContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluent.ApiServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluentImpl.TlsSidecarContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluentImpl.CruiseControlContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluentImpl.ApiServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/MetadataTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/MetadataTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/MetadataTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluent.ContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluent.ServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluentImpl.ContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluentImpl.ServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.JmxSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.ClusterRoleBindingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.InitContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.KafkaContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.ClusterCaCertNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.PersistentVolumeClaimNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.PerPodIngressNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.ExternalBootstrapIngressNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.PerPodRouteNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.ExternalBootstrapRouteNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.PerPodServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.ExternalBootstrapServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.BrokersServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.BootstrapServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.PodSetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.StatefulsetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.JmxSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.ClusterRoleBindingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.InitContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.KafkaContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.ClusterCaCertNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.PersistentVolumeClaimNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.PerPodIngressNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.ExternalBootstrapIngressNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.PerPodRouteNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.ExternalBootstrapRouteNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.PerPodServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.ExternalBootstrapServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.BrokersServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.BootstrapServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.PodSetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.StatefulsetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluent.MirrorMakerContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluentImpl.MirrorMakerContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ContainerTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ContainerTemplateFluent.EnvNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ContainerTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ContainerTemplateFluentImpl.EnvNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ContainerTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.JmxSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.BuildServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.ClusterRoleBindingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.BuildConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.BuildContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.InitContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.ConnectContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.ApiServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.BuildPodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.JmxSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.BuildServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.ClusterRoleBindingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.BuildConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.BuildContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.InitContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.ConnectContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.ApiServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.BuildPodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaUserTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaUserTemplateFluent.SecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaUserTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaUserTemplateFluentImpl.SecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaUserTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransQueryTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransQueryTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransQueryTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluent.BridgeContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluent.ApiServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluentImpl.BridgeContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluentImpl.ApiServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/InternalServiceTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/InternalServiceTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/InternalServiceTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/InternalServiceTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/InternalServiceTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ResourceTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ResourceTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ResourceTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ResourceTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ResourceTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateFluent.ContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateFluentImpl.ContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.JmxSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.ZookeeperContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.PersistentVolumeClaimNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.NodesServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.ClientServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.PodSetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.StatefulsetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.JmxSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.ZookeeperContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.PersistentVolumeClaimNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.NodesServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.ClientServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.PodSetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.StatefulsetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/DeploymentTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/DeploymentTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/DeploymentTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/DeploymentTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/DeploymentTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/BuildConfigTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/BuildConfigTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/BuildConfigTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/BuildConfigTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/BuildConfigTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluent.JmxSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluent.MirrorMaker2ContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluent.ApiServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluentImpl.JmxSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluentImpl.MirrorMaker2ContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluentImpl.ApiServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluent.QuotasNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluent.KafkaUserAuthorizationSimpleNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluent.KafkaUserScramSha512ClientAuthenticationNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluent.KafkaUserTlsExternalClientAuthenticationNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluent.KafkaUserTlsClientAuthenticationNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluentImpl.QuotasNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluentImpl.KafkaUserAuthorizationSimpleNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluentImpl.KafkaUserScramSha512ClientAuthenticationNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluentImpl.KafkaUserTlsExternalClientAuthenticationNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluentImpl.KafkaUserTlsClientAuthenticationNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxOptionsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxOptionsFluent.KafkaJmxAuthenticationPasswordNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxOptionsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxOptionsFluentImpl.KafkaJmxAuthenticationPasswordNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxOptionsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/JbodStorageFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/JbodStorageFluent.EphemeralStorageVolumesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/JbodStorageFluent.PersistentClaimStorageVolumesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/JbodStorageFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/JbodStorageFluentImpl.EphemeralStorageVolumesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/JbodStorageFluentImpl.PersistentClaimStorageVolumesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/JbodStorageBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/PersistentClaimStorageOverrideFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/PersistentClaimStorageOverrideFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/PersistentClaimStorageOverrideBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/EphemeralStorageFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/EphemeralStorageFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/EphemeralStorageBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/PersistentClaimStorageFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/PersistentClaimStorageFluent.OverridesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/PersistentClaimStorageFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/PersistentClaimStorageFluentImpl.OverridesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/PersistentClaimStorageBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/RackFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/RackFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/RackBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsClientAuthenticationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsClientAuthenticationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsClientAuthenticationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleFluent.AclRuleClusterResourceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleFluent.AclRuleTransactionalIdResourceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleFluent.AclRuleTopicResourceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleFluent.AclRuleGroupResourceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleFluentImpl.AclRuleClusterResourceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleFluentImpl.AclRuleTransactionalIdResourceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleFluentImpl.AclRuleTopicResourceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleFluentImpl.AclRuleGroupResourceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2Fluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2Fluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2Fluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2Fluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2FluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2FluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2FluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2FluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ClientTlsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ClientTlsFluent.TrustedCertificatesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ClientTlsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ClientTlsFluentImpl.TrustedCertificatesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ClientTlsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserTlsClientAuthenticationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserTlsClientAuthenticationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserTlsClientAuthenticationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeHttpConfigFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeHttpConfigFluent.CorsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeHttpConfigFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeHttpConfigFluentImpl.CorsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeHttpConfigBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxPrometheusExporterMetricsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxPrometheusExporterMetricsFluent.ValueFromNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxPrometheusExporterMetricsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxPrometheusExporterMetricsFluentImpl.ValueFromNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxPrometheusExporterMetricsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/SystemPropertyFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/SystemPropertyFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/SystemPropertyBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluent.JmxPrometheusExporterMetricsConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluent.BrokerCapacityNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluent.TlsSidecarNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluentImpl.JmxPrometheusExporterMetricsConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluentImpl.BrokerCapacityNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluentImpl.TlsSidecarNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserTlsExternalClientAuthenticationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserTlsExternalClientAuthenticationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserTlsExternalClientAuthenticationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.JmxPrometheusExporterMetricsConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.JmxOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.EphemeralStorageNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.PersistentClaimStorageNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.JmxPrometheusExporterMetricsConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.JmxOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.EphemeralStorageNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.PersistentClaimStorageNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ConnectorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ConnectorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ConnectorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/class-use/BrokerCapacityFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/class-use/BrokerCapacityFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/class-use/BrokerCapacityBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/class-use/JaegerTracingFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/class-use/JaegerTracingFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/class-use/JaegerTracingBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.ExternalConfigurationNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.JaegerTracingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.JmxPrometheusExporterMetricsConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.JmxOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.ExternalConfigurationNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.JaegerTracingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.JmxPrometheusExporterMetricsConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.JmxOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxAuthenticationPasswordFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxAuthenticationPasswordFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxAuthenticationPasswordBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2MirrorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2MirrorSpecFluent.HeartbeatConnectorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2MirrorSpecFluent.CheckpointConnectorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2MirrorSpecFluent.SourceConnectorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2MirrorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2MirrorSpecFluentImpl.HeartbeatConnectorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2MirrorSpecFluentImpl.CheckpointConnectorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2MirrorSpecFluentImpl.SourceConnectorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2MirrorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha256Fluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha256Fluent.PasswordSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha256FluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha256FluentImpl.PasswordSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha256Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthFluent.TlsTrustedCertificatesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthFluent.RefreshTokenNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthFluent.AccessTokenNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthFluent.ClientSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthFluentImpl.TlsTrustedCertificatesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthFluentImpl.RefreshTokenNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthFluentImpl.AccessTokenNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthFluentImpl.ClientSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha512Fluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha512Fluent.PasswordSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha512FluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha512FluentImpl.PasswordSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha512Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationPlainFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationPlainFluent.PasswordSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationPlainFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationPlainFluentImpl.PasswordSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationPlainBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationTlsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationTlsFluent.CertificateAndKeyNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationTlsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationTlsFluentImpl.CertificateAndKeyNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationTlsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsSidecarFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsSidecarFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsSidecarFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsSidecarFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsSidecarFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsSidecarFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsSidecarBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertificateAuthorityFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertificateAuthorityFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertificateAuthorityBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeConsumerSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeConsumerSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeConsumerSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluent.BuildNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluent.KafkaClientAuthenticationTlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluent.KafkaClientAuthenticationOAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluent.KafkaClientAuthenticationPlainNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluent.KafkaClientAuthenticationScramSha256Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluent.KafkaClientAuthenticationScramSha512Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluent.TlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluent.RackNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluentImpl.BuildNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluentImpl.KafkaClientAuthenticationTlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluentImpl.KafkaClientAuthenticationOAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluentImpl.KafkaClientAuthenticationPlainNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluentImpl.KafkaClientAuthenticationScramSha256NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluentImpl.KafkaClientAuthenticationScramSha512NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluentImpl.TlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluentImpl.RackNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleTopicResourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleTopicResourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleTopicResourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleGroupResourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleGroupResourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleGroupResourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractConnectorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractConnectorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecFluent.TlsSidecarNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecFluent.UserOperatorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecFluent.TopicOperatorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecFluentImpl.TlsSidecarNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecFluentImpl.UserOperatorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecFluentImpl.TopicOperatorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeHttpCorsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeHttpCorsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeHttpCorsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluent.StartupProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluentImpl.StartupProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/SidecarFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/SidecarFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/SidecarBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JvmOptionsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JvmOptionsFluent.JavaSystemPropertiesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JvmOptionsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JvmOptionsFluentImpl.JavaSystemPropertiesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JvmOptionsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluent.KafkaClientAuthenticationTlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluent.KafkaClientAuthenticationOAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluent.KafkaClientAuthenticationPlainNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluent.KafkaClientAuthenticationScramSha256Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluent.KafkaClientAuthenticationScramSha512Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluent.TlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluentImpl.KafkaClientAuthenticationTlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluentImpl.KafkaClientAuthenticationOAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluentImpl.KafkaClientAuthenticationPlainNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluentImpl.KafkaClientAuthenticationScramSha256NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluentImpl.KafkaClientAuthenticationScramSha512NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluentImpl.TlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationOpaFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationOpaFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationOpaBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeProducerSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeProducerSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeProducerSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ExternalLoggingFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ExternalLoggingFluent.ValueFromNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ExternalLoggingFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ExternalLoggingFluentImpl.ValueFromNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ExternalLoggingBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordSecretSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordSecretSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordSecretSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationCustomFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationCustomFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationCustomBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordFluent.ValueFromNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordFluentImpl.ValueFromNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluent.KafkaClientAuthenticationTlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluent.KafkaClientAuthenticationOAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluent.KafkaClientAuthenticationPlainNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluent.KafkaClientAuthenticationScramSha256Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluent.KafkaClientAuthenticationScramSha512Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluent.TlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluentImpl.KafkaClientAuthenticationTlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluentImpl.KafkaClientAuthenticationOAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluentImpl.KafkaClientAuthenticationPlainNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluentImpl.KafkaClientAuthenticationScramSha256NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluentImpl.KafkaClientAuthenticationScramSha512NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluentImpl.TlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserScramSha512ClientAuthenticationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserScramSha512ClientAuthenticationFluent.PasswordNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserScramSha512ClientAuthenticationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserScramSha512ClientAuthenticationFluentImpl.PasswordNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserScramSha512ClientAuthenticationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertAndKeySecretSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertAndKeySecretSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertAndKeySecretSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerConsumerSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluent.ClientsCaNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluent.CruiseControlNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluent.KafkaExporterNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluent.JmxTransNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluent.ClusterCaNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluent.EntityOperatorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluent.ZookeeperNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluent.KafkaNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerConsumerSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerConsumerSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertSecretSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertSecretSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertSecretSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluentImpl.ClientsCaNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluentImpl.CruiseControlNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluentImpl.KafkaExporterNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluentImpl.JmxTransNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluentImpl.ClusterCaNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluentImpl.EntityOperatorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluentImpl.ZookeeperNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluentImpl.KafkaNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/InlineLoggingFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/InlineLoggingFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/InlineLoggingBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserQuotasFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserQuotasFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserQuotasBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ExternalConfigurationReferenceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ExternalConfigurationReferenceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ExternalConfigurationReferenceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationSimpleFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationSimpleFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationSimpleBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleTransactionalIdResourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleTransactionalIdResourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleTransactionalIdResourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerProducerSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerProducerSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerProducerSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/GenericSecretSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/GenericSecretSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/GenericSecretSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransSpecFluent.KafkaQueriesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransSpecFluent.OutputDefinitionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransSpecFluentImpl.KafkaQueriesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransSpecFluentImpl.OutputDefinitionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.KafkaAuthorizationSimpleNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.KafkaAuthorizationKeycloakNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.KafkaAuthorizationCustomNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.KafkaAuthorizationOpaNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.ListenersNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.JmxPrometheusExporterMetricsConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.JmxOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.RackNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.JbodStorageNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.EphemeralStorageNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.PersistentClaimStorageNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.KafkaAuthorizationSimpleNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.KafkaAuthorizationKeycloakNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.KafkaAuthorizationCustomNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.KafkaAuthorizationOpaNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.ListenersNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.JmxPrometheusExporterMetricsConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.JmxOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.RackNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.JbodStorageNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.EphemeralStorageNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.PersistentClaimStorageNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/SpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/SpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeClientSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeClientSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ContainerEnvVarFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ContainerEnvVarFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ContainerEnvVarBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationKeycloakFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationKeycloakFluent.TlsTrustedCertificatesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationKeycloakFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationKeycloakFluentImpl.TlsTrustedCertificatesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationKeycloakBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeAdminClientSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeAdminClientSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeAdminClientSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleClusterResourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleClusterResourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleClusterResourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.JaegerTracingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.JmxPrometheusExporterMetricsConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.ProducerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.ConsumerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.JaegerTracingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.JmxPrometheusExporterMetricsConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.ProducerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.ConsumerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/package-use.html...
Building index for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/overview-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/index-all.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/allclasses-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/allpackages-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/deprecated-list.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/index.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/overview-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/help-doc.html...
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/api/target/api-0.29.0-SNAPSHOT-javadoc.jar
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:test-jar (default) @ api ---
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:integration-test (default) @ api ---
[WARNING] useSystemClassLoader setting has no effect when not forking
[WARNING] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:verify (default) @ api ---
[INFO] 
[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (analyze) @ api ---
[INFO] No dependency problems found
[INFO] 
[INFO] ------------------------< io.strimzi:mockkube >-------------------------
[INFO] Building mockkube 0.29.0-SNAPSHOT                                 [6/10]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ mockkube ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ mockkube ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ mockkube ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/mockkube/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ mockkube ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/mockkube/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ mockkube ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ mockkube ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ mockkube ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M5:test (default-test) @ mockkube ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ mockkube ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:report (report) @ mockkube ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] >>> maven-source-plugin:3.0.1:jar (attach-sources) > generate-sources @ mockkube >>>
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ mockkube ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ mockkube ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ mockkube ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/mockkube/target/jacoco.exec
[INFO] 
[INFO] <<< maven-source-plugin:3.0.1:jar (attach-sources) < generate-sources @ mockkube <<<
[INFO] 
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar (attach-sources) @ mockkube ---
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/mockkube/target/mockkube-0.29.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] --- maven-javadoc-plugin:3.1.0:jar (attach-javadocs) @ mockkube ---
[INFO] 
Loading source file /home/cloud-user/strimzi-kafka-operator/mockkube/src/main/java/io/strimzi/test/mockkube/Observer.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/mockkube/src/main/java/io/strimzi/test/mockkube/PredicatedWatcher.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/mockkube/src/main/java/io/strimzi/test/mockkube/CustomResourceMockBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/mockkube/src/main/java/io/strimzi/test/mockkube/ServiceMockBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/mockkube/src/main/java/io/strimzi/test/mockkube/DeploymentMockBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/mockkube/src/main/java/io/strimzi/test/mockkube/MockBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/mockkube/src/main/java/io/strimzi/test/mockkube/MockKube.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/mockkube/src/main/java/io/strimzi/test/mockkube/StatefulSetMockBuilder.java...
Constructing Javadoc information...
Standard Doclet version 11.0.11
Building tree for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/io/strimzi/test/mockkube/MockKube.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/io/strimzi/test/mockkube/MockKube.MockedCrd.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/io/strimzi/test/mockkube/Observer.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/io/strimzi/test/mockkube/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/io/strimzi/test/mockkube/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/constant-values.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/io/strimzi/test/mockkube/class-use/Observer.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/io/strimzi/test/mockkube/class-use/MockKube.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/io/strimzi/test/mockkube/class-use/MockKube.MockedCrd.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/io/strimzi/test/mockkube/package-use.html...
Building index for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/overview-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/index-all.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/allclasses-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/allpackages-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/deprecated-list.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/index.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/help-doc.html...
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/mockkube/target/mockkube-0.29.0-SNAPSHOT-javadoc.jar
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:integration-test (default) @ mockkube ---
[WARNING] useSystemClassLoader setting has no effect when not forking
[WARNING] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:verify (default) @ mockkube ---
[INFO] 
[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (analyze) @ mockkube ---
[INFO] No dependency problems found
[INFO] 
[INFO] ----------------------< io.strimzi:config-model >-----------------------
[INFO] Building config-model 0.29.0-SNAPSHOT                             [7/10]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ config-model ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ config-model ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ config-model ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/config-model/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ config-model ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/config-model/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ config-model ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ config-model ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/config-model/src/test/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ config-model ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M5:test (default-test) @ config-model ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ config-model ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:report (report) @ config-model ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] >>> maven-source-plugin:3.0.1:jar (attach-sources) > generate-sources @ config-model >>>
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ config-model ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ config-model ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ config-model ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/config-model/target/jacoco.exec
[INFO] 
[INFO] <<< maven-source-plugin:3.0.1:jar (attach-sources) < generate-sources @ config-model <<<
[INFO] 
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar (attach-sources) @ config-model ---
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/config-model/target/config-model-0.29.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] --- maven-javadoc-plugin:3.1.0:jar (attach-javadocs) @ config-model ---
[INFO] 
Loading source file /home/cloud-user/strimzi-kafka-operator/config-model/src/main/java/io/strimzi/kafka/config/model/ConfigModel.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/config-model/src/main/java/io/strimzi/kafka/config/model/ConfigModels.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/config-model/src/main/java/io/strimzi/kafka/config/model/Scope.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/config-model/src/main/java/io/strimzi/kafka/config/model/Type.java...
Constructing Javadoc information...
Standard Doclet version 11.0.11
Building tree for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/ConfigModel.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/ConfigModels.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/Scope.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/Type.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/constant-values.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/class-use/ConfigModel.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/class-use/ConfigModels.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/class-use/Scope.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/class-use/Type.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/package-use.html...
Building index for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/overview-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/index-all.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/allclasses-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/allpackages-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/deprecated-list.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/index.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/help-doc.html...
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/config-model/target/config-model-0.29.0-SNAPSHOT-javadoc.jar
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:integration-test (default) @ config-model ---
[WARNING] useSystemClassLoader setting has no effect when not forking
[WARNING] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:verify (default) @ config-model ---
[INFO] 
[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (analyze) @ config-model ---
[INFO] No dependency problems found
[INFO] 
[INFO] -------------------< io.strimzi:certificate-manager >-------------------
[INFO] Building certificate-manager 0.29.0-SNAPSHOT                      [8/10]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ certificate-manager ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ certificate-manager ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ certificate-manager ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/certificate-manager/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ certificate-manager ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ certificate-manager ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ certificate-manager ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ certificate-manager ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M5:test (default-test) @ certificate-manager ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ certificate-manager ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:report (report) @ certificate-manager ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] >>> maven-source-plugin:3.0.1:jar (attach-sources) > generate-sources @ certificate-manager >>>
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ certificate-manager ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ certificate-manager ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ certificate-manager ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/certificate-manager/target/jacoco.exec
[INFO] 
[INFO] <<< maven-source-plugin:3.0.1:jar (attach-sources) < generate-sources @ certificate-manager <<<
[INFO] 
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar (attach-sources) @ certificate-manager ---
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/certificate-manager-0.29.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] --- maven-javadoc-plugin:3.1.0:jar (attach-javadocs) @ certificate-manager ---
[INFO] 
Loading source file /home/cloud-user/strimzi-kafka-operator/certificate-manager/src/main/java/io/strimzi/certs/CertAndKey.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/certificate-manager/src/main/java/io/strimzi/certs/CertManager.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/certificate-manager/src/main/java/io/strimzi/certs/OpenSslCertManager.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/certificate-manager/src/main/java/io/strimzi/certs/SecretCertProvider.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/certificate-manager/src/main/java/io/strimzi/certs/Subject.java...
Constructing Javadoc information...
Standard Doclet version 11.0.11
Building tree for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/CertAndKey.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/CertManager.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/OpenSslCertManager.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/SecretCertProvider.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/Subject.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/Subject.Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/constant-values.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/class-use/CertAndKey.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/class-use/CertManager.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/class-use/OpenSslCertManager.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/class-use/SecretCertProvider.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/class-use/Subject.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/class-use/Subject.Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/package-use.html...
Building index for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/overview-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/index-all.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/allclasses-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/allpackages-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/deprecated-list.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/index.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/help-doc.html...
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/certificate-manager-0.29.0-SNAPSHOT-javadoc.jar
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:integration-test (default) @ certificate-manager ---
[WARNING] useSystemClassLoader setting has no effect when not forking
[WARNING] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:verify (default) @ certificate-manager ---
[INFO] 
[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (analyze) @ certificate-manager ---
[INFO] No dependency problems found
[INFO] 
[INFO] ---------------------< io.strimzi:operator-common >---------------------
[INFO] Building operator-common 0.29.0-SNAPSHOT                          [9/10]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ operator-common ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ operator-common ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ operator-common ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/operator-common/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ operator-common ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ operator-common ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ operator-common ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 9 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ operator-common ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M5:test (default-test) @ operator-common ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ operator-common ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:report (report) @ operator-common ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] >>> maven-source-plugin:3.0.1:jar (attach-sources) > generate-sources @ operator-common >>>
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ operator-common ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ operator-common ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ operator-common ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/operator-common/target/jacoco.exec
[INFO] 
[INFO] <<< maven-source-plugin:3.0.1:jar (attach-sources) < generate-sources @ operator-common <<<
[INFO] 
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar (attach-sources) @ operator-common ---
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/operator-common/target/operator-common-0.29.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] --- maven-javadoc-plugin:3.1.0:jar (attach-javadocs) @ operator-common ---
[INFO] 
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/model/InvalidResourceException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/model/NoSuchResourceException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/model/Ca.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/model/ClientsCa.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/model/NodeUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/model/StatusDiff.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlEndpoints.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlUserTaskStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlGoals.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlConfigurationParameters.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlLoadParameters.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlParameters.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlRebalanceKeys.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/AdminClientProvider.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/BackOff.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/InvalidConfigurationException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/MaxAttemptsExceededException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/MetricsProvider.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/PasswordGenerator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/model/NamespaceAndName.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/model/Labels.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/model/OrderedProperties.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/model/ResourceVisitor.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/model/ValidationVisitor.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/DeploymentOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/IngressV1Beta1Operator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/PodDisruptionBudgetOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/PvcOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/ResourceSupport.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/ServiceAccountOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/StatusUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/AbstractNonNamespacedResourceOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/AbstractScalableResourceOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/ClusterRoleOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/ReconcileResult.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/CrdOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/TimeoutException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/AbstractJsonDiff.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/AbstractResourceOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/AbstractWatchableStatusedResourceOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/BuildOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/EndpointOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/ImageStreamOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/NetworkPolicyOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/NoStackTraceTimeoutException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/RoleBindingOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/RoleOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/SecretOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/BuildConfigOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/ClusterRoleBindingOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/ConfigMapOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/DeploymentConfigOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/IngressOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/NodeOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/PodOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/ResourceDiff.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/RouteOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/ServiceOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/StorageClassOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/AbstractReadyResourceOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/AbstractWatchableResourceOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/PodDisruptionBudgetV1Beta1Operator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/process/ProcessHelper.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/MetricsAndLogging.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/ReconciliationException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/AbstractOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/Annotations.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/DefaultAdminClientProvider.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/InvalidConfigParameterException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/MicrometerMetricsProvider.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/Operator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/OperatorWatcher.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/Reconciliation.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/ReconciliationLogger.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/Util.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/KubernetesVersion.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/PlatformFeaturesAvailability.java...
Constructing Javadoc information...
Standard Doclet version 11.0.11
Building tree for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/AbstractOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/AdminClientProvider.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/Annotations.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/BackOff.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/DefaultAdminClientProvider.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/InvalidConfigParameterException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/InvalidConfigurationException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/MaxAttemptsExceededException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/MetricsAndLogging.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/MetricsProvider.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/MicrometerMetricsProvider.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/Operator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/PasswordGenerator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/Reconciliation.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/ReconciliationException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/ReconciliationLogger.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/Util.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlConfigurationParameters.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlEndpoints.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlGoals.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlLoadParameters.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlParameters.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlRebalanceKeys.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlUserTaskStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/Ca.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/ClientsCa.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/InvalidResourceException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/NodeUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/NoSuchResourceException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/StatusDiff.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/Labels.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/NamespaceAndName.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/OrderedProperties.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/ResourceVisitor.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/ResourceVisitor.Property.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/ResourceVisitor.Visitor.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/ValidationVisitor.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/KubernetesVersion.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/PlatformFeaturesAvailability.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/process/ProcessHelper.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/process/ProcessHelper.ProcessResult.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/AbstractJsonDiff.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/AbstractNonNamespacedResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/AbstractReadyResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/AbstractResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/AbstractScalableResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/AbstractWatchableResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/AbstractWatchableStatusedResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/BuildConfigOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/BuildOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ClusterRoleBindingOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ClusterRoleOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ConfigMapOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/CrdOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/DeploymentConfigOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/DeploymentOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/EndpointOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ImageStreamOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/IngressOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/IngressV1Beta1Operator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/NetworkPolicyOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/NodeOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/NoStackTraceTimeoutException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/PodDisruptionBudgetOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/PodDisruptionBudgetV1Beta1Operator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/PodOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/PvcOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ReconcileResult.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ReconcileResult.Created.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ReconcileResult.Noop.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ReconcileResult.Patched.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ResourceSupport.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/RoleBindingOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/RoleOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/RouteOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/SecretOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ServiceAccountOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ServiceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/StatusUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/StorageClassOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/TimeoutException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/process/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/process/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/constant-values.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/serialized-form.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/class-use/InvalidResourceException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/class-use/NoSuchResourceException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/class-use/Ca.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/class-use/ClientsCa.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/class-use/NodeUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/class-use/StatusDiff.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/class-use/CruiseControlEndpoints.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/class-use/CruiseControlUserTaskStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/class-use/CruiseControlGoals.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/class-use/CruiseControlConfigurationParameters.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/class-use/CruiseControlLoadParameters.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/class-use/CruiseControlParameters.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/class-use/CruiseControlRebalanceKeys.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/AdminClientProvider.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/BackOff.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/InvalidConfigurationException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/MaxAttemptsExceededException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/MetricsProvider.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/PasswordGenerator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/class-use/NamespaceAndName.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/class-use/Labels.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/class-use/OrderedProperties.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/class-use/ResourceVisitor.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/class-use/ResourceVisitor.Property.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/class-use/ResourceVisitor.Visitor.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/class-use/ValidationVisitor.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/DeploymentOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/IngressV1Beta1Operator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/PodDisruptionBudgetOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/PvcOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ResourceSupport.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ServiceAccountOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/StatusUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/AbstractNonNamespacedResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/AbstractScalableResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ClusterRoleOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ReconcileResult.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ReconcileResult.Patched.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ReconcileResult.Created.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ReconcileResult.Noop.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/CrdOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/TimeoutException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/AbstractJsonDiff.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/AbstractResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/AbstractWatchableStatusedResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/BuildOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/EndpointOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ImageStreamOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/NetworkPolicyOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/NoStackTraceTimeoutException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/RoleBindingOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/RoleOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/SecretOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/BuildConfigOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ClusterRoleBindingOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ConfigMapOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/DeploymentConfigOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/IngressOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/NodeOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/PodOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/RouteOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ServiceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/StorageClassOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/AbstractReadyResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/AbstractWatchableResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/PodDisruptionBudgetV1Beta1Operator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/process/class-use/ProcessHelper.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/process/class-use/ProcessHelper.ProcessResult.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/MetricsAndLogging.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/ReconciliationException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/AbstractOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/Annotations.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/DefaultAdminClientProvider.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/InvalidConfigParameterException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/MicrometerMetricsProvider.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/Operator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/Reconciliation.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/ReconciliationLogger.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/Util.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/class-use/KubernetesVersion.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/class-use/PlatformFeaturesAvailability.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/process/package-use.html...
Building index for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/overview-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/index-all.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/allclasses-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/allpackages-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/deprecated-list.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/index.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/overview-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/help-doc.html...
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/operator-common/target/operator-common-0.29.0-SNAPSHOT-javadoc.jar
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:test-jar (default) @ operator-common ---
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:integration-test (default) @ operator-common ---
[WARNING] useSystemClassLoader setting has no effect when not forking
[WARNING] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:verify (default) @ operator-common ---
[INFO] 
[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (analyze) @ operator-common ---
[INFO] No dependency problems found
[INFO] 
[INFO] -----------------------< io.strimzi:systemtest >------------------------
[INFO] Building systemtest 0.29.0-SNAPSHOT                              [10/10]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ systemtest ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ systemtest ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ systemtest ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/systemtest/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ systemtest ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 3 resources
[INFO] Copying 2 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ systemtest ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ systemtest ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 32 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ systemtest ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M5:test (default-test) @ systemtest ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[WARNING] Corrupted STDOUT by directly writing to native stream in forked JVM 1. See FAQ web page and the dump file /home/cloud-user/strimzi-kafka-operator/systemtest/target/surefire-reports/2022-03-28T06-45-40_602-jvmRun1.dumpstream
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ systemtest ---
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/systemtest/target/systemtest-0.29.0-SNAPSHOT.jar
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:report (report) @ systemtest ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] >>> maven-source-plugin:3.0.1:jar (attach-sources) > generate-sources @ systemtest >>>
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ systemtest ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ systemtest ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ systemtest ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/systemtest/target/jacoco.exec
[INFO] 
[INFO] <<< maven-source-plugin:3.0.1:jar (attach-sources) < generate-sources @ systemtest <<<
[INFO] 
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar (attach-sources) @ systemtest ---
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/systemtest/target/systemtest-0.29.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] --- maven-javadoc-plugin:3.1.0:jar (attach-javadocs) @ systemtest ---
[INFO] 
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/MultiNodeClusterOnly.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/MultiNodeClusterOnlyCondition.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/OpenShiftOnly.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/RequiredMinKubeApiVersion.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/RequiredMinKubeApiVersionCondition.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/IsolatedTest.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/ParallelNamespaceTest.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/IsolatedSuite.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/OpenShiftOnlyCondition.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/ParallelSuite.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/ParallelTest.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/StatefulSetTest.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/StatefulSetTestCondition.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/StrimziPodSetTest.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/StrimziPodSetTestCondition.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/cli/KafkaCmdClient.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/enums/DefaultNetworkPolicy.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/enums/ClusterOperatorInstallType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/enums/CustomResourceStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/enums/OlmInstallationStrategy.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/enums/ClusterOperatorRBACType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/enums/DeploymentTypes.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/k8s/Events.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/clientproperties/ProducerProperties.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/clientproperties/AbstractKafkaClientProperties.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/clientproperties/ConsumerProperties.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/AbstractKafkaClient.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/KafkaClientOperations.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/clients/ClientArgument.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/clients/ClientArgumentMap.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/clients/ClientType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/clients/InternalKafkaClient.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/clients/VerifiableClient.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/externalClients/ExternalKafkaClient.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/internalClients/AdminClientOperations.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/internalClients/BaseClients.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/internalClients/BridgeClients.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/internalClients/KafkaAdminClients.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/internalClients/KafkaClients.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/internalClients/KafkaOauthClients.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/internalClients/KafkaTracingClients.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/keycloak/KeycloakInstance.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/listeners/ExecutionListener.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/listeners/OrderTestSuites.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/logs/LogCollector.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/logs/TestExecutionWatcher.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/matchers/HasAllOfReasons.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/matchers/HasAnyOfReasons.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/matchers/HasNoneOfReasons.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/matchers/Matchers.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/matchers/LogHasNoUnexpectedErrors.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaBridgeResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaConnectResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaConnectorResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaMirrorMaker2Resource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaTopicResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaClientsResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaMirrorMakerResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaRebalanceResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaUserResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/StrimziPodSetResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/operator/BundleResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/operator/specific/HelmResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/operator/specific/SpecificResourceType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/operator/specific/OlmResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/operator/SetupClusterOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/ResourceCondition.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/ResourceItem.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/ResourceType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/ThrowableRunner.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/ServiceResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/ClusterOperatorCustomResourceDefinition.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/ClusterRoleBindingResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/ClusterRoleResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/ConfigMapResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/JobResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/NetworkPolicyResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/RoleBindingResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/RoleResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/SecretResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/ServiceAccountResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/ValidatingWebhookConfigurationResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/DeploymentResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/ComponentType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/ResourceManager.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/draincleaner/DrainCleanerResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/draincleaner/SetupDrainCleaner.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/ResourceOperation.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/security/CertAndKeyFiles.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/security/SystemTestCertAndKey.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/security/SystemTestCertAndKeyBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/security/SystemTestCertManager.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaBridgeUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaConnectUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaConnectorUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaMirrorMaker2Utils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaMirrorMakerUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaRebalanceUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaTopicUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUserUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/controllers/ConfigMapUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/controllers/DeploymentUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/controllers/JobUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/controllers/StatefulSetUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/controllers/StrimziPodSetUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/objects/NamespaceUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/objects/NodeUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/objects/PersistentVolumeClaimUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/objects/PodUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/objects/SecretUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/objects/ServiceUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/specific/BridgeUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/specific/CruiseControlUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/specific/JmxUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/specific/OlmUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/specific/TracingUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/specific/KeycloakUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/ClientUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/FileUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/HttpUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/RollingUpdateUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/StUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/TestKafkaVersion.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/interfaces/IndicativeSentences.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaBridgeTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaClientsTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaConnectTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaConnectorTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaMirrorMaker2Templates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaMirrorMakerTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaRebalanceTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaTopicTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaUserTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/kubernetes/ServiceTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/kubernetes/ClusterRoleBindingTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/kubernetes/NetworkPolicyTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/specific/ScraperTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/BeforeAllOnce.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/metrics/MetricsCollector.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/parallel/SuiteThreadController.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/parallel/TestSuiteNamespaceManager.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/storage/TestStorage.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/Constants.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/Environment.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaOauthClientsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaOauthClientsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaOauthClientsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/BridgeClientsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/BridgeClientsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/BridgeClientsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaTracingClientsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaTracingClientsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaTracingClientsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/BaseClientsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/BaseClientsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaClientsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaClientsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaClientsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaAdminClientsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaAdminClientsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaAdminClientsBuilder.java...
Constructing Javadoc information...
Standard Doclet version 11.0.11
Building tree for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/IsolatedSuite.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/IsolatedTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/MultiNodeClusterOnly.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/MultiNodeClusterOnlyCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/OpenShiftOnly.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/OpenShiftOnlyCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/ParallelNamespaceTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/ParallelSuite.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/ParallelTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/RequiredMinKubeApiVersion.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/RequiredMinKubeApiVersionCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/StatefulSetTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/StatefulSetTestCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/StrimziPodSetTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/StrimziPodSetTestCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/cli/KafkaCmdClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/AbstractKafkaClientProperties.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/AbstractKafkaClientProperties.KafkaClientPropertiesBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/ConsumerProperties.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/ConsumerProperties.ConsumerPropertiesBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/ProducerProperties.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/ProducerProperties.ProducerPropertiesBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/ClientArgument.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/ClientArgumentMap.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/ClientType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/InternalKafkaClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/InternalKafkaClient.Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/VerifiableClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/VerifiableClient.VerifiableClientBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/ConfigMapUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/DeploymentUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/JobUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/StatefulSetUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/StrimziPodSetUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/KafkaBridgeResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/KafkaClientsResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/KafkaConnectorResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/KafkaConnectResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/KafkaMirrorMaker2Resource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/KafkaMirrorMakerResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/KafkaRebalanceResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/KafkaResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/KafkaTopicResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/KafkaUserResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/StrimziPodSetResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/KafkaBridgeTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/KafkaClientsTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/KafkaConnectorTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/KafkaConnectTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/KafkaMirrorMaker2Templates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/KafkaMirrorMakerTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/KafkaRebalanceTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/KafkaTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/KafkaTopicTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/KafkaUserTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/draincleaner/DrainCleanerResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/draincleaner/SetupDrainCleaner.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/ClusterOperatorInstallType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/ClusterOperatorRBACType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/CustomResourceStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/DefaultNetworkPolicy.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/DeploymentTypes.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/OlmInstallationStrategy.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/externalClients/ExternalKafkaClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/externalClients/ExternalKafkaClient.Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/interfaces/IndicativeSentences.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/AdminClientOperations.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/BaseClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/BaseClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/BaseClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/BridgeClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/BridgeClientsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/BridgeClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/BridgeClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaAdminClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaAdminClientsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaAdminClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaAdminClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaClientsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaOauthClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaOauthClientsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaOauthClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaOauthClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaTracingClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaTracingClientsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaTracingClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaTracingClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/k8s/Events.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/AbstractKafkaClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/AbstractKafkaClient.Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/KafkaClientOperations.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/KafkaBridgeUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/KafkaConnectorUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/KafkaConnectUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/KafkaMirrorMaker2Utils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/KafkaMirrorMakerUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/KafkaRebalanceUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/KafkaTopicUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/KafkaUserUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/keycloak/KeycloakInstance.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/ClusterOperatorCustomResourceDefinition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/ClusterRoleBindingResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/ClusterRoleResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/ConfigMapResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/DeploymentResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/JobResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/NetworkPolicyResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/RoleBindingResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/RoleResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/SecretResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/ServiceAccountResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/ServiceResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/ValidatingWebhookConfigurationResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/kubernetes/ClusterRoleBindingTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/kubernetes/NetworkPolicyTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/kubernetes/ServiceTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/listeners/ExecutionListener.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/listeners/OrderTestSuites.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/logs/LogCollector.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/logs/TestExecutionWatcher.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/HasAllOfReasons.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/HasAnyOfReasons.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/HasNoneOfReasons.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/LogHasNoUnexpectedErrors.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/Matchers.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/metrics/MetricsCollector.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/metrics/MetricsCollector.Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/NamespaceUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/NodeUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/PersistentVolumeClaimUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/PodUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/SecretUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/ServiceUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/BundleResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/BundleResource.BundleResourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/SetupClusterOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/SetupClusterOperator.SetupClusterOperatorBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/parallel/SuiteThreadController.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/parallel/TestSuiteNamespaceManager.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/ComponentType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/ResourceCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/ResourceItem.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/ResourceManager.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/ResourceOperation.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/ResourceType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/ThrowableRunner.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/CertAndKeyFiles.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/SystemTestCertAndKey.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/SystemTestCertAndKeyBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/SystemTestCertManager.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/specific/HelmResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/specific/OlmResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/specific/SpecificResourceType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/specific/ScraperTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/BridgeUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/CruiseControlUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/CruiseControlUtils.SupportedHttpMethods.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/CruiseControlUtils.SupportedSchemes.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/JmxUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/KeycloakUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/OlmUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/TracingUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/storage/TestStorage.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/BeforeAllOnce.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/Constants.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/Environment.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/ClientUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/FileUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/HttpUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/RollingUpdateUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/StUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/TestKafkaVersion.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/cli/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/cli/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/interfaces/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/interfaces/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/k8s/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/k8s/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/externalClients/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/externalClients/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/keycloak/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/keycloak/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/listeners/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/listeners/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/logs/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/logs/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/metrics/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/metrics/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/parallel/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/parallel/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/draincleaner/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/draincleaner/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/specific/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/specific/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/storage/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/storage/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/kubernetes/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/kubernetes/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/specific/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/specific/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/constant-values.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/MultiNodeClusterOnly.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/MultiNodeClusterOnlyCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/OpenShiftOnly.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/RequiredMinKubeApiVersion.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/RequiredMinKubeApiVersionCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/IsolatedTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/ParallelNamespaceTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/IsolatedSuite.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/OpenShiftOnlyCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/ParallelSuite.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/ParallelTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/StatefulSetTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/StatefulSetTestCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/StrimziPodSetTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/StrimziPodSetTestCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/cli/class-use/KafkaCmdClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/class-use/DefaultNetworkPolicy.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/class-use/ClusterOperatorInstallType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/class-use/CustomResourceStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/class-use/OlmInstallationStrategy.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/class-use/ClusterOperatorRBACType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/class-use/DeploymentTypes.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/k8s/class-use/Events.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/class-use/ProducerProperties.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/class-use/ProducerProperties.ProducerPropertiesBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/class-use/AbstractKafkaClientProperties.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/class-use/AbstractKafkaClientProperties.KafkaClientPropertiesBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/class-use/ConsumerProperties.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/class-use/ConsumerProperties.ConsumerPropertiesBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/class-use/AbstractKafkaClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/class-use/AbstractKafkaClient.Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/class-use/KafkaClientOperations.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/class-use/ClientArgument.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/class-use/ClientArgumentMap.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/class-use/ClientType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/class-use/InternalKafkaClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/class-use/InternalKafkaClient.Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/class-use/VerifiableClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/class-use/VerifiableClient.VerifiableClientBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/externalClients/class-use/ExternalKafkaClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/externalClients/class-use/ExternalKafkaClient.Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/AdminClientOperations.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/BaseClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/BridgeClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaAdminClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaOauthClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaTracingClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/keycloak/class-use/KeycloakInstance.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/listeners/class-use/ExecutionListener.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/listeners/class-use/OrderTestSuites.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/logs/class-use/LogCollector.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/logs/class-use/TestExecutionWatcher.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/class-use/HasAllOfReasons.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/class-use/HasAnyOfReasons.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/class-use/HasNoneOfReasons.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/class-use/Matchers.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/class-use/LogHasNoUnexpectedErrors.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/KafkaBridgeResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/KafkaConnectResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/KafkaConnectorResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/KafkaMirrorMaker2Resource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/KafkaTopicResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/KafkaClientsResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/KafkaMirrorMakerResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/KafkaRebalanceResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/KafkaResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/KafkaUserResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/StrimziPodSetResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/class-use/BundleResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/class-use/BundleResource.BundleResourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/specific/class-use/HelmResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/specific/class-use/SpecificResourceType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/specific/class-use/OlmResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/class-use/SetupClusterOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/class-use/SetupClusterOperator.SetupClusterOperatorBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/class-use/ResourceCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/class-use/ResourceItem.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/class-use/ResourceType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/class-use/ThrowableRunner.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/ServiceResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/ClusterOperatorCustomResourceDefinition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/ClusterRoleBindingResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/ClusterRoleResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/ConfigMapResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/JobResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/NetworkPolicyResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/RoleBindingResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/RoleResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/SecretResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/ServiceAccountResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/ValidatingWebhookConfigurationResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/DeploymentResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/class-use/ComponentType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/class-use/ResourceManager.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/draincleaner/class-use/DrainCleanerResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/draincleaner/class-use/SetupDrainCleaner.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/class-use/ResourceOperation.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/class-use/CertAndKeyFiles.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/class-use/SystemTestCertAndKey.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/class-use/SystemTestCertAndKeyBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/class-use/SystemTestCertManager.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/class-use/KafkaBridgeUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/class-use/KafkaConnectUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/class-use/KafkaConnectorUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/class-use/KafkaMirrorMaker2Utils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/class-use/KafkaMirrorMakerUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/class-use/KafkaRebalanceUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/class-use/KafkaTopicUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/class-use/KafkaUserUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/class-use/KafkaUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/class-use/ConfigMapUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/class-use/DeploymentUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/class-use/JobUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/class-use/StatefulSetUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/class-use/StrimziPodSetUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/class-use/NamespaceUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/class-use/NodeUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/class-use/PersistentVolumeClaimUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/class-use/PodUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/class-use/SecretUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/class-use/ServiceUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/class-use/BridgeUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/class-use/CruiseControlUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/class-use/CruiseControlUtils.SupportedSchemes.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/class-use/CruiseControlUtils.SupportedHttpMethods.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/class-use/JmxUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/class-use/OlmUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/class-use/TracingUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/class-use/KeycloakUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/class-use/ClientUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/class-use/FileUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/class-use/HttpUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/class-use/RollingUpdateUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/class-use/StUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/class-use/TestKafkaVersion.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/interfaces/class-use/IndicativeSentences.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/class-use/KafkaBridgeTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/class-use/KafkaClientsTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/class-use/KafkaConnectTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/class-use/KafkaConnectorTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/class-use/KafkaMirrorMaker2Templates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/class-use/KafkaMirrorMakerTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/class-use/KafkaRebalanceTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/class-use/KafkaTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/class-use/KafkaTopicTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/class-use/KafkaUserTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/kubernetes/class-use/ServiceTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/kubernetes/class-use/ClusterRoleBindingTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/kubernetes/class-use/NetworkPolicyTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/specific/class-use/ScraperTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/class-use/BeforeAllOnce.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/metrics/class-use/MetricsCollector.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/metrics/class-use/MetricsCollector.Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/parallel/class-use/SuiteThreadController.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/parallel/class-use/TestSuiteNamespaceManager.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/storage/class-use/TestStorage.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/class-use/Constants.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/class-use/Environment.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaOauthClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaOauthClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaOauthClientsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/BridgeClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/BridgeClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/BridgeClientsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaTracingClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaTracingClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaTracingClientsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/BaseClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/BaseClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaClientsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaAdminClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaAdminClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaAdminClientsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/cli/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/interfaces/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/k8s/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/externalClients/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/keycloak/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/listeners/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/logs/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/metrics/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/parallel/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/draincleaner/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/specific/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/storage/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/kubernetes/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/specific/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/package-use.html...
Building index for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/overview-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/index-all.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/allclasses-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/allpackages-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/deprecated-list.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/index.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/overview-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/help-doc.html...
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/systemtest/target/systemtest-0.29.0-SNAPSHOT-javadoc.jar
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:integration-test (default) @ systemtest ---
[WARNING] useSystemClassLoader setting has no effect when not forking
[WARNING] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
2022-03-28 10:47:05 [main] INFO  [TestExecutionListener:29] =======================================================================
2022-03-28 10:47:05 [main] INFO  [TestExecutionListener:30] =======================================================================
2022-03-28 10:47:05 [main] INFO  [TestExecutionListener:31]                         Test run started
2022-03-28 10:47:05 [main] INFO  [TestExecutionListener:32] =======================================================================
2022-03-28 10:47:05 [main] INFO  [TestExecutionListener:33] =======================================================================
2022-03-28 10:47:05 [main] INFO  [TestExecutionListener:48] Following testclasses are selected for run:
2022-03-28 10:47:05 [main] INFO  [TestExecutionListener:51] -> io.strimzi.systemtest.bridge.HttpBridgeScramShaST
2022-03-28 10:47:05 [main] INFO  [TestExecutionListener:51] -> io.strimzi.systemtest.bridge.HttpBridgeTlsST
2022-03-28 10:47:05 [main] INFO  [TestExecutionListener:51] -> io.strimzi.systemtest.operators.topic.ThrottlingQuotaST
2022-03-28 10:47:05 [main] INFO  [TestExecutionListener:51] -> io.strimzi.systemtest.operators.topic.TopicST
2022-03-28 10:47:05 [main] INFO  [TestExecutionListener:51] -> io.strimzi.systemtest.operators.user.UserST
2022-03-28 10:47:05 [main] INFO  [TestExecutionListener:51] -> io.strimzi.systemtest.operators.ReconciliationST
2022-03-28 10:47:05 [main] INFO  [TestExecutionListener:51] -> io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST
2022-03-28 10:47:05 [main] INFO  [TestExecutionListener:52] =======================================================================
2022-03-28 10:47:05 [main] INFO  [TestExecutionListener:53] =======================================================================
[INFO] Running io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST
[INFO] Running io.strimzi.systemtest.bridge.HttpBridgeScramShaST
[INFO] Running io.strimzi.systemtest.operators.topic.ThrottlingQuotaST
[INFO] Running io.strimzi.systemtest.operators.topic.TopicST
[INFO] Running io.strimzi.systemtest.operators.user.UserST
[INFO] Running io.strimzi.systemtest.bridge.HttpBridgeTlsST
2022-03-28 10:47:05 [ForkJoinPool-1-worker-9] DEBUG [Environment:271] Json configuration is not provided or cannot be processed!
2022-03-28 10:47:06 [ForkJoinPool-1-worker-9] INFO  [Environment:219] Used environment variables:
2022-03-28 10:47:06 [ForkJoinPool-1-worker-9] INFO  [Environment:220] CONFIG: /home/cloud-user/strimzi-kafka-operator/systemtest/config.json
2022-03-28 10:47:06 [ForkJoinPool-1-worker-9] INFO  [Environment:221] STRIMZI_RBAC_SCOPE: CLUSTER
2022-03-28 10:47:06 [ForkJoinPool-1-worker-9] INFO  [Environment:221] OLM_APP_BUNDLE_PREFIX: strimzi-cluster-operator
2022-03-28 10:47:06 [ForkJoinPool-1-worker-9] INFO  [Environment:221] TEST_CLIENTS_VERSION: 0.2.0
2022-03-28 10:47:06 [ForkJoinPool-1-worker-9] INFO  [Environment:221] OLM_SOURCE_NAMESPACE: openshift-marketplace
2022-03-28 10:47:06 [ForkJoinPool-1-worker-9] INFO  [Environment:221] CLUSTER_OPERATOR_INSTALL_TYPE: BUNDLE
2022-03-28 10:47:06 [ForkJoinPool-1-worker-9] INFO  [Environment:221] STRIMZI_COMPONENTS_LOG_LEVEL: INFO
2022-03-28 10:47:06 [ForkJoinPool-1-worker-9] INFO  [Environment:221] SKIP_TEARDOWN: false
2022-03-28 10:47:06 [ForkJoinPool-1-worker-9] INFO  [Environment:221] LB_FINALIZERS: false
2022-03-28 10:47:06 [ForkJoinPool-1-worker-9] INFO  [Environment:221] OLM_OPERATOR_DEPLOYMENT_NAME: strimzi-cluster-operator
2022-03-28 10:47:06 [ForkJoinPool-1-worker-9] INFO  [Environment:221] DOCKER_ORG: strimzi
2022-03-28 10:47:06 [ForkJoinPool-1-worker-9] INFO  [Environment:221] TEST_LOG_DIR: /home/cloud-user/strimzi-kafka-operator/systemtest/../systemtest/target/logs/
2022-03-28 10:47:06 [ForkJoinPool-1-worker-9] INFO  [Environment:221] COMPONENTS_IMAGE_PULL_POLICY: IfNotPresent
2022-03-28 10:47:06 [ForkJoinPool-1-worker-9] INFO  [Environment:221] DOCKER_REGISTRY: quay.io
2022-03-28 10:47:06 [ForkJoinPool-1-worker-9] INFO  [Environment:221] TEST_CLIENT_IMAGE: quay.io/strimzi/test-client:latest-kafka-3.1.0
2022-03-28 10:47:06 [ForkJoinPool-1-worker-9] INFO  [Environment:221] SYSTEM_TEST_STRIMZI_IMAGE_PULL_SECRET: 
2022-03-28 10:47:06 [ForkJoinPool-1-worker-9] INFO  [Environment:221] TEST_ADMIN_IMAGE: quay.io/strimzi-test-clients/test-client-kafka-admin:0.2.0-kafka-3.1.0
2022-03-28 10:47:06 [ForkJoinPool-1-worker-9] INFO  [Environment:221] TEST_HTTP_PRODUCER_IMAGE: quay.io/strimzi-test-clients/test-client-http-producer:0.2.0
2022-03-28 10:47:06 [ForkJoinPool-1-worker-9] INFO  [Environment:221] OLM_OPERATOR_NAME: strimzi-kafka-operator
2022-03-28 10:47:06 [ForkJoinPool-1-worker-9] INFO  [Environment:221] DOCKER_TAG: latest
2022-03-28 10:47:06 [ForkJoinPool-1-worker-9] INFO  [Environment:221] OLM_SOURCE_NAME: community-operators
2022-03-28 10:47:06 [ForkJoinPool-1-worker-9] INFO  [Environment:221] STRIMZI_FEATURE_GATES: 
2022-03-28 10:47:06 [ForkJoinPool-1-worker-9] INFO  [Environment:221] CLIENTS_KAFKA_VERSION: 3.1.0
2022-03-28 10:47:06 [ForkJoinPool-1-worker-9] INFO  [Environment:221] TEST_HTTP_CONSUMER_IMAGE: quay.io/strimzi-test-clients/test-client-http-consumer:0.2.0
2022-03-28 10:47:06 [ForkJoinPool-1-worker-9] INFO  [Environment:221] STRIMZI_LOG_LEVEL: DEBUG
2022-03-28 10:47:06 [ForkJoinPool-1-worker-9] INFO  [Environment:221] ST_KAFKA_VERSION: 3.1.0
2022-03-28 10:47:06 [ForkJoinPool-1-worker-9] INFO  [Environment:221] OPERATOR_IMAGE_PULL_POLICY: Always
2022-03-28 10:47:06 [ForkJoinPool-1-worker-9] INFO  [Environment:221] DEFAULT_TO_DENY_NETWORK_POLICIES: true
2022-03-28 10:47:06 [ForkJoinPool-1-worker-9] INFO  [Environment:221] TEST_PRODUCER_IMAGE: quay.io/strimzi-test-clients/test-client-kafka-producer:0.2.0-kafka-3.1.0
2022-03-28 10:47:06 [ForkJoinPool-1-worker-9] INFO  [Environment:221] BRIDGE_IMAGE: latest-released
2022-03-28 10:47:06 [ForkJoinPool-1-worker-9] INFO  [Environment:221] TEST_STREAMS_IMAGE: quay.io/strimzi-test-clients/test-client-kafka-streams:0.2.0-kafka-3.1.0
2022-03-28 10:47:06 [ForkJoinPool-1-worker-9] INFO  [Environment:221] TEST_CONSUMER_IMAGE: quay.io/strimzi-test-clients/test-client-kafka-consumer:0.2.0-kafka-3.1.0
2022-03-28 10:47:06 [ForkJoinPool-1-worker-9] INFO  [Environment:221] OLM_OPERATOR_VERSION: 
2022-03-28 10:47:06 [ForkJoinPool-1-worker-11] DEBUG [BeforeAllOnce:51] ============================================================================
2022-03-28 10:47:06 [ForkJoinPool-1-worker-11] DEBUG [BeforeAllOnce:52] [io.strimzi.systemtest.operators.topic.TopicST - Before Suite] - Setup Suite environment
2022-03-28 10:47:06 [ForkJoinPool-1-worker-11] DEBUG [Config:540] Trying to configure client from Kubernetes config...
2022-03-28 10:47:06 [ForkJoinPool-1-worker-11] DEBUG [Config:549] Found for Kubernetes config at: [/home/cloud-user/.kube/config].
2022-03-28 10:47:06 [ForkJoinPool-1-worker-11] DEBUG [Config:540] Trying to configure client from Kubernetes config...
2022-03-28 10:47:06 [ForkJoinPool-1-worker-11] DEBUG [Config:549] Found for Kubernetes config at: [/home/cloud-user/.kube/config].
2022-03-28 10:47:06 [ForkJoinPool-1-worker-11] DEBUG [KubeCluster:80] Cluster minikube is not installed!
2022-03-28 10:47:06 [ForkJoinPool-1-worker-11] DEBUG [KubeCluster:71] Cluster kubectl is installed
2022-03-28 10:47:06 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - kubectl cluster-info
2022-03-28 10:47:11 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: kubectl cluster-info
2022-03-28 10:47:11 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 10:47:11 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - kubectl api-resources
2022-03-28 10:47:12 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: kubectl api-resources
2022-03-28 10:47:12 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 10:47:12 [ForkJoinPool-1-worker-11] DEBUG [KubeCluster:77] Cluster kubectl is not running!
2022-03-28 10:47:12 [ForkJoinPool-1-worker-11] DEBUG [KubeCluster:71] Cluster oc is installed
2022-03-28 10:47:12 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc status -n default
[INFO] Running io.strimzi.systemtest.operators.ReconciliationST
2022-03-28 10:47:13 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc status -n default
2022-03-28 10:47:13 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 10:47:13 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc api-resources
2022-03-28 10:47:14 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc api-resources
2022-03-28 10:47:14 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 10:47:14 [ForkJoinPool-1-worker-11] DEBUG [KubeCluster:73] Cluster oc is running
2022-03-28 10:47:14 [ForkJoinPool-1-worker-11] INFO  [KubeCluster:87] Using cluster: oc
2022-03-28 10:47:14 [ForkJoinPool-1-worker-11] INFO  [KubeClusterResource:60] Cluster default namespace is 'default'
2022-03-28 10:47:14 [ForkJoinPool-1-worker-11] INFO  [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@4f354689
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-03-28 10:47:14 [ForkJoinPool-1-worker-11] DEBUG [SetupClusterOperator:198] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@57380991, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@4f354689, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='*', bindingsNamespaces=[infra-namespace], operationTimeout=300000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-03-28 10:47:14 [ForkJoinPool-1-worker-11] INFO  [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-03-28 10:47:14 [ForkJoinPool-1-worker-11] WARN  [KubeClusterResource:151] Namespace infra-namespace is already created, going to delete it
2022-03-28 10:47:14 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Namespace infra-namespace removal
2022-03-28 10:47:14 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace default get Namespace infra-namespace -o yaml
2022-03-28 10:47:15 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace default get Namespace infra-namespace -o yaml
2022-03-28 10:47:15 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 10:47:15 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace infra-namespace removal not ready, will try again in 1000 ms (479741ms till timeout)
2022-03-28 10:47:16 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace default get Namespace infra-namespace -o yaml
2022-03-28 10:47:16 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace default get Namespace infra-namespace -o yaml
2022-03-28 10:47:16 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 10:47:16 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace infra-namespace removal not ready, will try again in 1000 ms (478542ms till timeout)
2022-03-28 10:47:17 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace default get Namespace infra-namespace -o yaml
2022-03-28 10:47:17 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace default get Namespace infra-namespace -o yaml
2022-03-28 10:47:17 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 10:47:17 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace infra-namespace removal not ready, will try again in 1000 ms (477315ms till timeout)
2022-03-28 10:47:18 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace default get Namespace infra-namespace -o yaml
2022-03-28 10:47:18 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace default get Namespace infra-namespace -o yaml
2022-03-28 10:47:18 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 10:47:18 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace infra-namespace removal not ready, will try again in 1000 ms (476063ms till timeout)
2022-03-28 10:47:19 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace default get Namespace infra-namespace -o yaml
2022-03-28 10:47:20 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace default get Namespace infra-namespace -o yaml
2022-03-28 10:47:20 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 10:47:20 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace infra-namespace removal not ready, will try again in 1000 ms (474855ms till timeout)
2022-03-28 10:47:21 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace default get Namespace infra-namespace -o yaml
2022-03-28 10:47:21 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace default get Namespace infra-namespace -o yaml
2022-03-28 10:47:21 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 10:47:21 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace infra-namespace removal not ready, will try again in 1000 ms (473630ms till timeout)
2022-03-28 10:47:22 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace default get Namespace infra-namespace -o yaml
2022-03-28 10:47:22 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace default get Namespace infra-namespace -o yaml
2022-03-28 10:47:22 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 10:47:22 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace infra-namespace removal not ready, will try again in 1000 ms (472443ms till timeout)
2022-03-28 10:47:23 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace default get Namespace infra-namespace -o yaml
2022-03-28 10:47:23 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace default get Namespace infra-namespace -o yaml
2022-03-28 10:47:23 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 10:47:23 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace infra-namespace removal not ready, will try again in 1000 ms (471133ms till timeout)
2022-03-28 10:47:24 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace default get Namespace infra-namespace -o yaml
2022-03-28 10:47:25 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace default get Namespace infra-namespace -o yaml
2022-03-28 10:47:25 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 10:47:25 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace infra-namespace removal not ready, will try again in 1000 ms (469633ms till timeout)
2022-03-28 10:47:26 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace default get Namespace infra-namespace -o yaml
2022-03-28 10:47:26 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace default get Namespace infra-namespace -o yaml
2022-03-28 10:47:26 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 10:47:26 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace infra-namespace removal not ready, will try again in 1000 ms (468383ms till timeout)
2022-03-28 10:47:27 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace default get Namespace infra-namespace -o yaml
2022-03-28 10:47:27 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace default get Namespace infra-namespace -o yaml
2022-03-28 10:47:27 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 10:47:27 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace infra-namespace removal not ready, will try again in 1000 ms (467052ms till timeout)
2022-03-28 10:47:28 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace default get Namespace infra-namespace -o yaml
2022-03-28 10:47:29 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace default get Namespace infra-namespace -o yaml
2022-03-28 10:47:29 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 10:47:29 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace infra-namespace removal not ready, will try again in 1000 ms (465839ms till timeout)
2022-03-28 10:47:30 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace default get Namespace infra-namespace -o yaml
2022-03-28 10:47:30 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace default get Namespace infra-namespace -o yaml
2022-03-28 10:47:30 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 10:47:30 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace infra-namespace removal not ready, will try again in 1000 ms (464643ms till timeout)
2022-03-28 10:47:31 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace default get Namespace infra-namespace -o yaml
2022-03-28 10:47:31 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Failed to exec command: oc --namespace default get Namespace infra-namespace -o yaml
2022-03-28 10:47:31 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 1
2022-03-28 10:47:31 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 10:47:31 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Error from server (NotFound): namespaces "infra-namespace" not found
2022-03-28 10:47:31 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] ======STDERR END======
2022-03-28 10:47:31 [ForkJoinPool-1-worker-11] INFO  [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-03-28 10:47:31 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Namespace infra-namespace
2022-03-28 10:47:31 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace default get Namespace infra-namespace -o json
2022-03-28 10:47:31 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace default get Namespace infra-namespace -o json
2022-03-28 10:47:31 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 10:47:31 [ForkJoinPool-1-worker-11] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c25,c5",
            "openshift.io/sa.scc.supplemental-groups": "1000610000/10000",
            "openshift.io/sa.scc.uid-range": "1000610000/10000"
        },
        "creationTimestamp": "2022-03-28T10:47:29Z",
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T10:46:59Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:status": {
                        "f:phase": {}
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T10:47:29Z"
            }
        ],
        "name": "infra-namespace",
        "resourceVersion": "1863892",
        "selfLink": "/api/v1/namespaces/infra-namespace",
        "uid": "7da94d2d-4c85-495c-bb19-127b3d507730"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 10:47:31 [ForkJoinPool-1-worker-11] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace]}
2022-03-28 10:47:31 [ForkJoinPool-1-worker-11] INFO  [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-28 10:47:31 [ForkJoinPool-1-worker-11] DEBUG [SetupClusterOperator:478] Installation resource type: ServiceAccount
2022-03-28 10:47:31 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-28 10:47:31 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-28 10:47:32 [ForkJoinPool-1-worker-11] DEBUG [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-28 10:47:32 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-28 10:47:32 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-28 10:47:32 [ForkJoinPool-1-worker-11] DEBUG [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-28 10:47:32 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-28 10:47:32 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-28 10:47:32 [ForkJoinPool-1-worker-11] DEBUG [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-28 10:47:32 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-28 10:47:32 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-28 10:47:32 [ForkJoinPool-1-worker-11] DEBUG [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-28 10:47:32 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-28 10:47:33 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-28 10:47:33 [ForkJoinPool-1-worker-11] DEBUG [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-28 10:47:33 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-28 10:47:33 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-28 10:47:33 [ForkJoinPool-1-worker-11] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 10:47:33 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-28 10:47:34 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-28 10:47:34 [ForkJoinPool-1-worker-11] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 10:47:34 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-28 10:47:35 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-28 10:47:35 [ForkJoinPool-1-worker-11] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 10:47:35 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-28 10:47:35 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-28 10:47:35 [ForkJoinPool-1-worker-11] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 10:47:35 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-28 10:47:36 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-28 10:47:36 [ForkJoinPool-1-worker-11] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 10:47:36 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-28 10:47:36 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-28 10:47:36 [ForkJoinPool-1-worker-11] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 10:47:36 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-28 10:47:36 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-28 10:47:36 [ForkJoinPool-1-worker-11] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 10:47:36 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-28 10:47:36 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-28 10:47:37 [ForkJoinPool-1-worker-11] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 10:47:37 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-28 10:47:37 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-28 10:47:37 [ForkJoinPool-1-worker-11] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 10:47:37 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-28 10:47:37 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-28 10:47:37 [ForkJoinPool-1-worker-11] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 10:47:37 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-28 10:47:37 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-28 10:47:37 [ForkJoinPool-1-worker-11] DEBUG [SetupClusterOperator:478] Installation resource type: ConfigMap
2022-03-28 10:47:37 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-28 10:47:38 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-28 10:47:38 [ForkJoinPool-1-worker-11] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 10:47:38 [ForkJoinPool-1-worker-11] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-03-28 10:47:38 [ForkJoinPool-1-worker-11] INFO  [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/cloud-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-28 10:47:38 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-28 10:47:38 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-28 10:47:38 [ForkJoinPool-1-worker-11] INFO  [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/cloud-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-28 10:47:38 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-28 10:47:38 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-28 10:47:38 [ForkJoinPool-1-worker-11] INFO  [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/cloud-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-28 10:47:38 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-28 10:47:38 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-28 10:47:38 [ForkJoinPool-1-worker-11] INFO  [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/cloud-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-28 10:47:38 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-28 10:47:38 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-28 10:47:38 [ForkJoinPool-1-worker-11] INFO  [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/cloud-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-28 10:47:38 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-28 10:47:39 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-28 10:47:39 [ForkJoinPool-1-worker-11] INFO  [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-28 10:47:39 [ForkJoinPool-1-worker-11] INFO  [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-28 10:47:39 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-28 10:47:39 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-28 10:47:39 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-28 10:47:39 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-28 10:47:39 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-28 10:47:40 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-28 10:47:40 [ForkJoinPool-1-worker-11] INFO  [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-28 10:47:40 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-28 10:47:40 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (479987ms till timeout)
2022-03-28 10:47:41 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (478945ms till timeout)
2022-03-28 10:47:42 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (477914ms till timeout)
2022-03-28 10:47:43 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (476857ms till timeout)
2022-03-28 10:47:44 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (475726ms till timeout)
2022-03-28 10:47:45 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (474673ms till timeout)
2022-03-28 10:47:46 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (473631ms till timeout)
2022-03-28 10:47:47 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (472588ms till timeout)
2022-03-28 10:47:48 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (471529ms till timeout)
2022-03-28 10:47:49 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (470503ms till timeout)
2022-03-28 10:47:50 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (469465ms till timeout)
2022-03-28 10:47:51 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (468438ms till timeout)
2022-03-28 10:47:52 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (467364ms till timeout)
2022-03-28 10:47:53 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (466305ms till timeout)
2022-03-28 10:47:55 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (465209ms till timeout)
2022-03-28 10:47:56 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (464164ms till timeout)
2022-03-28 10:47:57 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (463092ms till timeout)
2022-03-28 10:47:58 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (462041ms till timeout)
2022-03-28 10:47:59 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (461003ms till timeout)
2022-03-28 10:48:00 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (459978ms till timeout)
2022-03-28 10:48:01 [ForkJoinPool-1-worker-11] INFO  [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-28 10:48:01 [ForkJoinPool-1-worker-11] INFO  [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-03-28 10:48:01 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-28 10:48:01 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-869dcfb9f7-twrp8 not ready: strimzi-cluster-operator)
2022-03-28 10:48:01 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-869dcfb9f7-twrp8 are ready
2022-03-28 10:48:01 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599947ms till timeout)
2022-03-28 10:48:02 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-869dcfb9f7-twrp8 not ready: strimzi-cluster-operator)
2022-03-28 10:48:02 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-869dcfb9f7-twrp8 are ready
2022-03-28 10:48:02 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598900ms till timeout)
2022-03-28 10:48:03 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-869dcfb9f7-twrp8 not ready: strimzi-cluster-operator)
2022-03-28 10:48:03 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-869dcfb9f7-twrp8 are ready
2022-03-28 10:48:03 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597839ms till timeout)
2022-03-28 10:48:04 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-869dcfb9f7-twrp8 not ready: strimzi-cluster-operator)
2022-03-28 10:48:04 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-869dcfb9f7-twrp8 are ready
2022-03-28 10:48:04 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596800ms till timeout)
2022-03-28 10:48:05 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-869dcfb9f7-twrp8 not ready: strimzi-cluster-operator)
2022-03-28 10:48:05 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-869dcfb9f7-twrp8 are ready
2022-03-28 10:48:05 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595736ms till timeout)
2022-03-28 10:48:06 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-869dcfb9f7-twrp8 not ready: strimzi-cluster-operator)
2022-03-28 10:48:06 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-869dcfb9f7-twrp8 are ready
2022-03-28 10:48:06 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594717ms till timeout)
2022-03-28 10:48:07 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-869dcfb9f7-twrp8 not ready: strimzi-cluster-operator)
2022-03-28 10:48:07 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-869dcfb9f7-twrp8 are ready
2022-03-28 10:48:07 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593676ms till timeout)
2022-03-28 10:48:08 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-869dcfb9f7-twrp8 not ready: strimzi-cluster-operator)
2022-03-28 10:48:08 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-869dcfb9f7-twrp8 are ready
2022-03-28 10:48:08 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592650ms till timeout)
2022-03-28 10:48:09 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-869dcfb9f7-twrp8 not ready: strimzi-cluster-operator)
2022-03-28 10:48:09 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-869dcfb9f7-twrp8 are ready
2022-03-28 10:48:09 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591595ms till timeout)
2022-03-28 10:48:10 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-869dcfb9f7-twrp8 not ready: strimzi-cluster-operator)
2022-03-28 10:48:10 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-869dcfb9f7-twrp8 are ready
2022-03-28 10:48:10 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590557ms till timeout)
2022-03-28 10:48:11 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-869dcfb9f7-twrp8 not ready: strimzi-cluster-operator)
2022-03-28 10:48:11 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-869dcfb9f7-twrp8 are ready
2022-03-28 10:48:11 [ForkJoinPool-1-worker-11] INFO  [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-03-28 10:48:11 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:666] ============================================================================
2022-03-28 10:48:11 [ForkJoinPool-1-worker-11] DEBUG [AbstractST:666] ============================================================================
2022-03-28 10:48:11 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:666] ============================================================================
2022-03-28 10:48:11 [ForkJoinPool-1-worker-11] DEBUG [AbstractST:667] [operators.topic.TopicST - Before All] - Setup test suite environment
2022-03-28 10:48:11 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:666] ============================================================================
2022-03-28 10:48:11 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:666] ============================================================================
2022-03-28 10:48:11 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:666] ============================================================================
2022-03-28 10:48:11 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:667] [operators.user.UserST - Before All] - Setup test suite environment
2022-03-28 10:48:11 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:667] [bridge.HttpBridgeTlsST - Before All] - Setup test suite environment
2022-03-28 10:48:11 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:667] [operators.topic.ThrottlingQuotaST - Before All] - Setup test suite environment
2022-03-28 10:48:11 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:69] [operators.user.UserST] - Adding parallel suite: UserST
2022-03-28 10:48:11 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:69] [bridge.HttpBridgeTlsST] - Adding parallel suite: HttpBridgeTlsST
2022-03-28 10:48:11 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:73] [operators.user.UserST] - Parallel suites count: 1
2022-03-28 10:48:11 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:73] [bridge.HttpBridgeTlsST] - Parallel suites count: 2
2022-03-28 10:48:11 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:184] UserST suite now can proceed its execution
2022-03-28 10:48:11 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:184] HttpBridgeTlsST suite now can proceed its execution
2022-03-28 10:48:11 [ForkJoinPool-1-worker-15] DEBUG [AbstractST:666] ============================================================================
2022-03-28 10:48:11 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:667] [cruisecontrol.CruiseControlConfigurationST - Before All] - Setup test suite environment
2022-03-28 10:48:11 [ForkJoinPool-1-worker-7] DEBUG [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-28 10:48:11 [ForkJoinPool-1-worker-11] DEBUG [SuiteThreadController:69] [operators.topic.TopicST] - Adding parallel suite: TopicST
2022-03-28 10:48:11 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:69] [operators.topic.ThrottlingQuotaST] - Adding parallel suite: ThrottlingQuotaST
2022-03-28 10:48:11 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:667] [bridge.HttpBridgeScramShaST - Before All] - Setup test suite environment
2022-03-28 10:48:11 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:69] [cruisecontrol.CruiseControlConfigurationST] - Adding parallel suite: CruiseControlConfigurationST
2022-03-28 10:48:11 [ForkJoinPool-1-worker-13] DEBUG [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-28 10:48:11 [ForkJoinPool-1-worker-13] DEBUG [TestSuiteNamespaceManager:129] Test suite `UserST` creates these additional namespaces:[user-st]
2022-03-28 10:48:11 [ForkJoinPool-1-worker-15] DEBUG [AbstractST:667] [operators.ReconciliationST - Before All] - Setup test suite environment
2022-03-28 10:48:11 [ForkJoinPool-1-worker-11] DEBUG [SuiteThreadController:73] [operators.topic.TopicST] - Parallel suites count: 3
2022-03-28 10:48:11 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:73] [cruisecontrol.CruiseControlConfigurationST] - Parallel suites count: 5
2022-03-28 10:48:11 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:69] [bridge.HttpBridgeScramShaST] - Adding parallel suite: HttpBridgeScramShaST
2022-03-28 10:48:11 [ForkJoinPool-1-worker-7] DEBUG [TestSuiteNamespaceManager:129] Test suite `HttpBridgeTlsST` creates these additional namespaces:[http-bridge-tls-st]
2022-03-28 10:48:11 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:184] CruiseControlConfigurationST suite now can proceed its execution
2022-03-28 10:48:11 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:73] [bridge.HttpBridgeScramShaST] - Parallel suites count: 6
2022-03-28 10:48:11 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:73] [operators.topic.ThrottlingQuotaST] - Parallel suites count: 4
2022-03-28 10:48:11 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:69] [operators.ReconciliationST] - Adding parallel suite: ReconciliationST
2022-03-28 10:48:11 [ForkJoinPool-1-worker-3] DEBUG [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-28 10:48:11 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:73] [operators.ReconciliationST] - Parallel suites count: 7
2022-03-28 10:48:11 [ForkJoinPool-1-worker-11] DEBUG [SuiteThreadController:184] TopicST suite now can proceed its execution
2022-03-28 10:48:11 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:184] ThrottlingQuotaST suite now can proceed its execution
2022-03-28 10:48:11 [ForkJoinPool-1-worker-3] DEBUG [TestSuiteNamespaceManager:129] Test suite `CruiseControlConfigurationST` creates these additional namespaces:[cruise-control-configuration-st]
2022-03-28 10:48:11 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:184] HttpBridgeScramShaST suite now can proceed its execution
2022-03-28 10:48:11 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:159] [ReconciliationST] moved to the WaitZone, because current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:48:11 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:48:11 [ForkJoinPool-1-worker-9] DEBUG [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-28 10:48:11 [ForkJoinPool-1-worker-11] DEBUG [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-28 10:48:11 [ForkJoinPool-1-worker-5] DEBUG [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-28 10:48:11 [ForkJoinPool-1-worker-9] DEBUG [TestSuiteNamespaceManager:129] Test suite `ThrottlingQuotaST` creates these additional namespaces:[throttling-quota-st]
2022-03-28 10:48:11 [ForkJoinPool-1-worker-5] DEBUG [TestSuiteNamespaceManager:129] Test suite `HttpBridgeScramShaST` creates these additional namespaces:[http-bridge-scram-sha-st]
2022-03-28 10:48:11 [ForkJoinPool-1-worker-11] DEBUG [TestSuiteNamespaceManager:129] Test suite `TopicST` creates these additional namespaces:[topic-st]
2022-03-28 10:48:11 [ForkJoinPool-1-worker-13] INFO  [KubeClusterResource:156] Creating Namespace: user-st
2022-03-28 10:48:11 [ForkJoinPool-1-worker-3] INFO  [KubeClusterResource:156] Creating Namespace: cruise-control-configuration-st
2022-03-28 10:48:11 [ForkJoinPool-1-worker-7] INFO  [KubeClusterResource:156] Creating Namespace: http-bridge-tls-st
2022-03-28 10:48:11 [ForkJoinPool-1-worker-9] INFO  [KubeClusterResource:156] Creating Namespace: throttling-quota-st
2022-03-28 10:48:11 [ForkJoinPool-1-worker-5] INFO  [KubeClusterResource:156] Creating Namespace: http-bridge-scram-sha-st
2022-03-28 10:48:11 [ForkJoinPool-1-worker-11] INFO  [KubeClusterResource:156] Creating Namespace: topic-st
2022-03-28 10:48:11 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Namespace user-st
2022-03-28 10:48:11 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace infra-namespace get Namespace user-st -o json
2022-03-28 10:48:11 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Namespace http-bridge-tls-st
2022-03-28 10:48:11 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace infra-namespace get Namespace http-bridge-tls-st -o json
2022-03-28 10:48:11 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Namespace http-bridge-scram-sha-st
2022-03-28 10:48:11 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace infra-namespace get Namespace http-bridge-scram-sha-st -o json
2022-03-28 10:48:11 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Namespace throttling-quota-st
2022-03-28 10:48:11 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace infra-namespace get Namespace throttling-quota-st -o json
2022-03-28 10:48:11 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Namespace cruise-control-configuration-st
2022-03-28 10:48:11 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Namespace topic-st
2022-03-28 10:48:11 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace infra-namespace get Namespace cruise-control-configuration-st -o json
2022-03-28 10:48:11 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace infra-namespace get Namespace topic-st -o json
2022-03-28 10:48:12 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace infra-namespace get Namespace http-bridge-tls-st -o json
2022-03-28 10:48:12 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 10:48:12 [ForkJoinPool-1-worker-7] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c25,c15",
            "openshift.io/sa.scc.supplemental-groups": "1000630000/10000",
            "openshift.io/sa.scc.uid-range": "1000630000/10000"
        },
        "creationTimestamp": "2022-03-28T10:48:09Z",
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T10:47:39Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:status": {
                        "f:phase": {}
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T10:48:09Z"
            }
        ],
        "name": "http-bridge-tls-st",
        "resourceVersion": "1864169",
        "selfLink": "/api/v1/namespaces/http-bridge-tls-st",
        "uid": "549dd1e0-e128-453f-a7db-9eb3b2f972f1"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 10:48:12 [ForkJoinPool-1-worker-7] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st]}
2022-03-28 10:48:12 [ForkJoinPool-1-worker-7] INFO  [KubeClusterResource:82] Client use Namespace: http-bridge-tls-st
2022-03-28 10:48:12 [ForkJoinPool-1-worker-7] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=http-bridge-tls-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 10:48:12 [ForkJoinPool-1-worker-7] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: http-bridge-tls-st
2022-03-28 10:48:12 [ForkJoinPool-1-worker-7] INFO  [HttpBridgeTlsST:129] Deploy Kafka and KafkaBridge before tests
2022-03-28 10:48:12 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace infra-namespace get Namespace cruise-control-configuration-st -o json
2022-03-28 10:48:12 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:48:12 [ForkJoinPool-1-worker-3] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c25,c20",
            "openshift.io/sa.scc.supplemental-groups": "1000640000/10000",
            "openshift.io/sa.scc.uid-range": "1000640000/10000"
        },
        "creationTimestamp": "2022-03-28T10:48:09Z",
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T10:47:39Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:status": {
                        "f:phase": {}
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T10:48:09Z"
            }
        ],
        "name": "cruise-control-configuration-st",
        "resourceVersion": "1864184",
        "selfLink": "/api/v1/namespaces/cruise-control-configuration-st",
        "uid": "b3df5114-0b35-4fc7-af8b-6c858ab835e4"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 10:48:12 [ForkJoinPool-1-worker-3] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st]}
2022-03-28 10:48:12 [ForkJoinPool-1-worker-3] INFO  [KubeClusterResource:82] Client use Namespace: cruise-control-configuration-st
2022-03-28 10:48:12 [ForkJoinPool-1-worker-3] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=cruise-control-configuration-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 10:48:12 [ForkJoinPool-1-worker-3] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: cruise-control-configuration-st
2022-03-28 10:48:12 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:23] ############################################################################
2022-03-28 10:48:12 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testCapacityFile-STARTED
2022-03-28 10:48:12 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:658] ============================================================================
2022-03-28 10:48:12 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:659] [cruisecontrol.CruiseControlConfigurationST - Before Each] - Setup test case environment
2022-03-28 10:48:12 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:77] [cruisecontrol.CruiseControlConfigurationST] - Adding parallel test: testCapacityFile
2022-03-28 10:48:12 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:81] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 1
2022-03-28 10:48:12 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:230] testCapacityFile test now can proceed its execution
2022-03-28 10:48:12 [ForkJoinPool-1-worker-3] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 10:48:12 [ForkJoinPool-1-worker-3] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testCapacityFile=my-cluster-fd0fb61a}
2022-03-28 10:48:12 [ForkJoinPool-1-worker-3] TRACE [AbstractST:607] USERS_NAME_MAP: {testCapacityFile=my-user-937015144-1959439396}
2022-03-28 10:48:12 [ForkJoinPool-1-worker-3] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testCapacityFile=my-topic-1155641705-902584520}
2022-03-28 10:48:12 [ForkJoinPool-1-worker-3] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testCapacityFile=my-cluster-fd0fb61a-kafka-clients}
2022-03-28 10:48:12 [ForkJoinPool-1-worker-3] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-0 for test case:testCapacityFile
2022-03-28 10:48:12 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace infra-namespace get Namespace topic-st -o json
2022-03-28 10:48:12 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 10:48:12 [ForkJoinPool-1-worker-11] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c26,c0",
            "openshift.io/sa.scc.supplemental-groups": "1000650000/10000",
            "openshift.io/sa.scc.uid-range": "1000650000/10000"
        },
        "creationTimestamp": "2022-03-28T10:48:09Z",
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T10:47:39Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:status": {
                        "f:phase": {}
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T10:48:09Z"
            }
        ],
        "name": "topic-st",
        "resourceVersion": "1864210",
        "selfLink": "/api/v1/namespaces/topic-st",
        "uid": "44950359-c95b-4cae-a937-d7bfd6da7dac"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 10:48:12 [ForkJoinPool-1-worker-11] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st]}
2022-03-28 10:48:12 [ForkJoinPool-1-worker-11] INFO  [KubeClusterResource:82] Client use Namespace: topic-st
2022-03-28 10:48:12 [ForkJoinPool-1-worker-11] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=topic-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 10:48:12 [ForkJoinPool-1-worker-11] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: topic-st
2022-03-28 10:48:12 [ForkJoinPool-1-worker-11] INFO  [TopicST:491] Deploying shared Kafka across all test cases in topic-st namespace
2022-03-28 10:48:12 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace infra-namespace get Namespace user-st -o json
2022-03-28 10:48:12 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 10:48:12 [ForkJoinPool-1-worker-13] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c25,c10",
            "openshift.io/sa.scc.supplemental-groups": "1000620000/10000",
            "openshift.io/sa.scc.uid-range": "1000620000/10000"
        },
        "creationTimestamp": "2022-03-28T10:48:09Z",
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T10:47:39Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:status": {
                        "f:phase": {}
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T10:48:09Z"
            }
        ],
        "name": "user-st",
        "resourceVersion": "1864154",
        "selfLink": "/api/v1/namespaces/user-st",
        "uid": "5075e961-4ebb-4370-877f-d396a6443444"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 10:48:12 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace infra-namespace get Namespace http-bridge-scram-sha-st -o json
2022-03-28 10:48:12 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 10:48:12 [ForkJoinPool-1-worker-13] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st]}
2022-03-28 10:48:12 [ForkJoinPool-1-worker-5] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c26,c10",
            "openshift.io/sa.scc.supplemental-groups": "1000670000/10000",
            "openshift.io/sa.scc.uid-range": "1000670000/10000"
        },
        "creationTimestamp": "2022-03-28T10:48:09Z",
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T10:47:39Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:status": {
                        "f:phase": {}
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T10:48:09Z"
            }
        ],
        "name": "http-bridge-scram-sha-st",
        "resourceVersion": "1864246",
        "selfLink": "/api/v1/namespaces/http-bridge-scram-sha-st",
        "uid": "597208a6-63f8-47a1-ba74-52b18245bb27"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 10:48:12 [ForkJoinPool-1-worker-13] INFO  [KubeClusterResource:82] Client use Namespace: user-st
2022-03-28 10:48:12 [ForkJoinPool-1-worker-5] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st]}
2022-03-28 10:48:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace infra-namespace get Namespace throttling-quota-st -o json
2022-03-28 10:48:12 [ForkJoinPool-1-worker-13] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=user-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 10:48:12 [ForkJoinPool-1-worker-5] INFO  [KubeClusterResource:82] Client use Namespace: http-bridge-scram-sha-st
2022-03-28 10:48:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:48:12 [ForkJoinPool-1-worker-13] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: user-st
2022-03-28 10:48:12 [ForkJoinPool-1-worker-9] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c26,c5",
            "openshift.io/sa.scc.supplemental-groups": "1000660000/10000",
            "openshift.io/sa.scc.uid-range": "1000660000/10000"
        },
        "creationTimestamp": "2022-03-28T10:48:09Z",
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T10:47:39Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:status": {
                        "f:phase": {}
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T10:48:09Z"
            }
        ],
        "name": "throttling-quota-st",
        "resourceVersion": "1864227",
        "selfLink": "/api/v1/namespaces/throttling-quota-st",
        "uid": "06f0a364-4108-484d-a027-a56ba40ad4a1"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 10:48:12 [ForkJoinPool-1-worker-5] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=http-bridge-scram-sha-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 10:48:12 [ForkJoinPool-1-worker-5] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: http-bridge-scram-sha-st
2022-03-28 10:48:12 [ForkJoinPool-1-worker-9] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st]}
2022-03-28 10:48:12 [ForkJoinPool-1-worker-9] INFO  [KubeClusterResource:82] Client use Namespace: throttling-quota-st
2022-03-28 10:48:12 [ForkJoinPool-1-worker-5] INFO  [HttpBridgeScramShaST:123] Deploy Kafka and KafkaBridge before tests
2022-03-28 10:48:12 [ForkJoinPool-1-worker-9] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=throttling-quota-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 10:48:12 [ForkJoinPool-1-worker-9] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: throttling-quota-st
2022-03-28 10:48:12 [ForkJoinPool-1-worker-9] INFO  [ThrottlingQuotaST:304] Deploying shared Kafka across all test cases in throttling-quota-st namespace
2022-03-28 10:48:12 [ForkJoinPool-1-worker-3] INFO  [KubeClusterResource:156] Creating Namespace: namespace-0
2022-03-28 10:48:12 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Namespace namespace-0
2022-03-28 10:48:12 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st get Namespace namespace-0 -o json
2022-03-28 10:48:12 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update Kafka user-cluster-name in namespace user-st
2022-03-28 10:48:12 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update Kafka topic-cluster-name in namespace topic-st
2022-03-28 10:48:12 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update Kafka http-bridge-tls-cluster-name in namespace http-bridge-tls-st
2022-03-28 10:48:12 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update Kafka http-bridge-scram-sha-cluster-name in namespace http-bridge-scram-sha-st
2022-03-28 10:48:12 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update Kafka quota-cluster in namespace throttling-quota-st
2022-03-28 10:48:12 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st get Namespace namespace-0 -o json
2022-03-28 10:48:12 [ForkJoinPool-1-worker-5] WARN  [VersionUsageUtils:60] The client is using resource type 'kafkas' with unstable version 'v1beta2'
2022-03-28 10:48:12 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:48:12 [ForkJoinPool-1-worker-3] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c26,c15",
            "openshift.io/sa.scc.supplemental-groups": "1000680000/10000",
            "openshift.io/sa.scc.uid-range": "1000680000/10000"
        },
        "creationTimestamp": "2022-03-28T10:48:09Z",
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T10:47:39Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:status": {
                        "f:phase": {}
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T10:48:09Z"
            }
        ],
        "name": "namespace-0",
        "resourceVersion": "1864299",
        "selfLink": "/api/v1/namespaces/namespace-0",
        "uid": "13a920f0-fd71-41fd-b01c-87d53779ad19"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 10:48:12 [ForkJoinPool-1-worker-3] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@3e255cd9=[namespace-0]}
2022-03-28 10:48:12 [ForkJoinPool-1-worker-3] INFO  [KubeClusterResource:82] Client use Namespace: namespace-0
2022-03-28 10:48:12 [ForkJoinPool-1-worker-3] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-0, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 10:48:12 [ForkJoinPool-1-worker-3] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-0
2022-03-28 10:48:12 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-fd0fb61a in namespace namespace-0
2022-03-28 10:48:12 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:164] Using Namespace: namespace-0
2022-03-28 10:48:12 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:quota-cluster
2022-03-28 10:48:12 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:user-cluster-name
2022-03-28 10:48:12 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:topic-cluster-name
2022-03-28 10:48:12 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:http-bridge-tls-cluster-name
2022-03-28 10:48:12 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:http-bridge-scram-sha-cluster-name
2022-03-28 10:48:12 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-fd0fb61a
2022-03-28 10:48:12 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:433] Wait for Kafka: quota-cluster will have desired state: Ready
2022-03-28 10:48:12 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:433] Wait for Kafka: user-cluster-name will have desired state: Ready
2022-03-28 10:48:12 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Kafka: quota-cluster will have desired state: Ready
2022-03-28 10:48:12 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Kafka: user-cluster-name will have desired state: Ready
2022-03-28 10:48:12 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:433] Wait for Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready
2022-03-28 10:48:12 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready
2022-03-28 10:48:12 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-fd0fb61a will have desired state: Ready
2022-03-28 10:48:12 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:433] Wait for Kafka: topic-cluster-name will have desired state: Ready
2022-03-28 10:48:12 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-fd0fb61a will have desired state: Ready
2022-03-28 10:48:12 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Kafka: topic-cluster-name will have desired state: Ready
2022-03-28 10:48:12 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:433] Wait for Kafka: http-bridge-tls-cluster-name will have desired state: Ready
2022-03-28 10:48:12 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Kafka: http-bridge-tls-cluster-name will have desired state: Ready
2022-03-28 10:48:13 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (839966ms till timeout)
2022-03-28 10:48:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (839966ms till timeout)
2022-03-28 10:48:13 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (839982ms till timeout)
2022-03-28 10:48:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (839950ms till timeout)
2022-03-28 10:48:13 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (839949ms till timeout)
2022-03-28 10:48:13 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1319947ms till timeout)
2022-03-28 10:48:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (838927ms till timeout)
2022-03-28 10:48:14 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (838860ms till timeout)
2022-03-28 10:48:14 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (838870ms till timeout)
2022-03-28 10:48:14 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (838840ms till timeout)
2022-03-28 10:48:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (838812ms till timeout)
2022-03-28 10:48:14 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1318812ms till timeout)
2022-03-28 10:48:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (837878ms till timeout)
2022-03-28 10:48:15 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (837776ms till timeout)
2022-03-28 10:48:15 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (837759ms till timeout)
2022-03-28 10:48:15 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (837763ms till timeout)
2022-03-28 10:48:15 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1317744ms till timeout)
2022-03-28 10:48:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (837718ms till timeout)
2022-03-28 10:48:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (836750ms till timeout)
2022-03-28 10:48:16 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (836692ms till timeout)
2022-03-28 10:48:16 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (836699ms till timeout)
2022-03-28 10:48:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (836668ms till timeout)
2022-03-28 10:48:16 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1316668ms till timeout)
2022-03-28 10:48:16 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (836652ms till timeout)
2022-03-28 10:48:16 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:48:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (835712ms till timeout)
2022-03-28 10:48:17 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (835627ms till timeout)
2022-03-28 10:48:17 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1315603ms till timeout)
2022-03-28 10:48:17 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (835595ms till timeout)
2022-03-28 10:48:17 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (835590ms till timeout)
2022-03-28 10:48:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (835531ms till timeout)
2022-03-28 10:48:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (834619ms till timeout)
2022-03-28 10:48:18 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (834578ms till timeout)
2022-03-28 10:48:18 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1314552ms till timeout)
2022-03-28 10:48:18 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (834509ms till timeout)
2022-03-28 10:48:18 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (834505ms till timeout)
2022-03-28 10:48:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (834456ms till timeout)
2022-03-28 10:48:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (833590ms till timeout)
2022-03-28 10:48:19 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (833497ms till timeout)
2022-03-28 10:48:19 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1313497ms till timeout)
2022-03-28 10:48:19 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (833458ms till timeout)
2022-03-28 10:48:19 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (833454ms till timeout)
2022-03-28 10:48:19 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (833387ms till timeout)
2022-03-28 10:48:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (832571ms till timeout)
2022-03-28 10:48:20 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (832468ms till timeout)
2022-03-28 10:48:20 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1312479ms till timeout)
2022-03-28 10:48:20 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (832437ms till timeout)
2022-03-28 10:48:20 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (832427ms till timeout)
2022-03-28 10:48:20 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (832372ms till timeout)
2022-03-28 10:48:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (831524ms till timeout)
2022-03-28 10:48:21 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (831432ms till timeout)
2022-03-28 10:48:21 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1311404ms till timeout)
2022-03-28 10:48:21 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (831375ms till timeout)
2022-03-28 10:48:21 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (831378ms till timeout)
2022-03-28 10:48:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (831321ms till timeout)
2022-03-28 10:48:21 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:48:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (830468ms till timeout)
2022-03-28 10:48:22 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (830411ms till timeout)
2022-03-28 10:48:22 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1310382ms till timeout)
2022-03-28 10:48:22 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (830354ms till timeout)
2022-03-28 10:48:22 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (830314ms till timeout)
2022-03-28 10:48:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (830246ms till timeout)
2022-03-28 10:48:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (829395ms till timeout)
2022-03-28 10:48:23 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (829335ms till timeout)
2022-03-28 10:48:23 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1309345ms till timeout)
2022-03-28 10:48:23 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (829317ms till timeout)
2022-03-28 10:48:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (829182ms till timeout)
2022-03-28 10:48:23 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (829146ms till timeout)
2022-03-28 10:48:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (828380ms till timeout)
2022-03-28 10:48:24 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (828300ms till timeout)
2022-03-28 10:48:24 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1308314ms till timeout)
2022-03-28 10:48:24 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (828297ms till timeout)
2022-03-28 10:48:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (828158ms till timeout)
2022-03-28 10:48:24 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (828131ms till timeout)
2022-03-28 10:48:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (827353ms till timeout)
2022-03-28 10:48:25 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (827242ms till timeout)
2022-03-28 10:48:25 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1307224ms till timeout)
2022-03-28 10:48:25 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (827220ms till timeout)
2022-03-28 10:48:25 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (827087ms till timeout)
2022-03-28 10:48:25 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (827004ms till timeout)
2022-03-28 10:48:26 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:48:26 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (826094ms till timeout)
2022-03-28 10:48:26 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1306101ms till timeout)
2022-03-28 10:48:26 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (826095ms till timeout)
2022-03-28 10:48:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (826076ms till timeout)
2022-03-28 10:48:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (826057ms till timeout)
2022-03-28 10:48:27 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (825940ms till timeout)
2022-03-28 10:48:27 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (825049ms till timeout)
2022-03-28 10:48:27 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1305071ms till timeout)
2022-03-28 10:48:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (825034ms till timeout)
2022-03-28 10:48:27 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (825051ms till timeout)
2022-03-28 10:48:27 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (825032ms till timeout)
2022-03-28 10:48:28 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (824881ms till timeout)
2022-03-28 10:48:28 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (824005ms till timeout)
2022-03-28 10:48:28 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1304027ms till timeout)
2022-03-28 10:48:28 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (824016ms till timeout)
2022-03-28 10:48:28 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (824020ms till timeout)
2022-03-28 10:48:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (823998ms till timeout)
2022-03-28 10:48:29 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (823844ms till timeout)
2022-03-28 10:48:30 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (822926ms till timeout)
2022-03-28 10:48:30 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (822790ms till timeout)
2022-03-28 10:48:30 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (822767ms till timeout)
2022-03-28 10:48:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (822747ms till timeout)
2022-03-28 10:48:30 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (822769ms till timeout)
2022-03-28 10:48:30 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1302769ms till timeout)
2022-03-28 10:48:31 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (821871ms till timeout)
2022-03-28 10:48:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (821636ms till timeout)
2022-03-28 10:48:31 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (821643ms till timeout)
2022-03-28 10:48:31 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1301647ms till timeout)
2022-03-28 10:48:31 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (821647ms till timeout)
2022-03-28 10:48:31 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (821636ms till timeout)
2022-03-28 10:48:31 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:48:32 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (820768ms till timeout)
2022-03-28 10:48:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (820448ms till timeout)
2022-03-28 10:48:32 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (820470ms till timeout)
2022-03-28 10:48:32 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (820465ms till timeout)
2022-03-28 10:48:32 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (820465ms till timeout)
2022-03-28 10:48:32 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1300465ms till timeout)
2022-03-28 10:48:33 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (819687ms till timeout)
2022-03-28 10:48:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (819417ms till timeout)
2022-03-28 10:48:33 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (819434ms till timeout)
2022-03-28 10:48:33 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (819439ms till timeout)
2022-03-28 10:48:33 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (819438ms till timeout)
2022-03-28 10:48:33 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1299425ms till timeout)
2022-03-28 10:48:34 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (818654ms till timeout)
2022-03-28 10:48:34 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (818412ms till timeout)
2022-03-28 10:48:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (818394ms till timeout)
2022-03-28 10:48:34 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (818415ms till timeout)
2022-03-28 10:48:34 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (818403ms till timeout)
2022-03-28 10:48:34 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1298403ms till timeout)
2022-03-28 10:48:35 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (817570ms till timeout)
2022-03-28 10:48:35 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (817305ms till timeout)
2022-03-28 10:48:35 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1297305ms till timeout)
2022-03-28 10:48:35 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (817279ms till timeout)
2022-03-28 10:48:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (817261ms till timeout)
2022-03-28 10:48:35 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (817283ms till timeout)
2022-03-28 10:48:36 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (816509ms till timeout)
2022-03-28 10:48:36 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (816260ms till timeout)
2022-03-28 10:48:36 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (816233ms till timeout)
2022-03-28 10:48:36 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1296233ms till timeout)
2022-03-28 10:48:36 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (816193ms till timeout)
2022-03-28 10:48:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (816155ms till timeout)
2022-03-28 10:48:36 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:48:37 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (815361ms till timeout)
2022-03-28 10:48:37 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (815133ms till timeout)
2022-03-28 10:48:37 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1295133ms till timeout)
2022-03-28 10:48:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (815094ms till timeout)
2022-03-28 10:48:37 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (815112ms till timeout)
2022-03-28 10:48:37 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (815115ms till timeout)
2022-03-28 10:48:38 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (814271ms till timeout)
2022-03-28 10:48:38 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (814067ms till timeout)
2022-03-28 10:48:38 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (814058ms till timeout)
2022-03-28 10:48:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (814040ms till timeout)
2022-03-28 10:48:38 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1294052ms till timeout)
2022-03-28 10:48:38 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (814052ms till timeout)
2022-03-28 10:48:39 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (813225ms till timeout)
2022-03-28 10:48:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (813039ms till timeout)
2022-03-28 10:48:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (812956ms till timeout)
2022-03-28 10:48:40 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (812963ms till timeout)
2022-03-28 10:48:40 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1292967ms till timeout)
2022-03-28 10:48:40 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (812954ms till timeout)
2022-03-28 10:48:40 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (812192ms till timeout)
2022-03-28 10:48:41 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (811969ms till timeout)
2022-03-28 10:48:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (811927ms till timeout)
2022-03-28 10:48:41 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1291917ms till timeout)
2022-03-28 10:48:41 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (811913ms till timeout)
2022-03-28 10:48:41 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (811900ms till timeout)
2022-03-28 10:48:41 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (811132ms till timeout)
2022-03-28 10:48:41 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:48:42 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (810942ms till timeout)
2022-03-28 10:48:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (810892ms till timeout)
2022-03-28 10:48:42 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1290879ms till timeout)
2022-03-28 10:48:42 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (810841ms till timeout)
2022-03-28 10:48:42 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (810837ms till timeout)
2022-03-28 10:48:42 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (810112ms till timeout)
2022-03-28 10:48:43 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (809861ms till timeout)
2022-03-28 10:48:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (809822ms till timeout)
2022-03-28 10:48:43 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1289826ms till timeout)
2022-03-28 10:48:43 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (809774ms till timeout)
2022-03-28 10:48:43 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (809760ms till timeout)
2022-03-28 10:48:43 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (808984ms till timeout)
2022-03-28 10:48:44 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (808788ms till timeout)
2022-03-28 10:48:44 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1288707ms till timeout)
2022-03-28 10:48:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (808684ms till timeout)
2022-03-28 10:48:44 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (808670ms till timeout)
2022-03-28 10:48:44 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (808625ms till timeout)
2022-03-28 10:48:45 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (807928ms till timeout)
2022-03-28 10:48:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (807746ms till timeout)
2022-03-28 10:48:45 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1287679ms till timeout)
2022-03-28 10:48:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (807640ms till timeout)
2022-03-28 10:48:45 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (807630ms till timeout)
2022-03-28 10:48:45 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (807587ms till timeout)
2022-03-28 10:48:46 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (806872ms till timeout)
2022-03-28 10:48:46 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (806696ms till timeout)
2022-03-28 10:48:46 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1286550ms till timeout)
2022-03-28 10:48:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (806528ms till timeout)
2022-03-28 10:48:46 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (806549ms till timeout)
2022-03-28 10:48:46 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (806509ms till timeout)
2022-03-28 10:48:46 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:48:47 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (805766ms till timeout)
2022-03-28 10:48:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (805631ms till timeout)
2022-03-28 10:48:47 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1285493ms till timeout)
2022-03-28 10:48:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (805471ms till timeout)
2022-03-28 10:48:47 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (805474ms till timeout)
2022-03-28 10:48:47 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (805458ms till timeout)
2022-03-28 10:48:48 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (804698ms till timeout)
2022-03-28 10:48:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (804608ms till timeout)
2022-03-28 10:48:48 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1284457ms till timeout)
2022-03-28 10:48:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (804424ms till timeout)
2022-03-28 10:48:48 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (804436ms till timeout)
2022-03-28 10:48:48 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (804432ms till timeout)
2022-03-28 10:48:49 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (803631ms till timeout)
2022-03-28 10:48:49 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (803582ms till timeout)
2022-03-28 10:48:49 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1283413ms till timeout)
2022-03-28 10:48:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (803338ms till timeout)
2022-03-28 10:48:49 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (803360ms till timeout)
2022-03-28 10:48:49 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (803332ms till timeout)
2022-03-28 10:48:50 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (802589ms till timeout)
2022-03-28 10:48:50 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (802537ms till timeout)
2022-03-28 10:48:50 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1282393ms till timeout)
2022-03-28 10:48:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (802323ms till timeout)
2022-03-28 10:48:50 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (802333ms till timeout)
2022-03-28 10:48:50 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (802308ms till timeout)
2022-03-28 10:48:51 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (801510ms till timeout)
2022-03-28 10:48:51 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (801498ms till timeout)
2022-03-28 10:48:51 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1281327ms till timeout)
2022-03-28 10:48:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (801231ms till timeout)
2022-03-28 10:48:51 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (801253ms till timeout)
2022-03-28 10:48:51 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (801218ms till timeout)
2022-03-28 10:48:51 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:48:52 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (800479ms till timeout)
2022-03-28 10:48:52 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (800476ms till timeout)
2022-03-28 10:48:52 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1280283ms till timeout)
2022-03-28 10:48:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (800183ms till timeout)
2022-03-28 10:48:52 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (800175ms till timeout)
2022-03-28 10:48:52 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (800171ms till timeout)
2022-03-28 10:48:53 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (799441ms till timeout)
2022-03-28 10:48:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (799450ms till timeout)
2022-03-28 10:48:53 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1279216ms till timeout)
2022-03-28 10:48:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (799141ms till timeout)
2022-03-28 10:48:53 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (799148ms till timeout)
2022-03-28 10:48:53 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (799123ms till timeout)
2022-03-28 10:48:54 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (798396ms till timeout)
2022-03-28 10:48:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (798392ms till timeout)
2022-03-28 10:48:54 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1278169ms till timeout)
2022-03-28 10:48:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (798084ms till timeout)
2022-03-28 10:48:54 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (798083ms till timeout)
2022-03-28 10:48:54 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (798087ms till timeout)
2022-03-28 10:48:55 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (797286ms till timeout)
2022-03-28 10:48:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (797300ms till timeout)
2022-03-28 10:48:55 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1277078ms till timeout)
2022-03-28 10:48:55 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (797020ms till timeout)
2022-03-28 10:48:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (796998ms till timeout)
2022-03-28 10:48:55 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (797016ms till timeout)
2022-03-28 10:48:56 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (796256ms till timeout)
2022-03-28 10:48:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (796229ms till timeout)
2022-03-28 10:48:56 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:48:56 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1276043ms till timeout)
2022-03-28 10:48:56 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (795992ms till timeout)
2022-03-28 10:48:57 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (795972ms till timeout)
2022-03-28 10:48:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (795932ms till timeout)
2022-03-28 10:48:57 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (795229ms till timeout)
2022-03-28 10:48:57 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (795197ms till timeout)
2022-03-28 10:48:57 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1275026ms till timeout)
2022-03-28 10:48:58 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (794965ms till timeout)
2022-03-28 10:48:58 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (794941ms till timeout)
2022-03-28 10:48:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (794901ms till timeout)
2022-03-28 10:48:58 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (794175ms till timeout)
2022-03-28 10:48:58 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (794182ms till timeout)
2022-03-28 10:48:58 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1273998ms till timeout)
2022-03-28 10:48:59 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (793922ms till timeout)
2022-03-28 10:48:59 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (793894ms till timeout)
2022-03-28 10:48:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (793846ms till timeout)
2022-03-28 10:48:59 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (793123ms till timeout)
2022-03-28 10:48:59 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (793071ms till timeout)
2022-03-28 10:49:00 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1272962ms till timeout)
2022-03-28 10:49:00 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (792868ms till timeout)
2022-03-28 10:49:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (792815ms till timeout)
2022-03-28 10:49:00 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (792833ms till timeout)
2022-03-28 10:49:00 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (792082ms till timeout)
2022-03-28 10:49:00 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (792030ms till timeout)
2022-03-28 10:49:01 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1271913ms till timeout)
2022-03-28 10:49:01 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (791810ms till timeout)
2022-03-28 10:49:01 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (791786ms till timeout)
2022-03-28 10:49:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (791762ms till timeout)
2022-03-28 10:49:01 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:49:01 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (790970ms till timeout)
2022-03-28 10:49:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (790967ms till timeout)
2022-03-28 10:49:02 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1270777ms till timeout)
2022-03-28 10:49:02 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (790772ms till timeout)
2022-03-28 10:49:02 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (790732ms till timeout)
2022-03-28 10:49:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (790694ms till timeout)
2022-03-28 10:49:03 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (789916ms till timeout)
2022-03-28 10:49:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (789898ms till timeout)
2022-03-28 10:49:03 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1269742ms till timeout)
2022-03-28 10:49:03 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (789717ms till timeout)
2022-03-28 10:49:03 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (789688ms till timeout)
2022-03-28 10:49:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (789593ms till timeout)
2022-03-28 10:49:04 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (788649ms till timeout)
2022-03-28 10:49:04 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (788634ms till timeout)
2022-03-28 10:49:04 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1268612ms till timeout)
2022-03-28 10:49:04 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (788566ms till timeout)
2022-03-28 10:49:04 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (788557ms till timeout)
2022-03-28 10:49:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (788522ms till timeout)
2022-03-28 10:49:05 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (787596ms till timeout)
2022-03-28 10:49:05 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1267584ms till timeout)
2022-03-28 10:49:05 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (787575ms till timeout)
2022-03-28 10:49:05 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (787519ms till timeout)
2022-03-28 10:49:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (787438ms till timeout)
2022-03-28 10:49:05 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (787456ms till timeout)
2022-03-28 10:49:06 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (786467ms till timeout)
2022-03-28 10:49:06 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1266485ms till timeout)
2022-03-28 10:49:06 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (786485ms till timeout)
2022-03-28 10:49:06 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (786433ms till timeout)
2022-03-28 10:49:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (786397ms till timeout)
2022-03-28 10:49:06 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (786404ms till timeout)
2022-03-28 10:49:06 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:49:07 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (785360ms till timeout)
2022-03-28 10:49:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (785322ms till timeout)
2022-03-28 10:49:07 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1265328ms till timeout)
2022-03-28 10:49:07 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (785290ms till timeout)
2022-03-28 10:49:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (785294ms till timeout)
2022-03-28 10:49:07 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (785294ms till timeout)
2022-03-28 10:49:08 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (784252ms till timeout)
2022-03-28 10:49:08 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1264265ms till timeout)
2022-03-28 10:49:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (784227ms till timeout)
2022-03-28 10:49:08 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (784210ms till timeout)
2022-03-28 10:49:08 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (784214ms till timeout)
2022-03-28 10:49:08 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (784189ms till timeout)
2022-03-28 10:49:09 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (783037ms till timeout)
2022-03-28 10:49:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (783015ms till timeout)
2022-03-28 10:49:09 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (783033ms till timeout)
2022-03-28 10:49:09 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1263023ms till timeout)
2022-03-28 10:49:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (783023ms till timeout)
2022-03-28 10:49:09 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (782968ms till timeout)
2022-03-28 10:49:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (781945ms till timeout)
2022-03-28 10:49:11 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (781967ms till timeout)
2022-03-28 10:49:11 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (781929ms till timeout)
2022-03-28 10:49:11 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1261933ms till timeout)
2022-03-28 10:49:11 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (781870ms till timeout)
2022-03-28 10:49:11 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (781801ms till timeout)
2022-03-28 10:49:11 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:49:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (780918ms till timeout)
2022-03-28 10:49:12 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (780911ms till timeout)
2022-03-28 10:49:12 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (780877ms till timeout)
2022-03-28 10:49:12 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1260881ms till timeout)
2022-03-28 10:49:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (780819ms till timeout)
2022-03-28 10:49:12 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (780756ms till timeout)
2022-03-28 10:49:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (779893ms till timeout)
2022-03-28 10:49:13 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (779880ms till timeout)
2022-03-28 10:49:13 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1259846ms till timeout)
2022-03-28 10:49:13 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (779842ms till timeout)
2022-03-28 10:49:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (779783ms till timeout)
2022-03-28 10:49:13 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (779631ms till timeout)
2022-03-28 10:49:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (778792ms till timeout)
2022-03-28 10:49:14 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (778810ms till timeout)
2022-03-28 10:49:14 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1258810ms till timeout)
2022-03-28 10:49:14 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (778799ms till timeout)
2022-03-28 10:49:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (778719ms till timeout)
2022-03-28 10:49:14 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (778511ms till timeout)
2022-03-28 10:49:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (777746ms till timeout)
2022-03-28 10:49:15 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (777736ms till timeout)
2022-03-28 10:49:15 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1257740ms till timeout)
2022-03-28 10:49:15 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (777740ms till timeout)
2022-03-28 10:49:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (777645ms till timeout)
2022-03-28 10:49:15 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (777440ms till timeout)
2022-03-28 10:49:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (776658ms till timeout)
2022-03-28 10:49:16 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1256657ms till timeout)
2022-03-28 10:49:16 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (776625ms till timeout)
2022-03-28 10:49:16 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (776611ms till timeout)
2022-03-28 10:49:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (776549ms till timeout)
2022-03-28 10:49:16 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (776428ms till timeout)
2022-03-28 10:49:16 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:49:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (775623ms till timeout)
2022-03-28 10:49:17 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1255578ms till timeout)
2022-03-28 10:49:17 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (775511ms till timeout)
2022-03-28 10:49:17 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (775511ms till timeout)
2022-03-28 10:49:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (775503ms till timeout)
2022-03-28 10:49:17 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (775388ms till timeout)
2022-03-28 10:49:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (774563ms till timeout)
2022-03-28 10:49:18 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1254512ms till timeout)
2022-03-28 10:49:18 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (774458ms till timeout)
2022-03-28 10:49:18 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (774428ms till timeout)
2022-03-28 10:49:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (774431ms till timeout)
2022-03-28 10:49:18 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (774364ms till timeout)
2022-03-28 10:49:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (773524ms till timeout)
2022-03-28 10:49:19 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1253494ms till timeout)
2022-03-28 10:49:19 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (773429ms till timeout)
2022-03-28 10:49:19 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (773406ms till timeout)
2022-03-28 10:49:19 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (773410ms till timeout)
2022-03-28 10:49:19 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (773324ms till timeout)
2022-03-28 10:49:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (772476ms till timeout)
2022-03-28 10:49:20 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1252474ms till timeout)
2022-03-28 10:49:20 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (772391ms till timeout)
2022-03-28 10:49:20 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (772381ms till timeout)
2022-03-28 10:49:20 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (772377ms till timeout)
2022-03-28 10:49:20 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (772284ms till timeout)
2022-03-28 10:49:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (771446ms till timeout)
2022-03-28 10:49:21 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1251440ms till timeout)
2022-03-28 10:49:21 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (771345ms till timeout)
2022-03-28 10:49:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (771312ms till timeout)
2022-03-28 10:49:21 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (771308ms till timeout)
2022-03-28 10:49:21 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (771223ms till timeout)
2022-03-28 10:49:21 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:49:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (770408ms till timeout)
2022-03-28 10:49:22 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1250418ms till timeout)
2022-03-28 10:49:22 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (770320ms till timeout)
2022-03-28 10:49:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (770285ms till timeout)
2022-03-28 10:49:22 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (770255ms till timeout)
2022-03-28 10:49:22 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (770181ms till timeout)
2022-03-28 10:49:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (769380ms till timeout)
2022-03-28 10:49:23 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1249388ms till timeout)
2022-03-28 10:49:23 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (769301ms till timeout)
2022-03-28 10:49:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (769266ms till timeout)
2022-03-28 10:49:23 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (769225ms till timeout)
2022-03-28 10:49:23 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (769102ms till timeout)
2022-03-28 10:49:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (768317ms till timeout)
2022-03-28 10:49:24 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1248335ms till timeout)
2022-03-28 10:49:24 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (768268ms till timeout)
2022-03-28 10:49:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (768225ms till timeout)
2022-03-28 10:49:24 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (768178ms till timeout)
2022-03-28 10:49:24 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (768043ms till timeout)
2022-03-28 10:49:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (767264ms till timeout)
2022-03-28 10:49:25 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1247283ms till timeout)
2022-03-28 10:49:25 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (767250ms till timeout)
2022-03-28 10:49:25 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (767174ms till timeout)
2022-03-28 10:49:25 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (767093ms till timeout)
2022-03-28 10:49:25 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (766986ms till timeout)
2022-03-28 10:49:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (766231ms till timeout)
2022-03-28 10:49:26 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1246226ms till timeout)
2022-03-28 10:49:26 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (766193ms till timeout)
2022-03-28 10:49:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (766152ms till timeout)
2022-03-28 10:49:26 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:49:26 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (766068ms till timeout)
2022-03-28 10:49:27 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (765913ms till timeout)
2022-03-28 10:49:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (765182ms till timeout)
2022-03-28 10:49:27 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1245154ms till timeout)
2022-03-28 10:49:27 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (765154ms till timeout)
2022-03-28 10:49:27 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (765104ms till timeout)
2022-03-28 10:49:27 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (765055ms till timeout)
2022-03-28 10:49:28 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (764841ms till timeout)
2022-03-28 10:49:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (764076ms till timeout)
2022-03-28 10:49:28 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1244074ms till timeout)
2022-03-28 10:49:28 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (764074ms till timeout)
2022-03-28 10:49:28 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (763997ms till timeout)
2022-03-28 10:49:28 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (764001ms till timeout)
2022-03-28 10:49:29 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (763782ms till timeout)
2022-03-28 10:49:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (763045ms till timeout)
2022-03-28 10:49:29 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1243035ms till timeout)
2022-03-28 10:49:29 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (763035ms till timeout)
2022-03-28 10:49:30 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (762984ms till timeout)
2022-03-28 10:49:30 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (762950ms till timeout)
2022-03-28 10:49:30 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (762756ms till timeout)
2022-03-28 10:49:31 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (761879ms till timeout)
2022-03-28 10:49:31 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (761871ms till timeout)
2022-03-28 10:49:31 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (761875ms till timeout)
2022-03-28 10:49:31 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1241875ms till timeout)
2022-03-28 10:49:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (761852ms till timeout)
2022-03-28 10:49:31 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (761707ms till timeout)
2022-03-28 10:49:31 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:49:32 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (760814ms till timeout)
2022-03-28 10:49:32 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1240789ms till timeout)
2022-03-28 10:49:32 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (760783ms till timeout)
2022-03-28 10:49:32 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (760779ms till timeout)
2022-03-28 10:49:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (760737ms till timeout)
2022-03-28 10:49:32 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (760601ms till timeout)
2022-03-28 10:49:33 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (759758ms till timeout)
2022-03-28 10:49:33 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1239701ms till timeout)
2022-03-28 10:49:33 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (759697ms till timeout)
2022-03-28 10:49:33 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (759701ms till timeout)
2022-03-28 10:49:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (759678ms till timeout)
2022-03-28 10:49:33 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (759511ms till timeout)
2022-03-28 10:49:34 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (758648ms till timeout)
2022-03-28 10:49:34 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (758644ms till timeout)
2022-03-28 10:49:34 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (758648ms till timeout)
2022-03-28 10:49:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (758619ms till timeout)
2022-03-28 10:49:34 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1238641ms till timeout)
2022-03-28 10:49:34 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (758430ms till timeout)
2022-03-28 10:49:35 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (757606ms till timeout)
2022-03-28 10:49:35 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (757606ms till timeout)
2022-03-28 10:49:35 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (757602ms till timeout)
2022-03-28 10:49:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (757566ms till timeout)
2022-03-28 10:49:35 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1237573ms till timeout)
2022-03-28 10:49:35 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (757378ms till timeout)
2022-03-28 10:49:36 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (756465ms till timeout)
2022-03-28 10:49:36 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1236465ms till timeout)
2022-03-28 10:49:36 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (756461ms till timeout)
2022-03-28 10:49:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (756443ms till timeout)
2022-03-28 10:49:36 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (756464ms till timeout)
2022-03-28 10:49:36 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (756288ms till timeout)
2022-03-28 10:49:36 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:49:37 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (755388ms till timeout)
2022-03-28 10:49:37 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (755369ms till timeout)
2022-03-28 10:49:37 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1235373ms till timeout)
2022-03-28 10:49:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (755351ms till timeout)
2022-03-28 10:49:37 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (755371ms till timeout)
2022-03-28 10:49:37 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (755183ms till timeout)
2022-03-28 10:49:38 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (754362ms till timeout)
2022-03-28 10:49:38 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (754318ms till timeout)
2022-03-28 10:49:38 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1234318ms till timeout)
2022-03-28 10:49:38 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (754313ms till timeout)
2022-03-28 10:49:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (754283ms till timeout)
2022-03-28 10:49:38 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (754126ms till timeout)
2022-03-28 10:49:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (753319ms till timeout)
2022-03-28 10:49:39 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (753245ms till timeout)
2022-03-28 10:49:39 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1233185ms till timeout)
2022-03-28 10:49:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (753163ms till timeout)
2022-03-28 10:49:39 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (753168ms till timeout)
2022-03-28 10:49:39 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (753062ms till timeout)
2022-03-28 10:49:40 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (752277ms till timeout)
2022-03-28 10:49:40 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (752215ms till timeout)
2022-03-28 10:49:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (752146ms till timeout)
2022-03-28 10:49:40 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1232160ms till timeout)
2022-03-28 10:49:40 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (752156ms till timeout)
2022-03-28 10:49:40 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (752050ms till timeout)
2022-03-28 10:49:41 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (751164ms till timeout)
2022-03-28 10:49:41 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (751156ms till timeout)
2022-03-28 10:49:41 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:49:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (751117ms till timeout)
2022-03-28 10:49:41 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1231113ms till timeout)
2022-03-28 10:49:41 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (751093ms till timeout)
2022-03-28 10:49:41 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (751010ms till timeout)
2022-03-28 10:49:42 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (750064ms till timeout)
2022-03-28 10:49:42 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (750047ms till timeout)
2022-03-28 10:49:42 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (750043ms till timeout)
2022-03-28 10:49:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (750025ms till timeout)
2022-03-28 10:49:42 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1230021ms till timeout)
2022-03-28 10:49:43 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (749962ms till timeout)
2022-03-28 10:49:44 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (748979ms till timeout)
2022-03-28 10:49:44 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (748978ms till timeout)
2022-03-28 10:49:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (748941ms till timeout)
2022-03-28 10:49:44 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (748959ms till timeout)
2022-03-28 10:49:44 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1228961ms till timeout)
2022-03-28 10:49:44 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (748929ms till timeout)
2022-03-28 10:49:45 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (747932ms till timeout)
2022-03-28 10:49:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (747870ms till timeout)
2022-03-28 10:49:45 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (747848ms till timeout)
2022-03-28 10:49:45 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (747830ms till timeout)
2022-03-28 10:49:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (747852ms till timeout)
2022-03-28 10:49:45 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1227835ms till timeout)
2022-03-28 10:49:46 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (746882ms till timeout)
2022-03-28 10:49:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (746822ms till timeout)
2022-03-28 10:49:46 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (746764ms till timeout)
2022-03-28 10:49:46 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (746786ms till timeout)
2022-03-28 10:49:46 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (746777ms till timeout)
2022-03-28 10:49:46 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1226745ms till timeout)
2022-03-28 10:49:46 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:49:47 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (745802ms till timeout)
2022-03-28 10:49:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (745761ms till timeout)
2022-03-28 10:49:47 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (745686ms till timeout)
2022-03-28 10:49:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (745616ms till timeout)
2022-03-28 10:49:47 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1225608ms till timeout)
2022-03-28 10:49:47 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (745604ms till timeout)
2022-03-28 10:49:48 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (744708ms till timeout)
2022-03-28 10:49:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (744652ms till timeout)
2022-03-28 10:49:48 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (744647ms till timeout)
2022-03-28 10:49:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (744552ms till timeout)
2022-03-28 10:49:48 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1224533ms till timeout)
2022-03-28 10:49:48 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (744489ms till timeout)
2022-03-28 10:49:49 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (743620ms till timeout)
2022-03-28 10:49:49 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (743582ms till timeout)
2022-03-28 10:49:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (743555ms till timeout)
2022-03-28 10:49:49 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (743480ms till timeout)
2022-03-28 10:49:49 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1223477ms till timeout)
2022-03-28 10:49:49 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (743465ms till timeout)
2022-03-28 10:49:50 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (742570ms till timeout)
2022-03-28 10:49:50 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (742514ms till timeout)
2022-03-28 10:49:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (742482ms till timeout)
2022-03-28 10:49:50 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (742441ms till timeout)
2022-03-28 10:49:50 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1222408ms till timeout)
2022-03-28 10:49:50 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (742404ms till timeout)
2022-03-28 10:49:51 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (741535ms till timeout)
2022-03-28 10:49:51 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (741491ms till timeout)
2022-03-28 10:49:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (741464ms till timeout)
2022-03-28 10:49:51 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (741394ms till timeout)
2022-03-28 10:49:51 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1221294ms till timeout)
2022-03-28 10:49:51 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (741290ms till timeout)
2022-03-28 10:49:51 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:49:52 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (740450ms till timeout)
2022-03-28 10:49:52 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (740471ms till timeout)
2022-03-28 10:49:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (740449ms till timeout)
2022-03-28 10:49:52 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (740376ms till timeout)
2022-03-28 10:49:52 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1220273ms till timeout)
2022-03-28 10:49:52 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (740226ms till timeout)
2022-03-28 10:49:53 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (739418ms till timeout)
2022-03-28 10:49:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (739404ms till timeout)
2022-03-28 10:49:53 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (739426ms till timeout)
2022-03-28 10:49:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (739341ms till timeout)
2022-03-28 10:49:53 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1219192ms till timeout)
2022-03-28 10:49:53 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (739110ms till timeout)
2022-03-28 10:49:54 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (738337ms till timeout)
2022-03-28 10:49:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (738318ms till timeout)
2022-03-28 10:49:54 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (738340ms till timeout)
2022-03-28 10:49:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (738322ms till timeout)
2022-03-28 10:49:54 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1218157ms till timeout)
2022-03-28 10:49:54 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (738048ms till timeout)
2022-03-28 10:49:55 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (737275ms till timeout)
2022-03-28 10:49:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (737275ms till timeout)
2022-03-28 10:49:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (737286ms till timeout)
2022-03-28 10:49:55 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (737281ms till timeout)
2022-03-28 10:49:55 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1217063ms till timeout)
2022-03-28 10:49:56 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (736974ms till timeout)
2022-03-28 10:49:56 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (736235ms till timeout)
2022-03-28 10:49:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (736235ms till timeout)
2022-03-28 10:49:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (736254ms till timeout)
2022-03-28 10:49:56 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (736249ms till timeout)
2022-03-28 10:49:56 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:49:56 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1216035ms till timeout)
2022-03-28 10:49:57 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (735916ms till timeout)
2022-03-28 10:49:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (735168ms till timeout)
2022-03-28 10:49:57 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (735168ms till timeout)
2022-03-28 10:49:57 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (735182ms till timeout)
2022-03-28 10:49:57 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (735170ms till timeout)
2022-03-28 10:49:58 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1214906ms till timeout)
2022-03-28 10:49:58 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (734879ms till timeout)
2022-03-28 10:49:58 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (734080ms till timeout)
2022-03-28 10:49:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (734055ms till timeout)
2022-03-28 10:49:58 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (734046ms till timeout)
2022-03-28 10:49:58 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (734046ms till timeout)
2022-03-28 10:49:59 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1213822ms till timeout)
2022-03-28 10:49:59 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (733777ms till timeout)
2022-03-28 10:49:59 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (733045ms till timeout)
2022-03-28 10:49:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (733025ms till timeout)
2022-03-28 10:49:59 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (733027ms till timeout)
2022-03-28 10:49:59 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (733016ms till timeout)
2022-03-28 10:50:00 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1212762ms till timeout)
2022-03-28 10:50:00 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (732745ms till timeout)
2022-03-28 10:50:00 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (732021ms till timeout)
2022-03-28 10:50:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (731972ms till timeout)
2022-03-28 10:50:01 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (731950ms till timeout)
2022-03-28 10:50:01 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (731949ms till timeout)
2022-03-28 10:50:01 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1211713ms till timeout)
2022-03-28 10:50:01 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (731678ms till timeout)
2022-03-28 10:50:01 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:50:02 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (730889ms till timeout)
2022-03-28 10:50:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (730889ms till timeout)
2022-03-28 10:50:02 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (730856ms till timeout)
2022-03-28 10:50:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (730855ms till timeout)
2022-03-28 10:50:02 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1210639ms till timeout)
2022-03-28 10:50:02 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (730611ms till timeout)
2022-03-28 10:50:03 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (729828ms till timeout)
2022-03-28 10:50:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (729802ms till timeout)
2022-03-28 10:50:03 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (729816ms till timeout)
2022-03-28 10:50:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (729800ms till timeout)
2022-03-28 10:50:03 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1209616ms till timeout)
2022-03-28 10:50:03 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (729556ms till timeout)
2022-03-28 10:50:04 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (728776ms till timeout)
2022-03-28 10:50:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (728718ms till timeout)
2022-03-28 10:50:04 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (728736ms till timeout)
2022-03-28 10:50:04 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (728701ms till timeout)
2022-03-28 10:50:04 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1208577ms till timeout)
2022-03-28 10:50:04 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (728460ms till timeout)
2022-03-28 10:50:05 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (727706ms till timeout)
2022-03-28 10:50:05 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (727707ms till timeout)
2022-03-28 10:50:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (727675ms till timeout)
2022-03-28 10:50:05 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (727667ms till timeout)
2022-03-28 10:50:05 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1207532ms till timeout)
2022-03-28 10:50:05 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (727421ms till timeout)
2022-03-28 10:50:06 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (726672ms till timeout)
2022-03-28 10:50:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (726606ms till timeout)
2022-03-28 10:50:06 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (726624ms till timeout)
2022-03-28 10:50:06 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (726603ms till timeout)
2022-03-28 10:50:06 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1206447ms till timeout)
2022-03-28 10:50:06 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (726398ms till timeout)
2022-03-28 10:50:06 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:50:07 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (725582ms till timeout)
2022-03-28 10:50:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (725532ms till timeout)
2022-03-28 10:50:07 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (725537ms till timeout)
2022-03-28 10:50:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (725526ms till timeout)
2022-03-28 10:50:07 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1205416ms till timeout)
2022-03-28 10:50:07 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (725350ms till timeout)
2022-03-28 10:50:08 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (724528ms till timeout)
2022-03-28 10:50:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (724485ms till timeout)
2022-03-28 10:50:08 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (724443ms till timeout)
2022-03-28 10:50:08 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (724443ms till timeout)
2022-03-28 10:50:08 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1204347ms till timeout)
2022-03-28 10:50:08 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (724312ms till timeout)
2022-03-28 10:50:09 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (723510ms till timeout)
2022-03-28 10:50:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (723464ms till timeout)
2022-03-28 10:50:09 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (723380ms till timeout)
2022-03-28 10:50:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (723353ms till timeout)
2022-03-28 10:50:09 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1203315ms till timeout)
2022-03-28 10:50:09 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (723290ms till timeout)
2022-03-28 10:50:10 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (722439ms till timeout)
2022-03-28 10:50:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (722420ms till timeout)
2022-03-28 10:50:10 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (722351ms till timeout)
2022-03-28 10:50:10 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (722335ms till timeout)
2022-03-28 10:50:10 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1202275ms till timeout)
2022-03-28 10:50:10 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (722248ms till timeout)
2022-03-28 10:50:11 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (721382ms till timeout)
2022-03-28 10:50:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (721362ms till timeout)
2022-03-28 10:50:11 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (721323ms till timeout)
2022-03-28 10:50:11 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (721309ms till timeout)
2022-03-28 10:50:11 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1201252ms till timeout)
2022-03-28 10:50:11 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:444] Kafka: http-bridge-scram-sha-cluster-name is in desired state: Ready
2022-03-28 10:50:11 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update KafkaUser my-user-1569591476-1283230835 in namespace http-bridge-scram-sha-st
2022-03-28 10:50:11 [ForkJoinPool-1-worker-5] WARN  [VersionUsageUtils:60] The client is using resource type 'kafkausers' with unstable version 'v1beta2'
2022-03-28 10:50:11 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:50:11 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-1569591476-1283230835
2022-03-28 10:50:11 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-1569591476-1283230835 will have desired state: Ready
2022-03-28 10:50:11 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-1569591476-1283230835 will have desired state: Ready
2022-03-28 10:50:11 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaUser: my-user-1569591476-1283230835 will have desired state: Ready not ready, will try again in 1000 ms (179974ms till timeout)
2022-03-28 10:50:12 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (720284ms till timeout)
2022-03-28 10:50:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (720283ms till timeout)
2022-03-28 10:50:12 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (720275ms till timeout)
2022-03-28 10:50:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (720252ms till timeout)
2022-03-28 10:50:12 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1200145ms till timeout)
2022-03-28 10:50:12 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:444] KafkaUser: my-user-1569591476-1283230835 is in desired state: Ready
2022-03-28 10:50:13 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update Deployment http-bridge-scram-sha-st-shared-kafka-clients in namespace http-bridge-scram-sha-st
2022-03-28 10:50:13 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:http-bridge-scram-sha-st-shared-kafka-clients
2022-03-28 10:50:13 [ForkJoinPool-1-worker-5] INFO  [DeploymentUtils:161] Wait for Deployment: http-bridge-scram-sha-st-shared-kafka-clients will be ready
2022-03-28 10:50:13 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Wait for Deployment: http-bridge-scram-sha-st-shared-kafka-clients will be ready
2022-03-28 10:50:13 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Wait for Deployment: http-bridge-scram-sha-st-shared-kafka-clients will be ready not ready, will try again in 1000 ms (479921ms till timeout)
2022-03-28 10:50:13 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (719189ms till timeout)
2022-03-28 10:50:13 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (719191ms till timeout)
2022-03-28 10:50:13 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:444] Kafka: quota-cluster is in desired state: Ready
2022-03-28 10:50:13 [ForkJoinPool-1-worker-9] INFO  [TestSeparator:23] ############################################################################
2022-03-28 10:50:13 [ForkJoinPool-1-worker-9] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testThrottlingQuotasCreateTopic-STARTED
2022-03-28 10:50:13 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:658] ============================================================================
2022-03-28 10:50:13 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:659] [operators.topic.ThrottlingQuotaST - Before Each] - Setup test case environment
2022-03-28 10:50:13 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:77] [operators.topic.ThrottlingQuotaST] - Adding parallel test: testThrottlingQuotasCreateTopic
2022-03-28 10:50:13 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:81] [operators.topic.ThrottlingQuotaST] - Parallel test count: 2
2022-03-28 10:50:13 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:230] testThrottlingQuotasCreateTopic test now can proceed its execution
2022-03-28 10:50:13 [ForkJoinPool-1-worker-9] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 10:50:13 [ForkJoinPool-1-worker-9] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8, testCapacityFile=my-cluster-fd0fb61a}
2022-03-28 10:50:13 [ForkJoinPool-1-worker-9] TRACE [AbstractST:607] USERS_NAME_MAP: {testThrottlingQuotasCreateTopic=my-user-1131806865-947217230, testCapacityFile=my-user-937015144-1959439396}
2022-03-28 10:50:13 [ForkJoinPool-1-worker-9] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testThrottlingQuotasCreateTopic=my-topic-728672490-1194703402, testCapacityFile=my-topic-1155641705-902584520}
2022-03-28 10:50:13 [ForkJoinPool-1-worker-9] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8-kafka-clients, testCapacityFile=my-cluster-fd0fb61a-kafka-clients}
2022-03-28 10:50:13 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update KafkaUser my-user-1131806865-947217230 in namespace throttling-quota-st
2022-03-28 10:50:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (719173ms till timeout)
2022-03-28 10:50:13 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1199091ms till timeout)
2022-03-28 10:50:13 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-1131806865-947217230
2022-03-28 10:50:14 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-1131806865-947217230 will have desired state: Ready
2022-03-28 10:50:14 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-1131806865-947217230 will have desired state: Ready
2022-03-28 10:50:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] KafkaUser: my-user-1131806865-947217230 will have desired state: Ready not ready, will try again in 1000 ms (179890ms till timeout)
2022-03-28 10:50:14 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Wait for Deployment: http-bridge-scram-sha-st-shared-kafka-clients will be ready not ready, will try again in 1000 ms (478867ms till timeout)
2022-03-28 10:50:14 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:444] Kafka: user-cluster-name is in desired state: Ready
2022-03-28 10:50:14 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:23] ############################################################################
2022-03-28 10:50:14 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testCreatingUsersWithSecretPrefix-STARTED
2022-03-28 10:50:14 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:658] ============================================================================
2022-03-28 10:50:14 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:659] [operators.user.UserST - Before Each] - Setup test case environment
2022-03-28 10:50:14 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:77] [operators.user.UserST] - Adding parallel test: testCreatingUsersWithSecretPrefix
2022-03-28 10:50:14 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:81] [operators.user.UserST] - Parallel test count: 3
2022-03-28 10:50:14 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:230] testCreatingUsersWithSecretPrefix test now can proceed its execution
2022-03-28 10:50:14 [ForkJoinPool-1-worker-13] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 10:50:14 [ForkJoinPool-1-worker-13] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8, testCapacityFile=my-cluster-fd0fb61a, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c}
2022-03-28 10:50:14 [ForkJoinPool-1-worker-13] TRACE [AbstractST:607] USERS_NAME_MAP: {testThrottlingQuotasCreateTopic=my-user-1131806865-947217230, testCapacityFile=my-user-937015144-1959439396, testCreatingUsersWithSecretPrefix=my-user-1450786684-1808961062}
2022-03-28 10:50:14 [ForkJoinPool-1-worker-13] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testThrottlingQuotasCreateTopic=my-topic-728672490-1194703402, testCapacityFile=my-topic-1155641705-902584520, testCreatingUsersWithSecretPrefix=my-topic-1479520843-618891199}
2022-03-28 10:50:14 [ForkJoinPool-1-worker-13] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8-kafka-clients, testCapacityFile=my-cluster-fd0fb61a-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c-kafka-clients}
2022-03-28 10:50:14 [ForkJoinPool-1-worker-13] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-1 for test case:testCreatingUsersWithSecretPrefix
2022-03-28 10:50:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (718143ms till timeout)
2022-03-28 10:50:14 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (718139ms till timeout)
2022-03-28 10:50:14 [ForkJoinPool-1-worker-13] INFO  [KubeClusterResource:156] Creating Namespace: namespace-1
2022-03-28 10:50:14 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Namespace namespace-1
2022-03-28 10:50:14 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-0 get Namespace namespace-1 -o json
2022-03-28 10:50:14 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1198053ms till timeout)
2022-03-28 10:50:15 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:444] KafkaUser: my-user-1131806865-947217230 is in desired state: Ready
2022-03-28 10:50:15 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-0 get Namespace namespace-1 -o json
2022-03-28 10:50:15 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 10:50:15 [ForkJoinPool-1-worker-13] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c26,c20",
            "openshift.io/sa.scc.supplemental-groups": "1000690000/10000",
            "openshift.io/sa.scc.uid-range": "1000690000/10000"
        },
        "creationTimestamp": "2022-03-28T10:50:12Z",
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T10:49:42Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:status": {
                        "f:phase": {}
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T10:50:12Z"
            }
        ],
        "name": "namespace-1",
        "resourceVersion": "1866066",
        "selfLink": "/api/v1/namespaces/namespace-1",
        "uid": "a69a7394-6868-4dff-aaf9-71137df916d2"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 10:50:15 [ForkJoinPool-1-worker-13] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[namespace-1], io.strimzi.test.logs.CollectorElement@3e255cd9=[namespace-0]}
2022-03-28 10:50:15 [ForkJoinPool-1-worker-13] INFO  [KubeClusterResource:82] Client use Namespace: namespace-1
2022-03-28 10:50:15 [ForkJoinPool-1-worker-13] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-1, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 10:50:15 [ForkJoinPool-1-worker-13] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-1
2022-03-28 10:50:15 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-ede8537c in namespace namespace-1
2022-03-28 10:50:15 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:164] Using Namespace: namespace-1
2022-03-28 10:50:15 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update Job create-admin-my-cluster-83f3f4b8-kafka-clients in namespace throttling-quota-st
2022-03-28 10:50:15 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-ede8537c
2022-03-28 10:50:15 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:create-admin-my-cluster-83f3f4b8-kafka-clients
2022-03-28 10:50:15 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-ede8537c will have desired state: Ready
2022-03-28 10:50:15 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-ede8537c will have desired state: Ready
2022-03-28 10:50:15 [ForkJoinPool-1-worker-9] INFO  [JobUtils:81] Waiting for job: create-admin-my-cluster-83f3f4b8-kafka-clients will be in active state
2022-03-28 10:50:15 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 10:50:15 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (839890ms till timeout)
2022-03-28 10:50:15 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Wait for Deployment: http-bridge-scram-sha-st-shared-kafka-clients will be ready not ready, will try again in 1000 ms (477794ms till timeout)
2022-03-28 10:50:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job active not ready, will try again in 1000 ms (179892ms till timeout)
2022-03-28 10:50:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (717090ms till timeout)
2022-03-28 10:50:15 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (717065ms till timeout)
2022-03-28 10:50:15 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1197027ms till timeout)
2022-03-28 10:50:16 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (838794ms till timeout)
2022-03-28 10:50:16 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Wait for Deployment: http-bridge-scram-sha-st-shared-kafka-clients will be ready not ready, will try again in 1000 ms (476675ms till timeout)
2022-03-28 10:50:16 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:50:16 [ForkJoinPool-1-worker-9] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 10:50:16 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 10:50:16 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:444] Kafka: http-bridge-tls-cluster-name is in desired state: Ready
2022-03-28 10:50:16 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update KafkaUser my-user-1252185471-65885922 in namespace http-bridge-tls-st
2022-03-28 10:50:17 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (715976ms till timeout)
2022-03-28 10:50:17 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1195935ms till timeout)
2022-03-28 10:50:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (299785ms till timeout)
2022-03-28 10:50:17 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-1252185471-65885922
2022-03-28 10:50:17 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-1252185471-65885922 will have desired state: Ready
2022-03-28 10:50:17 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-1252185471-65885922 will have desired state: Ready
2022-03-28 10:50:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaUser: my-user-1252185471-65885922 will have desired state: Ready not ready, will try again in 1000 ms (179927ms till timeout)
2022-03-28 10:50:17 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (837716ms till timeout)
2022-03-28 10:50:17 [ForkJoinPool-1-worker-5] INFO  [DeploymentUtils:168] Deployment: http-bridge-scram-sha-st-shared-kafka-clients is ready
2022-03-28 10:50:18 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (714923ms till timeout)
2022-03-28 10:50:18 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update KafkaBridge http-bridge-scram-sha-cluster-name in namespace http-bridge-scram-sha-st
2022-03-28 10:50:18 [ForkJoinPool-1-worker-5] WARN  [VersionUsageUtils:60] The client is using resource type 'kafkabridges' with unstable version 'v1beta2'
2022-03-28 10:50:18 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1194877ms till timeout)
2022-03-28 10:50:18 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaBridge:http-bridge-scram-sha-cluster-name
2022-03-28 10:50:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (298654ms till timeout)
2022-03-28 10:50:18 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:433] Wait for KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready
2022-03-28 10:50:18 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready
2022-03-28 10:50:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaUser: my-user-1252185471-65885922 will have desired state: Ready not ready, will try again in 1000 ms (178761ms till timeout)
2022-03-28 10:50:18 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (479808ms till timeout)
2022-03-28 10:50:18 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (836624ms till timeout)
2022-03-28 10:50:19 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (713862ms till timeout)
2022-03-28 10:50:19 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1193854ms till timeout)
2022-03-28 10:50:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (297532ms till timeout)
2022-03-28 10:50:19 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:444] KafkaUser: my-user-1252185471-65885922 is in desired state: Ready
2022-03-28 10:50:19 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (478733ms till timeout)
2022-03-28 10:50:19 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update Deployment http-bridge-tls-st-kafka-clients in namespace http-bridge-tls-st
2022-03-28 10:50:19 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients
2022-03-28 10:50:19 [ForkJoinPool-1-worker-7] INFO  [DeploymentUtils:161] Wait for Deployment: http-bridge-tls-st-kafka-clients will be ready
2022-03-28 10:50:19 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Wait for Deployment: http-bridge-tls-st-kafka-clients will be ready
2022-03-28 10:50:19 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Wait for Deployment: http-bridge-tls-st-kafka-clients will be ready not ready, will try again in 1000 ms (479962ms till timeout)
2022-03-28 10:50:20 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (835541ms till timeout)
2022-03-28 10:50:20 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (712791ms till timeout)
2022-03-28 10:50:20 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1192776ms till timeout)
2022-03-28 10:50:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (296464ms till timeout)
2022-03-28 10:50:20 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (477708ms till timeout)
2022-03-28 10:50:20 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Wait for Deployment: http-bridge-tls-st-kafka-clients will be ready not ready, will try again in 1000 ms (478929ms till timeout)
2022-03-28 10:50:21 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (834485ms till timeout)
2022-03-28 10:50:21 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:444] Kafka: topic-cluster-name is in desired state: Ready
2022-03-28 10:50:21 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1191715ms till timeout)
2022-03-28 10:50:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (295339ms till timeout)
2022-03-28 10:50:21 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (476686ms till timeout)
2022-03-28 10:50:21 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:50:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Wait for Deployment: http-bridge-tls-st-kafka-clients will be ready not ready, will try again in 1000 ms (477890ms till timeout)
2022-03-28 10:50:22 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (833448ms till timeout)
2022-03-28 10:50:22 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1190655ms till timeout)
2022-03-28 10:50:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (294270ms till timeout)
2022-03-28 10:50:22 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (475645ms till timeout)
2022-03-28 10:50:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Wait for Deployment: http-bridge-tls-st-kafka-clients will be ready not ready, will try again in 1000 ms (476859ms till timeout)
2022-03-28 10:50:23 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (832420ms till timeout)
2022-03-28 10:50:23 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1189531ms till timeout)
2022-03-28 10:50:23 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (474625ms till timeout)
2022-03-28 10:50:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (293185ms till timeout)
2022-03-28 10:50:24 [ForkJoinPool-1-worker-7] INFO  [DeploymentUtils:168] Deployment: http-bridge-tls-st-kafka-clients is ready
2022-03-28 10:50:24 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update KafkaBridge http-bridge-tls-cluster-name in namespace http-bridge-tls-st
2022-03-28 10:50:24 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (831382ms till timeout)
2022-03-28 10:50:24 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaBridge:http-bridge-tls-cluster-name
2022-03-28 10:50:24 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:433] Wait for KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready
2022-03-28 10:50:24 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready
2022-03-28 10:50:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (479982ms till timeout)
2022-03-28 10:50:24 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1188501ms till timeout)
2022-03-28 10:50:24 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (473487ms till timeout)
2022-03-28 10:50:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (292011ms till timeout)
2022-03-28 10:50:25 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (478901ms till timeout)
2022-03-28 10:50:25 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (830212ms till timeout)
2022-03-28 10:50:25 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1187359ms till timeout)
2022-03-28 10:50:25 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (472429ms till timeout)
2022-03-28 10:50:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (290840ms till timeout)
2022-03-28 10:50:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (477842ms till timeout)
2022-03-28 10:50:26 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (829034ms till timeout)
2022-03-28 10:50:26 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1186294ms till timeout)
2022-03-28 10:50:26 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:50:26 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (471370ms till timeout)
2022-03-28 10:50:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (289688ms till timeout)
2022-03-28 10:50:27 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (476758ms till timeout)
2022-03-28 10:50:27 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (828001ms till timeout)
2022-03-28 10:50:27 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1185231ms till timeout)
2022-03-28 10:50:28 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (470238ms till timeout)
2022-03-28 10:50:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (288597ms till timeout)
2022-03-28 10:50:28 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (475694ms till timeout)
2022-03-28 10:50:28 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (826970ms till timeout)
2022-03-28 10:50:28 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1184203ms till timeout)
2022-03-28 10:50:29 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (469199ms till timeout)
2022-03-28 10:50:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (287521ms till timeout)
2022-03-28 10:50:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (474640ms till timeout)
2022-03-28 10:50:29 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (825943ms till timeout)
2022-03-28 10:50:29 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1183186ms till timeout)
2022-03-28 10:50:30 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (468157ms till timeout)
2022-03-28 10:50:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (286449ms till timeout)
2022-03-28 10:50:30 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (473608ms till timeout)
2022-03-28 10:50:30 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (824921ms till timeout)
2022-03-28 10:50:30 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1182160ms till timeout)
2022-03-28 10:50:31 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (467102ms till timeout)
2022-03-28 10:50:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (285395ms till timeout)
2022-03-28 10:50:31 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (472538ms till timeout)
2022-03-28 10:50:31 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (823877ms till timeout)
2022-03-28 10:50:31 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:50:31 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1181124ms till timeout)
2022-03-28 10:50:32 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (466066ms till timeout)
2022-03-28 10:50:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (284324ms till timeout)
2022-03-28 10:50:32 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (471521ms till timeout)
2022-03-28 10:50:32 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (822849ms till timeout)
2022-03-28 10:50:32 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1180099ms till timeout)
2022-03-28 10:50:33 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (465015ms till timeout)
2022-03-28 10:50:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (283239ms till timeout)
2022-03-28 10:50:33 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (470468ms till timeout)
2022-03-28 10:50:33 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (821806ms till timeout)
2022-03-28 10:50:33 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1179069ms till timeout)
2022-03-28 10:50:34 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (463924ms till timeout)
2022-03-28 10:50:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (282200ms till timeout)
2022-03-28 10:50:34 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (469365ms till timeout)
2022-03-28 10:50:34 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (820726ms till timeout)
2022-03-28 10:50:34 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1177996ms till timeout)
2022-03-28 10:50:35 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (462858ms till timeout)
2022-03-28 10:50:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (281148ms till timeout)
2022-03-28 10:50:35 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (468312ms till timeout)
2022-03-28 10:50:35 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (819657ms till timeout)
2022-03-28 10:50:36 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1176910ms till timeout)
2022-03-28 10:50:36 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (461800ms till timeout)
2022-03-28 10:50:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (280091ms till timeout)
2022-03-28 10:50:36 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:50:36 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (467277ms till timeout)
2022-03-28 10:50:36 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (818640ms till timeout)
2022-03-28 10:50:37 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1175883ms till timeout)
2022-03-28 10:50:37 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (460777ms till timeout)
2022-03-28 10:50:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (279026ms till timeout)
2022-03-28 10:50:38 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (466248ms till timeout)
2022-03-28 10:50:38 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (817553ms till timeout)
2022-03-28 10:50:38 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1174782ms till timeout)
2022-03-28 10:50:38 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (459701ms till timeout)
2022-03-28 10:50:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (277935ms till timeout)
2022-03-28 10:50:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (465134ms till timeout)
2022-03-28 10:50:39 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (816460ms till timeout)
2022-03-28 10:50:39 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1173744ms till timeout)
2022-03-28 10:50:39 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (458630ms till timeout)
2022-03-28 10:50:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (276900ms till timeout)
2022-03-28 10:50:40 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (464097ms till timeout)
2022-03-28 10:50:40 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (815444ms till timeout)
2022-03-28 10:50:40 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1172677ms till timeout)
2022-03-28 10:50:40 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:444] KafkaBridge: http-bridge-scram-sha-cluster-name is in desired state: Ready
2022-03-28 10:50:40 [ForkJoinPool-1-worker-5] INFO  [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-03-28 10:50:40 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:23] ############################################################################
2022-03-28 10:50:40 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeScramShaST.testReceiveSimpleMessageTlsScramSha-STARTED
2022-03-28 10:50:40 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:658] ============================================================================
2022-03-28 10:50:40 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:659] [bridge.HttpBridgeScramShaST - Before Each] - Setup test case environment
2022-03-28 10:50:40 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:77] [bridge.HttpBridgeScramShaST] - Adding parallel test: testReceiveSimpleMessageTlsScramSha
2022-03-28 10:50:40 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:81] [bridge.HttpBridgeScramShaST] - Parallel test count: 4
2022-03-28 10:50:40 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:230] testReceiveSimpleMessageTlsScramSha test now can proceed its execution
2022-03-28 10:50:40 [ForkJoinPool-1-worker-5] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 10:50:40 [ForkJoinPool-1-worker-5] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8, testCapacityFile=my-cluster-fd0fb61a, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c}
2022-03-28 10:50:40 [ForkJoinPool-1-worker-5] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-1915916223-1265293973, testThrottlingQuotasCreateTopic=my-user-1131806865-947217230, testCapacityFile=my-user-937015144-1959439396, testCreatingUsersWithSecretPrefix=my-user-1450786684-1808961062}
2022-03-28 10:50:40 [ForkJoinPool-1-worker-5] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-1834944466-2094449929, testThrottlingQuotasCreateTopic=my-topic-728672490-1194703402, testCapacityFile=my-topic-1155641705-902584520, testCreatingUsersWithSecretPrefix=my-topic-1479520843-618891199}
2022-03-28 10:50:40 [ForkJoinPool-1-worker-5] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8-kafka-clients, testCapacityFile=my-cluster-fd0fb61a-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c-kafka-clients}
2022-03-28 10:50:40 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update KafkaTopic my-topic-1936097768-1459518187 in namespace http-bridge-scram-sha-st
2022-03-28 10:50:40 [ForkJoinPool-1-worker-5] WARN  [VersionUsageUtils:60] The client is using resource type 'kafkatopics' with unstable version 'v1beta2'
2022-03-28 10:50:40 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1936097768-1459518187
2022-03-28 10:50:40 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-1936097768-1459518187 will have desired state: Ready
2022-03-28 10:50:40 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-1936097768-1459518187 will have desired state: Ready
2022-03-28 10:50:40 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaTopic: my-topic-1936097768-1459518187 will have desired state: Ready not ready, will try again in 1000 ms (179978ms till timeout)
2022-03-28 10:50:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (275844ms till timeout)
2022-03-28 10:50:41 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (463048ms till timeout)
2022-03-28 10:50:41 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (814368ms till timeout)
2022-03-28 10:50:41 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1171647ms till timeout)
2022-03-28 10:50:41 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:444] KafkaTopic: my-topic-1936097768-1459518187 is in desired state: Ready
2022-03-28 10:50:41 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update Job consumer-1508550712 in namespace http-bridge-scram-sha-st
2022-03-28 10:50:41 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:50:41 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:consumer-1508550712
2022-03-28 10:50:41 [ForkJoinPool-1-worker-5] INFO  [JobUtils:81] Waiting for job: consumer-1508550712 will be in active state
2022-03-28 10:50:41 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 10:50:41 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job active not ready, will try again in 1000 ms (179968ms till timeout)
2022-03-28 10:50:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (274691ms till timeout)
2022-03-28 10:50:42 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (462015ms till timeout)
2022-03-28 10:50:42 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (813319ms till timeout)
2022-03-28 10:50:42 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1170567ms till timeout)
2022-03-28 10:50:43 [ForkJoinPool-1-worker-5] INFO  [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-03-28 10:50:43 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update Job producer-838381648 in namespace http-bridge-scram-sha-st
2022-03-28 10:50:43 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:producer-838381648
2022-03-28 10:50:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (273633ms till timeout)
2022-03-28 10:50:43 [ForkJoinPool-1-worker-5] INFO  [JobUtils:81] Waiting for job: producer-838381648 will be in active state
2022-03-28 10:50:43 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 10:50:43 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (460988ms till timeout)
2022-03-28 10:50:43 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] job active not ready, will try again in 1000 ms (179934ms till timeout)
2022-03-28 10:50:43 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (812276ms till timeout)
2022-03-28 10:50:43 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1169547ms till timeout)
2022-03-28 10:50:44 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (459886ms till timeout)
2022-03-28 10:50:44 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (811227ms till timeout)
2022-03-28 10:50:44 [ForkJoinPool-1-worker-5] INFO  [ClientUtils:61] Waiting till producer producer-838381648 and consumer consumer-1508550712 finish
2022-03-28 10:50:44 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for clients finished
2022-03-28 10:50:44 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (219967ms till timeout)
2022-03-28 10:50:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (272359ms till timeout)
2022-03-28 10:50:44 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1168462ms till timeout)
2022-03-28 10:50:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (458831ms till timeout)
2022-03-28 10:50:45 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (810139ms till timeout)
2022-03-28 10:50:45 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (218913ms till timeout)
2022-03-28 10:50:45 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1167408ms till timeout)
2022-03-28 10:50:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (271273ms till timeout)
2022-03-28 10:50:46 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (457776ms till timeout)
2022-03-28 10:50:46 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (809034ms till timeout)
2022-03-28 10:50:46 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (217765ms till timeout)
2022-03-28 10:50:46 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1166345ms till timeout)
2022-03-28 10:50:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (270182ms till timeout)
2022-03-28 10:50:46 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:50:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (456734ms till timeout)
2022-03-28 10:50:47 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (807978ms till timeout)
2022-03-28 10:50:47 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (216667ms till timeout)
2022-03-28 10:50:47 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1165252ms till timeout)
2022-03-28 10:50:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (269036ms till timeout)
2022-03-28 10:50:48 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:444] KafkaBridge: http-bridge-tls-cluster-name is in desired state: Ready
2022-03-28 10:50:48 [ForkJoinPool-1-worker-7] INFO  [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-03-28 10:50:48 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:23] ############################################################################
2022-03-28 10:50:48 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeTlsST.testSendSimpleMessageTls-STARTED
2022-03-28 10:50:48 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:658] ============================================================================
2022-03-28 10:50:48 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:659] [bridge.HttpBridgeTlsST - Before Each] - Setup test case environment
2022-03-28 10:50:48 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:77] [bridge.HttpBridgeTlsST] - Adding parallel test: testSendSimpleMessageTls
2022-03-28 10:50:48 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:81] [bridge.HttpBridgeTlsST] - Parallel test count: 5
2022-03-28 10:50:48 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:230] testSendSimpleMessageTls test now can proceed its execution
2022-03-28 10:50:48 [ForkJoinPool-1-worker-7] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 10:50:48 [ForkJoinPool-1-worker-7] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8, testCapacityFile=my-cluster-fd0fb61a, testSendSimpleMessageTls=my-cluster-27ddd6ba, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c}
2022-03-28 10:50:48 [ForkJoinPool-1-worker-7] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-1915916223-1265293973, testThrottlingQuotasCreateTopic=my-user-1131806865-947217230, testCapacityFile=my-user-937015144-1959439396, testSendSimpleMessageTls=my-user-738229993-1095121647, testCreatingUsersWithSecretPrefix=my-user-1450786684-1808961062}
2022-03-28 10:50:48 [ForkJoinPool-1-worker-7] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-1834944466-2094449929, testThrottlingQuotasCreateTopic=my-topic-728672490-1194703402, testCapacityFile=my-topic-1155641705-902584520, testSendSimpleMessageTls=my-topic-563023482-1697359353, testCreatingUsersWithSecretPrefix=my-topic-1479520843-618891199}
2022-03-28 10:50:48 [ForkJoinPool-1-worker-7] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8-kafka-clients, testCapacityFile=my-cluster-fd0fb61a-kafka-clients, testSendSimpleMessageTls=my-cluster-27ddd6ba-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c-kafka-clients}
2022-03-28 10:50:48 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update KafkaTopic my-topic-1526630576-133645002 in namespace http-bridge-tls-st
2022-03-28 10:50:48 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (806943ms till timeout)
2022-03-28 10:50:48 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1526630576-133645002
2022-03-28 10:50:48 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-1526630576-133645002 will have desired state: Ready
2022-03-28 10:50:48 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-1526630576-133645002 will have desired state: Ready
2022-03-28 10:50:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic: my-topic-1526630576-133645002 will have desired state: Ready not ready, will try again in 1000 ms (179962ms till timeout)
2022-03-28 10:50:48 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (215644ms till timeout)
2022-03-28 10:50:48 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1164227ms till timeout)
2022-03-28 10:50:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (267921ms till timeout)
2022-03-28 10:50:49 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (805909ms till timeout)
2022-03-28 10:50:49 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:444] KafkaTopic: my-topic-1526630576-133645002 is in desired state: Ready
2022-03-28 10:50:49 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update Job producer-3918978 in namespace http-bridge-tls-st
2022-03-28 10:50:49 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1163196ms till timeout)
2022-03-28 10:50:49 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (214598ms till timeout)
2022-03-28 10:50:49 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:producer-3918978
2022-03-28 10:50:49 [ForkJoinPool-1-worker-7] INFO  [JobUtils:81] Waiting for job: producer-3918978 will be in active state
2022-03-28 10:50:49 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 10:50:49 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job active not ready, will try again in 1000 ms (179964ms till timeout)
2022-03-28 10:50:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (266743ms till timeout)
2022-03-28 10:50:50 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (804825ms till timeout)
2022-03-28 10:50:50 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (213579ms till timeout)
2022-03-28 10:50:50 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1162171ms till timeout)
2022-03-28 10:50:51 [ForkJoinPool-1-worker-7] INFO  [ClientUtils:76] Waiting for producer/consumer:producer-3918978 to finished
2022-03-28 10:50:51 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 10:50:51 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job producer-3918978 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:50:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:50:51 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (219949ms till timeout)
2022-03-28 10:50:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (265619ms till timeout)
2022-03-28 10:50:51 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (803782ms till timeout)
2022-03-28 10:50:51 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (212544ms till timeout)
2022-03-28 10:50:51 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:50:51 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1161135ms till timeout)
2022-03-28 10:50:52 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job producer-3918978 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:50:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:50:52 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (218888ms till timeout)
2022-03-28 10:50:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (264503ms till timeout)
2022-03-28 10:50:52 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (802735ms till timeout)
2022-03-28 10:50:52 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1160091ms till timeout)
2022-03-28 10:50:52 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (211440ms till timeout)
2022-03-28 10:50:53 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job producer-3918978 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:50:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:50:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (217736ms till timeout)
2022-03-28 10:50:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (263370ms till timeout)
2022-03-28 10:50:53 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1158991ms till timeout)
2022-03-28 10:50:53 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (801619ms till timeout)
2022-03-28 10:50:54 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (210279ms till timeout)
2022-03-28 10:50:54 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job producer-3918978 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:50:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:50:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (216505ms till timeout)
2022-03-28 10:50:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (262229ms till timeout)
2022-03-28 10:50:55 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (800561ms till timeout)
2022-03-28 10:50:55 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1157923ms till timeout)
2022-03-28 10:50:55 [ForkJoinPool-1-worker-5] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment producer-838381648 deletion
2022-03-28 10:50:55 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for ReplicaSet producer-838381648 to be deleted
2022-03-28 10:50:55 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] ReplicaSet producer-838381648 to be deleted not ready, will try again in 5000 ms (179953ms till timeout)
2022-03-28 10:50:55 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job producer-3918978 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:50:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:50:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (215389ms till timeout)
2022-03-28 10:50:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (261117ms till timeout)
2022-03-28 10:50:56 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (799482ms till timeout)
2022-03-28 10:50:56 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-fd0fb61a will have desired state: Ready not ready, will try again in 1000 ms (1156827ms till timeout)
2022-03-28 10:50:56 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job producer-3918978 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:50:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:50:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (214303ms till timeout)
2022-03-28 10:50:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (260013ms till timeout)
2022-03-28 10:50:56 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:50:57 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (798416ms till timeout)
2022-03-28 10:50:57 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:444] Kafka: my-cluster-fd0fb61a is in desired state: Ready
2022-03-28 10:50:57 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-0 exec my-cluster-fd0fb61a-cruise-control-54945c676d-hzgkq -- /bin/bash -c cat /tmp/capacity.json
2022-03-28 10:50:57 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job producer-3918978 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:50:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:50:57 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (213160ms till timeout)
2022-03-28 10:50:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (258925ms till timeout)
2022-03-28 10:50:58 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (797329ms till timeout)
2022-03-28 10:50:58 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-0 exec my-cluster-fd0fb61a-cruise-control-54945c676d-hzgkq -- /bin/bash -c cat /tmp/capacity.json
2022-03-28 10:50:58 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 10:50:58 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:23] ############################################################################
2022-03-28 10:50:58 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeScramShaST.testSendSimpleMessageTlsScramSha-STARTED
2022-03-28 10:50:58 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:658] ============================================================================
2022-03-28 10:50:58 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:659] [bridge.HttpBridgeScramShaST - Before Each] - Setup test case environment
2022-03-28 10:50:58 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:77] [bridge.HttpBridgeScramShaST] - Adding parallel test: testSendSimpleMessageTlsScramSha
2022-03-28 10:50:58 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:81] [bridge.HttpBridgeScramShaST] - Parallel test count: 6
2022-03-28 10:50:58 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:230] testSendSimpleMessageTlsScramSha test now can proceed its execution
2022-03-28 10:50:58 [ForkJoinPool-1-worker-1] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 10:50:58 [ForkJoinPool-1-worker-1] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8, testCapacityFile=my-cluster-fd0fb61a, testSendSimpleMessageTls=my-cluster-27ddd6ba, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5}
2022-03-28 10:50:58 [ForkJoinPool-1-worker-1] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-1915916223-1265293973, testThrottlingQuotasCreateTopic=my-user-1131806865-947217230, testCapacityFile=my-user-937015144-1959439396, testSendSimpleMessageTls=my-user-738229993-1095121647, testCreatingUsersWithSecretPrefix=my-user-1450786684-1808961062, testSendSimpleMessageTlsScramSha=my-user-1499386234-1249154328}
2022-03-28 10:50:58 [ForkJoinPool-1-worker-1] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-1834944466-2094449929, testThrottlingQuotasCreateTopic=my-topic-728672490-1194703402, testCapacityFile=my-topic-1155641705-902584520, testSendSimpleMessageTls=my-topic-563023482-1697359353, testCreatingUsersWithSecretPrefix=my-topic-1479520843-618891199, testSendSimpleMessageTlsScramSha=my-topic-1028951580-1656411053}
2022-03-28 10:50:58 [ForkJoinPool-1-worker-1] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8-kafka-clients, testCapacityFile=my-cluster-fd0fb61a-kafka-clients, testSendSimpleMessageTls=my-cluster-27ddd6ba-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5-kafka-clients}
2022-03-28 10:50:58 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:155] Create/Update KafkaTopic my-topic-1457742994-2068824825 in namespace http-bridge-scram-sha-st
2022-03-28 10:50:58 [ForkJoinPool-1-worker-3] INFO  [CruiseControlConfigurationST:80] We got only one configuration of broker-capacities
2022-03-28 10:50:58 [ForkJoinPool-1-worker-3] INFO  [CruiseControlConfigurationST:83] Verifying cruise control configuration.
2022-03-28 10:50:58 [ForkJoinPool-1-worker-3] INFO  [CruiseControlConfigurationST:92] Verifying default cruise control capacities
2022-03-28 10:50:58 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:674] ============================================================================
2022-03-28 10:50:58 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:675] [cruisecontrol.CruiseControlConfigurationST - After Each] - Clean up after test
2022-03-28 10:50:58 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:344] ############################################################################
2022-03-28 10:50:58 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:348] Delete all resources for testCapacityFile
2022-03-28 10:50:58 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:241] Delete of Kafka my-cluster-fd0fb61a in namespace namespace-0
2022-03-28 10:50:58 [ForkJoinPool-1-worker-3] INFO  [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-0, for cruise control Kafka cluster my-cluster-fd0fb61a
2022-03-28 10:50:58 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1457742994-2068824825
2022-03-28 10:50:58 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job producer-3918978 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:50:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:50:58 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-1457742994-2068824825 will have desired state: Ready
2022-03-28 10:50:58 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-1457742994-2068824825 will have desired state: Ready
2022-03-28 10:50:58 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (212060ms till timeout)
2022-03-28 10:50:59 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] KafkaTopic: my-topic-1457742994-2068824825 will have desired state: Ready not ready, will try again in 1000 ms (179969ms till timeout)
2022-03-28 10:50:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (257811ms till timeout)
2022-03-28 10:50:59 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (796240ms till timeout)
2022-03-28 10:50:59 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-fd0fb61a
2022-03-28 10:50:59 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-fd0fb61a not ready, will try again in 10000 ms (839925ms till timeout)
2022-03-28 10:51:00 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job producer-3918978 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:50:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:51:00 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:444] KafkaTopic: my-topic-1457742994-2068824825 is in desired state: Ready
2022-03-28 10:51:00 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:155] Create/Update Job producer-1468544431 in namespace http-bridge-scram-sha-st
2022-03-28 10:51:00 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (210814ms till timeout)
2022-03-28 10:51:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (256582ms till timeout)
2022-03-28 10:51:00 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:producer-1468544431
2022-03-28 10:51:00 [ForkJoinPool-1-worker-1] INFO  [JobUtils:81] Waiting for job: producer-1468544431 will be in active state
2022-03-28 10:51:00 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 10:51:00 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (795101ms till timeout)
2022-03-28 10:51:00 [ForkJoinPool-1-worker-5] DEBUG [JobUtils:40] Job producer-838381648 was deleted
2022-03-28 10:51:00 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job active not ready, will try again in 1000 ms (179871ms till timeout)
2022-03-28 10:51:00 [ForkJoinPool-1-worker-5] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment consumer-1508550712 deletion
2022-03-28 10:51:00 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for ReplicaSet consumer-1508550712 to be deleted
2022-03-28 10:51:01 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] ReplicaSet consumer-1508550712 to be deleted not ready, will try again in 5000 ms (179878ms till timeout)
2022-03-28 10:51:01 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job producer-3918978 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:50:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:51:01 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (209509ms till timeout)
2022-03-28 10:51:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (255253ms till timeout)
2022-03-28 10:51:01 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (794012ms till timeout)
2022-03-28 10:51:01 [ForkJoinPool-1-worker-1] INFO  [ClientUtils:76] Waiting for producer/consumer:producer-1468544431 to finished
2022-03-28 10:51:01 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 10:51:01 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:51:01 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job producer-1468544431 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:50:57Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:51:02 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (219722ms till timeout)
2022-03-28 10:51:02 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (792905ms till timeout)
2022-03-28 10:51:02 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job producer-3918978 in namespace http-bridge-tls-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T10:50:58Z, conditions=[JobCondition(lastProbeTime=2022-03-28T10:50:58Z, lastTransitionTime=2022-03-28T10:50:58Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T10:50:47Z, succeeded=1, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:51:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (254023ms till timeout)
2022-03-28 10:51:03 [ForkJoinPool-1-worker-7] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment producer-3918978 deletion
2022-03-28 10:51:03 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for ReplicaSet producer-3918978 to be deleted
2022-03-28 10:51:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] ReplicaSet producer-3918978 to be deleted not ready, will try again in 5000 ms (179922ms till timeout)
2022-03-28 10:51:03 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job producer-1468544431 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:50:57Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:51:03 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (218356ms till timeout)
2022-03-28 10:51:03 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (791821ms till timeout)
2022-03-28 10:51:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (252863ms till timeout)
2022-03-28 10:51:04 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job producer-1468544431 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:50:57Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:51:04 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (217099ms till timeout)
2022-03-28 10:51:04 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (790795ms till timeout)
2022-03-28 10:51:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (251720ms till timeout)
2022-03-28 10:51:05 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job producer-1468544431 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:50:57Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:51:05 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (215954ms till timeout)
2022-03-28 10:51:05 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (789697ms till timeout)
2022-03-28 10:51:06 [ForkJoinPool-1-worker-5] DEBUG [JobUtils:40] Job consumer-1508550712 was deleted
2022-03-28 10:51:06 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:674] ============================================================================
2022-03-28 10:51:06 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:675] [bridge.HttpBridgeScramShaST - After Each] - Clean up after test
2022-03-28 10:51:06 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:344] ############################################################################
2022-03-28 10:51:06 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:348] Delete all resources for testReceiveSimpleMessageTlsScramSha
2022-03-28 10:51:06 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of Job consumer-1508550712 in namespace http-bridge-scram-sha-st
2022-03-28 10:51:06 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:consumer-1508550712
2022-03-28 10:51:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (250517ms till timeout)
2022-03-28 10:51:06 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of Job producer-838381648 in namespace http-bridge-scram-sha-st
2022-03-28 10:51:06 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:producer-838381648
2022-03-28 10:51:06 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of KafkaTopic my-topic-1936097768-1459518187 in namespace http-bridge-scram-sha-st
2022-03-28 10:51:06 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1936097768-1459518187
2022-03-28 10:51:06 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1936097768-1459518187 not ready, will try again in 10000 ms (179871ms till timeout)
2022-03-28 10:51:06 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:51:06 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job producer-1468544431 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:50:57Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:51:06 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (214836ms till timeout)
2022-03-28 10:51:06 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (788670ms till timeout)
2022-03-28 10:51:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (249448ms till timeout)
2022-03-28 10:51:07 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job producer-1468544431 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:50:57Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:51:08 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (213715ms till timeout)
2022-03-28 10:51:08 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (787557ms till timeout)
2022-03-28 10:51:08 [ForkJoinPool-1-worker-7] DEBUG [JobUtils:40] Job producer-3918978 was deleted
2022-03-28 10:51:08 [ForkJoinPool-1-worker-7] INFO  [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-03-28 10:51:08 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update Job consumer-1035607454 in namespace http-bridge-tls-st
2022-03-28 10:51:08 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:consumer-1035607454
2022-03-28 10:51:08 [ForkJoinPool-1-worker-7] INFO  [JobUtils:81] Waiting for job: consumer-1035607454 will be in active state
2022-03-28 10:51:08 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 10:51:08 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job active not ready, will try again in 1000 ms (179975ms till timeout)
2022-03-28 10:51:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (248373ms till timeout)
2022-03-28 10:51:09 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (786511ms till timeout)
2022-03-28 10:51:09 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job producer-1468544431 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:50:57Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:51:09 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (212632ms till timeout)
2022-03-28 10:51:09 [ForkJoinPool-1-worker-7] INFO  [ClientUtils:76] Waiting for producer/consumer:consumer-1035607454 to finished
2022-03-28 10:51:09 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 10:51:09 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job consumer-1035607454 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:51:05Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:51:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (219948ms till timeout)
2022-03-28 10:51:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (247319ms till timeout)
2022-03-28 10:51:09 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:369] ############################################################################
2022-03-28 10:51:09 [ForkJoinPool-1-worker-3] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-0 for test case:testCapacityFile
2022-03-28 10:51:09 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Namespace namespace-0 removal
2022-03-28 10:51:09 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:10 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:10 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:51:10 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (479663ms till timeout)
2022-03-28 10:51:10 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (785487ms till timeout)
2022-03-28 10:51:10 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job producer-1468544431 in namespace http-bridge-scram-sha-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T10:51:06Z, conditions=[JobCondition(lastProbeTime=2022-03-28T10:51:06Z, lastTransitionTime=2022-03-28T10:51:06Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T10:50:57Z, succeeded=1, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:51:10 [ForkJoinPool-1-worker-1] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment producer-1468544431 deletion
2022-03-28 10:51:10 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for ReplicaSet producer-1468544431 to be deleted
2022-03-28 10:51:10 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] ReplicaSet producer-1468544431 to be deleted not ready, will try again in 5000 ms (179943ms till timeout)
2022-03-28 10:51:10 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job consumer-1035607454 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:51:05Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:51:10 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (218887ms till timeout)
2022-03-28 10:51:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (246262ms till timeout)
2022-03-28 10:51:11 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:11 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (784433ms till timeout)
2022-03-28 10:51:11 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job consumer-1035607454 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:51:05Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:51:11 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (217846ms till timeout)
2022-03-28 10:51:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (245166ms till timeout)
2022-03-28 10:51:11 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:51:12 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (783387ms till timeout)
2022-03-28 10:51:12 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job consumer-1035607454 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:51:05Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:51:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (216759ms till timeout)
2022-03-28 10:51:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (244002ms till timeout)
2022-03-28 10:51:13 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (782278ms till timeout)
2022-03-28 10:51:13 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job consumer-1035607454 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:51:05Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:51:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (215673ms till timeout)
2022-03-28 10:51:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (242882ms till timeout)
2022-03-28 10:51:14 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (781215ms till timeout)
2022-03-28 10:51:14 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job consumer-1035607454 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:51:05Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:51:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (214505ms till timeout)
2022-03-28 10:51:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (241771ms till timeout)
2022-03-28 10:51:15 [ForkJoinPool-1-worker-1] DEBUG [JobUtils:40] Job producer-1468544431 was deleted
2022-03-28 10:51:15 [ForkJoinPool-1-worker-1] INFO  [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-03-28 10:51:15 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (780181ms till timeout)
2022-03-28 10:51:15 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:155] Create/Update Job consumer-1966473481 in namespace http-bridge-scram-sha-st
2022-03-28 10:51:15 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:consumer-1966473481
2022-03-28 10:51:15 [ForkJoinPool-1-worker-1] INFO  [JobUtils:81] Waiting for job: consumer-1966473481 will be in active state
2022-03-28 10:51:15 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 10:51:15 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job active not ready, will try again in 1000 ms (179950ms till timeout)
2022-03-28 10:51:15 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job consumer-1035607454 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:51:05Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:51:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (213360ms till timeout)
2022-03-28 10:51:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (240653ms till timeout)
2022-03-28 10:51:16 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:16 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:51:16 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (473242ms till timeout)
2022-03-28 10:51:16 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (779038ms till timeout)
2022-03-28 10:51:16 [ForkJoinPool-1-worker-1] INFO  [ClientUtils:76] Waiting for producer/consumer:consumer-1966473481 to finished
2022-03-28 10:51:16 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 10:51:16 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job consumer-1966473481 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:51:13Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:51:16 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:369] ############################################################################
2022-03-28 10:51:16 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:267] testReceiveSimpleMessageTlsScramSha - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTls, testSendSimpleMessageTlsScramSha] to and randomly select one to start execution
2022-03-28 10:51:16 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:93] [bridge.HttpBridgeScramShaST] - Removing parallel test: testReceiveSimpleMessageTlsScramSha
2022-03-28 10:51:16 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:97] [bridge.HttpBridgeScramShaST] - Parallel test count: 5
2022-03-28 10:51:16 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeScramShaST.testReceiveSimpleMessageTlsScramSha-FINISHED
2022-03-28 10:51:16 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:30] ############################################################################
2022-03-28 10:51:16 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (219903ms till timeout)
2022-03-28 10:51:16 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:51:17 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job consumer-1035607454 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:51:05Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:51:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (212194ms till timeout)
2022-03-28 10:51:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (239556ms till timeout)
2022-03-28 10:51:17 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:17 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (778005ms till timeout)
2022-03-28 10:51:17 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job consumer-1966473481 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:51:13Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:51:17 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (218809ms till timeout)
2022-03-28 10:51:18 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job consumer-1035607454 in namespace http-bridge-tls-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T10:51:15Z, conditions=[JobCondition(lastProbeTime=2022-03-28T10:51:15Z, lastTransitionTime=2022-03-28T10:51:15Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T10:51:05Z, succeeded=1, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:51:18 [ForkJoinPool-1-worker-7] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment consumer-1035607454 deletion
2022-03-28 10:51:18 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for ReplicaSet consumer-1035607454 to be deleted
2022-03-28 10:51:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (238485ms till timeout)
2022-03-28 10:51:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] ReplicaSet consumer-1035607454 to be deleted not ready, will try again in 5000 ms (179946ms till timeout)
2022-03-28 10:51:18 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (776969ms till timeout)
2022-03-28 10:51:18 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job consumer-1966473481 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:51:13Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:51:19 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (217726ms till timeout)
2022-03-28 10:51:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (237391ms till timeout)
2022-03-28 10:51:19 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (775884ms till timeout)
2022-03-28 10:51:20 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job consumer-1966473481 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:51:13Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:51:20 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (216668ms till timeout)
2022-03-28 10:51:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (236260ms till timeout)
2022-03-28 10:51:20 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (774820ms till timeout)
2022-03-28 10:51:21 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job consumer-1966473481 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:51:13Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:51:21 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (215571ms till timeout)
2022-03-28 10:51:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (235239ms till timeout)
2022-03-28 10:51:21 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (773795ms till timeout)
2022-03-28 10:51:21 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:51:22 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job consumer-1966473481 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:51:13Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:51:22 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (214491ms till timeout)
2022-03-28 10:51:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (234151ms till timeout)
2022-03-28 10:51:22 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:22 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:51:22 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (466897ms till timeout)
2022-03-28 10:51:22 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (772742ms till timeout)
2022-03-28 10:51:23 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job consumer-1966473481 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:51:13Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:51:23 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (213466ms till timeout)
2022-03-28 10:51:23 [ForkJoinPool-1-worker-7] DEBUG [JobUtils:40] Job consumer-1035607454 was deleted
2022-03-28 10:51:23 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:674] ============================================================================
2022-03-28 10:51:23 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:675] [bridge.HttpBridgeTlsST - After Each] - Clean up after test
2022-03-28 10:51:23 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:344] ############################################################################
2022-03-28 10:51:23 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:348] Delete all resources for testSendSimpleMessageTls
2022-03-28 10:51:23 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of Job producer-3918978 in namespace http-bridge-tls-st
2022-03-28 10:51:23 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:producer-3918978
2022-03-28 10:51:23 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of Job consumer-1035607454 in namespace http-bridge-tls-st
2022-03-28 10:51:23 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:consumer-1035607454
2022-03-28 10:51:23 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of KafkaTopic my-topic-1526630576-133645002 in namespace http-bridge-tls-st
2022-03-28 10:51:23 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1526630576-133645002
2022-03-28 10:51:23 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (233054ms till timeout)
2022-03-28 10:51:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1526630576-133645002 not ready, will try again in 10000 ms (179801ms till timeout)
2022-03-28 10:51:23 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (771722ms till timeout)
2022-03-28 10:51:24 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:24 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:51:24 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (465585ms till timeout)
2022-03-28 10:51:24 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job consumer-1966473481 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:51:13Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:51:24 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (212334ms till timeout)
2022-03-28 10:51:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (231953ms till timeout)
2022-03-28 10:51:24 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (770672ms till timeout)
2022-03-28 10:51:25 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:25 [ForkJoinPool-1-worker-1] DEBUG [ClientUtils:79] Job consumer-1966473481 in namespace http-bridge-scram-sha-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T10:51:22Z, conditions=[JobCondition(lastProbeTime=2022-03-28T10:51:22Z, lastTransitionTime=2022-03-28T10:51:22Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T10:51:13Z, succeeded=1, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:51:25 [ForkJoinPool-1-worker-1] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment consumer-1966473481 deletion
2022-03-28 10:51:25 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for ReplicaSet consumer-1966473481 to be deleted
2022-03-28 10:51:25 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] ReplicaSet consumer-1966473481 to be deleted not ready, will try again in 5000 ms (179882ms till timeout)
2022-03-28 10:51:26 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (769607ms till timeout)
2022-03-28 10:51:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (230792ms till timeout)
2022-03-28 10:51:26 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:51:27 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (768556ms till timeout)
2022-03-28 10:51:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (229680ms till timeout)
2022-03-28 10:51:28 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (767508ms till timeout)
2022-03-28 10:51:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (228591ms till timeout)
2022-03-28 10:51:29 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (766477ms till timeout)
2022-03-28 10:51:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (227517ms till timeout)
2022-03-28 10:51:30 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (765435ms till timeout)
2022-03-28 10:51:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (226455ms till timeout)
2022-03-28 10:51:30 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:30 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:51:30 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (459220ms till timeout)
2022-03-28 10:51:31 [ForkJoinPool-1-worker-1] DEBUG [JobUtils:40] Job consumer-1966473481 was deleted
2022-03-28 10:51:31 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:674] ============================================================================
2022-03-28 10:51:31 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:675] [bridge.HttpBridgeScramShaST - After Each] - Clean up after test
2022-03-28 10:51:31 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:344] ############################################################################
2022-03-28 10:51:31 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:348] Delete all resources for testSendSimpleMessageTlsScramSha
2022-03-28 10:51:31 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:241] Delete of Job producer-1468544431 in namespace http-bridge-scram-sha-st
2022-03-28 10:51:31 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:producer-1468544431
2022-03-28 10:51:31 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:241] Delete of Job consumer-1966473481 in namespace http-bridge-scram-sha-st
2022-03-28 10:51:31 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:consumer-1966473481
2022-03-28 10:51:31 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:241] Delete of KafkaTopic my-topic-1457742994-2068824825 in namespace http-bridge-scram-sha-st
2022-03-28 10:51:31 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (764387ms till timeout)
2022-03-28 10:51:31 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1457742994-2068824825
2022-03-28 10:51:31 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1457742994-2068824825 not ready, will try again in 10000 ms (179893ms till timeout)
2022-03-28 10:51:31 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (225358ms till timeout)
2022-03-28 10:51:31 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:31 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:51:31 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (457904ms till timeout)
2022-03-28 10:51:31 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:51:32 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (763369ms till timeout)
2022-03-28 10:51:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (224260ms till timeout)
2022-03-28 10:51:32 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:33 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:33 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:51:33 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (456559ms till timeout)
2022-03-28 10:51:33 [ForkJoinPool-1-worker-17] INFO  [TestSeparator:23] ############################################################################
2022-03-28 10:51:33 [ForkJoinPool-1-worker-17] INFO  [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationFileIsCreated-STARTED
2022-03-28 10:51:33 [ForkJoinPool-1-worker-17] DEBUG [AbstractST:658] ============================================================================
2022-03-28 10:51:33 [ForkJoinPool-1-worker-17] DEBUG [AbstractST:659] [cruisecontrol.CruiseControlConfigurationST - Before Each] - Setup test case environment
2022-03-28 10:51:33 [ForkJoinPool-1-worker-17] DEBUG [SuiteThreadController:77] [cruisecontrol.CruiseControlConfigurationST] - Adding parallel test: testConfigurationFileIsCreated
2022-03-28 10:51:33 [ForkJoinPool-1-worker-17] DEBUG [SuiteThreadController:81] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 6
2022-03-28 10:51:33 [ForkJoinPool-1-worker-17] DEBUG [SuiteThreadController:230] testConfigurationFileIsCreated test now can proceed its execution
2022-03-28 10:51:33 [ForkJoinPool-1-worker-17] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 10:51:33 [ForkJoinPool-1-worker-17] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383, testConfigurationFileIsCreated=my-cluster-e0fac774, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8, testCapacityFile=my-cluster-fd0fb61a, testSendSimpleMessageTls=my-cluster-27ddd6ba, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5}
2022-03-28 10:51:33 [ForkJoinPool-1-worker-17] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-1915916223-1265293973, testConfigurationFileIsCreated=my-user-180611180-50530491, testThrottlingQuotasCreateTopic=my-user-1131806865-947217230, testCapacityFile=my-user-937015144-1959439396, testSendSimpleMessageTls=my-user-738229993-1095121647, testCreatingUsersWithSecretPrefix=my-user-1450786684-1808961062, testSendSimpleMessageTlsScramSha=my-user-1499386234-1249154328}
2022-03-28 10:51:33 [ForkJoinPool-1-worker-17] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-1834944466-2094449929, testConfigurationFileIsCreated=my-topic-1296407588-313880708, testThrottlingQuotasCreateTopic=my-topic-728672490-1194703402, testCapacityFile=my-topic-1155641705-902584520, testSendSimpleMessageTls=my-topic-563023482-1697359353, testCreatingUsersWithSecretPrefix=my-topic-1479520843-618891199, testSendSimpleMessageTlsScramSha=my-topic-1028951580-1656411053}
2022-03-28 10:51:33 [ForkJoinPool-1-worker-17] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383-kafka-clients, testConfigurationFileIsCreated=my-cluster-e0fac774-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8-kafka-clients, testCapacityFile=my-cluster-fd0fb61a-kafka-clients, testSendSimpleMessageTls=my-cluster-27ddd6ba-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5-kafka-clients}
2022-03-28 10:51:33 [ForkJoinPool-1-worker-17] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-2 for test case:testConfigurationFileIsCreated
2022-03-28 10:51:33 [ForkJoinPool-1-worker-17] INFO  [KubeClusterResource:156] Creating Namespace: namespace-2
2022-03-28 10:51:33 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for Namespace namespace-2
2022-03-28 10:51:33 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-2 -o json
2022-03-28 10:51:33 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (762272ms till timeout)
2022-03-28 10:51:33 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-2 -o json
2022-03-28 10:51:33 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 10:51:33 [ForkJoinPool-1-worker-17] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c26,c25",
            "openshift.io/sa.scc.supplemental-groups": "1000700000/10000",
            "openshift.io/sa.scc.uid-range": "1000700000/10000"
        },
        "creationTimestamp": "2022-03-28T10:51:30Z",
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T10:51:00Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:status": {
                        "f:phase": {}
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T10:51:30Z"
            }
        ],
        "name": "namespace-2",
        "resourceVersion": "1867554",
        "selfLink": "/api/v1/namespaces/namespace-2",
        "uid": "4c821595-119a-4aac-a5a1-28b83ca69b80"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 10:51:33 [ForkJoinPool-1-worker-17] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[namespace-1], io.strimzi.test.logs.CollectorElement@3e255cd9=[namespace-0], io.strimzi.test.logs.CollectorElement@aec142ef=[namespace-2]}
2022-03-28 10:51:33 [ForkJoinPool-1-worker-17] INFO  [KubeClusterResource:82] Client use Namespace: namespace-2
2022-03-28 10:51:33 [ForkJoinPool-1-worker-17] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-2, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 10:51:33 [ForkJoinPool-1-worker-17] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-2
2022-03-28 10:51:33 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-e0fac774 in namespace namespace-2
2022-03-28 10:51:33 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:164] Using Namespace: namespace-2
2022-03-28 10:51:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (222927ms till timeout)
2022-03-28 10:51:34 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-e0fac774
2022-03-28 10:51:34 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:369] ############################################################################
2022-03-28 10:51:34 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:267] testSendSimpleMessageTls - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTls, testSendSimpleMessageTlsScramSha, testConfigurationFileIsCreated] to and randomly select one to start execution
2022-03-28 10:51:34 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:93] [bridge.HttpBridgeTlsST] - Removing parallel test: testSendSimpleMessageTls
2022-03-28 10:51:34 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:97] [bridge.HttpBridgeTlsST] - Parallel test count: 5
2022-03-28 10:51:34 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeTlsST.testSendSimpleMessageTls-FINISHED
2022-03-28 10:51:34 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:30] ############################################################################
2022-03-28 10:51:34 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:23] ############################################################################
2022-03-28 10:51:34 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeTlsST.testReceiveSimpleMessageTls-STARTED
2022-03-28 10:51:34 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:658] ============================================================================
2022-03-28 10:51:34 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:659] [bridge.HttpBridgeTlsST - Before Each] - Setup test case environment
2022-03-28 10:51:34 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:77] [bridge.HttpBridgeTlsST] - Adding parallel test: testReceiveSimpleMessageTls
2022-03-28 10:51:34 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:81] [bridge.HttpBridgeTlsST] - Parallel test count: 6
2022-03-28 10:51:34 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:230] testReceiveSimpleMessageTls test now can proceed its execution
2022-03-28 10:51:34 [ForkJoinPool-1-worker-7] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 10:51:34 [ForkJoinPool-1-worker-7] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383, testConfigurationFileIsCreated=my-cluster-e0fac774, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8, testCapacityFile=my-cluster-fd0fb61a, testSendSimpleMessageTls=my-cluster-27ddd6ba, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c, testReceiveSimpleMessageTls=my-cluster-c6a15963, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5}
2022-03-28 10:51:34 [ForkJoinPool-1-worker-7] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-1915916223-1265293973, testConfigurationFileIsCreated=my-user-180611180-50530491, testThrottlingQuotasCreateTopic=my-user-1131806865-947217230, testCapacityFile=my-user-937015144-1959439396, testSendSimpleMessageTls=my-user-738229993-1095121647, testCreatingUsersWithSecretPrefix=my-user-1450786684-1808961062, testReceiveSimpleMessageTls=my-user-6088058-1142028780, testSendSimpleMessageTlsScramSha=my-user-1499386234-1249154328}
2022-03-28 10:51:34 [ForkJoinPool-1-worker-7] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-1834944466-2094449929, testConfigurationFileIsCreated=my-topic-1296407588-313880708, testThrottlingQuotasCreateTopic=my-topic-728672490-1194703402, testCapacityFile=my-topic-1155641705-902584520, testSendSimpleMessageTls=my-topic-563023482-1697359353, testCreatingUsersWithSecretPrefix=my-topic-1479520843-618891199, testReceiveSimpleMessageTls=my-topic-478514536-2132782608, testSendSimpleMessageTlsScramSha=my-topic-1028951580-1656411053}
2022-03-28 10:51:34 [ForkJoinPool-1-worker-7] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383-kafka-clients, testConfigurationFileIsCreated=my-cluster-e0fac774-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8-kafka-clients, testCapacityFile=my-cluster-fd0fb61a-kafka-clients, testSendSimpleMessageTls=my-cluster-27ddd6ba-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c-kafka-clients, testReceiveSimpleMessageTls=my-cluster-c6a15963-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5-kafka-clients}
2022-03-28 10:51:34 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update KafkaTopic my-topic-819761185-2131351185 in namespace http-bridge-tls-st
2022-03-28 10:51:34 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:34 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-e0fac774 will have desired state: Ready
2022-03-28 10:51:34 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-e0fac774 will have desired state: Ready
2022-03-28 10:51:34 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-819761185-2131351185
2022-03-28 10:51:34 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1319908ms till timeout)
2022-03-28 10:51:34 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-819761185-2131351185 will have desired state: Ready
2022-03-28 10:51:34 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-819761185-2131351185 will have desired state: Ready
2022-03-28 10:51:34 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic: my-topic-819761185-2131351185 will have desired state: Ready not ready, will try again in 1000 ms (179966ms till timeout)
2022-03-28 10:51:34 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (761200ms till timeout)
2022-03-28 10:51:34 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:34 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:51:34 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (455180ms till timeout)
2022-03-28 10:51:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (221647ms till timeout)
2022-03-28 10:51:35 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1318851ms till timeout)
2022-03-28 10:51:35 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:444] KafkaTopic: my-topic-819761185-2131351185 is in desired state: Ready
2022-03-28 10:51:35 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update Job consumer-197971465 in namespace http-bridge-tls-st
2022-03-28 10:51:35 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:35 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:consumer-197971465
2022-03-28 10:51:35 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (760105ms till timeout)
2022-03-28 10:51:35 [ForkJoinPool-1-worker-7] INFO  [JobUtils:81] Waiting for job: consumer-197971465 will be in active state
2022-03-28 10:51:35 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 10:51:35 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job active not ready, will try again in 1000 ms (179959ms till timeout)
2022-03-28 10:51:35 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:35 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:51:35 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (453810ms till timeout)
2022-03-28 10:51:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (220564ms till timeout)
2022-03-28 10:51:36 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1317785ms till timeout)
2022-03-28 10:51:36 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (759082ms till timeout)
2022-03-28 10:51:36 [ForkJoinPool-1-worker-7] INFO  [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-03-28 10:51:36 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update Job producer-1990229902 in namespace http-bridge-tls-st
2022-03-28 10:51:36 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:producer-1990229902
2022-03-28 10:51:36 [ForkJoinPool-1-worker-7] INFO  [JobUtils:81] Waiting for job: producer-1990229902 will be in active state
2022-03-28 10:51:36 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 10:51:36 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job active not ready, will try again in 1000 ms (179950ms till timeout)
2022-03-28 10:51:36 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:51:36 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:37 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:37 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:51:37 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (452578ms till timeout)
2022-03-28 10:51:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (219422ms till timeout)
2022-03-28 10:51:37 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1316744ms till timeout)
2022-03-28 10:51:37 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (758045ms till timeout)
2022-03-28 10:51:37 [ForkJoinPool-1-worker-7] INFO  [ClientUtils:61] Waiting till producer producer-1990229902 and consumer consumer-197971465 finish
2022-03-28 10:51:37 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for clients finished
2022-03-28 10:51:37 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (219927ms till timeout)
2022-03-28 10:51:38 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:38 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:38 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:51:38 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (451346ms till timeout)
2022-03-28 10:51:38 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1315683ms till timeout)
2022-03-28 10:51:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (218267ms till timeout)
2022-03-28 10:51:38 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (757011ms till timeout)
2022-03-28 10:51:38 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (218797ms till timeout)
2022-03-28 10:51:39 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:39 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1314610ms till timeout)
2022-03-28 10:51:39 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:39 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:51:39 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (450052ms till timeout)
2022-03-28 10:51:39 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (755947ms till timeout)
2022-03-28 10:51:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (217142ms till timeout)
2022-03-28 10:51:40 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (217743ms till timeout)
2022-03-28 10:51:40 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:40 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1313532ms till timeout)
2022-03-28 10:51:40 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (754842ms till timeout)
2022-03-28 10:51:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (215990ms till timeout)
2022-03-28 10:51:41 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:41 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:51:41 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (448599ms till timeout)
2022-03-28 10:51:41 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (216704ms till timeout)
2022-03-28 10:51:41 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:369] ############################################################################
2022-03-28 10:51:41 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:267] testSendSimpleMessageTlsScramSha - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTls, testSendSimpleMessageTlsScramSha, testConfigurationFileIsCreated, testReceiveSimpleMessageTls] to and randomly select one to start execution
2022-03-28 10:51:41 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:93] [bridge.HttpBridgeScramShaST] - Removing parallel test: testSendSimpleMessageTlsScramSha
2022-03-28 10:51:41 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:97] [bridge.HttpBridgeScramShaST] - Parallel test count: 5
2022-03-28 10:51:41 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeScramShaST.testSendSimpleMessageTlsScramSha-FINISHED
2022-03-28 10:51:41 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:30] ############################################################################
2022-03-28 10:51:41 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:23] ############################################################################
2022-03-28 10:51:41 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testDeployAndUnDeployCruiseControl-STARTED
2022-03-28 10:51:41 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:658] ============================================================================
2022-03-28 10:51:41 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:659] [cruisecontrol.CruiseControlConfigurationST - Before Each] - Setup test case environment
2022-03-28 10:51:41 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:77] [cruisecontrol.CruiseControlConfigurationST] - Adding parallel test: testDeployAndUnDeployCruiseControl
2022-03-28 10:51:41 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:81] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 6
2022-03-28 10:51:41 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:230] testDeployAndUnDeployCruiseControl test now can proceed its execution
2022-03-28 10:51:41 [ForkJoinPool-1-worker-1] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 10:51:41 [ForkJoinPool-1-worker-1] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383, testDeployAndUnDeployCruiseControl=my-cluster-c0830cbe, testConfigurationFileIsCreated=my-cluster-e0fac774, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8, testCapacityFile=my-cluster-fd0fb61a, testSendSimpleMessageTls=my-cluster-27ddd6ba, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c, testReceiveSimpleMessageTls=my-cluster-c6a15963, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5}
2022-03-28 10:51:41 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:689] ============================================================================
2022-03-28 10:51:41 [ForkJoinPool-1-worker-1] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-1915916223-1265293973, testDeployAndUnDeployCruiseControl=my-user-1988711145-439861161, testConfigurationFileIsCreated=my-user-180611180-50530491, testThrottlingQuotasCreateTopic=my-user-1131806865-947217230, testCapacityFile=my-user-937015144-1959439396, testSendSimpleMessageTls=my-user-738229993-1095121647, testCreatingUsersWithSecretPrefix=my-user-1450786684-1808961062, testReceiveSimpleMessageTls=my-user-6088058-1142028780, testSendSimpleMessageTlsScramSha=my-user-1499386234-1249154328}
2022-03-28 10:51:41 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:690] [bridge.HttpBridgeScramShaST - After All] - Clean up after test suite
2022-03-28 10:51:41 [ForkJoinPool-1-worker-1] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-1834944466-2094449929, testDeployAndUnDeployCruiseControl=my-topic-1511079187-95055629, testConfigurationFileIsCreated=my-topic-1296407588-313880708, testThrottlingQuotasCreateTopic=my-topic-728672490-1194703402, testCapacityFile=my-topic-1155641705-902584520, testSendSimpleMessageTls=my-topic-563023482-1697359353, testCreatingUsersWithSecretPrefix=my-topic-1479520843-618891199, testReceiveSimpleMessageTls=my-topic-478514536-2132782608, testSendSimpleMessageTlsScramSha=my-topic-1028951580-1656411053}
2022-03-28 10:51:41 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:344] ############################################################################
2022-03-28 10:51:41 [ForkJoinPool-1-worker-1] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-c0830cbe-kafka-clients, testConfigurationFileIsCreated=my-cluster-e0fac774-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8-kafka-clients, testCapacityFile=my-cluster-fd0fb61a-kafka-clients, testSendSimpleMessageTls=my-cluster-27ddd6ba-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c-kafka-clients, testReceiveSimpleMessageTls=my-cluster-c6a15963-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5-kafka-clients}
2022-03-28 10:51:41 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:348] Delete all resources for HttpBridgeScramShaST
2022-03-28 10:51:41 [ForkJoinPool-1-worker-1] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-3 for test case:testDeployAndUnDeployCruiseControl
2022-03-28 10:51:41 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of Deployment http-bridge-scram-sha-st-shared-kafka-clients in namespace http-bridge-scram-sha-st
2022-03-28 10:51:41 [ForkJoinPool-1-worker-1] INFO  [KubeClusterResource:156] Creating Namespace: namespace-3
2022-03-28 10:51:41 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Namespace namespace-3
2022-03-28 10:51:41 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace namespace-3 -o json
2022-03-28 10:51:41 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:http-bridge-scram-sha-st-shared-kafka-clients
2022-03-28 10:51:41 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1312455ms till timeout)
2022-03-28 10:51:41 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:51:41 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (753748ms till timeout)
2022-03-28 10:51:41 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace namespace-3 -o json
2022-03-28 10:51:41 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 10:51:41 [ForkJoinPool-1-worker-1] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c27,c4",
            "openshift.io/sa.scc.supplemental-groups": "1000710000/10000",
            "openshift.io/sa.scc.uid-range": "1000710000/10000"
        },
        "creationTimestamp": "2022-03-28T10:51:39Z",
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T10:51:09Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:status": {
                        "f:phase": {}
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T10:51:39Z"
            }
        ],
        "name": "namespace-3",
        "resourceVersion": "1867774",
        "selfLink": "/api/v1/namespaces/namespace-3",
        "uid": "801378f5-be5a-4628-8148-fee943a3d990"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 10:51:41 [ForkJoinPool-1-worker-1] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[namespace-1], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-3], io.strimzi.test.logs.CollectorElement@3e255cd9=[namespace-0], io.strimzi.test.logs.CollectorElement@aec142ef=[namespace-2]}
2022-03-28 10:51:41 [ForkJoinPool-1-worker-1] INFO  [KubeClusterResource:82] Client use Namespace: namespace-3
2022-03-28 10:51:41 [ForkJoinPool-1-worker-1] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-3, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 10:51:41 [ForkJoinPool-1-worker-1] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-3
2022-03-28 10:51:41 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-c0830cbe in namespace namespace-3
2022-03-28 10:51:41 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:164] Using Namespace: namespace-3
2022-03-28 10:51:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (214823ms till timeout)
2022-03-28 10:51:42 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-scram-sha-st-shared-kafka-clients not ready, will try again in 10000 ms (479606ms till timeout)
2022-03-28 10:51:42 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:42 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-c0830cbe
2022-03-28 10:51:42 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (215574ms till timeout)
2022-03-28 10:51:42 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-c0830cbe will have desired state: Ready
2022-03-28 10:51:42 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-c0830cbe will have desired state: Ready
2022-03-28 10:51:42 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1319876ms till timeout)
2022-03-28 10:51:42 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:42 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:51:42 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (447160ms till timeout)
2022-03-28 10:51:42 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1311334ms till timeout)
2022-03-28 10:51:42 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (752726ms till timeout)
2022-03-28 10:51:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (213676ms till timeout)
2022-03-28 10:51:43 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (214526ms till timeout)
2022-03-28 10:51:43 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1318791ms till timeout)
2022-03-28 10:51:43 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:43 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:43 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:51:43 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (445813ms till timeout)
2022-03-28 10:51:43 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1310268ms till timeout)
2022-03-28 10:51:44 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (751603ms till timeout)
2022-03-28 10:51:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (212548ms till timeout)
2022-03-28 10:51:44 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (213467ms till timeout)
2022-03-28 10:51:44 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1317744ms till timeout)
2022-03-28 10:51:44 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:45 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1309187ms till timeout)
2022-03-28 10:51:45 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (750515ms till timeout)
2022-03-28 10:51:45 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:45 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:51:45 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (444523ms till timeout)
2022-03-28 10:51:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (212252ms till timeout)
2022-03-28 10:51:45 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1316690ms till timeout)
2022-03-28 10:51:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (211205ms till timeout)
2022-03-28 10:51:46 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1308055ms till timeout)
2022-03-28 10:51:46 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:46 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (749404ms till timeout)
2022-03-28 10:51:46 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:46 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:51:46 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (443081ms till timeout)
2022-03-28 10:51:46 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (211191ms till timeout)
2022-03-28 10:51:46 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1315635ms till timeout)
2022-03-28 10:51:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (210127ms till timeout)
2022-03-28 10:51:46 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:51:47 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1306990ms till timeout)
2022-03-28 10:51:47 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (748256ms till timeout)
2022-03-28 10:51:47 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:47 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1314545ms till timeout)
2022-03-28 10:51:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (210042ms till timeout)
2022-03-28 10:51:47 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:47 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:51:47 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (441775ms till timeout)
2022-03-28 10:51:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (208884ms till timeout)
2022-03-28 10:51:48 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1305882ms till timeout)
2022-03-28 10:51:48 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (747212ms till timeout)
2022-03-28 10:51:48 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1313509ms till timeout)
2022-03-28 10:51:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (208973ms till timeout)
2022-03-28 10:51:48 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (207660ms till timeout)
2022-03-28 10:51:49 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:49 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:51:49 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (440436ms till timeout)
2022-03-28 10:51:49 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1304773ms till timeout)
2022-03-28 10:51:49 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (746127ms till timeout)
2022-03-28 10:51:49 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1312466ms till timeout)
2022-03-28 10:51:49 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (207904ms till timeout)
2022-03-28 10:51:50 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (206571ms till timeout)
2022-03-28 10:51:50 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1303744ms till timeout)
2022-03-28 10:51:50 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (745093ms till timeout)
2022-03-28 10:51:50 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:50 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:51:50 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (439146ms till timeout)
2022-03-28 10:51:50 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1311336ms till timeout)
2022-03-28 10:51:51 [ForkJoinPool-1-worker-7] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment producer-1990229902 deletion
2022-03-28 10:51:51 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for ReplicaSet producer-1990229902 to be deleted
2022-03-28 10:51:51 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] ReplicaSet producer-1990229902 to be deleted not ready, will try again in 5000 ms (179975ms till timeout)
2022-03-28 10:51:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (205504ms till timeout)
2022-03-28 10:51:51 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1302708ms till timeout)
2022-03-28 10:51:51 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:51 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (744072ms till timeout)
2022-03-28 10:51:51 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:51:51 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:51 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:51:51 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (437723ms till timeout)
2022-03-28 10:51:52 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1310282ms till timeout)
2022-03-28 10:51:52 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-scram-sha-st-shared-kafka-clients not ready, will try again in 10000 ms (469356ms till timeout)
2022-03-28 10:51:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (204449ms till timeout)
2022-03-28 10:51:52 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1301664ms till timeout)
2022-03-28 10:51:52 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (743008ms till timeout)
2022-03-28 10:51:52 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:53 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1309244ms till timeout)
2022-03-28 10:51:53 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:53 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:51:53 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (436417ms till timeout)
2022-03-28 10:51:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (203386ms till timeout)
2022-03-28 10:51:53 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1300582ms till timeout)
2022-03-28 10:51:53 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (741962ms till timeout)
2022-03-28 10:51:54 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1308207ms till timeout)
2022-03-28 10:51:54 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:54 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:54 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:51:54 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (435045ms till timeout)
2022-03-28 10:51:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (202218ms till timeout)
2022-03-28 10:51:54 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1299521ms till timeout)
2022-03-28 10:51:54 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (740908ms till timeout)
2022-03-28 10:51:55 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1307152ms till timeout)
2022-03-28 10:51:55 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:55 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1298262ms till timeout)
2022-03-28 10:51:55 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (739680ms till timeout)
2022-03-28 10:51:56 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:56 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:51:56 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (433626ms till timeout)
2022-03-28 10:51:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (200763ms till timeout)
2022-03-28 10:51:56 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1306105ms till timeout)
2022-03-28 10:51:56 [ForkJoinPool-1-worker-7] DEBUG [JobUtils:40] Job producer-1990229902 was deleted
2022-03-28 10:51:56 [ForkJoinPool-1-worker-7] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment consumer-197971465 deletion
2022-03-28 10:51:56 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for ReplicaSet consumer-197971465 to be deleted
2022-03-28 10:51:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] ReplicaSet consumer-197971465 to be deleted not ready, will try again in 5000 ms (179947ms till timeout)
2022-03-28 10:51:56 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:51:56 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1297229ms till timeout)
2022-03-28 10:51:56 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (738647ms till timeout)
2022-03-28 10:51:57 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:57 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1305054ms till timeout)
2022-03-28 10:51:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (199610ms till timeout)
2022-03-28 10:51:57 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:57 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:51:57 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (432336ms till timeout)
2022-03-28 10:51:58 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (737587ms till timeout)
2022-03-28 10:51:58 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1296080ms till timeout)
2022-03-28 10:51:58 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:58 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1303948ms till timeout)
2022-03-28 10:51:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (198420ms till timeout)
2022-03-28 10:51:58 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:58 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:51:58 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (430957ms till timeout)
2022-03-28 10:51:59 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (736560ms till timeout)
2022-03-28 10:51:59 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1295045ms till timeout)
2022-03-28 10:51:59 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1302913ms till timeout)
2022-03-28 10:51:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (197261ms till timeout)
2022-03-28 10:51:59 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:59 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-1 get Namespace namespace-0 -o yaml
2022-03-28 10:51:59 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 1
2022-03-28 10:51:59 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 10:51:59 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-0" not found
2022-03-28 10:51:59 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] ======STDERR END======
2022-03-28 10:51:59 [ForkJoinPool-1-worker-3] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[namespace-1], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-3], io.strimzi.test.logs.CollectorElement@3e255cd9=[], io.strimzi.test.logs.CollectorElement@aec142ef=[namespace-2]}
2022-03-28 10:51:59 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:267] testCapacityFile - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTls, testSendSimpleMessageTlsScramSha, testConfigurationFileIsCreated, testReceiveSimpleMessageTls, testDeployAndUnDeployCruiseControl] to and randomly select one to start execution
2022-03-28 10:51:59 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:93] [cruisecontrol.CruiseControlConfigurationST] - Removing parallel test: testCapacityFile
2022-03-28 10:51:59 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:97] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 5
2022-03-28 10:51:59 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testCapacityFile-FINISHED
2022-03-28 10:51:59 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:30] ############################################################################
2022-03-28 10:51:59 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:23] ############################################################################
2022-03-28 10:51:59 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods-STARTED
2022-03-28 10:51:59 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:658] ============================================================================
2022-03-28 10:51:59 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:659] [cruisecontrol.CruiseControlConfigurationST - Before Each] - Setup test case environment
2022-03-28 10:51:59 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:77] [cruisecontrol.CruiseControlConfigurationST] - Adding parallel test: testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods
2022-03-28 10:51:59 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:81] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 6
2022-03-28 10:51:59 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:230] testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods test now can proceed its execution
2022-03-28 10:51:59 [ForkJoinPool-1-worker-3] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 10:51:59 [ForkJoinPool-1-worker-3] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383, testDeployAndUnDeployCruiseControl=my-cluster-c0830cbe, testConfigurationFileIsCreated=my-cluster-e0fac774, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8, testCapacityFile=my-cluster-fd0fb61a, testSendSimpleMessageTls=my-cluster-27ddd6ba, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c, testReceiveSimpleMessageTls=my-cluster-c6a15963, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-f50d425d, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5}
2022-03-28 10:51:59 [ForkJoinPool-1-worker-3] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-1915916223-1265293973, testDeployAndUnDeployCruiseControl=my-user-1988711145-439861161, testConfigurationFileIsCreated=my-user-180611180-50530491, testThrottlingQuotasCreateTopic=my-user-1131806865-947217230, testCapacityFile=my-user-937015144-1959439396, testSendSimpleMessageTls=my-user-738229993-1095121647, testCreatingUsersWithSecretPrefix=my-user-1450786684-1808961062, testReceiveSimpleMessageTls=my-user-6088058-1142028780, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-745974575-1852384454, testSendSimpleMessageTlsScramSha=my-user-1499386234-1249154328}
2022-03-28 10:51:59 [ForkJoinPool-1-worker-3] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-1834944466-2094449929, testDeployAndUnDeployCruiseControl=my-topic-1511079187-95055629, testConfigurationFileIsCreated=my-topic-1296407588-313880708, testThrottlingQuotasCreateTopic=my-topic-728672490-1194703402, testCapacityFile=my-topic-1155641705-902584520, testSendSimpleMessageTls=my-topic-563023482-1697359353, testCreatingUsersWithSecretPrefix=my-topic-1479520843-618891199, testReceiveSimpleMessageTls=my-topic-478514536-2132782608, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-254848724-189700675, testSendSimpleMessageTlsScramSha=my-topic-1028951580-1656411053}
2022-03-28 10:51:59 [ForkJoinPool-1-worker-3] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-c0830cbe-kafka-clients, testConfigurationFileIsCreated=my-cluster-e0fac774-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8-kafka-clients, testCapacityFile=my-cluster-fd0fb61a-kafka-clients, testSendSimpleMessageTls=my-cluster-27ddd6ba-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c-kafka-clients, testReceiveSimpleMessageTls=my-cluster-c6a15963-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-f50d425d-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5-kafka-clients}
2022-03-28 10:51:59 [ForkJoinPool-1-worker-3] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-4 for test case:testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods
2022-03-28 10:52:00 [ForkJoinPool-1-worker-3] INFO  [KubeClusterResource:156] Creating Namespace: namespace-4
2022-03-28 10:52:00 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (735499ms till timeout)
2022-03-28 10:52:00 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Namespace namespace-4
2022-03-28 10:52:00 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-3 get Namespace namespace-4 -o json
2022-03-28 10:52:00 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1293988ms till timeout)
2022-03-28 10:52:00 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-3 get Namespace namespace-4 -o json
2022-03-28 10:52:00 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:52:00 [ForkJoinPool-1-worker-3] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c27,c9",
            "openshift.io/sa.scc.supplemental-groups": "1000720000/10000",
            "openshift.io/sa.scc.uid-range": "1000720000/10000"
        },
        "creationTimestamp": "2022-03-28T10:51:57Z",
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T10:51:27Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:status": {
                        "f:phase": {}
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T10:51:57Z"
            }
        ],
        "name": "namespace-4",
        "resourceVersion": "1868128",
        "selfLink": "/api/v1/namespaces/namespace-4",
        "uid": "820e1e5e-d382-4825-a67d-217cde1b9dec"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 10:52:00 [ForkJoinPool-1-worker-3] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@3881d5f2=[namespace-4], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[namespace-1], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-3], io.strimzi.test.logs.CollectorElement@3e255cd9=[], io.strimzi.test.logs.CollectorElement@aec142ef=[namespace-2]}
2022-03-28 10:52:00 [ForkJoinPool-1-worker-3] INFO  [KubeClusterResource:82] Client use Namespace: namespace-4
2022-03-28 10:52:00 [ForkJoinPool-1-worker-3] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-4, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 10:52:00 [ForkJoinPool-1-worker-3] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-4
2022-03-28 10:52:00 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-f50d425d in namespace namespace-4
2022-03-28 10:52:00 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:164] Using Namespace: namespace-4
2022-03-28 10:52:00 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1301754ms till timeout)
2022-03-28 10:52:00 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-f50d425d
2022-03-28 10:52:00 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-f50d425d will have desired state: Ready
2022-03-28 10:52:00 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-f50d425d will have desired state: Ready
2022-03-28 10:52:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (196004ms till timeout)
2022-03-28 10:52:00 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1319932ms till timeout)
2022-03-28 10:52:01 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (734402ms till timeout)
2022-03-28 10:52:01 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1292943ms till timeout)
2022-03-28 10:52:01 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1300731ms till timeout)
2022-03-28 10:52:01 [ForkJoinPool-1-worker-7] DEBUG [JobUtils:40] Job consumer-197971465 was deleted
2022-03-28 10:52:01 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:674] ============================================================================
2022-03-28 10:52:01 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:675] [bridge.HttpBridgeTlsST - After Each] - Clean up after test
2022-03-28 10:52:01 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:344] ############################################################################
2022-03-28 10:52:01 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:348] Delete all resources for testReceiveSimpleMessageTls
2022-03-28 10:52:01 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of Job consumer-197971465 in namespace http-bridge-tls-st
2022-03-28 10:52:01 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:consumer-197971465
2022-03-28 10:52:01 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of Job producer-1990229902 in namespace http-bridge-tls-st
2022-03-28 10:52:01 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:producer-1990229902
2022-03-28 10:52:01 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of KafkaTopic my-topic-819761185-2131351185 in namespace http-bridge-tls-st
2022-03-28 10:52:01 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:52:01 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-819761185-2131351185
2022-03-28 10:52:01 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1318874ms till timeout)
2022-03-28 10:52:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (194905ms till timeout)
2022-03-28 10:52:01 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-819761185-2131351185 not ready, will try again in 10000 ms (179906ms till timeout)
2022-03-28 10:52:02 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (733346ms till timeout)
2022-03-28 10:52:02 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1291863ms till timeout)
2022-03-28 10:52:02 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-scram-sha-st-shared-kafka-clients not ready, will try again in 10000 ms (459030ms till timeout)
2022-03-28 10:52:02 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1299653ms till timeout)
2022-03-28 10:52:02 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1317806ms till timeout)
2022-03-28 10:52:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (193790ms till timeout)
2022-03-28 10:52:03 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (732185ms till timeout)
2022-03-28 10:52:03 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1290739ms till timeout)
2022-03-28 10:52:03 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1298608ms till timeout)
2022-03-28 10:52:04 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1316717ms till timeout)
2022-03-28 10:52:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (192671ms till timeout)
2022-03-28 10:52:04 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (731139ms till timeout)
2022-03-28 10:52:04 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1289643ms till timeout)
2022-03-28 10:52:04 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1297558ms till timeout)
2022-03-28 10:52:05 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1315672ms till timeout)
2022-03-28 10:52:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (191551ms till timeout)
2022-03-28 10:52:05 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (730049ms till timeout)
2022-03-28 10:52:05 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1288620ms till timeout)
2022-03-28 10:52:05 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1296514ms till timeout)
2022-03-28 10:52:06 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1314519ms till timeout)
2022-03-28 10:52:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (190422ms till timeout)
2022-03-28 10:52:06 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1287569ms till timeout)
2022-03-28 10:52:06 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (728977ms till timeout)
2022-03-28 10:52:06 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1295460ms till timeout)
2022-03-28 10:52:06 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:52:07 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1313396ms till timeout)
2022-03-28 10:52:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (189342ms till timeout)
2022-03-28 10:52:07 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1286508ms till timeout)
2022-03-28 10:52:07 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (727905ms till timeout)
2022-03-28 10:52:07 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1294433ms till timeout)
2022-03-28 10:52:08 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1312345ms till timeout)
2022-03-28 10:52:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (188196ms till timeout)
2022-03-28 10:52:08 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1285458ms till timeout)
2022-03-28 10:52:08 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (726857ms till timeout)
2022-03-28 10:52:08 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1293334ms till timeout)
2022-03-28 10:52:09 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1311304ms till timeout)
2022-03-28 10:52:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (187119ms till timeout)
2022-03-28 10:52:09 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1284378ms till timeout)
2022-03-28 10:52:09 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (725771ms till timeout)
2022-03-28 10:52:10 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1292268ms till timeout)
2022-03-28 10:52:10 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1310272ms till timeout)
2022-03-28 10:52:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (186035ms till timeout)
2022-03-28 10:52:10 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1283340ms till timeout)
2022-03-28 10:52:10 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (724686ms till timeout)
2022-03-28 10:52:11 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1291226ms till timeout)
2022-03-28 10:52:11 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1309237ms till timeout)
2022-03-28 10:52:11 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:52:11 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1282293ms till timeout)
2022-03-28 10:52:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (184938ms till timeout)
2022-03-28 10:52:11 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (723626ms till timeout)
2022-03-28 10:52:12 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:369] ############################################################################
2022-03-28 10:52:12 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:267] testReceiveSimpleMessageTls - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTls, testSendSimpleMessageTlsScramSha, testConfigurationFileIsCreated, testReceiveSimpleMessageTls, testDeployAndUnDeployCruiseControl, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods] to and randomly select one to start execution
2022-03-28 10:52:12 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:93] [bridge.HttpBridgeTlsST] - Removing parallel test: testReceiveSimpleMessageTls
2022-03-28 10:52:12 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:97] [bridge.HttpBridgeTlsST] - Parallel test count: 5
2022-03-28 10:52:12 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeTlsST.testReceiveSimpleMessageTls-FINISHED
2022-03-28 10:52:12 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:30] ############################################################################
2022-03-28 10:52:12 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:689] ============================================================================
2022-03-28 10:52:12 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:690] [bridge.HttpBridgeTlsST - After All] - Clean up after test suite
2022-03-28 10:52:12 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:344] ############################################################################
2022-03-28 10:52:12 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:348] Delete all resources for HttpBridgeTlsST
2022-03-28 10:52:12 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of Deployment http-bridge-tls-st-kafka-clients in namespace http-bridge-tls-st
2022-03-28 10:52:12 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients
2022-03-28 10:52:12 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1290161ms till timeout)
2022-03-28 10:52:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients not ready, will try again in 10000 ms (479834ms till timeout)
2022-03-28 10:52:12 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1308193ms till timeout)
2022-03-28 10:52:12 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-scram-sha-st-shared-kafka-clients not ready, will try again in 10000 ms (448796ms till timeout)
2022-03-28 10:52:12 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1281211ms till timeout)
2022-03-28 10:52:13 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (722556ms till timeout)
2022-03-28 10:52:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (183759ms till timeout)
2022-03-28 10:52:13 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1289037ms till timeout)
2022-03-28 10:52:13 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1307163ms till timeout)
2022-03-28 10:52:14 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1280166ms till timeout)
2022-03-28 10:52:14 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (721478ms till timeout)
2022-03-28 10:52:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (182652ms till timeout)
2022-03-28 10:52:14 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1287947ms till timeout)
2022-03-28 10:52:14 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1306121ms till timeout)
2022-03-28 10:52:15 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1279110ms till timeout)
2022-03-28 10:52:15 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (720431ms till timeout)
2022-03-28 10:52:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (181593ms till timeout)
2022-03-28 10:52:15 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1286822ms till timeout)
2022-03-28 10:52:15 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1305078ms till timeout)
2022-03-28 10:52:16 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1278030ms till timeout)
2022-03-28 10:52:16 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (719338ms till timeout)
2022-03-28 10:52:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (180327ms till timeout)
2022-03-28 10:52:16 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1285749ms till timeout)
2022-03-28 10:52:16 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1304031ms till timeout)
2022-03-28 10:52:16 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:52:17 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1276900ms till timeout)
2022-03-28 10:52:17 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-ede8537c will have desired state: Ready not ready, will try again in 1000 ms (718312ms till timeout)
2022-03-28 10:52:17 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1284606ms till timeout)
2022-03-28 10:52:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (179104ms till timeout)
2022-03-28 10:52:17 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1302963ms till timeout)
2022-03-28 10:52:18 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1275863ms till timeout)
2022-03-28 10:52:18 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:444] Kafka: my-cluster-ede8537c is in desired state: Ready
2022-03-28 10:52:18 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update KafkaTopic my-topic-1479520843-618891199 in namespace namespace-4
2022-03-28 10:52:18 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:164] Using Namespace: namespace-1
2022-03-28 10:52:18 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1479520843-618891199
2022-03-28 10:52:18 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-1479520843-618891199 will have desired state: Ready
2022-03-28 10:52:18 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-1479520843-618891199 will have desired state: Ready
2022-03-28 10:52:18 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] KafkaTopic: my-topic-1479520843-618891199 will have desired state: Ready not ready, will try again in 1000 ms (179897ms till timeout)
2022-03-28 10:52:18 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1283541ms till timeout)
2022-03-28 10:52:18 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1301892ms till timeout)
2022-03-28 10:52:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (177901ms till timeout)
2022-03-28 10:52:19 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1274833ms till timeout)
2022-03-28 10:52:19 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:444] KafkaTopic: my-topic-1479520843-618891199 is in desired state: Ready
2022-03-28 10:52:19 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update KafkaUser encrypted-leopold in namespace namespace-4
2022-03-28 10:52:19 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:164] Using Namespace: namespace-1
2022-03-28 10:52:19 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:encrypted-leopold
2022-03-28 10:52:19 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:433] Wait for KafkaUser: encrypted-leopold will have desired state: Ready
2022-03-28 10:52:19 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaUser: encrypted-leopold will have desired state: Ready
2022-03-28 10:52:19 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1282446ms till timeout)
2022-03-28 10:52:19 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] KafkaUser: encrypted-leopold will have desired state: Ready not ready, will try again in 1000 ms (179935ms till timeout)
2022-03-28 10:52:19 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1300840ms till timeout)
2022-03-28 10:52:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (176753ms till timeout)
2022-03-28 10:52:20 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1273807ms till timeout)
2022-03-28 10:52:20 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1281373ms till timeout)
2022-03-28 10:52:20 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:444] KafkaUser: encrypted-leopold is in desired state: Ready
2022-03-28 10:52:20 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update KafkaUser scramed-leopold in namespace namespace-4
2022-03-28 10:52:20 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:164] Using Namespace: namespace-1
2022-03-28 10:52:21 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1299747ms till timeout)
2022-03-28 10:52:21 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:scramed-leopold
2022-03-28 10:52:21 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:433] Wait for KafkaUser: scramed-leopold will have desired state: Ready
2022-03-28 10:52:21 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaUser: scramed-leopold will have desired state: Ready
2022-03-28 10:52:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (175575ms till timeout)
2022-03-28 10:52:21 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] KafkaUser: scramed-leopold will have desired state: Ready not ready, will try again in 1000 ms (179903ms till timeout)
2022-03-28 10:52:21 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1272696ms till timeout)
2022-03-28 10:52:21 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:52:21 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1280328ms till timeout)
2022-03-28 10:52:22 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1298661ms till timeout)
2022-03-28 10:52:22 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:444] KafkaUser: scramed-leopold is in desired state: Ready
2022-03-28 10:52:22 [ForkJoinPool-1-worker-13] INFO  [UserST:346] Deploying KafkaClients pod for TLS listener
2022-03-28 10:52:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (174423ms till timeout)
2022-03-28 10:52:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients not ready, will try again in 10000 ms (469547ms till timeout)
2022-03-28 10:52:22 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update Deployment my-cluster-ede8537c-tls-kafka-clients in namespace namespace-1
2022-03-28 10:52:22 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:164] Using Namespace: namespace-1
2022-03-28 10:52:22 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1271629ms till timeout)
2022-03-28 10:52:22 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-ede8537c-tls-kafka-clients
2022-03-28 10:52:22 [ForkJoinPool-1-worker-13] INFO  [DeploymentUtils:161] Wait for Deployment: my-cluster-ede8537c-tls-kafka-clients will be ready
2022-03-28 10:52:22 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Wait for Deployment: my-cluster-ede8537c-tls-kafka-clients will be ready
2022-03-28 10:52:22 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: my-cluster-ede8537c-tls-kafka-clients will be ready not ready, will try again in 1000 ms (479960ms till timeout)
2022-03-28 10:52:22 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of KafkaBridge http-bridge-scram-sha-cluster-name in namespace http-bridge-scram-sha-st
2022-03-28 10:52:23 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1279192ms till timeout)
2022-03-28 10:52:23 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1297637ms till timeout)
2022-03-28 10:52:23 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaBridge:http-bridge-scram-sha-cluster-name
2022-03-28 10:52:23 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaBridge:http-bridge-scram-sha-cluster-name not ready, will try again in 10000 ms (479800ms till timeout)
2022-03-28 10:52:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (173206ms till timeout)
2022-03-28 10:52:23 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1270494ms till timeout)
2022-03-28 10:52:24 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: my-cluster-ede8537c-tls-kafka-clients will be ready not ready, will try again in 1000 ms (478876ms till timeout)
2022-03-28 10:52:24 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1278160ms till timeout)
2022-03-28 10:52:24 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1296573ms till timeout)
2022-03-28 10:52:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (172130ms till timeout)
2022-03-28 10:52:24 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1269428ms till timeout)
2022-03-28 10:52:25 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: my-cluster-ede8537c-tls-kafka-clients will be ready not ready, will try again in 1000 ms (477846ms till timeout)
2022-03-28 10:52:25 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1277085ms till timeout)
2022-03-28 10:52:25 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1295521ms till timeout)
2022-03-28 10:52:25 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1268317ms till timeout)
2022-03-28 10:52:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (170964ms till timeout)
2022-03-28 10:52:26 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: my-cluster-ede8537c-tls-kafka-clients will be ready not ready, will try again in 1000 ms (476778ms till timeout)
2022-03-28 10:52:26 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1276004ms till timeout)
2022-03-28 10:52:26 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1294466ms till timeout)
2022-03-28 10:52:26 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:52:27 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1267146ms till timeout)
2022-03-28 10:52:27 [ForkJoinPool-1-worker-13] INFO  [DeploymentUtils:168] Deployment: my-cluster-ede8537c-tls-kafka-clients is ready
2022-03-28 10:52:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (169686ms till timeout)
2022-03-28 10:52:27 [ForkJoinPool-1-worker-13] INFO  [UserST:350] Deploying KafkaClients pod for PLAIN listener
2022-03-28 10:52:27 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update Deployment my-cluster-ede8537c-plain-kafka-clients in namespace namespace-1
2022-03-28 10:52:27 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:164] Using Namespace: namespace-1
2022-03-28 10:52:27 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-ede8537c-plain-kafka-clients
2022-03-28 10:52:27 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1274954ms till timeout)
2022-03-28 10:52:27 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1293394ms till timeout)
2022-03-28 10:52:27 [ForkJoinPool-1-worker-13] INFO  [DeploymentUtils:161] Wait for Deployment: my-cluster-ede8537c-plain-kafka-clients will be ready
2022-03-28 10:52:27 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Wait for Deployment: my-cluster-ede8537c-plain-kafka-clients will be ready
2022-03-28 10:52:27 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: my-cluster-ede8537c-plain-kafka-clients will be ready not ready, will try again in 1000 ms (479966ms till timeout)
2022-03-28 10:52:28 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1266093ms till timeout)
2022-03-28 10:52:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (168525ms till timeout)
2022-03-28 10:52:28 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1273892ms till timeout)
2022-03-28 10:52:28 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1292327ms till timeout)
2022-03-28 10:52:28 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: my-cluster-ede8537c-plain-kafka-clients will be ready not ready, will try again in 1000 ms (478921ms till timeout)
2022-03-28 10:52:29 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1265071ms till timeout)
2022-03-28 10:52:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (167409ms till timeout)
2022-03-28 10:52:29 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1272811ms till timeout)
2022-03-28 10:52:29 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1291244ms till timeout)
2022-03-28 10:52:29 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: my-cluster-ede8537c-plain-kafka-clients will be ready not ready, will try again in 1000 ms (477873ms till timeout)
2022-03-28 10:52:30 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1264053ms till timeout)
2022-03-28 10:52:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (166264ms till timeout)
2022-03-28 10:52:30 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1271706ms till timeout)
2022-03-28 10:52:30 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1290154ms till timeout)
2022-03-28 10:52:30 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: my-cluster-ede8537c-plain-kafka-clients will be ready not ready, will try again in 1000 ms (476838ms till timeout)
2022-03-28 10:52:31 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1263023ms till timeout)
2022-03-28 10:52:31 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1270580ms till timeout)
2022-03-28 10:52:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (165133ms till timeout)
2022-03-28 10:52:31 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1289059ms till timeout)
2022-03-28 10:52:31 [ForkJoinPool-1-worker-13] INFO  [DeploymentUtils:168] Deployment: my-cluster-ede8537c-plain-kafka-clients is ready
2022-03-28 10:52:31 [ForkJoinPool-1-worker-13] INFO  [UserST:357] Checking if user secrets with secret prefixes exists
2022-03-28 10:52:31 [ForkJoinPool-1-worker-13] INFO  [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-03-28 10:52:31 [ForkJoinPool-1-worker-13] INFO  [UserST:373] Checking if TLS user is able to send messages
2022-03-28 10:52:31 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:52:31 [ForkJoinPool-1-worker-13] DEBUG [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@2edd313e, which are set.
2022-03-28 10:52:31 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@48d49007, messages=[], arguments=[--bootstrap-server, my-cluster-ede8537c-kafka-bootstrap.namespace-1.svc:9093, USER=top_secret_encrypted_leopold, --max-messages, 100, --topic, my-topic-1936097768-1459518187], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-ede8537c-tls-kafka-clients-6cbf7d7bb4-hlwg7', podNamespace='namespace-1', bootstrapServer='my-cluster-ede8537c-kafka-bootstrap.namespace-1.svc:9093', topicName='my-topic-1936097768-1459518187', maxMessages=100, kafkaUsername='top-secret-encrypted-leopold', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@2edd313e}
2022-03-28 10:52:31 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:124] Producing 100 messages to my-cluster-ede8537c-kafka-bootstrap.namespace-1.svc:9093:my-topic-1936097768-1459518187 from pod my-cluster-ede8537c-tls-kafka-clients-6cbf7d7bb4-hlwg7
2022-03-28 10:52:31 [ForkJoinPool-1-worker-13] INFO  [VerifiableClient:192] Client command: oc exec my-cluster-ede8537c-tls-kafka-clients-6cbf7d7bb4-hlwg7 -n namespace-1 -- /opt/kafka/producer.sh --bootstrap-server my-cluster-ede8537c-kafka-bootstrap.namespace-1.svc:9093 USER=top_secret_encrypted_leopold --max-messages 100 --topic my-topic-1936097768-1459518187
2022-03-28 10:52:31 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc exec my-cluster-ede8537c-tls-kafka-clients-6cbf7d7bb4-hlwg7 -n namespace-1 -- /opt/kafka/producer.sh --bootstrap-server my-cluster-ede8537c-kafka-bootstrap.namespace-1.svc:9093 USER=top_secret_encrypted_leopold --max-messages 100 --topic my-topic-1936097768-1459518187
2022-03-28 10:52:32 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1261979ms till timeout)
2022-03-28 10:52:32 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1269545ms till timeout)
2022-03-28 10:52:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (164051ms till timeout)
2022-03-28 10:52:32 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1287980ms till timeout)
2022-03-28 10:52:32 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients not ready, will try again in 10000 ms (459272ms till timeout)
2022-03-28 10:52:33 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1260911ms till timeout)
2022-03-28 10:52:33 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of KafkaUser my-user-1569591476-1283230835 in namespace http-bridge-scram-sha-st
2022-03-28 10:52:33 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1569591476-1283230835
2022-03-28 10:52:33 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1268425ms till timeout)
2022-03-28 10:52:33 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1569591476-1283230835 not ready, will try again in 10000 ms (179821ms till timeout)
2022-03-28 10:52:33 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1286872ms till timeout)
2022-03-28 10:52:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (162904ms till timeout)
2022-03-28 10:52:34 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1259870ms till timeout)
2022-03-28 10:52:34 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1267383ms till timeout)
2022-03-28 10:52:34 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1285812ms till timeout)
2022-03-28 10:52:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (161669ms till timeout)
2022-03-28 10:52:35 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1258813ms till timeout)
2022-03-28 10:52:36 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1266294ms till timeout)
2022-03-28 10:52:36 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1284709ms till timeout)
2022-03-28 10:52:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (160526ms till timeout)
2022-03-28 10:52:36 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1257770ms till timeout)
2022-03-28 10:52:36 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:52:37 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1283667ms till timeout)
2022-03-28 10:52:37 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1265162ms till timeout)
2022-03-28 10:52:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (159435ms till timeout)
2022-03-28 10:52:37 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1256708ms till timeout)
2022-03-28 10:52:38 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1282462ms till timeout)
2022-03-28 10:52:38 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1263976ms till timeout)
2022-03-28 10:52:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (158335ms till timeout)
2022-03-28 10:52:38 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1255663ms till timeout)
2022-03-28 10:52:39 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1281408ms till timeout)
2022-03-28 10:52:39 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1262892ms till timeout)
2022-03-28 10:52:39 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1254634ms till timeout)
2022-03-28 10:52:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (157215ms till timeout)
2022-03-28 10:52:39 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:127] Producer finished correctly: true
2022-03-28 10:52:39 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:131] Producer produced 100 messages
2022-03-28 10:52:39 [ForkJoinPool-1-worker-13] DEBUG [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6303cf13, which are set.
2022-03-28 10:52:39 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@5840f46e, messages=[], arguments=[--bootstrap-server, my-cluster-ede8537c-kafka-bootstrap.namespace-1.svc:9093, --group-id, my-consumer-group-1555527971, USER=top_secret_encrypted_leopold, --max-messages, 100, --group-instance-id, instance326079724, --topic, my-topic-1936097768-1459518187], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-ede8537c-tls-kafka-clients-6cbf7d7bb4-hlwg7', podNamespace='namespace-1', bootstrapServer='my-cluster-ede8537c-kafka-bootstrap.namespace-1.svc:9093', topicName='my-topic-1936097768-1459518187', maxMessages=100, kafkaUsername='top-secret-encrypted-leopold', consumerGroupName='my-consumer-group-1555527971', consumerInstanceId='instance326079724', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6303cf13}
2022-03-28 10:52:39 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:192] Consuming 100 messages from my-cluster-ede8537c-kafka-bootstrap.namespace-1.svc:9093:my-topic-1936097768-1459518187 from pod my-cluster-ede8537c-tls-kafka-clients-6cbf7d7bb4-hlwg7
2022-03-28 10:52:39 [ForkJoinPool-1-worker-13] INFO  [VerifiableClient:192] Client command: oc exec my-cluster-ede8537c-tls-kafka-clients-6cbf7d7bb4-hlwg7 -n namespace-1 -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-ede8537c-kafka-bootstrap.namespace-1.svc:9093 --group-id my-consumer-group-1555527971 USER=top_secret_encrypted_leopold --max-messages 100 --group-instance-id instance326079724 --topic my-topic-1936097768-1459518187
2022-03-28 10:52:39 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc exec my-cluster-ede8537c-tls-kafka-clients-6cbf7d7bb4-hlwg7 -n namespace-1 -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-ede8537c-kafka-bootstrap.namespace-1.svc:9093 --group-id my-consumer-group-1555527971 USER=top_secret_encrypted_leopold --max-messages 100 --group-instance-id instance326079724 --topic my-topic-1936097768-1459518187
2022-03-28 10:52:40 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1280368ms till timeout)
2022-03-28 10:52:40 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1261801ms till timeout)
2022-03-28 10:52:40 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1253584ms till timeout)
2022-03-28 10:52:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (156110ms till timeout)
2022-03-28 10:52:41 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1279327ms till timeout)
2022-03-28 10:52:41 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1260749ms till timeout)
2022-03-28 10:52:41 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1252559ms till timeout)
2022-03-28 10:52:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (155069ms till timeout)
2022-03-28 10:52:41 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:52:42 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1278299ms till timeout)
2022-03-28 10:52:42 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1259702ms till timeout)
2022-03-28 10:52:42 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1251529ms till timeout)
2022-03-28 10:52:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (153965ms till timeout)
2022-03-28 10:52:42 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients not ready, will try again in 10000 ms (449114ms till timeout)
2022-03-28 10:52:43 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1277212ms till timeout)
2022-03-28 10:52:43 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1258667ms till timeout)
2022-03-28 10:52:43 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1250486ms till timeout)
2022-03-28 10:52:44 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of Kafka http-bridge-scram-sha-cluster-name in namespace http-bridge-scram-sha-st
2022-03-28 10:52:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (152654ms till timeout)
2022-03-28 10:52:44 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:http-bridge-scram-sha-cluster-name
2022-03-28 10:52:44 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:http-bridge-scram-sha-cluster-name not ready, will try again in 10000 ms (839911ms till timeout)
2022-03-28 10:52:44 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1276110ms till timeout)
2022-03-28 10:52:44 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1257618ms till timeout)
2022-03-28 10:52:44 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1249271ms till timeout)
2022-03-28 10:52:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (151392ms till timeout)
2022-03-28 10:52:45 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1256555ms till timeout)
2022-03-28 10:52:45 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1275041ms till timeout)
2022-03-28 10:52:45 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1248211ms till timeout)
2022-03-28 10:52:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (150253ms till timeout)
2022-03-28 10:52:46 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1255495ms till timeout)
2022-03-28 10:52:46 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1273955ms till timeout)
2022-03-28 10:52:46 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:52:47 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1247015ms till timeout)
2022-03-28 10:52:47 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1254353ms till timeout)
2022-03-28 10:52:47 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1272825ms till timeout)
2022-03-28 10:52:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (148739ms till timeout)
2022-03-28 10:52:48 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1245886ms till timeout)
2022-03-28 10:52:48 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1253316ms till timeout)
2022-03-28 10:52:49 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1271771ms till timeout)
2022-03-28 10:52:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (147648ms till timeout)
2022-03-28 10:52:49 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1244807ms till timeout)
2022-03-28 10:52:50 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1270741ms till timeout)
2022-03-28 10:52:50 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1252255ms till timeout)
2022-03-28 10:52:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (146513ms till timeout)
2022-03-28 10:52:50 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1243776ms till timeout)
2022-03-28 10:52:50 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-28 10:52:50 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:198] Consumer consumed 100 messages
2022-03-28 10:52:50 [ForkJoinPool-1-worker-13] INFO  [UserST:386] Checking if SCRAM-SHA user is able to send messages
2022-03-28 10:52:51 [ForkJoinPool-1-worker-13] DEBUG [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@7e82154e, which are set.
2022-03-28 10:52:51 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@7b9f2a77, messages=[], arguments=[--bootstrap-server, my-cluster-ede8537c-kafka-bootstrap.namespace-1.svc:9092, USER=top_secret_scramed_leopold, --max-messages, 100, --topic, my-topic-1936097768-1459518187], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-ede8537c-plain-kafka-clients-69b8c9d45c-5vrd7', podNamespace='namespace-1', bootstrapServer='my-cluster-ede8537c-kafka-bootstrap.namespace-1.svc:9092', topicName='my-topic-1936097768-1459518187', maxMessages=100, kafkaUsername='top-secret-scramed-leopold', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@7e82154e}
2022-03-28 10:52:51 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:94] Producing 100 messages to my-cluster-ede8537c-kafka-bootstrap.namespace-1.svc:9092:my-topic-1936097768-1459518187 from pod my-cluster-ede8537c-plain-kafka-clients-69b8c9d45c-5vrd7
2022-03-28 10:52:51 [ForkJoinPool-1-worker-13] INFO  [VerifiableClient:192] Client command: oc exec my-cluster-ede8537c-plain-kafka-clients-69b8c9d45c-5vrd7 -n namespace-1 -- /opt/kafka/producer.sh --bootstrap-server my-cluster-ede8537c-kafka-bootstrap.namespace-1.svc:9092 USER=top_secret_scramed_leopold --max-messages 100 --topic my-topic-1936097768-1459518187
2022-03-28 10:52:51 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc exec my-cluster-ede8537c-plain-kafka-clients-69b8c9d45c-5vrd7 -n namespace-1 -- /opt/kafka/producer.sh --bootstrap-server my-cluster-ede8537c-kafka-bootstrap.namespace-1.svc:9092 USER=top_secret_scramed_leopold --max-messages 100 --topic my-topic-1936097768-1459518187
2022-03-28 10:52:51 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1269677ms till timeout)
2022-03-28 10:52:51 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1251190ms till timeout)
2022-03-28 10:52:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (145413ms till timeout)
2022-03-28 10:52:51 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1242754ms till timeout)
2022-03-28 10:52:51 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:52:52 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1250101ms till timeout)
2022-03-28 10:52:52 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1268558ms till timeout)
2022-03-28 10:52:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (144318ms till timeout)
2022-03-28 10:52:52 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1241630ms till timeout)
2022-03-28 10:52:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients not ready, will try again in 10000 ms (438864ms till timeout)
2022-03-28 10:52:53 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1249015ms till timeout)
2022-03-28 10:52:53 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1267480ms till timeout)
2022-03-28 10:52:53 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1240550ms till timeout)
2022-03-28 10:52:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (143183ms till timeout)
2022-03-28 10:52:54 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:369] ############################################################################
2022-03-28 10:52:54 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1247946ms till timeout)
2022-03-28 10:52:54 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1266432ms till timeout)
2022-03-28 10:52:54 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Namespace http-bridge-scram-sha-st removal
2022-03-28 10:52:54 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 10:52:54 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-4 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 10:52:54 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 10:52:54 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (479744ms till timeout)
2022-03-28 10:52:54 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1239498ms till timeout)
2022-03-28 10:52:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (142038ms till timeout)
2022-03-28 10:52:55 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1246907ms till timeout)
2022-03-28 10:52:55 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1265377ms till timeout)
2022-03-28 10:52:55 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 10:52:55 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1238432ms till timeout)
2022-03-28 10:52:55 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-4 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 10:52:55 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 10:52:55 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (478461ms till timeout)
2022-03-28 10:52:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (140884ms till timeout)
2022-03-28 10:52:56 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:97] Producer finished correctly: true
2022-03-28 10:52:56 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:101] Producer produced 100 messages
2022-03-28 10:52:56 [ForkJoinPool-1-worker-13] DEBUG [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6191940, which are set.
2022-03-28 10:52:56 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@59257e69, messages=[], arguments=[--bootstrap-server, my-cluster-ede8537c-kafka-bootstrap.namespace-1.svc:9092, --group-id, my-consumer-group-1555527971, USER=top_secret_scramed_leopold, --max-messages, 100, --group-instance-id, instance1011850612, --topic, my-topic-1936097768-1459518187], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-ede8537c-plain-kafka-clients-69b8c9d45c-5vrd7', podNamespace='namespace-1', bootstrapServer='my-cluster-ede8537c-kafka-bootstrap.namespace-1.svc:9092', topicName='my-topic-1936097768-1459518187', maxMessages=100, kafkaUsername='top-secret-scramed-leopold', consumerGroupName='my-consumer-group-1555527971', consumerInstanceId='instance1011850612', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6191940}
2022-03-28 10:52:56 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:157] Consuming 100 messages from my-cluster-ede8537c-kafka-bootstrap.namespace-1.svc:9092#my-topic-1936097768-1459518187 from pod my-cluster-ede8537c-plain-kafka-clients-69b8c9d45c-5vrd7
2022-03-28 10:52:56 [ForkJoinPool-1-worker-13] INFO  [VerifiableClient:192] Client command: oc exec my-cluster-ede8537c-plain-kafka-clients-69b8c9d45c-5vrd7 -n namespace-1 -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-ede8537c-kafka-bootstrap.namespace-1.svc:9092 --group-id my-consumer-group-1555527971 USER=top_secret_scramed_leopold --max-messages 100 --group-instance-id instance1011850612 --topic my-topic-1936097768-1459518187
2022-03-28 10:52:56 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc exec my-cluster-ede8537c-plain-kafka-clients-69b8c9d45c-5vrd7 -n namespace-1 -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-ede8537c-kafka-bootstrap.namespace-1.svc:9092 --group-id my-consumer-group-1555527971 USER=top_secret_scramed_leopold --max-messages 100 --group-instance-id instance1011850612 --topic my-topic-1936097768-1459518187
2022-03-28 10:52:56 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1245818ms till timeout)
2022-03-28 10:52:56 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1264234ms till timeout)
2022-03-28 10:52:56 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1237392ms till timeout)
2022-03-28 10:52:56 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:52:56 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 10:52:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (139764ms till timeout)
2022-03-28 10:52:57 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-4 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 10:52:57 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 10:52:57 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (477166ms till timeout)
2022-03-28 10:52:57 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1244783ms till timeout)
2022-03-28 10:52:57 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1263193ms till timeout)
2022-03-28 10:52:57 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1236371ms till timeout)
2022-03-28 10:52:58 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 10:52:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (138525ms till timeout)
2022-03-28 10:52:58 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-4 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 10:52:58 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 10:52:58 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (475928ms till timeout)
2022-03-28 10:52:58 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1243731ms till timeout)
2022-03-28 10:52:58 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1262142ms till timeout)
2022-03-28 10:52:58 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1235306ms till timeout)
2022-03-28 10:52:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (137432ms till timeout)
2022-03-28 10:52:59 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 10:52:59 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1242689ms till timeout)
2022-03-28 10:52:59 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1261100ms till timeout)
2022-03-28 10:52:59 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-4 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 10:52:59 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 10:52:59 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (474677ms till timeout)
2022-03-28 10:53:00 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1234193ms till timeout)
2022-03-28 10:53:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (136364ms till timeout)
2022-03-28 10:53:00 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1241634ms till timeout)
2022-03-28 10:53:00 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1260085ms till timeout)
2022-03-28 10:53:00 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 10:53:00 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-4 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 10:53:00 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 10:53:00 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (473412ms till timeout)
2022-03-28 10:53:01 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1233142ms till timeout)
2022-03-28 10:53:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (135145ms till timeout)
2022-03-28 10:53:01 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1240522ms till timeout)
2022-03-28 10:53:01 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1258974ms till timeout)
2022-03-28 10:53:01 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:53:01 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 10:53:02 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1232049ms till timeout)
2022-03-28 10:53:02 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-4 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 10:53:02 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 10:53:02 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (472053ms till timeout)
2022-03-28 10:53:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (134058ms till timeout)
2022-03-28 10:53:02 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1239395ms till timeout)
2022-03-28 10:53:02 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1257839ms till timeout)
2022-03-28 10:53:03 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1230949ms till timeout)
2022-03-28 10:53:03 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of KafkaBridge http-bridge-tls-cluster-name in namespace http-bridge-tls-st
2022-03-28 10:53:03 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 10:53:03 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaBridge:http-bridge-tls-cluster-name
2022-03-28 10:53:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaBridge:http-bridge-tls-cluster-name not ready, will try again in 10000 ms (479740ms till timeout)
2022-03-28 10:53:03 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-4 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 10:53:03 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 10:53:03 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (470615ms till timeout)
2022-03-28 10:53:03 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1238311ms till timeout)
2022-03-28 10:53:04 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1256763ms till timeout)
2022-03-28 10:53:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (132801ms till timeout)
2022-03-28 10:53:04 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1229661ms till timeout)
2022-03-28 10:53:04 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 10:53:05 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1237254ms till timeout)
2022-03-28 10:53:05 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1255693ms till timeout)
2022-03-28 10:53:05 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-4 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 10:53:05 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 10:53:05 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (469291ms till timeout)
2022-03-28 10:53:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (131720ms till timeout)
2022-03-28 10:53:05 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1228592ms till timeout)
2022-03-28 10:53:06 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 10:53:06 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1236182ms till timeout)
2022-03-28 10:53:06 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1254542ms till timeout)
2022-03-28 10:53:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (130538ms till timeout)
2022-03-28 10:53:06 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-4 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 10:53:06 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 10:53:06 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (467854ms till timeout)
2022-03-28 10:53:06 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1227499ms till timeout)
2022-03-28 10:53:06 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:53:07 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1235148ms till timeout)
2022-03-28 10:53:07 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1253490ms till timeout)
2022-03-28 10:53:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (129398ms till timeout)
2022-03-28 10:53:07 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 10:53:07 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1226373ms till timeout)
2022-03-28 10:53:07 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-4 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 10:53:07 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 10:53:07 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (466542ms till timeout)
2022-03-28 10:53:08 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1234083ms till timeout)
2022-03-28 10:53:08 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1252341ms till timeout)
2022-03-28 10:53:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (128299ms till timeout)
2022-03-28 10:53:08 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 10:53:08 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1225328ms till timeout)
2022-03-28 10:53:09 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-4 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 10:53:09 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 10:53:09 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (465318ms till timeout)
2022-03-28 10:53:09 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1233006ms till timeout)
2022-03-28 10:53:09 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1251314ms till timeout)
2022-03-28 10:53:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (127163ms till timeout)
2022-03-28 10:53:09 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1224244ms till timeout)
2022-03-28 10:53:10 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 10:53:10 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1231963ms till timeout)
2022-03-28 10:53:10 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-4 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 10:53:10 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 10:53:10 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (463919ms till timeout)
2022-03-28 10:53:10 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1250282ms till timeout)
2022-03-28 10:53:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (126093ms till timeout)
2022-03-28 10:53:11 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1223193ms till timeout)
2022-03-28 10:53:11 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1230896ms till timeout)
2022-03-28 10:53:11 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 10:53:11 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1249250ms till timeout)
2022-03-28 10:53:11 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-4 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 10:53:11 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 10:53:11 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (462679ms till timeout)
2022-03-28 10:53:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (125011ms till timeout)
2022-03-28 10:53:11 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:53:12 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1222168ms till timeout)
2022-03-28 10:53:12 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1229835ms till timeout)
2022-03-28 10:53:12 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1248216ms till timeout)
2022-03-28 10:53:12 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 10:53:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (123913ms till timeout)
2022-03-28 10:53:12 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-4 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 10:53:12 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 10:53:12 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (461420ms till timeout)
2022-03-28 10:53:13 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1221106ms till timeout)
2022-03-28 10:53:13 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1228736ms till timeout)
2022-03-28 10:53:13 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1247073ms till timeout)
2022-03-28 10:53:13 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of KafkaUser my-user-1252185471-65885922 in namespace http-bridge-tls-st
2022-03-28 10:53:13 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 10:53:14 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1252185471-65885922
2022-03-28 10:53:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (122765ms till timeout)
2022-03-28 10:53:14 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1220048ms till timeout)
2022-03-28 10:53:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1252185471-65885922 not ready, will try again in 10000 ms (179826ms till timeout)
2022-03-28 10:53:14 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-4 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 10:53:14 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 10:53:14 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (460146ms till timeout)
2022-03-28 10:53:14 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1227702ms till timeout)
2022-03-28 10:53:14 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1246008ms till timeout)
2022-03-28 10:53:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (121697ms till timeout)
2022-03-28 10:53:15 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1219023ms till timeout)
2022-03-28 10:53:15 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 10:53:15 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-4 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 10:53:15 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 10:53:15 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (458765ms till timeout)
2022-03-28 10:53:15 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1226647ms till timeout)
2022-03-28 10:53:15 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1244991ms till timeout)
2022-03-28 10:53:16 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1217852ms till timeout)
2022-03-28 10:53:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (120475ms till timeout)
2022-03-28 10:53:16 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 10:53:16 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1225608ms till timeout)
2022-03-28 10:53:16 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:164] ReconciliationST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 10:53:16 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1243897ms till timeout)
2022-03-28 10:53:16 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-4 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 10:53:16 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 10:53:16 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (457509ms till timeout)
2022-03-28 10:53:17 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1216826ms till timeout)
2022-03-28 10:53:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (119426ms till timeout)
2022-03-28 10:53:17 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1224529ms till timeout)
2022-03-28 10:53:17 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 10:53:17 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1242830ms till timeout)
2022-03-28 10:53:18 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-4 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 10:53:18 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 1
2022-03-28 10:53:18 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 10:53:18 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Error from server (NotFound): namespaces "http-bridge-scram-sha-st" not found
2022-03-28 10:53:18 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] ======STDERR END======
2022-03-28 10:53:18 [ForkJoinPool-1-worker-5] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@3881d5f2=[namespace-4], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[namespace-1], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-3], io.strimzi.test.logs.CollectorElement@3e255cd9=[], io.strimzi.test.logs.CollectorElement@aec142ef=[namespace-2]}
2022-03-28 10:53:18 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:254] HttpBridgeScramShaST - Notifies waiting test suites:[UserST, HttpBridgeTlsST, TopicST, CruiseControlConfigurationST, HttpBridgeScramShaST, ThrottlingQuotaST, ReconciliationST] to and randomly select one to start execution
2022-03-28 10:53:18 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:85] [bridge.HttpBridgeScramShaST] - Removing parallel suite: HttpBridgeScramShaST
2022-03-28 10:53:18 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:89] [bridge.HttpBridgeScramShaST] - Parallel suites count: 6
[INFO] Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 372.64 s - in io.strimzi.systemtest.bridge.HttpBridgeScramShaST
2022-03-28 10:53:18 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:23] ############################################################################
2022-03-28 10:53:18 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationPerformanceOptions-STARTED
2022-03-28 10:53:18 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:658] ============================================================================
2022-03-28 10:53:18 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:659] [cruisecontrol.CruiseControlConfigurationST - Before Each] - Setup test case environment
2022-03-28 10:53:18 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:77] [cruisecontrol.CruiseControlConfigurationST] - Adding parallel test: testConfigurationPerformanceOptions
2022-03-28 10:53:18 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:81] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 6
2022-03-28 10:53:18 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:230] testConfigurationPerformanceOptions test now can proceed its execution
2022-03-28 10:53:18 [ForkJoinPool-1-worker-5] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 10:53:18 [ForkJoinPool-1-worker-5] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383, testDeployAndUnDeployCruiseControl=my-cluster-c0830cbe, testConfigurationFileIsCreated=my-cluster-e0fac774, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8, testCapacityFile=my-cluster-fd0fb61a, testSendSimpleMessageTls=my-cluster-27ddd6ba, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c, testReceiveSimpleMessageTls=my-cluster-c6a15963, testConfigurationPerformanceOptions=my-cluster-b6310693, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-f50d425d, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5}
2022-03-28 10:53:18 [ForkJoinPool-1-worker-5] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-1915916223-1265293973, testDeployAndUnDeployCruiseControl=my-user-1988711145-439861161, testConfigurationFileIsCreated=my-user-180611180-50530491, testThrottlingQuotasCreateTopic=my-user-1131806865-947217230, testCapacityFile=my-user-937015144-1959439396, testSendSimpleMessageTls=my-user-738229993-1095121647, testCreatingUsersWithSecretPrefix=my-user-1450786684-1808961062, testReceiveSimpleMessageTls=my-user-6088058-1142028780, testConfigurationPerformanceOptions=my-user-930975101-640523779, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-745974575-1852384454, testSendSimpleMessageTlsScramSha=my-user-1499386234-1249154328}
2022-03-28 10:53:18 [ForkJoinPool-1-worker-5] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-1834944466-2094449929, testDeployAndUnDeployCruiseControl=my-topic-1511079187-95055629, testConfigurationFileIsCreated=my-topic-1296407588-313880708, testThrottlingQuotasCreateTopic=my-topic-728672490-1194703402, testCapacityFile=my-topic-1155641705-902584520, testSendSimpleMessageTls=my-topic-563023482-1697359353, testCreatingUsersWithSecretPrefix=my-topic-1479520843-618891199, testReceiveSimpleMessageTls=my-topic-478514536-2132782608, testConfigurationPerformanceOptions=my-topic-213912446-1165431808, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-254848724-189700675, testSendSimpleMessageTlsScramSha=my-topic-1028951580-1656411053}
2022-03-28 10:53:18 [ForkJoinPool-1-worker-5] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-c0830cbe-kafka-clients, testConfigurationFileIsCreated=my-cluster-e0fac774-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8-kafka-clients, testCapacityFile=my-cluster-fd0fb61a-kafka-clients, testSendSimpleMessageTls=my-cluster-27ddd6ba-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c-kafka-clients, testReceiveSimpleMessageTls=my-cluster-c6a15963-kafka-clients, testConfigurationPerformanceOptions=my-cluster-b6310693-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-f50d425d-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5-kafka-clients}
2022-03-28 10:53:18 [ForkJoinPool-1-worker-5] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-5 for test case:testConfigurationPerformanceOptions
2022-03-28 10:53:18 [ForkJoinPool-1-worker-5] INFO  [KubeClusterResource:156] Creating Namespace: namespace-5
2022-03-28 10:53:18 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Namespace namespace-5
2022-03-28 10:53:18 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace namespace-5 -o json
2022-03-28 10:53:18 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1215751ms till timeout)
2022-03-28 10:53:18 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-4 get Namespace namespace-5 -o json
2022-03-28 10:53:18 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 10:53:18 [ForkJoinPool-1-worker-5] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c27,c14",
            "openshift.io/sa.scc.supplemental-groups": "1000730000/10000",
            "openshift.io/sa.scc.uid-range": "1000730000/10000"
        },
        "creationTimestamp": "2022-03-28T10:53:15Z",
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T10:52:45Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:status": {
                        "f:phase": {}
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T10:53:15Z"
            }
        ],
        "name": "namespace-5",
        "resourceVersion": "1869717",
        "selfLink": "/api/v1/namespaces/namespace-5",
        "uid": "8fdbfe9b-e391-42f1-aa9b-c1ea25c12892"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 10:53:18 [ForkJoinPool-1-worker-5] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[namespace-1], io.strimzi.test.logs.CollectorElement@aec142ef=[namespace-2], io.strimzi.test.logs.CollectorElement@f851b6c3=[namespace-5], io.strimzi.test.logs.CollectorElement@3881d5f2=[namespace-4], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-3], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 10:53:18 [ForkJoinPool-1-worker-5] INFO  [KubeClusterResource:82] Client use Namespace: namespace-5
2022-03-28 10:53:18 [ForkJoinPool-1-worker-5] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-5, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 10:53:18 [ForkJoinPool-1-worker-5] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-5
2022-03-28 10:53:18 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-b6310693 in namespace namespace-5
2022-03-28 10:53:18 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:164] Using Namespace: namespace-5
2022-03-28 10:53:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (118098ms till timeout)
2022-03-28 10:53:18 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1223496ms till timeout)
2022-03-28 10:53:18 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-b6310693
2022-03-28 10:53:18 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-b6310693 will have desired state: Ready
2022-03-28 10:53:18 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-b6310693 will have desired state: Ready
2022-03-28 10:53:18 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1319970ms till timeout)
2022-03-28 10:53:19 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1241769ms till timeout)
2022-03-28 10:53:19 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1214728ms till timeout)
2022-03-28 10:53:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (117007ms till timeout)
2022-03-28 10:53:19 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1222450ms till timeout)
2022-03-28 10:53:19 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1318915ms till timeout)
2022-03-28 10:53:20 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1240709ms till timeout)
2022-03-28 10:53:20 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1213709ms till timeout)
2022-03-28 10:53:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (115912ms till timeout)
2022-03-28 10:53:20 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1221331ms till timeout)
2022-03-28 10:53:20 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1317887ms till timeout)
2022-03-28 10:53:21 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1239649ms till timeout)
2022-03-28 10:53:21 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:160] Consumer finished correctly: true
2022-03-28 10:53:21 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:163] Consumer consumed 100 messages
2022-03-28 10:53:21 [ForkJoinPool-1-worker-13] INFO  [UserST:392] Checking owner reference - if the secret will be deleted when we delete KafkaUser
2022-03-28 10:53:21 [ForkJoinPool-1-worker-13] INFO  [UserST:394] Deleting KafkaUser:encrypted-leopold
2022-03-28 10:53:21 [ForkJoinPool-1-worker-13] INFO  [KafkaUserUtils:62] Waiting for KafkaUser deletion encrypted-leopold
2022-03-28 10:53:21 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaUser deletion encrypted-leopold
2022-03-28 10:53:21 [ForkJoinPool-1-worker-13] INFO  [KafkaUserUtils:75] KafkaUser encrypted-leopold deleted
2022-03-28 10:53:21 [ForkJoinPool-1-worker-13] INFO  [UserST:398] Deleting KafkaUser:scramed-leopold
2022-03-28 10:53:21 [ForkJoinPool-1-worker-13] INFO  [KafkaUserUtils:62] Waiting for KafkaUser deletion scramed-leopold
2022-03-28 10:53:21 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaUser deletion scramed-leopold
2022-03-28 10:53:21 [ForkJoinPool-1-worker-13] INFO  [KafkaUserUtils:75] KafkaUser scramed-leopold deleted
2022-03-28 10:53:21 [ForkJoinPool-1-worker-13] INFO  [UserST:402] Checking if secrets are deleted
2022-03-28 10:53:21 [ForkJoinPool-1-worker-13] INFO  [SecretUtils:54] Waiting for Secret deletion top-secret-encrypted-leopold
2022-03-28 10:53:21 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Expected secret top-secret-encrypted-leopold deleted
2022-03-28 10:53:21 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1212612ms till timeout)
2022-03-28 10:53:21 [ForkJoinPool-1-worker-13] INFO  [SecretUtils:58] Secret top-secret-encrypted-leopold deleted
2022-03-28 10:53:21 [ForkJoinPool-1-worker-13] INFO  [SecretUtils:54] Waiting for Secret deletion top-secret-scramed-leopold
2022-03-28 10:53:21 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Expected secret top-secret-scramed-leopold deleted
2022-03-28 10:53:21 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Expected secret top-secret-scramed-leopold deleted not ready, will try again in 1000 ms (179909ms till timeout)
2022-03-28 10:53:21 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:184] ReconciliationST suite now can proceed its execution
2022-03-28 10:53:21 [ForkJoinPool-1-worker-15] DEBUG [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-28 10:53:21 [ForkJoinPool-1-worker-15] DEBUG [TestSuiteNamespaceManager:129] Test suite `ReconciliationST` creates these additional namespaces:[reconciliation-st]
2022-03-28 10:53:21 [ForkJoinPool-1-worker-15] INFO  [KubeClusterResource:156] Creating Namespace: reconciliation-st
2022-03-28 10:53:21 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Namespace reconciliation-st
2022-03-28 10:53:21 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-5 get Namespace reconciliation-st -o json
2022-03-28 10:53:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (114782ms till timeout)
2022-03-28 10:53:22 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1316808ms till timeout)
2022-03-28 10:53:22 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1220219ms till timeout)
2022-03-28 10:53:22 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1238464ms till timeout)
2022-03-28 10:53:22 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-5 get Namespace reconciliation-st -o json
2022-03-28 10:53:22 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:53:22 [ForkJoinPool-1-worker-15] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c27,c19",
            "openshift.io/sa.scc.supplemental-groups": "1000740000/10000",
            "openshift.io/sa.scc.uid-range": "1000740000/10000"
        },
        "creationTimestamp": "2022-03-28T10:53:19Z",
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T10:52:49Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:status": {
                        "f:phase": {}
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T10:53:19Z"
            }
        ],
        "name": "reconciliation-st",
        "resourceVersion": "1869794",
        "selfLink": "/api/v1/namespaces/reconciliation-st",
        "uid": "37beb108-46a4-480c-8037-a90e3dc4d1e1"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 10:53:22 [ForkJoinPool-1-worker-15] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[namespace-1], io.strimzi.test.logs.CollectorElement@aec142ef=[namespace-2], io.strimzi.test.logs.CollectorElement@f851b6c3=[namespace-5], io.strimzi.test.logs.CollectorElement@3881d5f2=[namespace-4], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-3], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 10:53:22 [ForkJoinPool-1-worker-15] INFO  [KubeClusterResource:82] Client use Namespace: reconciliation-st
2022-03-28 10:53:22 [ForkJoinPool-1-worker-15] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=reconciliation-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 10:53:22 [ForkJoinPool-1-worker-15] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: reconciliation-st
2022-03-28 10:53:22 [ForkJoinPool-1-worker-15] INFO  [TestSeparator:23] ############################################################################
2022-03-28 10:53:22 [ForkJoinPool-1-worker-15] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.ReconciliationST.testPauseReconciliationInKafkaRebalanceAndTopic-STARTED
2022-03-28 10:53:22 [ForkJoinPool-1-worker-15] DEBUG [AbstractST:658] ============================================================================
2022-03-28 10:53:22 [ForkJoinPool-1-worker-15] DEBUG [AbstractST:659] [operators.ReconciliationST - Before Each] - Setup test case environment
2022-03-28 10:53:22 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:77] [operators.ReconciliationST] - Adding parallel test: testPauseReconciliationInKafkaRebalanceAndTopic
2022-03-28 10:53:22 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:81] [operators.ReconciliationST] - Parallel test count: 7
2022-03-28 10:53:22 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:205] [testPauseReconciliationInKafkaRebalanceAndTopic] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 10:53:22 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:210] testPauseReconciliationInKafkaRebalanceAndTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 10:53:22 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1211550ms till timeout)
2022-03-28 10:53:22 [ForkJoinPool-1-worker-13] INFO  [SecretUtils:58] Secret top-secret-scramed-leopold deleted
2022-03-28 10:53:22 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:674] ============================================================================
2022-03-28 10:53:22 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:675] [operators.user.UserST - After Each] - Clean up after test
2022-03-28 10:53:22 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:344] ############################################################################
2022-03-28 10:53:22 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:348] Delete all resources for testCreatingUsersWithSecretPrefix
2022-03-28 10:53:22 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of KafkaUser scramed-leopold in namespace namespace-1
2022-03-28 10:53:22 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:scramed-leopold
2022-03-28 10:53:23 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of Deployment my-cluster-ede8537c-plain-kafka-clients in namespace namespace-1
2022-03-28 10:53:23 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-ede8537c-plain-kafka-clients
2022-03-28 10:53:23 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1315702ms till timeout)
2022-03-28 10:53:23 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1219097ms till timeout)
2022-03-28 10:53:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (113622ms till timeout)
2022-03-28 10:53:23 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1237418ms till timeout)
2022-03-28 10:53:23 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-ede8537c-plain-kafka-clients not ready, will try again in 10000 ms (479745ms till timeout)
2022-03-28 10:53:23 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1210494ms till timeout)
2022-03-28 10:53:24 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of Kafka http-bridge-tls-cluster-name in namespace http-bridge-tls-st
2022-03-28 10:53:24 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1314578ms till timeout)
2022-03-28 10:53:24 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1217995ms till timeout)
2022-03-28 10:53:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (112482ms till timeout)
2022-03-28 10:53:24 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:http-bridge-tls-cluster-name
2022-03-28 10:53:24 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1236348ms till timeout)
2022-03-28 10:53:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:http-bridge-tls-cluster-name not ready, will try again in 10000 ms (839942ms till timeout)
2022-03-28 10:53:24 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1209422ms till timeout)
2022-03-28 10:53:25 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1313533ms till timeout)
2022-03-28 10:53:25 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1216921ms till timeout)
2022-03-28 10:53:25 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1235316ms till timeout)
2022-03-28 10:53:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (111292ms till timeout)
2022-03-28 10:53:25 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1208242ms till timeout)
2022-03-28 10:53:26 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1312493ms till timeout)
2022-03-28 10:53:26 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1215879ms till timeout)
2022-03-28 10:53:26 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1234252ms till timeout)
2022-03-28 10:53:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (109996ms till timeout)
2022-03-28 10:53:26 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1207208ms till timeout)
2022-03-28 10:53:27 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:230] testPauseReconciliationInKafkaRebalanceAndTopic test now can proceed its execution
2022-03-28 10:53:27 [ForkJoinPool-1-worker-15] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 10:53:27 [ForkJoinPool-1-worker-15] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-da91577a, testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383, testDeployAndUnDeployCruiseControl=my-cluster-c0830cbe, testConfigurationFileIsCreated=my-cluster-e0fac774, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8, testCapacityFile=my-cluster-fd0fb61a, testSendSimpleMessageTls=my-cluster-27ddd6ba, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c, testReceiveSimpleMessageTls=my-cluster-c6a15963, testConfigurationPerformanceOptions=my-cluster-b6310693, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-f50d425d, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5}
2022-03-28 10:53:27 [ForkJoinPool-1-worker-15] TRACE [AbstractST:607] USERS_NAME_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-user-1260379615-1050949907, testReceiveSimpleMessageTlsScramSha=my-user-1915916223-1265293973, testDeployAndUnDeployCruiseControl=my-user-1988711145-439861161, testConfigurationFileIsCreated=my-user-180611180-50530491, testThrottlingQuotasCreateTopic=my-user-1131806865-947217230, testCapacityFile=my-user-937015144-1959439396, testSendSimpleMessageTls=my-user-738229993-1095121647, testCreatingUsersWithSecretPrefix=my-user-1450786684-1808961062, testReceiveSimpleMessageTls=my-user-6088058-1142028780, testConfigurationPerformanceOptions=my-user-930975101-640523779, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-745974575-1852384454, testSendSimpleMessageTlsScramSha=my-user-1499386234-1249154328}
2022-03-28 10:53:27 [ForkJoinPool-1-worker-15] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-1674392683-1637010870, testReceiveSimpleMessageTlsScramSha=my-topic-1834944466-2094449929, testDeployAndUnDeployCruiseControl=my-topic-1511079187-95055629, testConfigurationFileIsCreated=my-topic-1296407588-313880708, testThrottlingQuotasCreateTopic=my-topic-728672490-1194703402, testCapacityFile=my-topic-1155641705-902584520, testSendSimpleMessageTls=my-topic-563023482-1697359353, testCreatingUsersWithSecretPrefix=my-topic-1479520843-618891199, testReceiveSimpleMessageTls=my-topic-478514536-2132782608, testConfigurationPerformanceOptions=my-topic-213912446-1165431808, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-254848724-189700675, testSendSimpleMessageTlsScramSha=my-topic-1028951580-1656411053}
2022-03-28 10:53:27 [ForkJoinPool-1-worker-15] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-da91577a-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-c0830cbe-kafka-clients, testConfigurationFileIsCreated=my-cluster-e0fac774-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8-kafka-clients, testCapacityFile=my-cluster-fd0fb61a-kafka-clients, testSendSimpleMessageTls=my-cluster-27ddd6ba-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c-kafka-clients, testReceiveSimpleMessageTls=my-cluster-c6a15963-kafka-clients, testConfigurationPerformanceOptions=my-cluster-b6310693-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-f50d425d-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5-kafka-clients}
2022-03-28 10:53:27 [ForkJoinPool-1-worker-15] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-6 for test case:testPauseReconciliationInKafkaRebalanceAndTopic
2022-03-28 10:53:27 [ForkJoinPool-1-worker-15] INFO  [KubeClusterResource:156] Creating Namespace: namespace-6
2022-03-28 10:53:27 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1311437ms till timeout)
2022-03-28 10:53:27 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Namespace namespace-6
2022-03-28 10:53:27 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace reconciliation-st get Namespace namespace-6 -o json
2022-03-28 10:53:27 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1214836ms till timeout)
2022-03-28 10:53:27 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1233143ms till timeout)
2022-03-28 10:53:27 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace reconciliation-st get Namespace namespace-6 -o json
2022-03-28 10:53:27 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:53:27 [ForkJoinPool-1-worker-15] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c27,c24",
            "openshift.io/sa.scc.supplemental-groups": "1000750000/10000",
            "openshift.io/sa.scc.uid-range": "1000750000/10000"
        },
        "creationTimestamp": "2022-03-28T10:53:24Z",
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T10:52:55Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:status": {
                        "f:phase": {}
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T10:53:24Z"
            }
        ],
        "name": "namespace-6",
        "resourceVersion": "1870047",
        "selfLink": "/api/v1/namespaces/namespace-6",
        "uid": "511c9b4b-6367-4054-90cd-db57527b42b7"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 10:53:27 [ForkJoinPool-1-worker-15] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[namespace-1], io.strimzi.test.logs.CollectorElement@aec142ef=[namespace-2], io.strimzi.test.logs.CollectorElement@f851b6c3=[namespace-5], io.strimzi.test.logs.CollectorElement@3881d5f2=[namespace-4], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-6], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-3], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 10:53:27 [ForkJoinPool-1-worker-15] INFO  [KubeClusterResource:82] Client use Namespace: namespace-6
2022-03-28 10:53:27 [ForkJoinPool-1-worker-15] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-6, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 10:53:27 [ForkJoinPool-1-worker-15] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-6
2022-03-28 10:53:27 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-da91577a in namespace namespace-6
2022-03-28 10:53:27 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:164] Using Namespace: namespace-6
2022-03-28 10:53:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (108845ms till timeout)
2022-03-28 10:53:28 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1206113ms till timeout)
2022-03-28 10:53:28 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-da91577a
2022-03-28 10:53:28 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-da91577a will have desired state: Ready
2022-03-28 10:53:28 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-da91577a will have desired state: Ready
2022-03-28 10:53:28 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1319962ms till timeout)
2022-03-28 10:53:28 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1310393ms till timeout)
2022-03-28 10:53:28 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1213767ms till timeout)
2022-03-28 10:53:28 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1231857ms till timeout)
2022-03-28 10:53:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (107664ms till timeout)
2022-03-28 10:53:29 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1204920ms till timeout)
2022-03-28 10:53:29 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1318911ms till timeout)
2022-03-28 10:53:29 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1309252ms till timeout)
2022-03-28 10:53:29 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1212634ms till timeout)
2022-03-28 10:53:29 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1230802ms till timeout)
2022-03-28 10:53:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (106603ms till timeout)
2022-03-28 10:53:30 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1203842ms till timeout)
2022-03-28 10:53:30 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1317831ms till timeout)
2022-03-28 10:53:30 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1211535ms till timeout)
2022-03-28 10:53:30 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1308101ms till timeout)
2022-03-28 10:53:31 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1229601ms till timeout)
2022-03-28 10:53:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (105375ms till timeout)
2022-03-28 10:53:31 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1202717ms till timeout)
2022-03-28 10:53:31 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1316753ms till timeout)
2022-03-28 10:53:31 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1210505ms till timeout)
2022-03-28 10:53:31 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1307000ms till timeout)
2022-03-28 10:53:32 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1228480ms till timeout)
2022-03-28 10:53:32 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1201568ms till timeout)
2022-03-28 10:53:32 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1315666ms till timeout)
2022-03-28 10:53:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (104139ms till timeout)
2022-03-28 10:53:32 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1209373ms till timeout)
2022-03-28 10:53:32 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1305889ms till timeout)
2022-03-28 10:53:33 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1227450ms till timeout)
2022-03-28 10:53:33 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-ede8537c-plain-kafka-clients not ready, will try again in 10000 ms (469540ms till timeout)
2022-03-28 10:53:33 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1200492ms till timeout)
2022-03-28 10:53:33 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1314591ms till timeout)
2022-03-28 10:53:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (103027ms till timeout)
2022-03-28 10:53:34 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1208277ms till timeout)
2022-03-28 10:53:34 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1304766ms till timeout)
2022-03-28 10:53:34 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1226434ms till timeout)
2022-03-28 10:53:34 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:369] ############################################################################
2022-03-28 10:53:34 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Namespace http-bridge-tls-st removal
2022-03-28 10:53:34 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace http-bridge-tls-st -o yaml
2022-03-28 10:53:34 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1199445ms till timeout)
2022-03-28 10:53:34 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1313520ms till timeout)
2022-03-28 10:53:34 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace http-bridge-tls-st -o yaml
2022-03-28 10:53:34 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 10:53:34 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (479692ms till timeout)
2022-03-28 10:53:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (101962ms till timeout)
2022-03-28 10:53:35 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1207263ms till timeout)
2022-03-28 10:53:35 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1303724ms till timeout)
2022-03-28 10:53:35 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1225397ms till timeout)
2022-03-28 10:53:35 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1198416ms till timeout)
2022-03-28 10:53:35 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1312493ms till timeout)
2022-03-28 10:53:35 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace http-bridge-tls-st -o yaml
2022-03-28 10:53:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (100869ms till timeout)
2022-03-28 10:53:36 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1206220ms till timeout)
2022-03-28 10:53:36 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace http-bridge-tls-st -o yaml
2022-03-28 10:53:36 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 10:53:36 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (478322ms till timeout)
2022-03-28 10:53:36 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1302609ms till timeout)
2022-03-28 10:53:36 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1224379ms till timeout)
2022-03-28 10:53:36 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1197372ms till timeout)
2022-03-28 10:53:36 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1311443ms till timeout)
2022-03-28 10:53:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (99764ms till timeout)
2022-03-28 10:53:37 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1205193ms till timeout)
2022-03-28 10:53:37 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace http-bridge-tls-st -o yaml
2022-03-28 10:53:37 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1301592ms till timeout)
2022-03-28 10:53:37 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1223362ms till timeout)
2022-03-28 10:53:37 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1196306ms till timeout)
2022-03-28 10:53:37 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1310384ms till timeout)
2022-03-28 10:53:38 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace http-bridge-tls-st -o yaml
2022-03-28 10:53:38 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 10:53:38 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (476327ms till timeout)
2022-03-28 10:53:38 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1204080ms till timeout)
2022-03-28 10:53:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (98600ms till timeout)
2022-03-28 10:53:38 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1300553ms till timeout)
2022-03-28 10:53:38 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1222300ms till timeout)
2022-03-28 10:53:38 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1195253ms till timeout)
2022-03-28 10:53:38 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1309351ms till timeout)
2022-03-28 10:53:39 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace http-bridge-tls-st -o yaml
2022-03-28 10:53:39 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1203033ms till timeout)
2022-03-28 10:53:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (97523ms till timeout)
2022-03-28 10:53:39 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1299533ms till timeout)
2022-03-28 10:53:39 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1221190ms till timeout)
2022-03-28 10:53:39 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace http-bridge-tls-st -o yaml
2022-03-28 10:53:39 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 10:53:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (474916ms till timeout)
2022-03-28 10:53:39 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1194230ms till timeout)
2022-03-28 10:53:39 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1308322ms till timeout)
2022-03-28 10:53:40 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1201994ms till timeout)
2022-03-28 10:53:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (96400ms till timeout)
2022-03-28 10:53:40 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1298425ms till timeout)
2022-03-28 10:53:40 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace http-bridge-tls-st -o yaml
2022-03-28 10:53:40 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1220152ms till timeout)
2022-03-28 10:53:40 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace http-bridge-tls-st -o yaml
2022-03-28 10:53:40 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 10:53:40 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (473550ms till timeout)
2022-03-28 10:53:40 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1193212ms till timeout)
2022-03-28 10:53:41 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1307274ms till timeout)
2022-03-28 10:53:41 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1200978ms till timeout)
2022-03-28 10:53:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (95344ms till timeout)
2022-03-28 10:53:41 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1297347ms till timeout)
2022-03-28 10:53:41 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1219132ms till timeout)
2022-03-28 10:53:41 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace http-bridge-tls-st -o yaml
2022-03-28 10:53:42 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1192133ms till timeout)
2022-03-28 10:53:42 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1306211ms till timeout)
2022-03-28 10:53:42 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace http-bridge-tls-st -o yaml
2022-03-28 10:53:42 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 10:53:42 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (472198ms till timeout)
2022-03-28 10:53:42 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1199955ms till timeout)
2022-03-28 10:53:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (94305ms till timeout)
2022-03-28 10:53:42 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1296292ms till timeout)
2022-03-28 10:53:42 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1218113ms till timeout)
2022-03-28 10:53:43 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1191075ms till timeout)
2022-03-28 10:53:43 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1305136ms till timeout)
2022-03-28 10:53:43 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace http-bridge-tls-st -o yaml
2022-03-28 10:53:43 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1198832ms till timeout)
2022-03-28 10:53:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (93249ms till timeout)
2022-03-28 10:53:43 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1295261ms till timeout)
2022-03-28 10:53:43 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace http-bridge-tls-st -o yaml
2022-03-28 10:53:43 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 10:53:43 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (470885ms till timeout)
2022-03-28 10:53:43 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1217061ms till timeout)
2022-03-28 10:53:43 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-ede8537c-plain-kafka-clients not ready, will try again in 10000 ms (459346ms till timeout)
2022-03-28 10:53:44 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1190046ms till timeout)
2022-03-28 10:53:44 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1304102ms till timeout)
2022-03-28 10:53:44 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1197796ms till timeout)
2022-03-28 10:53:44 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace http-bridge-tls-st -o yaml
2022-03-28 10:53:44 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1294183ms till timeout)
2022-03-28 10:53:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (92123ms till timeout)
2022-03-28 10:53:44 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1216037ms till timeout)
2022-03-28 10:53:45 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace http-bridge-tls-st -o yaml
2022-03-28 10:53:45 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 10:53:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (469525ms till timeout)
2022-03-28 10:53:45 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1189011ms till timeout)
2022-03-28 10:53:45 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1303074ms till timeout)
2022-03-28 10:53:45 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1196766ms till timeout)
2022-03-28 10:53:45 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1293146ms till timeout)
2022-03-28 10:53:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (91053ms till timeout)
2022-03-28 10:53:45 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1214977ms till timeout)
2022-03-28 10:53:46 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace http-bridge-tls-st -o yaml
2022-03-28 10:53:46 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1187963ms till timeout)
2022-03-28 10:53:46 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1301916ms till timeout)
2022-03-28 10:53:46 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace http-bridge-tls-st -o yaml
2022-03-28 10:53:46 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 10:53:46 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (468029ms till timeout)
2022-03-28 10:53:46 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1195731ms till timeout)
2022-03-28 10:53:46 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1292107ms till timeout)
2022-03-28 10:53:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (89883ms till timeout)
2022-03-28 10:53:46 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1213812ms till timeout)
2022-03-28 10:53:47 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1186924ms till timeout)
2022-03-28 10:53:47 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1300865ms till timeout)
2022-03-28 10:53:47 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace http-bridge-tls-st -o yaml
2022-03-28 10:53:47 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1194702ms till timeout)
2022-03-28 10:53:47 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1291093ms till timeout)
2022-03-28 10:53:47 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace http-bridge-tls-st -o yaml
2022-03-28 10:53:47 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 10:53:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (466606ms till timeout)
2022-03-28 10:53:48 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1212737ms till timeout)
2022-03-28 10:53:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (88806ms till timeout)
2022-03-28 10:53:48 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1185875ms till timeout)
2022-03-28 10:53:48 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1299740ms till timeout)
2022-03-28 10:53:48 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1193599ms till timeout)
2022-03-28 10:53:48 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1289987ms till timeout)
2022-03-28 10:53:48 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace http-bridge-tls-st -o yaml
2022-03-28 10:53:49 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1211435ms till timeout)
2022-03-28 10:53:49 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1184810ms till timeout)
2022-03-28 10:53:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (87422ms till timeout)
2022-03-28 10:53:49 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1298690ms till timeout)
2022-03-28 10:53:49 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1192549ms till timeout)
2022-03-28 10:53:49 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1288924ms till timeout)
2022-03-28 10:53:50 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1210384ms till timeout)
2022-03-28 10:53:50 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1183786ms till timeout)
2022-03-28 10:53:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (86369ms till timeout)
2022-03-28 10:53:50 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1297672ms till timeout)
2022-03-28 10:53:50 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1191534ms till timeout)
2022-03-28 10:53:51 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1287855ms till timeout)
2022-03-28 10:53:51 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1209345ms till timeout)
2022-03-28 10:53:51 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1182726ms till timeout)
2022-03-28 10:53:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (85326ms till timeout)
2022-03-28 10:53:51 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1296641ms till timeout)
2022-03-28 10:53:51 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1190515ms till timeout)
2022-03-28 10:53:52 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1286813ms till timeout)
2022-03-28 10:53:52 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1208319ms till timeout)
2022-03-28 10:53:52 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1181704ms till timeout)
2022-03-28 10:53:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (84279ms till timeout)
2022-03-28 10:53:52 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1295594ms till timeout)
2022-03-28 10:53:52 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1189479ms till timeout)
2022-03-28 10:53:53 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1285781ms till timeout)
2022-03-28 10:53:53 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1207279ms till timeout)
2022-03-28 10:53:53 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1180643ms till timeout)
2022-03-28 10:53:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (83244ms till timeout)
2022-03-28 10:53:53 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1294522ms till timeout)
2022-03-28 10:53:53 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1188378ms till timeout)
2022-03-28 10:53:54 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1284641ms till timeout)
2022-03-28 10:53:54 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-ede8537c-plain-kafka-clients not ready, will try again in 10000 ms (448893ms till timeout)
2022-03-28 10:53:54 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1206222ms till timeout)
2022-03-28 10:53:54 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1179596ms till timeout)
2022-03-28 10:53:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (82207ms till timeout)
2022-03-28 10:53:54 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1293510ms till timeout)
2022-03-28 10:53:54 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-6 get Namespace http-bridge-tls-st -o yaml
2022-03-28 10:53:54 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 1
2022-03-28 10:53:54 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 10:53:54 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Error from server (NotFound): namespaces "http-bridge-tls-st" not found
2022-03-28 10:53:54 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] ======STDERR END======
2022-03-28 10:53:54 [ForkJoinPool-1-worker-7] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[namespace-1], io.strimzi.test.logs.CollectorElement@aec142ef=[namespace-2], io.strimzi.test.logs.CollectorElement@f851b6c3=[namespace-5], io.strimzi.test.logs.CollectorElement@3881d5f2=[namespace-4], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-6], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-3], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 10:53:54 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:254] HttpBridgeTlsST - Notifies waiting test suites:[UserST, HttpBridgeTlsST, TopicST, CruiseControlConfigurationST, HttpBridgeScramShaST, ThrottlingQuotaST] to and randomly select one to start execution
2022-03-28 10:53:54 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:85] [bridge.HttpBridgeTlsST] - Removing parallel suite: HttpBridgeTlsST
2022-03-28 10:53:54 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:89] [bridge.HttpBridgeTlsST] - Parallel suites count: 5
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 409.297 s - in io.strimzi.systemtest.bridge.HttpBridgeTlsST
2022-03-28 10:53:54 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:23] ############################################################################
2022-03-28 10:53:54 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationReflection-STARTED
2022-03-28 10:53:54 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:658] ============================================================================
2022-03-28 10:53:54 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:659] [cruisecontrol.CruiseControlConfigurationST - Before Each] - Setup test case environment
2022-03-28 10:53:54 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:77] [cruisecontrol.CruiseControlConfigurationST] - Adding parallel test: testConfigurationReflection
2022-03-28 10:53:54 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:81] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 8
2022-03-28 10:53:54 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:205] [testConfigurationReflection] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:53:54 [ForkJoinPool-1-worker-7] TRACE [SuiteThreadController:210] testConfigurationReflection is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:53:54 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1187352ms till timeout)
2022-03-28 10:53:55 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1283610ms till timeout)
2022-03-28 10:53:55 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1205194ms till timeout)
2022-03-28 10:53:55 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1178530ms till timeout)
2022-03-28 10:53:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (81134ms till timeout)
2022-03-28 10:53:55 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1292413ms till timeout)
2022-03-28 10:53:55 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1186327ms till timeout)
2022-03-28 10:53:56 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1282548ms till timeout)
2022-03-28 10:53:56 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1204073ms till timeout)
2022-03-28 10:53:56 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1177429ms till timeout)
2022-03-28 10:53:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (80025ms till timeout)
2022-03-28 10:53:56 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1291384ms till timeout)
2022-03-28 10:53:57 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1185275ms till timeout)
2022-03-28 10:53:57 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1281510ms till timeout)
2022-03-28 10:53:57 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1176381ms till timeout)
2022-03-28 10:53:57 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1202950ms till timeout)
2022-03-28 10:53:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (78939ms till timeout)
2022-03-28 10:53:57 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1290354ms till timeout)
2022-03-28 10:53:58 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1184214ms till timeout)
2022-03-28 10:53:58 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1280448ms till timeout)
2022-03-28 10:53:58 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1175313ms till timeout)
2022-03-28 10:53:58 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1201892ms till timeout)
2022-03-28 10:53:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (77792ms till timeout)
2022-03-28 10:53:59 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1289233ms till timeout)
2022-03-28 10:53:59 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1183169ms till timeout)
2022-03-28 10:53:59 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1279303ms till timeout)
2022-03-28 10:53:59 [ForkJoinPool-1-worker-7] TRACE [SuiteThreadController:210] testConfigurationReflection is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:53:59 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1174284ms till timeout)
2022-03-28 10:53:59 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1200856ms till timeout)
2022-03-28 10:54:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (76709ms till timeout)
2022-03-28 10:54:00 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1288148ms till timeout)
2022-03-28 10:54:00 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1182147ms till timeout)
2022-03-28 10:54:00 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1278247ms till timeout)
2022-03-28 10:54:00 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1173245ms till timeout)
2022-03-28 10:54:00 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1199803ms till timeout)
2022-03-28 10:54:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (75640ms till timeout)
2022-03-28 10:54:01 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1181067ms till timeout)
2022-03-28 10:54:01 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1287049ms till timeout)
2022-03-28 10:54:01 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1277225ms till timeout)
2022-03-28 10:54:01 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1172208ms till timeout)
2022-03-28 10:54:02 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1198781ms till timeout)
2022-03-28 10:54:02 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1179986ms till timeout)
2022-03-28 10:54:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (74530ms till timeout)
2022-03-28 10:54:02 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1285971ms till timeout)
2022-03-28 10:54:02 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1276193ms till timeout)
2022-03-28 10:54:03 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1171184ms till timeout)
2022-03-28 10:54:03 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1197720ms till timeout)
2022-03-28 10:54:03 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1178952ms till timeout)
2022-03-28 10:54:03 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1284930ms till timeout)
2022-03-28 10:54:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (73484ms till timeout)
2022-03-28 10:54:03 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1275176ms till timeout)
2022-03-28 10:54:04 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1170052ms till timeout)
2022-03-28 10:54:04 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1196574ms till timeout)
2022-03-28 10:54:04 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-ede8537c-plain-kafka-clients not ready, will try again in 10000 ms (438781ms till timeout)
2022-03-28 10:54:04 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1177935ms till timeout)
2022-03-28 10:54:04 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1283911ms till timeout)
2022-03-28 10:54:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (72421ms till timeout)
2022-03-28 10:54:04 [ForkJoinPool-1-worker-7] TRACE [SuiteThreadController:210] testConfigurationReflection is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:54:04 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1274069ms till timeout)
2022-03-28 10:54:05 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1169023ms till timeout)
2022-03-28 10:54:05 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1195531ms till timeout)
2022-03-28 10:54:05 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1176898ms till timeout)
2022-03-28 10:54:05 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1282855ms till timeout)
2022-03-28 10:54:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (71344ms till timeout)
2022-03-28 10:54:05 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1273038ms till timeout)
2022-03-28 10:54:06 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1167989ms till timeout)
2022-03-28 10:54:06 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1194480ms till timeout)
2022-03-28 10:54:06 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1175868ms till timeout)
2022-03-28 10:54:06 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1281764ms till timeout)
2022-03-28 10:54:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (70247ms till timeout)
2022-03-28 10:54:06 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1271974ms till timeout)
2022-03-28 10:54:07 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1166917ms till timeout)
2022-03-28 10:54:07 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1193440ms till timeout)
2022-03-28 10:54:07 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1174844ms till timeout)
2022-03-28 10:54:07 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1280717ms till timeout)
2022-03-28 10:54:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (69197ms till timeout)
2022-03-28 10:54:07 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1270920ms till timeout)
2022-03-28 10:54:08 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1165865ms till timeout)
2022-03-28 10:54:08 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1192337ms till timeout)
2022-03-28 10:54:08 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-c0830cbe will have desired state: Ready not ready, will try again in 1000 ms (1173823ms till timeout)
2022-03-28 10:54:08 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1279665ms till timeout)
2022-03-28 10:54:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (68057ms till timeout)
2022-03-28 10:54:08 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1269897ms till timeout)
2022-03-28 10:54:09 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1164829ms till timeout)
2022-03-28 10:54:09 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1191308ms till timeout)
2022-03-28 10:54:09 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:444] Kafka: my-cluster-c0830cbe is in desired state: Ready
2022-03-28 10:54:09 [ForkJoinPool-1-worker-1] INFO  [CruiseControlConfigurationST:111] Removing Cruise Control to the classic Kafka.
2022-03-28 10:54:09 [ForkJoinPool-1-worker-1] INFO  [RollingUpdateUtils:73] Waiting for component with name: my-cluster-c0830cbe-kafka rolling update
2022-03-28 10:54:09 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:74] Waiting for rolling update of component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})
2022-03-28 10:54:09 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for component with name my-cluster-c0830cbe-kafka rolling update
2022-03-28 10:54:09 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=b9a4429e-22d8-41a5-b7e5-723851ceb10a, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:54:09 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1278630ms till timeout)
2022-03-28 10:54:09 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=b9a4429e-22d8-41a5-b7e5-723851ceb10a, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:54:09 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=b9a4429e-22d8-41a5-b7e5-723851ceb10a, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:54:09 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-2 hasn't rolled
2022-03-28 10:54:09 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1799955ms till timeout)
2022-03-28 10:54:09 [ForkJoinPool-1-worker-7] TRACE [SuiteThreadController:210] testConfigurationReflection is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:54:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (66954ms till timeout)
2022-03-28 10:54:10 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1268850ms till timeout)
2022-03-28 10:54:10 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1163813ms till timeout)
2022-03-28 10:54:10 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1190226ms till timeout)
2022-03-28 10:54:10 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1277601ms till timeout)
2022-03-28 10:54:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (65838ms till timeout)
2022-03-28 10:54:11 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1267821ms till timeout)
2022-03-28 10:54:11 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1162753ms till timeout)
2022-03-28 10:54:11 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1189157ms till timeout)
2022-03-28 10:54:11 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1276576ms till timeout)
2022-03-28 10:54:12 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1266778ms till timeout)
2022-03-28 10:54:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (64719ms till timeout)
2022-03-28 10:54:12 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1161710ms till timeout)
2022-03-28 10:54:12 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1188110ms till timeout)
2022-03-28 10:54:12 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1275541ms till timeout)
2022-03-28 10:54:13 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1265753ms till timeout)
2022-03-28 10:54:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (63588ms till timeout)
2022-03-28 10:54:13 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1160635ms till timeout)
2022-03-28 10:54:13 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1187037ms till timeout)
2022-03-28 10:54:13 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1274431ms till timeout)
2022-03-28 10:54:14 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1264682ms till timeout)
2022-03-28 10:54:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (62457ms till timeout)
2022-03-28 10:54:14 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of Deployment my-cluster-ede8537c-tls-kafka-clients in namespace namespace-1
2022-03-28 10:54:14 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-ede8537c-tls-kafka-clients
2022-03-28 10:54:14 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1159533ms till timeout)
2022-03-28 10:54:14 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=b9a4429e-22d8-41a5-b7e5-723851ceb10a, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:54:14 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-ede8537c-tls-kafka-clients not ready, will try again in 10000 ms (479821ms till timeout)
2022-03-28 10:54:14 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=b9a4429e-22d8-41a5-b7e5-723851ceb10a, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:54:14 [ForkJoinPool-1-worker-7] TRACE [SuiteThreadController:210] testConfigurationReflection is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:54:14 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=b9a4429e-22d8-41a5-b7e5-723851ceb10a, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:54:14 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-2 hasn't rolled
2022-03-28 10:54:14 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1794860ms till timeout)
2022-03-28 10:54:14 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1185912ms till timeout)
2022-03-28 10:54:14 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1273329ms till timeout)
2022-03-28 10:54:15 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1263664ms till timeout)
2022-03-28 10:54:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (61337ms till timeout)
2022-03-28 10:54:15 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1158482ms till timeout)
2022-03-28 10:54:15 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1184824ms till timeout)
2022-03-28 10:54:16 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1272237ms till timeout)
2022-03-28 10:54:16 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1262623ms till timeout)
2022-03-28 10:54:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (60289ms till timeout)
2022-03-28 10:54:16 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1157430ms till timeout)
2022-03-28 10:54:17 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1183764ms till timeout)
2022-03-28 10:54:17 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1271103ms till timeout)
2022-03-28 10:54:17 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1261521ms till timeout)
2022-03-28 10:54:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (59153ms till timeout)
2022-03-28 10:54:17 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1156355ms till timeout)
2022-03-28 10:54:18 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1182717ms till timeout)
2022-03-28 10:54:18 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1270042ms till timeout)
2022-03-28 10:54:18 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1260484ms till timeout)
2022-03-28 10:54:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (58086ms till timeout)
2022-03-28 10:54:18 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Kafka: my-cluster-e0fac774 will have desired state: Ready not ready, will try again in 1000 ms (1155316ms till timeout)
2022-03-28 10:54:19 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1181655ms till timeout)
2022-03-28 10:54:19 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1268982ms till timeout)
2022-03-28 10:54:19 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1259438ms till timeout)
2022-03-28 10:54:19 [ForkJoinPool-1-worker-7] TRACE [SuiteThreadController:210] testConfigurationReflection is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:54:19 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=b9a4429e-22d8-41a5-b7e5-723851ceb10a, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:54:19 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=b9a4429e-22d8-41a5-b7e5-723851ceb10a, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:54:19 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=b9a4429e-22d8-41a5-b7e5-723851ceb10a, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:54:19 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-2 hasn't rolled
2022-03-28 10:54:19 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1789782ms till timeout)
2022-03-28 10:54:19 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:444] Kafka: my-cluster-e0fac774 is in desired state: Ready
2022-03-28 10:54:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (56935ms till timeout)
2022-03-28 10:54:19 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-2 exec my-cluster-e0fac774-cruise-control-5b6b748dbd-6652q -- /bin/bash -c cat /tmp/cruisecontrol.properties
2022-03-28 10:54:20 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1180577ms till timeout)
2022-03-28 10:54:20 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1267904ms till timeout)
2022-03-28 10:54:20 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1258421ms till timeout)
2022-03-28 10:54:20 [ForkJoinPool-1-worker-9] INFO  [PodUtils:189] Message org.apache.kafka.common.errors.ThrottlingQuotaExceededException: The throttling quota has been exceeded. found in create-admin-my-cluster-83f3f4b8-kafka-clients-5p99p log
2022-03-28 10:54:21 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment create-admin-my-cluster-83f3f4b8-kafka-clients deletion
2022-03-28 10:54:21 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for ReplicaSet create-admin-my-cluster-83f3f4b8-kafka-clients to be deleted
2022-03-28 10:54:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] ReplicaSet create-admin-my-cluster-83f3f4b8-kafka-clients to be deleted not ready, will try again in 5000 ms (179942ms till timeout)
2022-03-28 10:54:21 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1179463ms till timeout)
2022-03-28 10:54:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1266856ms till timeout)
2022-03-28 10:54:21 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1257382ms till timeout)
2022-03-28 10:54:22 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1178395ms till timeout)
2022-03-28 10:54:22 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1265747ms till timeout)
2022-03-28 10:54:22 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1256254ms till timeout)
2022-03-28 10:54:23 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1177319ms till timeout)
2022-03-28 10:54:23 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1264661ms till timeout)
2022-03-28 10:54:23 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1255189ms till timeout)
2022-03-28 10:54:24 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1176266ms till timeout)
2022-03-28 10:54:24 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1263616ms till timeout)
2022-03-28 10:54:24 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1254135ms till timeout)
2022-03-28 10:54:24 [ForkJoinPool-1-worker-7] TRACE [SuiteThreadController:210] testConfigurationReflection is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:54:24 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=b9a4429e-22d8-41a5-b7e5-723851ceb10a, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:54:24 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-ede8537c-tls-kafka-clients not ready, will try again in 10000 ms (469582ms till timeout)
2022-03-28 10:54:24 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=b9a4429e-22d8-41a5-b7e5-723851ceb10a, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:54:24 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=b9a4429e-22d8-41a5-b7e5-723851ceb10a, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:54:24 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-2 hasn't rolled
2022-03-28 10:54:24 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1784663ms till timeout)
2022-03-28 10:54:25 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1175084ms till timeout)
2022-03-28 10:54:25 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1262545ms till timeout)
2022-03-28 10:54:25 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1253116ms till timeout)
2022-03-28 10:54:26 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:40] Job create-admin-my-cluster-83f3f4b8-kafka-clients was deleted
2022-03-28 10:54:26 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-101
2022-03-28 10:54:26 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1261405ms till timeout)
2022-03-28 10:54:26 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1173893ms till timeout)
2022-03-28 10:54:26 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1251990ms till timeout)
2022-03-28 10:54:27 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1250923ms till timeout)
2022-03-28 10:54:27 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1172805ms till timeout)
2022-03-28 10:54:27 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1260317ms till timeout)
2022-03-28 10:54:29 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1249822ms till timeout)
2022-03-28 10:54:29 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1259216ms till timeout)
2022-03-28 10:54:29 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-f50d425d will have desired state: Ready not ready, will try again in 1000 ms (1171704ms till timeout)
2022-03-28 10:54:29 [ForkJoinPool-1-worker-7] TRACE [SuiteThreadController:210] testConfigurationReflection is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:54:30 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=b9a4429e-22d8-41a5-b7e5-723851ceb10a, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:54:30 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1248732ms till timeout)
2022-03-28 10:54:30 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:444] Kafka: my-cluster-f50d425d is in desired state: Ready
2022-03-28 10:54:30 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1258133ms till timeout)
2022-03-28 10:54:30 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=b9a4429e-22d8-41a5-b7e5-723851ceb10a, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:54:30 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=b9a4429e-22d8-41a5-b7e5-723851ceb10a, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:54:30 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-2 hasn't rolled
2022-03-28 10:54:30 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1779497ms till timeout)
2022-03-28 10:54:30 [ForkJoinPool-1-worker-3] INFO  [CruiseControlConfigurationST:157] Changing the broker capacity of the cruise control
2022-03-28 10:54:30 [ForkJoinPool-1-worker-3] INFO  [CruiseControlConfigurationST:168] Verifying that CC pod is rolling, because of change size of disk
2022-03-28 10:54:30 [ForkJoinPool-1-worker-3] INFO  [DeploymentUtils:136] Waiting for Deployment my-cluster-f50d425d-cruise-control rolling update
2022-03-28 10:54:30 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Deployment my-cluster-f50d425d-cruise-control rolling update in namespace:namespace-4
2022-03-28 10:54:30 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-f50d425d-cruise-control-65f54f8b4d-lwmc4=90f7ed91-d8e3-496f-85cb-a3b47a769d64}
2022-03-28 10:54:30 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-f50d425d-cruise-control-65f54f8b4d-lwmc4=90f7ed91-d8e3-496f-85cb-a3b47a769d64}
2022-03-28 10:54:30 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-f50d425d-cruise-control-65f54f8b4d-lwmc4=90f7ed91-d8e3-496f-85cb-a3b47a769d64}
2022-03-28 10:54:30 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Deployment my-cluster-f50d425d-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (599833ms till timeout)
2022-03-28 10:54:31 [ForkJoinPool-1-worker-17] INFO  [Exec:417] Command: oc --namespace namespace-2 exec my-cluster-e0fac774-cruise-control-5b6b748dbd-6652q -- /bin/bash -c cat /tmp/cruisecontrol.properties
2022-03-28 10:54:31 [ForkJoinPool-1-worker-17] INFO  [Exec:417] Return code: 0
2022-03-28 10:54:31 [ForkJoinPool-1-worker-17] DEBUG [AbstractST:674] ============================================================================
2022-03-28 10:54:31 [ForkJoinPool-1-worker-17] DEBUG [AbstractST:675] [cruisecontrol.CruiseControlConfigurationST - After Each] - Clean up after test
2022-03-28 10:54:31 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:344] ############################################################################
2022-03-28 10:54:31 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:348] Delete all resources for testConfigurationFileIsCreated
2022-03-28 10:54:31 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:241] Delete of Kafka my-cluster-e0fac774 in namespace namespace-2
2022-03-28 10:54:31 [ForkJoinPool-1-worker-17] INFO  [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-2, for cruise control Kafka cluster my-cluster-e0fac774
2022-03-28 10:54:31 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1247652ms till timeout)
2022-03-28 10:54:31 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1257067ms till timeout)
2022-03-28 10:54:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-101
2022-03-28 10:54:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:31 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-106
2022-03-28 10:54:31 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-e0fac774
2022-03-28 10:54:32 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-e0fac774 not ready, will try again in 10000 ms (839882ms till timeout)
2022-03-28 10:54:32 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1255973ms till timeout)
2022-03-28 10:54:32 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1246514ms till timeout)
2022-03-28 10:54:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-106
2022-03-28 10:54:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:32 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-109
2022-03-28 10:54:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-109
2022-03-28 10:54:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:33 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-11
2022-03-28 10:54:33 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1254829ms till timeout)
2022-03-28 10:54:33 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1245386ms till timeout)
2022-03-28 10:54:34 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1244268ms till timeout)
2022-03-28 10:54:34 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1253683ms till timeout)
2022-03-28 10:54:34 [ForkJoinPool-1-worker-7] TRACE [SuiteThreadController:210] testConfigurationReflection is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:54:35 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=b9a4429e-22d8-41a5-b7e5-723851ceb10a, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:54:35 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:54:35 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:54:35 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-2 hasn't rolled
2022-03-28 10:54:35 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1774337ms till timeout)
2022-03-28 10:54:35 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-ede8537c-tls-kafka-clients not ready, will try again in 10000 ms (459202ms till timeout)
2022-03-28 10:54:35 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-f50d425d-cruise-control-65f54f8b4d-lwmc4=90f7ed91-d8e3-496f-85cb-a3b47a769d64}
2022-03-28 10:54:35 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1243111ms till timeout)
2022-03-28 10:54:35 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1252502ms till timeout)
2022-03-28 10:54:35 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-f50d425d-cruise-control-65f54f8b4d-lwmc4=90f7ed91-d8e3-496f-85cb-a3b47a769d64}
2022-03-28 10:54:35 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-f50d425d-cruise-control-65f54f8b4d-lwmc4=90f7ed91-d8e3-496f-85cb-a3b47a769d64}
2022-03-28 10:54:35 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Deployment my-cluster-f50d425d-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (594645ms till timeout)
2022-03-28 10:54:36 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1251400ms till timeout)
2022-03-28 10:54:36 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1241985ms till timeout)
2022-03-28 10:54:37 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1240929ms till timeout)
2022-03-28 10:54:37 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1250344ms till timeout)
2022-03-28 10:54:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-11
2022-03-28 10:54:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:38 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-110
2022-03-28 10:54:39 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1239854ms till timeout)
2022-03-28 10:54:39 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1249269ms till timeout)
2022-03-28 10:54:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-110
2022-03-28 10:54:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:39 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-112
2022-03-28 10:54:39 [ForkJoinPool-1-worker-7] TRACE [SuiteThreadController:210] testConfigurationReflection is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:54:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-112
2022-03-28 10:54:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:39 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-113
2022-03-28 10:54:40 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1248225ms till timeout)
2022-03-28 10:54:40 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1238810ms till timeout)
2022-03-28 10:54:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-113
2022-03-28 10:54:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:40 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-114
2022-03-28 10:54:40 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=b9a4429e-22d8-41a5-b7e5-723851ceb10a, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:54:40 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:54:40 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:54:40 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-2 hasn't rolled
2022-03-28 10:54:40 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1769234ms till timeout)
2022-03-28 10:54:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-114
2022-03-28 10:54:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:40 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-116
2022-03-28 10:54:40 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-f50d425d-cruise-control-65f54f8b4d-lwmc4=90f7ed91-d8e3-496f-85cb-a3b47a769d64}
2022-03-28 10:54:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-116
2022-03-28 10:54:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:40 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-12
2022-03-28 10:54:41 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-f50d425d-cruise-control-65f54f8b4d-lwmc4=90f7ed91-d8e3-496f-85cb-a3b47a769d64}
2022-03-28 10:54:41 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-f50d425d-cruise-control-65f54f8b4d-lwmc4=90f7ed91-d8e3-496f-85cb-a3b47a769d64}
2022-03-28 10:54:41 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Deployment my-cluster-f50d425d-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (589436ms till timeout)
2022-03-28 10:54:41 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1237713ms till timeout)
2022-03-28 10:54:41 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1247128ms till timeout)
2022-03-28 10:54:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-12
2022-03-28 10:54:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:41 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-124
2022-03-28 10:54:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-124
2022-03-28 10:54:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:41 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-128
2022-03-28 10:54:42 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:369] ############################################################################
2022-03-28 10:54:42 [ForkJoinPool-1-worker-17] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-2 for test case:testConfigurationFileIsCreated
2022-03-28 10:54:42 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for Namespace namespace-2 removal
2022-03-28 10:54:42 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1246030ms till timeout)
2022-03-28 10:54:42 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:54:42 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1236593ms till timeout)
2022-03-28 10:54:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-128
2022-03-28 10:54:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:42 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-13
2022-03-28 10:54:42 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:54:42 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:42 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (479668ms till timeout)
2022-03-28 10:54:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-13
2022-03-28 10:54:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:42 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-132
2022-03-28 10:54:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-132
2022-03-28 10:54:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:43 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-133
2022-03-28 10:54:43 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1244970ms till timeout)
2022-03-28 10:54:43 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1235496ms till timeout)
2022-03-28 10:54:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-133
2022-03-28 10:54:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:43 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-136
2022-03-28 10:54:43 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:54:43 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:54:43 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:43 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (478385ms till timeout)
2022-03-28 10:54:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-136
2022-03-28 10:54:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:44 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-137
2022-03-28 10:54:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-137
2022-03-28 10:54:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:44 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-139
2022-03-28 10:54:44 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1243840ms till timeout)
2022-03-28 10:54:44 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1234357ms till timeout)
2022-03-28 10:54:44 [ForkJoinPool-1-worker-7] TRACE [SuiteThreadController:210] testConfigurationReflection is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:54:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-139
2022-03-28 10:54:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:44 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-144
2022-03-28 10:54:44 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:54:45 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:54:45 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:45 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (477063ms till timeout)
2022-03-28 10:54:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-144
2022-03-28 10:54:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:45 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-145
2022-03-28 10:54:45 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=b9a4429e-22d8-41a5-b7e5-723851ceb10a, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:54:45 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1233253ms till timeout)
2022-03-28 10:54:45 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:54:45 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:54:45 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-2 hasn't rolled
2022-03-28 10:54:45 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1764032ms till timeout)
2022-03-28 10:54:45 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1242664ms till timeout)
2022-03-28 10:54:45 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-ede8537c-tls-kafka-clients not ready, will try again in 10000 ms (448836ms till timeout)
2022-03-28 10:54:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-145
2022-03-28 10:54:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:45 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-148
2022-03-28 10:54:46 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-f50d425d-cruise-control-65f54f8b4d-lwmc4=90f7ed91-d8e3-496f-85cb-a3b47a769d64}
2022-03-28 10:54:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-148
2022-03-28 10:54:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:46 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-149
2022-03-28 10:54:46 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-f50d425d-cruise-control-65f54f8b4d-lwmc4=90f7ed91-d8e3-496f-85cb-a3b47a769d64}
2022-03-28 10:54:46 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-f50d425d-cruise-control-65f54f8b4d-lwmc4=90f7ed91-d8e3-496f-85cb-a3b47a769d64}
2022-03-28 10:54:46 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Deployment my-cluster-f50d425d-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (584303ms till timeout)
2022-03-28 10:54:46 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:54:46 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:54:46 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:46 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (475689ms till timeout)
2022-03-28 10:54:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-149
2022-03-28 10:54:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:46 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-15
2022-03-28 10:54:46 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1241614ms till timeout)
2022-03-28 10:54:46 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1232199ms till timeout)
2022-03-28 10:54:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-15
2022-03-28 10:54:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:47 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-153
2022-03-28 10:54:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-153
2022-03-28 10:54:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:47 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-156
2022-03-28 10:54:47 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:54:47 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1231085ms till timeout)
2022-03-28 10:54:47 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1240444ms till timeout)
2022-03-28 10:54:47 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:54:47 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:47 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (474364ms till timeout)
2022-03-28 10:54:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-156
2022-03-28 10:54:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:48 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-158
2022-03-28 10:54:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-158
2022-03-28 10:54:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:48 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-16
2022-03-28 10:54:48 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1230020ms till timeout)
2022-03-28 10:54:48 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:54:48 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1239377ms till timeout)
2022-03-28 10:54:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-16
2022-03-28 10:54:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:49 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-160
2022-03-28 10:54:49 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:54:49 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:49 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (472875ms till timeout)
2022-03-28 10:54:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-160
2022-03-28 10:54:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:49 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-163
2022-03-28 10:54:49 [ForkJoinPool-1-worker-7] TRACE [SuiteThreadController:210] testConfigurationReflection is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:54:49 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1228983ms till timeout)
2022-03-28 10:54:49 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1238319ms till timeout)
2022-03-28 10:54:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-163
2022-03-28 10:54:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:50 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-165
2022-03-28 10:54:50 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:54:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-165
2022-03-28 10:54:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:50 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-166
2022-03-28 10:54:50 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=b9a4429e-22d8-41a5-b7e5-723851ceb10a, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:54:50 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:54:50 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:50 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (471619ms till timeout)
2022-03-28 10:54:50 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:54:50 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:54:50 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-2 hasn't rolled
2022-03-28 10:54:50 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1758926ms till timeout)
2022-03-28 10:54:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-166
2022-03-28 10:54:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:50 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-168
2022-03-28 10:54:50 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1227895ms till timeout)
2022-03-28 10:54:51 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1237297ms till timeout)
2022-03-28 10:54:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-168
2022-03-28 10:54:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:51 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-17
2022-03-28 10:54:51 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-f50d425d-cruise-control-65f54f8b4d-lwmc4=90f7ed91-d8e3-496f-85cb-a3b47a769d64}
2022-03-28 10:54:51 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-f50d425d-cruise-control-65f54f8b4d-lwmc4=90f7ed91-d8e3-496f-85cb-a3b47a769d64}
2022-03-28 10:54:51 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-f50d425d-cruise-control-65f54f8b4d-lwmc4=90f7ed91-d8e3-496f-85cb-a3b47a769d64}
2022-03-28 10:54:51 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Deployment my-cluster-f50d425d-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (579181ms till timeout)
2022-03-28 10:54:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-17
2022-03-28 10:54:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:51 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-170
2022-03-28 10:54:51 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:54:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-170
2022-03-28 10:54:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:51 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-172
2022-03-28 10:54:51 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:54:51 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:51 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (470269ms till timeout)
2022-03-28 10:54:52 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1226853ms till timeout)
2022-03-28 10:54:52 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1236233ms till timeout)
2022-03-28 10:54:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-172
2022-03-28 10:54:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:52 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-18
2022-03-28 10:54:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-18
2022-03-28 10:54:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:52 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-180
2022-03-28 10:54:52 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:54:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-180
2022-03-28 10:54:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:53 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-183
2022-03-28 10:54:53 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1225787ms till timeout)
2022-03-28 10:54:53 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1235187ms till timeout)
2022-03-28 10:54:53 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:54:53 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:53 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (468899ms till timeout)
2022-03-28 10:54:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-183
2022-03-28 10:54:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:53 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-19
2022-03-28 10:54:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-19
2022-03-28 10:54:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:53 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-192
2022-03-28 10:54:54 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1224717ms till timeout)
2022-03-28 10:54:54 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1234121ms till timeout)
2022-03-28 10:54:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-192
2022-03-28 10:54:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:54 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-193
2022-03-28 10:54:54 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:54:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-193
2022-03-28 10:54:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:54 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-195
2022-03-28 10:54:54 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:54:54 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:54 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (467665ms till timeout)
2022-03-28 10:54:54 [ForkJoinPool-1-worker-7] TRACE [SuiteThreadController:210] testConfigurationReflection is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:54:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-195
2022-03-28 10:54:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:54 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-196
2022-03-28 10:54:55 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1223695ms till timeout)
2022-03-28 10:54:55 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1233077ms till timeout)
2022-03-28 10:54:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-196
2022-03-28 10:54:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:55 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-197
2022-03-28 10:54:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-197
2022-03-28 10:54:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:55 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-198
2022-03-28 10:54:55 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:54:55 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=b9a4429e-22d8-41a5-b7e5-723851ceb10a, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:54:55 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:54:55 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:54:55 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-2 hasn't rolled
2022-03-28 10:54:55 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1753889ms till timeout)
2022-03-28 10:54:55 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of KafkaTopic my-topic-1479520843-618891199 in namespace namespace-1
2022-03-28 10:54:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-198
2022-03-28 10:54:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:55 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-200
2022-03-28 10:54:55 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:54:55 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:55 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (466363ms till timeout)
2022-03-28 10:54:55 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1479520843-618891199
2022-03-28 10:54:56 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1479520843-618891199 not ready, will try again in 10000 ms (179840ms till timeout)
2022-03-28 10:54:56 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1222629ms till timeout)
2022-03-28 10:54:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-200
2022-03-28 10:54:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:56 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-201
2022-03-28 10:54:56 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1231997ms till timeout)
2022-03-28 10:54:56 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-f50d425d-cruise-control-65f54f8b4d-lwmc4=90f7ed91-d8e3-496f-85cb-a3b47a769d64}
2022-03-28 10:54:56 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-f50d425d-cruise-control-65f54f8b4d-lwmc4=90f7ed91-d8e3-496f-85cb-a3b47a769d64}
2022-03-28 10:54:56 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-f50d425d-cruise-control-65f54f8b4d-lwmc4=90f7ed91-d8e3-496f-85cb-a3b47a769d64}
2022-03-28 10:54:56 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Deployment my-cluster-f50d425d-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (574004ms till timeout)
2022-03-28 10:54:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-201
2022-03-28 10:54:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:56 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-202
2022-03-28 10:54:56 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:54:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-202
2022-03-28 10:54:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:56 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-203
2022-03-28 10:54:57 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:54:57 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:57 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (465078ms till timeout)
2022-03-28 10:54:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-203
2022-03-28 10:54:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:57 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-204
2022-03-28 10:54:57 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1221598ms till timeout)
2022-03-28 10:54:57 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1230965ms till timeout)
2022-03-28 10:54:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-204
2022-03-28 10:54:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:57 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-205
2022-03-28 10:54:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-205
2022-03-28 10:54:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:58 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-206
2022-03-28 10:54:58 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:54:58 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1220529ms till timeout)
2022-03-28 10:54:58 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1229885ms till timeout)
2022-03-28 10:54:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-206
2022-03-28 10:54:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:58 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-207
2022-03-28 10:54:58 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:54:58 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:58 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (463746ms till timeout)
2022-03-28 10:54:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-207
2022-03-28 10:54:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:58 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-208
2022-03-28 10:54:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-208
2022-03-28 10:54:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:59 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-209
2022-03-28 10:54:59 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1219459ms till timeout)
2022-03-28 10:54:59 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1228857ms till timeout)
2022-03-28 10:54:59 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:54:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-209
2022-03-28 10:54:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:59 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-210
2022-03-28 10:54:59 [ForkJoinPool-1-worker-7] TRACE [SuiteThreadController:210] testConfigurationReflection is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:54:59 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:54:59 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 10:54:59 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (462409ms till timeout)
2022-03-28 10:55:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-210
2022-03-28 10:55:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:00 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-211
2022-03-28 10:55:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-211
2022-03-28 10:55:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:00 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-212
2022-03-28 10:55:00 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1218432ms till timeout)
2022-03-28 10:55:00 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1227820ms till timeout)
2022-03-28 10:55:00 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=b9a4429e-22d8-41a5-b7e5-723851ceb10a, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:55:00 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:55:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-212
2022-03-28 10:55:01 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:55:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:01 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-213
2022-03-28 10:55:01 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:55:01 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-2 hasn't rolled
2022-03-28 10:55:01 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1748480ms till timeout)
2022-03-28 10:55:01 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:55:01 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:01 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (461073ms till timeout)
2022-03-28 10:55:01 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-f50d425d-cruise-control-65f54f8b4d-lwmc4=90f7ed91-d8e3-496f-85cb-a3b47a769d64}
2022-03-28 10:55:01 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1217376ms till timeout)
2022-03-28 10:55:01 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1226788ms till timeout)
2022-03-28 10:55:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-213
2022-03-28 10:55:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:01 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-214
2022-03-28 10:55:01 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-f50d425d-cruise-control-65f54f8b4d-lwmc4=90f7ed91-d8e3-496f-85cb-a3b47a769d64}
2022-03-28 10:55:01 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-f50d425d-cruise-control-65f54f8b4d-lwmc4=90f7ed91-d8e3-496f-85cb-a3b47a769d64}
2022-03-28 10:55:01 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Deployment my-cluster-f50d425d-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (568844ms till timeout)
2022-03-28 10:55:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-214
2022-03-28 10:55:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:01 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-215
2022-03-28 10:55:02 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:55:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-215
2022-03-28 10:55:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:02 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-216
2022-03-28 10:55:02 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:55:02 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:02 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (459766ms till timeout)
2022-03-28 10:55:02 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1216294ms till timeout)
2022-03-28 10:55:02 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1225687ms till timeout)
2022-03-28 10:55:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-216
2022-03-28 10:55:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:02 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-217
2022-03-28 10:55:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-217
2022-03-28 10:55:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:03 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-218
2022-03-28 10:55:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-218
2022-03-28 10:55:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:03 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-219
2022-03-28 10:55:03 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:55:03 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1215176ms till timeout)
2022-03-28 10:55:03 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1224588ms till timeout)
2022-03-28 10:55:03 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:55:03 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:03 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (458554ms till timeout)
2022-03-28 10:55:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-219
2022-03-28 10:55:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:03 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-220
2022-03-28 10:55:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-220
2022-03-28 10:55:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:04 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-224
2022-03-28 10:55:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-224
2022-03-28 10:55:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:04 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-225
2022-03-28 10:55:04 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:55:04 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1223530ms till timeout)
2022-03-28 10:55:04 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1214114ms till timeout)
2022-03-28 10:55:04 [ForkJoinPool-1-worker-7] TRACE [SuiteThreadController:210] testConfigurationReflection is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:55:05 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:55:05 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:05 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (457172ms till timeout)
2022-03-28 10:55:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-225
2022-03-28 10:55:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:05 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-226
2022-03-28 10:55:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-226
2022-03-28 10:55:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:05 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-227
2022-03-28 10:55:05 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1222485ms till timeout)
2022-03-28 10:55:05 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1213051ms till timeout)
2022-03-28 10:55:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-227
2022-03-28 10:55:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:05 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-23
2022-03-28 10:55:06 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:55:06 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=b9a4429e-22d8-41a5-b7e5-723851ceb10a, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:55:06 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of KafkaUser encrypted-leopold in namespace namespace-1
2022-03-28 10:55:06 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:55:06 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:55:06 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-2 hasn't rolled
2022-03-28 10:55:06 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1743409ms till timeout)
2022-03-28 10:55:06 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:encrypted-leopold
2022-03-28 10:55:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-23
2022-03-28 10:55:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:06 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-232
2022-03-28 10:55:06 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:55:06 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:06 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (455849ms till timeout)
2022-03-28 10:55:06 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of Kafka my-cluster-ede8537c in namespace namespace-1
2022-03-28 10:55:06 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-ede8537c
2022-03-28 10:55:06 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-ede8537c not ready, will try again in 10000 ms (839952ms till timeout)
2022-03-28 10:55:06 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-f50d425d-cruise-control-65f54f8b4d-lwmc4=90f7ed91-d8e3-496f-85cb-a3b47a769d64}
2022-03-28 10:55:06 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-f50d425d-cruise-control-65f54f8b4d-lwmc4=90f7ed91-d8e3-496f-85cb-a3b47a769d64, my-cluster-f50d425d-cruise-control-8546d57b59-c8rgt=71096785-46b8-44d4-811e-8707410a33af}
2022-03-28 10:55:06 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-f50d425d-cruise-control-65f54f8b4d-lwmc4=90f7ed91-d8e3-496f-85cb-a3b47a769d64}
2022-03-28 10:55:06 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Deployment my-cluster-f50d425d-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (563756ms till timeout)
2022-03-28 10:55:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-232
2022-03-28 10:55:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:06 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-237
2022-03-28 10:55:06 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1221344ms till timeout)
2022-03-28 10:55:06 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1211927ms till timeout)
2022-03-28 10:55:07 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:55:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-237
2022-03-28 10:55:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:07 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-238
2022-03-28 10:55:07 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:55:07 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:07 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (454498ms till timeout)
2022-03-28 10:55:08 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1220234ms till timeout)
2022-03-28 10:55:08 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1210815ms till timeout)
2022-03-28 10:55:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-238
2022-03-28 10:55:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:08 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-242
2022-03-28 10:55:08 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:55:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-242
2022-03-28 10:55:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:08 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-243
2022-03-28 10:55:09 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:55:09 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:09 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (453219ms till timeout)
2022-03-28 10:55:09 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1219156ms till timeout)
2022-03-28 10:55:09 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1209741ms till timeout)
2022-03-28 10:55:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-243
2022-03-28 10:55:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:09 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-250
2022-03-28 10:55:09 [ForkJoinPool-1-worker-7] TRACE [SuiteThreadController:210] testConfigurationReflection is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:55:10 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:55:10 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1218085ms till timeout)
2022-03-28 10:55:10 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1208645ms till timeout)
2022-03-28 10:55:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-250
2022-03-28 10:55:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:10 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-252
2022-03-28 10:55:10 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:55:10 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:10 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (451841ms till timeout)
2022-03-28 10:55:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-252
2022-03-28 10:55:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:10 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-254
2022-03-28 10:55:11 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=b9a4429e-22d8-41a5-b7e5-723851ceb10a, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:55:11 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1216900ms till timeout)
2022-03-28 10:55:11 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1207485ms till timeout)
2022-03-28 10:55:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-254
2022-03-28 10:55:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:11 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-256
2022-03-28 10:55:11 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:55:11 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:55:11 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-2 hasn't rolled
2022-03-28 10:55:11 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1738239ms till timeout)
2022-03-28 10:55:11 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:55:11 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:55:11 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:11 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (450556ms till timeout)
2022-03-28 10:55:11 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-f50d425d-cruise-control-65f54f8b4d-lwmc4=90f7ed91-d8e3-496f-85cb-a3b47a769d64}
2022-03-28 10:55:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-256
2022-03-28 10:55:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:11 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-259
2022-03-28 10:55:11 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-f50d425d-cruise-control-65f54f8b4d-lwmc4=90f7ed91-d8e3-496f-85cb-a3b47a769d64, my-cluster-f50d425d-cruise-control-8546d57b59-c8rgt=71096785-46b8-44d4-811e-8707410a33af}
2022-03-28 10:55:11 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-f50d425d-cruise-control-65f54f8b4d-lwmc4=90f7ed91-d8e3-496f-85cb-a3b47a769d64}
2022-03-28 10:55:11 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Deployment my-cluster-f50d425d-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (558623ms till timeout)
2022-03-28 10:55:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-259
2022-03-28 10:55:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:12 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-262
2022-03-28 10:55:12 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1215827ms till timeout)
2022-03-28 10:55:12 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1206364ms till timeout)
2022-03-28 10:55:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-262
2022-03-28 10:55:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:12 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-267
2022-03-28 10:55:12 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:55:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-267
2022-03-28 10:55:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:12 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-268
2022-03-28 10:55:13 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:55:13 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:13 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (449256ms till timeout)
2022-03-28 10:55:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-268
2022-03-28 10:55:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:13 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-269
2022-03-28 10:55:13 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1214791ms till timeout)
2022-03-28 10:55:13 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1205318ms till timeout)
2022-03-28 10:55:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-269
2022-03-28 10:55:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:13 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-270
2022-03-28 10:55:14 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:55:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-270
2022-03-28 10:55:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:14 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-271
2022-03-28 10:55:14 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:55:14 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:14 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (447975ms till timeout)
2022-03-28 10:55:14 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1213688ms till timeout)
2022-03-28 10:55:14 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1204255ms till timeout)
2022-03-28 10:55:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-271
2022-03-28 10:55:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:14 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-272
2022-03-28 10:55:14 [ForkJoinPool-1-worker-7] TRACE [SuiteThreadController:210] testConfigurationReflection is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:55:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-272
2022-03-28 10:55:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:14 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-277
2022-03-28 10:55:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-277
2022-03-28 10:55:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:15 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-279
2022-03-28 10:55:15 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:55:15 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:55:15 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:15 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (446704ms till timeout)
2022-03-28 10:55:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-279
2022-03-28 10:55:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:15 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-284
2022-03-28 10:55:15 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1212648ms till timeout)
2022-03-28 10:55:15 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1203159ms till timeout)
2022-03-28 10:55:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-284
2022-03-28 10:55:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:16 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-286
2022-03-28 10:55:16 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=b9a4429e-22d8-41a5-b7e5-723851ceb10a, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:55:16 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:55:16 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:55:16 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-2 hasn't rolled
2022-03-28 10:55:16 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1733174ms till timeout)
2022-03-28 10:55:16 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:55:16 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1211611ms till timeout)
2022-03-28 10:55:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-286
2022-03-28 10:55:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:16 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-287
2022-03-28 10:55:16 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:369] ############################################################################
2022-03-28 10:55:16 [ForkJoinPool-1-worker-13] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-1 for test case:testCreatingUsersWithSecretPrefix
2022-03-28 10:55:16 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1202122ms till timeout)
2022-03-28 10:55:16 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Namespace namespace-1 removal
2022-03-28 10:55:16 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:16 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-f50d425d-cruise-control-65f54f8b4d-lwmc4=90f7ed91-d8e3-496f-85cb-a3b47a769d64}
2022-03-28 10:55:16 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:55:16 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:16 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (445332ms till timeout)
2022-03-28 10:55:17 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-f50d425d-cruise-control-65f54f8b4d-lwmc4=90f7ed91-d8e3-496f-85cb-a3b47a769d64, my-cluster-f50d425d-cruise-control-8546d57b59-c8rgt=71096785-46b8-44d4-811e-8707410a33af}
2022-03-28 10:55:17 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-f50d425d-cruise-control-65f54f8b4d-lwmc4=90f7ed91-d8e3-496f-85cb-a3b47a769d64}
2022-03-28 10:55:17 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Deployment my-cluster-f50d425d-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (553437ms till timeout)
2022-03-28 10:55:17 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:17 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:17 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (479664ms till timeout)
2022-03-28 10:55:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-287
2022-03-28 10:55:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:17 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-29
2022-03-28 10:55:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-29
2022-03-28 10:55:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:17 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-299
2022-03-28 10:55:17 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1210543ms till timeout)
2022-03-28 10:55:17 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1200996ms till timeout)
2022-03-28 10:55:17 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:55:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-299
2022-03-28 10:55:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:18 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-30
2022-03-28 10:55:18 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:18 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 10:55:18 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 1
2022-03-28 10:55:18 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 10:55:18 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-2" not found
2022-03-28 10:55:18 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] ======STDERR END======
2022-03-28 10:55:18 [ForkJoinPool-1-worker-17] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[namespace-1], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@f851b6c3=[namespace-5], io.strimzi.test.logs.CollectorElement@3881d5f2=[namespace-4], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-6], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-3], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 10:55:18 [ForkJoinPool-1-worker-17] DEBUG [SuiteThreadController:267] testConfigurationFileIsCreated - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTls, testSendSimpleMessageTlsScramSha, testConfigurationFileIsCreated, testReceiveSimpleMessageTls, testDeployAndUnDeployCruiseControl, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testConfigurationPerformanceOptions, testConfigurationReflection] to and randomly select one to start execution
2022-03-28 10:55:18 [ForkJoinPool-1-worker-17] DEBUG [SuiteThreadController:93] [cruisecontrol.CruiseControlConfigurationST] - Removing parallel test: testConfigurationFileIsCreated
2022-03-28 10:55:18 [ForkJoinPool-1-worker-17] DEBUG [SuiteThreadController:97] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 7
2022-03-28 10:55:18 [ForkJoinPool-1-worker-17] INFO  [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationFileIsCreated-FINISHED
2022-03-28 10:55:18 [ForkJoinPool-1-worker-17] INFO  [TestSeparator:30] ############################################################################
2022-03-28 10:55:18 [ForkJoinPool-1-worker-17] INFO  [TestSeparator:23] ############################################################################
2022-03-28 10:55:18 [ForkJoinPool-1-worker-17] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testThrottlingQuotasCreateAlterPartitions-STARTED
2022-03-28 10:55:18 [ForkJoinPool-1-worker-17] DEBUG [AbstractST:658] ============================================================================
2022-03-28 10:55:18 [ForkJoinPool-1-worker-17] DEBUG [AbstractST:659] [operators.topic.ThrottlingQuotaST - Before Each] - Setup test case environment
2022-03-28 10:55:18 [ForkJoinPool-1-worker-17] DEBUG [SuiteThreadController:77] [operators.topic.ThrottlingQuotaST] - Adding parallel test: testThrottlingQuotasCreateAlterPartitions
2022-03-28 10:55:18 [ForkJoinPool-1-worker-17] DEBUG [SuiteThreadController:81] [operators.topic.ThrottlingQuotaST] - Parallel test count: 8
2022-03-28 10:55:18 [ForkJoinPool-1-worker-17] DEBUG [SuiteThreadController:205] [testThrottlingQuotasCreateAlterPartitions] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:55:18 [ForkJoinPool-1-worker-17] TRACE [SuiteThreadController:210] testThrottlingQuotasCreateAlterPartitions is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:55:18 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:18 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:18 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (478334ms till timeout)
2022-03-28 10:55:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-30
2022-03-28 10:55:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:18 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-300
2022-03-28 10:55:18 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1209471ms till timeout)
2022-03-28 10:55:18 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1199957ms till timeout)
2022-03-28 10:55:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-300
2022-03-28 10:55:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:19 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-301
2022-03-28 10:55:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-301
2022-03-28 10:55:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:19 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-304
2022-03-28 10:55:19 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:19 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:19 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:19 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (477059ms till timeout)
2022-03-28 10:55:19 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:230] testConfigurationReflection test now can proceed its execution
2022-03-28 10:55:19 [ForkJoinPool-1-worker-7] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 10:55:19 [ForkJoinPool-1-worker-7] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383, testConfigurationFileIsCreated=my-cluster-e0fac774, testSendSimpleMessageTls=my-cluster-27ddd6ba, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c, testConfigurationReflection=my-cluster-d1e2168e, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-da91577a, testDeployAndUnDeployCruiseControl=my-cluster-c0830cbe, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8, testCapacityFile=my-cluster-fd0fb61a, testReceiveSimpleMessageTls=my-cluster-c6a15963, testConfigurationPerformanceOptions=my-cluster-b6310693, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-f50d425d, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5}
2022-03-28 10:55:19 [ForkJoinPool-1-worker-7] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-1915916223-1265293973, testConfigurationFileIsCreated=my-user-180611180-50530491, testSendSimpleMessageTls=my-user-738229993-1095121647, testCreatingUsersWithSecretPrefix=my-user-1450786684-1808961062, testConfigurationReflection=my-user-1445653023-93200027, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-1260379615-1050949907, testDeployAndUnDeployCruiseControl=my-user-1988711145-439861161, testThrottlingQuotasCreateTopic=my-user-1131806865-947217230, testCapacityFile=my-user-937015144-1959439396, testReceiveSimpleMessageTls=my-user-6088058-1142028780, testConfigurationPerformanceOptions=my-user-930975101-640523779, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-745974575-1852384454, testSendSimpleMessageTlsScramSha=my-user-1499386234-1249154328}
2022-03-28 10:55:19 [ForkJoinPool-1-worker-7] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-1834944466-2094449929, testConfigurationFileIsCreated=my-topic-1296407588-313880708, testSendSimpleMessageTls=my-topic-563023482-1697359353, testCreatingUsersWithSecretPrefix=my-topic-1479520843-618891199, testConfigurationReflection=my-topic-640815392-1092406112, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-1674392683-1637010870, testDeployAndUnDeployCruiseControl=my-topic-1511079187-95055629, testThrottlingQuotasCreateTopic=my-topic-728672490-1194703402, testCapacityFile=my-topic-1155641705-902584520, testReceiveSimpleMessageTls=my-topic-478514536-2132782608, testConfigurationPerformanceOptions=my-topic-213912446-1165431808, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-254848724-189700675, testSendSimpleMessageTlsScramSha=my-topic-1028951580-1656411053}
2022-03-28 10:55:19 [ForkJoinPool-1-worker-7] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383-kafka-clients, testConfigurationFileIsCreated=my-cluster-e0fac774-kafka-clients, testSendSimpleMessageTls=my-cluster-27ddd6ba-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c-kafka-clients, testConfigurationReflection=my-cluster-d1e2168e-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-da91577a-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-c0830cbe-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8-kafka-clients, testCapacityFile=my-cluster-fd0fb61a-kafka-clients, testReceiveSimpleMessageTls=my-cluster-c6a15963-kafka-clients, testConfigurationPerformanceOptions=my-cluster-b6310693-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-f50d425d-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5-kafka-clients}
2022-03-28 10:55:19 [ForkJoinPool-1-worker-7] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-7 for test case:testConfigurationReflection
2022-03-28 10:55:19 [ForkJoinPool-1-worker-7] INFO  [KubeClusterResource:156] Creating Namespace: namespace-7
2022-03-28 10:55:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-304
2022-03-28 10:55:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:19 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-305
2022-03-28 10:55:19 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1208322ms till timeout)
2022-03-28 10:55:19 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1198899ms till timeout)
2022-03-28 10:55:19 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Namespace namespace-7
2022-03-28 10:55:19 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-7 -o json
2022-03-28 10:55:20 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-7 -o json
2022-03-28 10:55:20 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:20 [ForkJoinPool-1-worker-7] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c28,c2",
            "openshift.io/sa.scc.supplemental-groups": "1000760000/10000",
            "openshift.io/sa.scc.uid-range": "1000760000/10000"
        },
        "creationTimestamp": "2022-03-28T10:55:17Z",
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T10:54:47Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:status": {
                        "f:phase": {}
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T10:55:17Z"
            }
        ],
        "name": "namespace-7",
        "resourceVersion": "1872405",
        "selfLink": "/api/v1/namespaces/namespace-7",
        "uid": "b6a87fd9-6214-4359-bbba-b3500b93a60e"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 10:55:20 [ForkJoinPool-1-worker-7] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[namespace-1], io.strimzi.test.logs.CollectorElement@c4c0ea0=[namespace-7], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@f851b6c3=[namespace-5], io.strimzi.test.logs.CollectorElement@3881d5f2=[namespace-4], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-6], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-3], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 10:55:20 [ForkJoinPool-1-worker-7] INFO  [KubeClusterResource:82] Client use Namespace: namespace-7
2022-03-28 10:55:20 [ForkJoinPool-1-worker-7] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-7, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 10:55:20 [ForkJoinPool-1-worker-7] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-7
2022-03-28 10:55:20 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-d1e2168e in namespace namespace-7
2022-03-28 10:55:20 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:164] Using Namespace: namespace-7
2022-03-28 10:55:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-305
2022-03-28 10:55:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:20 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-31
2022-03-28 10:55:20 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-d1e2168e
2022-03-28 10:55:20 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-d1e2168e will have desired state: Ready
2022-03-28 10:55:20 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-d1e2168e will have desired state: Ready
2022-03-28 10:55:20 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1319961ms till timeout)
2022-03-28 10:55:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-31
2022-03-28 10:55:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:20 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-319
2022-03-28 10:55:20 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1207280ms till timeout)
2022-03-28 10:55:21 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1197843ms till timeout)
2022-03-28 10:55:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-319
2022-03-28 10:55:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:21 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-320
2022-03-28 10:55:21 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:21 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:21 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (475745ms till timeout)
2022-03-28 10:55:21 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=b9a4429e-22d8-41a5-b7e5-723851ceb10a, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:55:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-320
2022-03-28 10:55:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:21 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-321
2022-03-28 10:55:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1318887ms till timeout)
2022-03-28 10:55:21 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:55:21 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:55:21 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-2 hasn't rolled
2022-03-28 10:55:21 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1728021ms till timeout)
2022-03-28 10:55:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-321
2022-03-28 10:55:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:21 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-322
2022-03-28 10:55:22 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1206247ms till timeout)
2022-03-28 10:55:22 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-f50d425d-cruise-control-65f54f8b4d-lwmc4=90f7ed91-d8e3-496f-85cb-a3b47a769d64}
2022-03-28 10:55:22 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:22 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1196774ms till timeout)
2022-03-28 10:55:22 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-f50d425d-cruise-control-65f54f8b4d-lwmc4=90f7ed91-d8e3-496f-85cb-a3b47a769d64, my-cluster-f50d425d-cruise-control-8546d57b59-c8rgt=71096785-46b8-44d4-811e-8707410a33af}
2022-03-28 10:55:22 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-f50d425d-cruise-control-65f54f8b4d-lwmc4=90f7ed91-d8e3-496f-85cb-a3b47a769d64}
2022-03-28 10:55:22 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Deployment my-cluster-f50d425d-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (548335ms till timeout)
2022-03-28 10:55:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-322
2022-03-28 10:55:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:22 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-323
2022-03-28 10:55:22 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:22 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:22 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (474500ms till timeout)
2022-03-28 10:55:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1317843ms till timeout)
2022-03-28 10:55:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-323
2022-03-28 10:55:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:22 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-324
2022-03-28 10:55:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-324
2022-03-28 10:55:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:22 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-325
2022-03-28 10:55:23 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1205211ms till timeout)
2022-03-28 10:55:23 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1195728ms till timeout)
2022-03-28 10:55:23 [ForkJoinPool-1-worker-17] TRACE [SuiteThreadController:210] testThrottlingQuotasCreateAlterPartitions is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:55:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-325
2022-03-28 10:55:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:23 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-326
2022-03-28 10:55:23 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:23 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:23 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:23 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (473210ms till timeout)
2022-03-28 10:55:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-326
2022-03-28 10:55:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:23 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-327
2022-03-28 10:55:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1316802ms till timeout)
2022-03-28 10:55:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-327
2022-03-28 10:55:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:23 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-328
2022-03-28 10:55:24 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1204155ms till timeout)
2022-03-28 10:55:24 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1194679ms till timeout)
2022-03-28 10:55:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-328
2022-03-28 10:55:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:24 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-329
2022-03-28 10:55:24 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-329
2022-03-28 10:55:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:24 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-330
2022-03-28 10:55:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1315731ms till timeout)
2022-03-28 10:55:24 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:24 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:24 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (471902ms till timeout)
2022-03-28 10:55:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-330
2022-03-28 10:55:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:25 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-331
2022-03-28 10:55:25 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1203103ms till timeout)
2022-03-28 10:55:25 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1193646ms till timeout)
2022-03-28 10:55:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-331
2022-03-28 10:55:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:25 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-332
2022-03-28 10:55:25 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1314625ms till timeout)
2022-03-28 10:55:25 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-332
2022-03-28 10:55:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:26 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-333
2022-03-28 10:55:26 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1201965ms till timeout)
2022-03-28 10:55:26 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1192550ms till timeout)
2022-03-28 10:55:26 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:26 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:26 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (470440ms till timeout)
2022-03-28 10:55:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-333
2022-03-28 10:55:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:26 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-334
2022-03-28 10:55:26 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=b9a4429e-22d8-41a5-b7e5-723851ceb10a, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:55:26 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:55:26 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:55:26 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-2 hasn't rolled
2022-03-28 10:55:26 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1722936ms till timeout)
2022-03-28 10:55:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-334
2022-03-28 10:55:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:26 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-335
2022-03-28 10:55:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1313529ms till timeout)
2022-03-28 10:55:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-335
2022-03-28 10:55:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:27 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-336
2022-03-28 10:55:27 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-f50d425d-cruise-control-65f54f8b4d-lwmc4=90f7ed91-d8e3-496f-85cb-a3b47a769d64}
2022-03-28 10:55:27 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-f50d425d-cruise-control-65f54f8b4d-lwmc4=90f7ed91-d8e3-496f-85cb-a3b47a769d64, my-cluster-f50d425d-cruise-control-8546d57b59-c8rgt=71096785-46b8-44d4-811e-8707410a33af}
2022-03-28 10:55:27 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-f50d425d-cruise-control-65f54f8b4d-lwmc4=90f7ed91-d8e3-496f-85cb-a3b47a769d64}
2022-03-28 10:55:27 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Deployment my-cluster-f50d425d-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (543200ms till timeout)
2022-03-28 10:55:27 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1200920ms till timeout)
2022-03-28 10:55:27 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:27 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1191472ms till timeout)
2022-03-28 10:55:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-336
2022-03-28 10:55:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:27 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-337
2022-03-28 10:55:27 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:27 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:27 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (469151ms till timeout)
2022-03-28 10:55:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-337
2022-03-28 10:55:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:27 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-338
2022-03-28 10:55:28 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1312416ms till timeout)
2022-03-28 10:55:28 [ForkJoinPool-1-worker-17] TRACE [SuiteThreadController:210] testThrottlingQuotasCreateAlterPartitions is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:55:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-338
2022-03-28 10:55:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:28 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-339
2022-03-28 10:55:28 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1199870ms till timeout)
2022-03-28 10:55:28 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1190410ms till timeout)
2022-03-28 10:55:28 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-339
2022-03-28 10:55:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:28 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-341
2022-03-28 10:55:28 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:28 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:28 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (467851ms till timeout)
2022-03-28 10:55:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-341
2022-03-28 10:55:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:29 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-343
2022-03-28 10:55:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1311321ms till timeout)
2022-03-28 10:55:29 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1198782ms till timeout)
2022-03-28 10:55:29 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1189367ms till timeout)
2022-03-28 10:55:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-343
2022-03-28 10:55:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:29 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-345
2022-03-28 10:55:29 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-345
2022-03-28 10:55:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:30 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-346
2022-03-28 10:55:30 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:30 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:30 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (466615ms till timeout)
2022-03-28 10:55:30 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1310189ms till timeout)
2022-03-28 10:55:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-346
2022-03-28 10:55:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:30 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-349
2022-03-28 10:55:30 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1197737ms till timeout)
2022-03-28 10:55:30 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1188322ms till timeout)
2022-03-28 10:55:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-349
2022-03-28 10:55:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:31 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-355
2022-03-28 10:55:31 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:31 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1309164ms till timeout)
2022-03-28 10:55:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-355
2022-03-28 10:55:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:31 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-356
2022-03-28 10:55:31 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:31 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:31 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (465341ms till timeout)
2022-03-28 10:55:31 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1187294ms till timeout)
2022-03-28 10:55:31 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1196681ms till timeout)
2022-03-28 10:55:31 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=b9a4429e-22d8-41a5-b7e5-723851ceb10a, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:55:31 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:55:31 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:55:31 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-2 hasn't rolled
2022-03-28 10:55:31 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1717836ms till timeout)
2022-03-28 10:55:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-356
2022-03-28 10:55:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:31 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-357
2022-03-28 10:55:32 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-f50d425d-cruise-control-65f54f8b4d-lwmc4=90f7ed91-d8e3-496f-85cb-a3b47a769d64}
2022-03-28 10:55:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-357
2022-03-28 10:55:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:32 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-361
2022-03-28 10:55:32 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1308134ms till timeout)
2022-03-28 10:55:32 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-f50d425d-cruise-control-65f54f8b4d-lwmc4=90f7ed91-d8e3-496f-85cb-a3b47a769d64, my-cluster-f50d425d-cruise-control-8546d57b59-c8rgt=71096785-46b8-44d4-811e-8707410a33af}
2022-03-28 10:55:32 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-f50d425d-cruise-control-65f54f8b4d-lwmc4=90f7ed91-d8e3-496f-85cb-a3b47a769d64}
2022-03-28 10:55:32 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Deployment my-cluster-f50d425d-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (538117ms till timeout)
2022-03-28 10:55:32 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:32 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1186256ms till timeout)
2022-03-28 10:55:32 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1195560ms till timeout)
2022-03-28 10:55:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-361
2022-03-28 10:55:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:32 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-362
2022-03-28 10:55:32 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:32 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:32 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (464039ms till timeout)
2022-03-28 10:55:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-362
2022-03-28 10:55:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:33 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-364
2022-03-28 10:55:33 [ForkJoinPool-1-worker-17] TRACE [SuiteThreadController:210] testThrottlingQuotasCreateAlterPartitions is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:55:33 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1307112ms till timeout)
2022-03-28 10:55:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-364
2022-03-28 10:55:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:33 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-366
2022-03-28 10:55:33 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1185223ms till timeout)
2022-03-28 10:55:33 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:33 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1194456ms till timeout)
2022-03-28 10:55:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-366
2022-03-28 10:55:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:33 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-368
2022-03-28 10:55:34 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:34 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:34 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (462820ms till timeout)
2022-03-28 10:55:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-368
2022-03-28 10:55:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:34 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-369
2022-03-28 10:55:34 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1306072ms till timeout)
2022-03-28 10:55:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-369
2022-03-28 10:55:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:34 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-374
2022-03-28 10:55:34 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1184156ms till timeout)
2022-03-28 10:55:34 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1193414ms till timeout)
2022-03-28 10:55:35 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-374
2022-03-28 10:55:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:35 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-375
2022-03-28 10:55:35 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:35 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:35 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (461558ms till timeout)
2022-03-28 10:55:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-375
2022-03-28 10:55:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:35 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-378
2022-03-28 10:55:35 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1305021ms till timeout)
2022-03-28 10:55:35 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1183089ms till timeout)
2022-03-28 10:55:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-378
2022-03-28 10:55:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:35 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-379
2022-03-28 10:55:35 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1192353ms till timeout)
2022-03-28 10:55:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-379
2022-03-28 10:55:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:36 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-381
2022-03-28 10:55:36 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:36 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1303983ms till timeout)
2022-03-28 10:55:36 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:36 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:36 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (460268ms till timeout)
2022-03-28 10:55:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-381
2022-03-28 10:55:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:36 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-382
2022-03-28 10:55:36 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=b9a4429e-22d8-41a5-b7e5-723851ceb10a, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:55:36 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1182054ms till timeout)
2022-03-28 10:55:36 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:55:36 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:55:36 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-2 hasn't rolled
2022-03-28 10:55:36 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1712781ms till timeout)
2022-03-28 10:55:37 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1191295ms till timeout)
2022-03-28 10:55:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-382
2022-03-28 10:55:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:37 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-383
2022-03-28 10:55:37 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-f50d425d-cruise-control-65f54f8b4d-lwmc4=90f7ed91-d8e3-496f-85cb-a3b47a769d64}
2022-03-28 10:55:37 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-f50d425d-cruise-control-65f54f8b4d-lwmc4=90f7ed91-d8e3-496f-85cb-a3b47a769d64, my-cluster-f50d425d-cruise-control-8546d57b59-c8rgt=71096785-46b8-44d4-811e-8707410a33af}
2022-03-28 10:55:37 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-f50d425d-cruise-control-65f54f8b4d-lwmc4=90f7ed91-d8e3-496f-85cb-a3b47a769d64}
2022-03-28 10:55:37 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Deployment my-cluster-f50d425d-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (532980ms till timeout)
2022-03-28 10:55:37 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-383
2022-03-28 10:55:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:37 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-384
2022-03-28 10:55:37 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1302900ms till timeout)
2022-03-28 10:55:37 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1181006ms till timeout)
2022-03-28 10:55:37 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:37 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:37 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (458935ms till timeout)
2022-03-28 10:55:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-384
2022-03-28 10:55:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:37 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-387
2022-03-28 10:55:38 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1190253ms till timeout)
2022-03-28 10:55:38 [ForkJoinPool-1-worker-17] TRACE [SuiteThreadController:210] testThrottlingQuotasCreateAlterPartitions is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:55:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-387
2022-03-28 10:55:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:38 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-388
2022-03-28 10:55:38 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1301799ms till timeout)
2022-03-28 10:55:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-388
2022-03-28 10:55:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:38 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-389
2022-03-28 10:55:38 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:38 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1179926ms till timeout)
2022-03-28 10:55:39 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1189215ms till timeout)
2022-03-28 10:55:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-389
2022-03-28 10:55:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:39 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-392
2022-03-28 10:55:39 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:39 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:39 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (457612ms till timeout)
2022-03-28 10:55:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-392
2022-03-28 10:55:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:39 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-393
2022-03-28 10:55:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1300738ms till timeout)
2022-03-28 10:55:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-393
2022-03-28 10:55:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:39 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-397
2022-03-28 10:55:40 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1178852ms till timeout)
2022-03-28 10:55:40 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1188125ms till timeout)
2022-03-28 10:55:40 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-397
2022-03-28 10:55:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:40 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-4
2022-03-28 10:55:40 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:40 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:40 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (456331ms till timeout)
2022-03-28 10:55:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-4
2022-03-28 10:55:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:40 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-400
2022-03-28 10:55:40 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1299701ms till timeout)
2022-03-28 10:55:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-400
2022-03-28 10:55:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:40 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-404
2022-03-28 10:55:41 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1177792ms till timeout)
2022-03-28 10:55:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-404
2022-03-28 10:55:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:41 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-405
2022-03-28 10:55:41 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1187107ms till timeout)
2022-03-28 10:55:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-405
2022-03-28 10:55:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:41 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-406
2022-03-28 10:55:41 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:41 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:41 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:41 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (455103ms till timeout)
2022-03-28 10:55:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-406
2022-03-28 10:55:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:41 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-408
2022-03-28 10:55:41 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=b9a4429e-22d8-41a5-b7e5-723851ceb10a, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:55:41 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1298559ms till timeout)
2022-03-28 10:55:41 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:55:41 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:55:41 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-2 hasn't rolled
2022-03-28 10:55:41 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1707699ms till timeout)
2022-03-28 10:55:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-408
2022-03-28 10:55:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:42 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-409
2022-03-28 10:55:42 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1176770ms till timeout)
2022-03-28 10:55:42 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1186054ms till timeout)
2022-03-28 10:55:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-409
2022-03-28 10:55:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:42 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-410
2022-03-28 10:55:42 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-f50d425d-cruise-control-65f54f8b4d-lwmc4=90f7ed91-d8e3-496f-85cb-a3b47a769d64}
2022-03-28 10:55:42 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-f50d425d-cruise-control-65f54f8b4d-lwmc4=90f7ed91-d8e3-496f-85cb-a3b47a769d64, my-cluster-f50d425d-cruise-control-8546d57b59-c8rgt=71096785-46b8-44d4-811e-8707410a33af}
2022-03-28 10:55:42 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-f50d425d-cruise-control-65f54f8b4d-lwmc4=90f7ed91-d8e3-496f-85cb-a3b47a769d64}
2022-03-28 10:55:42 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Deployment my-cluster-f50d425d-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (527889ms till timeout)
2022-03-28 10:55:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-410
2022-03-28 10:55:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:42 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-411
2022-03-28 10:55:42 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-411
2022-03-28 10:55:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:42 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-414
2022-03-28 10:55:42 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:42 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:42 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (453844ms till timeout)
2022-03-28 10:55:43 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1297518ms till timeout)
2022-03-28 10:55:43 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1175733ms till timeout)
2022-03-28 10:55:43 [ForkJoinPool-1-worker-17] TRACE [SuiteThreadController:210] testThrottlingQuotasCreateAlterPartitions is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:55:43 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-da91577a will have desired state: Ready not ready, will try again in 1000 ms (1185018ms till timeout)
2022-03-28 10:55:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-414
2022-03-28 10:55:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:43 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-415
2022-03-28 10:55:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-415
2022-03-28 10:55:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:43 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-416
2022-03-28 10:55:43 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-416
2022-03-28 10:55:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:43 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-418
2022-03-28 10:55:44 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1296495ms till timeout)
2022-03-28 10:55:44 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1174664ms till timeout)
2022-03-28 10:55:44 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:44 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:44 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (452571ms till timeout)
2022-03-28 10:55:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-418
2022-03-28 10:55:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:44 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-419
2022-03-28 10:55:44 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:444] Kafka: my-cluster-da91577a is in desired state: Ready
2022-03-28 10:55:44 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:155] Create/Update KafkaTopic my-topic-1674392683-1637010870 in namespace namespace-7
2022-03-28 10:55:44 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:164] Using Namespace: namespace-6
2022-03-28 10:55:44 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1674392683-1637010870
2022-03-28 10:55:44 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-1674392683-1637010870 will have desired state: Ready
2022-03-28 10:55:44 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-1674392683-1637010870 will have desired state: Ready
2022-03-28 10:55:44 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic: my-topic-1674392683-1637010870 will have desired state: Ready not ready, will try again in 1000 ms (179930ms till timeout)
2022-03-28 10:55:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-419
2022-03-28 10:55:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:44 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-421
2022-03-28 10:55:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-421
2022-03-28 10:55:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:44 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-422
2022-03-28 10:55:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1295409ms till timeout)
2022-03-28 10:55:45 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:45 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1173592ms till timeout)
2022-03-28 10:55:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-422
2022-03-28 10:55:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:45 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-423
2022-03-28 10:55:45 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:444] KafkaTopic: my-topic-1674392683-1637010870 is in desired state: Ready
2022-03-28 10:55:45 [ForkJoinPool-1-worker-15] INFO  [ReconciliationST:147] Adding pause annotation into KafkaTopic resource and changing replication factor
2022-03-28 10:55:45 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:45 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:45 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (451236ms till timeout)
2022-03-28 10:55:45 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-1674392683-1637010870 will have desired state: ReconciliationPaused
2022-03-28 10:55:45 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-1674392683-1637010870 will have desired state: ReconciliationPaused
2022-03-28 10:55:45 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic: my-topic-1674392683-1637010870 will have desired state: ReconciliationPaused not ready, will try again in 1000 ms (179966ms till timeout)
2022-03-28 10:55:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-423
2022-03-28 10:55:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:45 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-43
2022-03-28 10:55:46 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1294383ms till timeout)
2022-03-28 10:55:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-43
2022-03-28 10:55:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:46 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-431
2022-03-28 10:55:46 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-b6310693 will have desired state: Ready not ready, will try again in 1000 ms (1172554ms till timeout)
2022-03-28 10:55:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-431
2022-03-28 10:55:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:46 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-44
2022-03-28 10:55:46 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:46 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:444] KafkaTopic: my-topic-1674392683-1637010870 is in desired state: ReconciliationPaused
2022-03-28 10:55:46 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-7 exec my-cluster-da91577a-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-1674392683-1637010870 --describe --bootstrap-server my-cluster-da91577a-kafka-bootstrap:9092
2022-03-28 10:55:46 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:46 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:46 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (449902ms till timeout)
2022-03-28 10:55:46 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=b9a4429e-22d8-41a5-b7e5-723851ceb10a, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:55:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-44
2022-03-28 10:55:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:47 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-440
2022-03-28 10:55:47 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:55:47 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:55:47 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-2 hasn't rolled
2022-03-28 10:55:47 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1702627ms till timeout)
2022-03-28 10:55:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1293302ms till timeout)
2022-03-28 10:55:47 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-7 exec my-cluster-da91577a-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-1674392683-1637010870 --describe --bootstrap-server my-cluster-da91577a-kafka-bootstrap:9092
2022-03-28 10:55:47 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 10:55:47 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 10:55:47 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-da91577a-kafka-0" not found
2022-03-28 10:55:47 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 10:55:47 [ForkJoinPool-1-worker-15] ERROR [TestExecutionWatcher:28] ReconciliationST - Exception `oc --namespace namespace-7 exec my-cluster-da91577a-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-1674392683-1637010870 --describe --bootstrap-server my-cluster-da91577a-kafka-bootstrap:9092` got status code 1 and stderr:
------
Error from server (NotFound): pods "my-cluster-da91577a-kafka-0" not found

------
and stdout:
------

------ has been thrown in @Test. Going to collect logs from components.
2022-03-28 10:55:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-440
2022-03-28 10:55:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:47 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-441
2022-03-28 10:55:47 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:444] Kafka: my-cluster-b6310693 is in desired state: Ready
2022-03-28 10:55:47 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-f50d425d-cruise-control-65f54f8b4d-lwmc4=90f7ed91-d8e3-496f-85cb-a3b47a769d64}
2022-03-28 10:55:47 [ForkJoinPool-1-worker-15] DEBUG [LogCollector:158] [reconciliation-st] adding to all namespaces, which should be collected: [infra-namespace, namespace-6, reconciliation-st]
2022-03-28 10:55:47 [ForkJoinPool-1-worker-15] INFO  [LogCollector:252] Collecting events in Namespace infra-namespace
2022-03-28 10:55:47 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace infra-namespace get events
2022-03-28 10:55:47 [ForkJoinPool-1-worker-5] INFO  [CruiseControlConfigurationST:271] Changing cruise control performance tuning options
2022-03-28 10:55:47 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-f50d425d-cruise-control-8546d57b59-c8rgt=71096785-46b8-44d4-811e-8707410a33af}
2022-03-28 10:55:47 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:119] All pods seem to have rolled
2022-03-28 10:55:47 [ForkJoinPool-1-worker-3] INFO  [DeploymentUtils:161] Wait for Deployment: my-cluster-f50d425d-cruise-control will be ready
2022-03-28 10:55:47 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Wait for Deployment: my-cluster-f50d425d-cruise-control will be ready
2022-03-28 10:55:47 [ForkJoinPool-1-worker-5] INFO  [CruiseControlConfigurationST:277] Verifying that CC pod is rolling, after changing options
2022-03-28 10:55:47 [ForkJoinPool-1-worker-5] INFO  [DeploymentUtils:136] Waiting for Deployment my-cluster-b6310693-cruise-control rolling update
2022-03-28 10:55:47 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Deployment my-cluster-b6310693-cruise-control rolling update in namespace:namespace-5
2022-03-28 10:55:47 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-b6310693-cruise-control-7c74956698-lqfpx=dddfd390-e0dd-4449-a1ae-af7eeeb85c68}
2022-03-28 10:55:47 [ForkJoinPool-1-worker-3] INFO  [DeploymentUtils:168] Deployment: my-cluster-f50d425d-cruise-control is ready
2022-03-28 10:55:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-441
2022-03-28 10:55:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:47 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-442
2022-03-28 10:55:47 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-f50d425d, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-f50d425d-cruise-control}, additionalProperties={})to be ready
2022-03-28 10:55:47 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-b6310693-cruise-control-7c74956698-lqfpx=dddfd390-e0dd-4449-a1ae-af7eeeb85c68}
2022-03-28 10:55:47 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-b6310693-cruise-control-7c74956698-lqfpx=dddfd390-e0dd-4449-a1ae-af7eeeb85c68}
2022-03-28 10:55:47 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Deployment my-cluster-b6310693-cruise-control rolling update in namespace:namespace-5 not ready, will try again in 5000 ms (599878ms till timeout)
2022-03-28 10:55:47 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:47 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-f50d425d-cruise-control-8546d57b59-c8rgt not ready: cruise-control)
2022-03-28 10:55:47 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-f50d425d-cruise-control-8546d57b59-c8rgt not ready: tls-sidecar)
2022-03-28 10:55:47 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-f50d425d-cruise-control-8546d57b59-c8rgt are ready
2022-03-28 10:55:47 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-f50d425d, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-f50d425d-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599926ms till timeout)
2022-03-28 10:55:47 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace infra-namespace get events
2022-03-28 10:55:47 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:47 [ForkJoinPool-1-worker-15] INFO  [LogCollector:259] Collecting ConfigMaps in Namespace infra-namespace
2022-03-28 10:55:48 [ForkJoinPool-1-worker-15] INFO  [LogCollector:217] Collecting logs for Pod(s) in Namespace infra-namespace
2022-03-28 10:55:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-442
2022-03-28 10:55:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:48 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-443
2022-03-28 10:55:48 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:48 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:48 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (448629ms till timeout)
2022-03-28 10:55:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1292262ms till timeout)
2022-03-28 10:55:48 [ForkJoinPool-1-worker-17] TRACE [SuiteThreadController:210] testThrottlingQuotasCreateAlterPartitions is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:55:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-443
2022-03-28 10:55:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:48 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-444
2022-03-28 10:55:48 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace infra-namespace describe pod strimzi-cluster-operator-869dcfb9f7-twrp8
2022-03-28 10:55:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-444
2022-03-28 10:55:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:48 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-445
2022-03-28 10:55:48 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace infra-namespace describe pod strimzi-cluster-operator-869dcfb9f7-twrp8
2022-03-28 10:55:48 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:48 [ForkJoinPool-1-worker-15] INFO  [LogCollector:266] Collecting Deployments in Namespace infra-namespace
2022-03-28 10:55:48 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace infra-namespace get Deployment -o yaml
2022-03-28 10:55:48 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-f50d425d-cruise-control-8546d57b59-c8rgt not ready: cruise-control)
2022-03-28 10:55:48 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-f50d425d-cruise-control-8546d57b59-c8rgt not ready: tls-sidecar)
2022-03-28 10:55:48 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-f50d425d-cruise-control-8546d57b59-c8rgt are ready
2022-03-28 10:55:48 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-f50d425d, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-f50d425d-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598893ms till timeout)
2022-03-28 10:55:49 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace infra-namespace get Deployment -o yaml
2022-03-28 10:55:49 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:49 [ForkJoinPool-1-worker-15] INFO  [LogCollector:271] Collecting StatefulSets in Namespace infra-namespace
2022-03-28 10:55:49 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace infra-namespace get StatefulSet -o yaml
2022-03-28 10:55:49 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-445
2022-03-28 10:55:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:49 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-446
2022-03-28 10:55:49 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1291224ms till timeout)
2022-03-28 10:55:49 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:49 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:49 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (447380ms till timeout)
2022-03-28 10:55:49 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace infra-namespace get StatefulSet -o yaml
2022-03-28 10:55:49 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:49 [ForkJoinPool-1-worker-15] INFO  [LogCollector:276] Collecting ReplicaSets in Namespace infra-namespace
2022-03-28 10:55:49 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace infra-namespace get replicaset -o yaml
2022-03-28 10:55:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-446
2022-03-28 10:55:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:49 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-447
2022-03-28 10:55:49 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace infra-namespace get replicaset -o yaml
2022-03-28 10:55:49 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:49 [ForkJoinPool-1-worker-15] INFO  [LogCollector:281] Collecting Strimzi in Namespace infra-namespace
2022-03-28 10:55:49 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc get strimzi -o yaml -n infra-namespace
2022-03-28 10:55:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-447
2022-03-28 10:55:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:49 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-448
2022-03-28 10:55:50 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-f50d425d-cruise-control-8546d57b59-c8rgt not ready: cruise-control)
2022-03-28 10:55:50 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-f50d425d-cruise-control-8546d57b59-c8rgt not ready: tls-sidecar)
2022-03-28 10:55:50 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-f50d425d-cruise-control-8546d57b59-c8rgt are ready
2022-03-28 10:55:50 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-f50d425d, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-f50d425d-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597851ms till timeout)
2022-03-28 10:55:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-448
2022-03-28 10:55:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:50 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-449
2022-03-28 10:55:50 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1290191ms till timeout)
2022-03-28 10:55:50 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-449
2022-03-28 10:55:50 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:50 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-450
2022-03-28 10:55:50 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-6 get Namespace namespace-1 -o yaml
2022-03-28 10:55:50 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 1
2022-03-28 10:55:50 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 10:55:50 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-1" not found
2022-03-28 10:55:50 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] ======STDERR END======
2022-03-28 10:55:50 [ForkJoinPool-1-worker-13] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[namespace-7], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@f851b6c3=[namespace-5], io.strimzi.test.logs.CollectorElement@3881d5f2=[namespace-4], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[infra-namespace, namespace-6, reconciliation-st], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-3], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 10:55:50 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:267] testCreatingUsersWithSecretPrefix - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTls, testSendSimpleMessageTlsScramSha, testConfigurationFileIsCreated, testReceiveSimpleMessageTls, testDeployAndUnDeployCruiseControl, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testConfigurationPerformanceOptions, testThrottlingQuotasCreateAlterPartitions] to and randomly select one to start execution
2022-03-28 10:55:50 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:93] [operators.user.UserST] - Removing parallel test: testCreatingUsersWithSecretPrefix
2022-03-28 10:55:50 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:97] [operators.user.UserST] - Parallel test count: 7
2022-03-28 10:55:50 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testCreatingUsersWithSecretPrefix-FINISHED
2022-03-28 10:55:50 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:30] ############################################################################
2022-03-28 10:55:50 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:23] ############################################################################
2022-03-28 10:55:50 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testScramUserWithQuotas-STARTED
2022-03-28 10:55:50 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:658] ============================================================================
2022-03-28 10:55:50 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:659] [operators.user.UserST - Before Each] - Setup test case environment
2022-03-28 10:55:50 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:77] [operators.user.UserST] - Adding parallel test: testScramUserWithQuotas
2022-03-28 10:55:50 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:81] [operators.user.UserST] - Parallel test count: 8
2022-03-28 10:55:50 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:205] [testScramUserWithQuotas] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:55:50 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testScramUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:55:50 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc get strimzi -o yaml -n infra-namespace
2022-03-28 10:55:50 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:50 [ForkJoinPool-1-worker-15] INFO  [LogCollector:287] Collecting cluster status
2022-03-28 10:55:50 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc describe nodes
2022-03-28 10:55:51 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-f50d425d-cruise-control-8546d57b59-c8rgt not ready: cruise-control)
2022-03-28 10:55:51 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-f50d425d-cruise-control-8546d57b59-c8rgt not ready: tls-sidecar)
2022-03-28 10:55:51 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-f50d425d-cruise-control-8546d57b59-c8rgt are ready
2022-03-28 10:55:51 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-f50d425d, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-f50d425d-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596721ms till timeout)
2022-03-28 10:55:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-450
2022-03-28 10:55:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:51 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-451
2022-03-28 10:55:51 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1289122ms till timeout)
2022-03-28 10:55:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-451
2022-03-28 10:55:51 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:51 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-452
2022-03-28 10:55:52 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=b9a4429e-22d8-41a5-b7e5-723851ceb10a, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:55:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-452
2022-03-28 10:55:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:52 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-454
2022-03-28 10:55:52 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:55:52 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:55:52 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-2 hasn't rolled
2022-03-28 10:55:52 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1697593ms till timeout)
2022-03-28 10:55:52 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-f50d425d-cruise-control-8546d57b59-c8rgt not ready: cruise-control)
2022-03-28 10:55:52 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-f50d425d-cruise-control-8546d57b59-c8rgt not ready: tls-sidecar)
2022-03-28 10:55:52 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-f50d425d-cruise-control-8546d57b59-c8rgt are ready
2022-03-28 10:55:52 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-f50d425d, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-f50d425d-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595678ms till timeout)
2022-03-28 10:55:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-454
2022-03-28 10:55:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:52 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-455
2022-03-28 10:55:52 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1288061ms till timeout)
2022-03-28 10:55:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-455
2022-03-28 10:55:52 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:52 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-456
2022-03-28 10:55:52 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-b6310693-cruise-control-7c74956698-lqfpx=dddfd390-e0dd-4449-a1ae-af7eeeb85c68}
2022-03-28 10:55:53 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-b6310693-cruise-control-7c74956698-lqfpx=dddfd390-e0dd-4449-a1ae-af7eeeb85c68}
2022-03-28 10:55:53 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-b6310693-cruise-control-7c74956698-lqfpx=dddfd390-e0dd-4449-a1ae-af7eeeb85c68}
2022-03-28 10:55:53 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Deployment my-cluster-b6310693-cruise-control rolling update in namespace:namespace-5 not ready, will try again in 5000 ms (594748ms till timeout)
2022-03-28 10:55:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-456
2022-03-28 10:55:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:53 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-457
2022-03-28 10:55:53 [ForkJoinPool-1-worker-17] DEBUG [SuiteThreadController:230] testThrottlingQuotasCreateAlterPartitions test now can proceed its execution
2022-03-28 10:55:53 [ForkJoinPool-1-worker-17] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 10:55:53 [ForkJoinPool-1-worker-17] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383, testConfigurationFileIsCreated=my-cluster-e0fac774, testThrottlingQuotasCreateAlterPartitions=my-cluster-28d9dd2e, testSendSimpleMessageTls=my-cluster-27ddd6ba, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c, testConfigurationReflection=my-cluster-d1e2168e, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-da91577a, testDeployAndUnDeployCruiseControl=my-cluster-c0830cbe, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8, testCapacityFile=my-cluster-fd0fb61a, testReceiveSimpleMessageTls=my-cluster-c6a15963, testConfigurationPerformanceOptions=my-cluster-b6310693, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-f50d425d, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5}
2022-03-28 10:55:53 [ForkJoinPool-1-worker-17] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-1915916223-1265293973, testConfigurationFileIsCreated=my-user-180611180-50530491, testThrottlingQuotasCreateAlterPartitions=my-user-1933547616-1024431140, testSendSimpleMessageTls=my-user-738229993-1095121647, testCreatingUsersWithSecretPrefix=my-user-1450786684-1808961062, testConfigurationReflection=my-user-1445653023-93200027, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-1260379615-1050949907, testDeployAndUnDeployCruiseControl=my-user-1988711145-439861161, testThrottlingQuotasCreateTopic=my-user-1131806865-947217230, testCapacityFile=my-user-937015144-1959439396, testReceiveSimpleMessageTls=my-user-6088058-1142028780, testConfigurationPerformanceOptions=my-user-930975101-640523779, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-745974575-1852384454, testSendSimpleMessageTlsScramSha=my-user-1499386234-1249154328}
2022-03-28 10:55:53 [ForkJoinPool-1-worker-17] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-1834944466-2094449929, testConfigurationFileIsCreated=my-topic-1296407588-313880708, testThrottlingQuotasCreateAlterPartitions=my-topic-609849288-1810034325, testSendSimpleMessageTls=my-topic-563023482-1697359353, testCreatingUsersWithSecretPrefix=my-topic-1479520843-618891199, testConfigurationReflection=my-topic-640815392-1092406112, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-1674392683-1637010870, testDeployAndUnDeployCruiseControl=my-topic-1511079187-95055629, testThrottlingQuotasCreateTopic=my-topic-728672490-1194703402, testCapacityFile=my-topic-1155641705-902584520, testReceiveSimpleMessageTls=my-topic-478514536-2132782608, testConfigurationPerformanceOptions=my-topic-213912446-1165431808, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-254848724-189700675, testSendSimpleMessageTlsScramSha=my-topic-1028951580-1656411053}
2022-03-28 10:55:53 [ForkJoinPool-1-worker-17] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383-kafka-clients, testConfigurationFileIsCreated=my-cluster-e0fac774-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-28d9dd2e-kafka-clients, testSendSimpleMessageTls=my-cluster-27ddd6ba-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c-kafka-clients, testConfigurationReflection=my-cluster-d1e2168e-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-da91577a-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-c0830cbe-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8-kafka-clients, testCapacityFile=my-cluster-fd0fb61a-kafka-clients, testReceiveSimpleMessageTls=my-cluster-c6a15963-kafka-clients, testConfigurationPerformanceOptions=my-cluster-b6310693-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-f50d425d-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5-kafka-clients}
2022-03-28 10:55:53 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:155] Create/Update KafkaUser my-user-1933547616-1024431140 in namespace throttling-quota-st
2022-03-28 10:55:53 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-f50d425d-cruise-control-8546d57b59-c8rgt not ready: cruise-control)
2022-03-28 10:55:53 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-f50d425d-cruise-control-8546d57b59-c8rgt not ready: tls-sidecar)
2022-03-28 10:55:53 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-f50d425d-cruise-control-8546d57b59-c8rgt are ready
2022-03-28 10:55:53 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-f50d425d, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-f50d425d-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594599ms till timeout)
2022-03-28 10:55:53 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-1933547616-1024431140
2022-03-28 10:55:53 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc describe nodes
2022-03-28 10:55:53 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:53 [ForkJoinPool-1-worker-15] INFO  [LogCollector:252] Collecting events in Namespace namespace-6
2022-03-28 10:55:53 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-6 get events
2022-03-28 10:55:53 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-1933547616-1024431140 will have desired state: Ready
2022-03-28 10:55:53 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-1933547616-1024431140 will have desired state: Ready
2022-03-28 10:55:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-457
2022-03-28 10:55:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:53 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-458
2022-03-28 10:55:53 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] KafkaUser: my-user-1933547616-1024431140 will have desired state: Ready not ready, will try again in 1000 ms (179950ms till timeout)
2022-03-28 10:55:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1286991ms till timeout)
2022-03-28 10:55:53 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-6 get events
2022-03-28 10:55:53 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:53 [ForkJoinPool-1-worker-15] INFO  [LogCollector:259] Collecting ConfigMaps in Namespace namespace-6
2022-03-28 10:55:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-458
2022-03-28 10:55:53 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:53 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-459
2022-03-28 10:55:53 [ForkJoinPool-1-worker-15] INFO  [LogCollector:217] Collecting logs for Pod(s) in Namespace namespace-6
2022-03-28 10:55:53 [ForkJoinPool-1-worker-15] DEBUG [LogCollector:208] Collecting logs for TestCase: testPauseReconciliationInKafkaRebalanceAndTopic, and Pod: my-cluster-da91577a-cruise-control-65b4686599-2mq8v
2022-03-28 10:55:54 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-6 describe pod my-cluster-da91577a-cruise-control-65b4686599-2mq8v
2022-03-28 10:55:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-459
2022-03-28 10:55:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:54 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-46
2022-03-28 10:55:54 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-f50d425d-cruise-control-8546d57b59-c8rgt not ready: cruise-control)
2022-03-28 10:55:54 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-f50d425d-cruise-control-8546d57b59-c8rgt not ready: tls-sidecar)
2022-03-28 10:55:54 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-f50d425d-cruise-control-8546d57b59-c8rgt are ready
2022-03-28 10:55:54 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-f50d425d, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-f50d425d-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593522ms till timeout)
2022-03-28 10:55:54 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:444] KafkaUser: my-user-1933547616-1024431140 is in desired state: Ready
2022-03-28 10:55:54 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-6 describe pod my-cluster-da91577a-cruise-control-65b4686599-2mq8v
2022-03-28 10:55:54 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:54 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:155] Create/Update Job create-admin-my-cluster-28d9dd2e-kafka-clients in namespace throttling-quota-st
2022-03-28 10:55:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1285901ms till timeout)
2022-03-28 10:55:54 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-6 describe pod my-cluster-da91577a-cruise-control-65b4686599-2mq8v
2022-03-28 10:55:54 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:create-admin-my-cluster-28d9dd2e-kafka-clients
2022-03-28 10:55:54 [ForkJoinPool-1-worker-17] INFO  [JobUtils:81] Waiting for job: create-admin-my-cluster-28d9dd2e-kafka-clients will be in active state
2022-03-28 10:55:54 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 10:55:54 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job active not ready, will try again in 1000 ms (179951ms till timeout)
2022-03-28 10:55:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-46
2022-03-28 10:55:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:54 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-461
2022-03-28 10:55:55 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-6 describe pod my-cluster-da91577a-cruise-control-65b4686599-2mq8v
2022-03-28 10:55:55 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:55 [ForkJoinPool-1-worker-15] DEBUG [LogCollector:208] Collecting logs for TestCase: testPauseReconciliationInKafkaRebalanceAndTopic, and Pod: my-cluster-da91577a-entity-operator-6c9f579f98-p68pz
2022-03-28 10:55:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-461
2022-03-28 10:55:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:55 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-462
2022-03-28 10:55:55 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-6 describe pod my-cluster-da91577a-entity-operator-6c9f579f98-p68pz
2022-03-28 10:55:55 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-f50d425d-cruise-control-8546d57b59-c8rgt not ready: cruise-control)
2022-03-28 10:55:55 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-f50d425d-cruise-control-8546d57b59-c8rgt not ready: tls-sidecar)
2022-03-28 10:55:55 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-f50d425d-cruise-control-8546d57b59-c8rgt are ready
2022-03-28 10:55:55 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-f50d425d, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-f50d425d-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592400ms till timeout)
2022-03-28 10:55:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1284790ms till timeout)
2022-03-28 10:55:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-462
2022-03-28 10:55:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:55 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-463
2022-03-28 10:55:55 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testScramUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:55:55 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-6 describe pod my-cluster-da91577a-entity-operator-6c9f579f98-p68pz
2022-03-28 10:55:55 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:55 [ForkJoinPool-1-worker-17] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 10:55:55 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 10:55:55 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-6 describe pod my-cluster-da91577a-entity-operator-6c9f579f98-p68pz
2022-03-28 10:55:56 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (299917ms till timeout)
2022-03-28 10:55:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-463
2022-03-28 10:55:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:56 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-469
2022-03-28 10:55:56 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-6 describe pod my-cluster-da91577a-entity-operator-6c9f579f98-p68pz
2022-03-28 10:55:56 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:56 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-6 describe pod my-cluster-da91577a-entity-operator-6c9f579f98-p68pz
2022-03-28 10:55:56 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-f50d425d-cruise-control-8546d57b59-c8rgt not ready: cruise-control)
2022-03-28 10:55:56 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-f50d425d-cruise-control-8546d57b59-c8rgt not ready: tls-sidecar)
2022-03-28 10:55:56 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-f50d425d-cruise-control-8546d57b59-c8rgt are ready
2022-03-28 10:55:56 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-f50d425d, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-f50d425d-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591285ms till timeout)
2022-03-28 10:55:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-469
2022-03-28 10:55:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:56 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-470
2022-03-28 10:55:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1283744ms till timeout)
2022-03-28 10:55:56 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-6 describe pod my-cluster-da91577a-entity-operator-6c9f579f98-p68pz
2022-03-28 10:55:56 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:56 [ForkJoinPool-1-worker-15] DEBUG [LogCollector:208] Collecting logs for TestCase: testPauseReconciliationInKafkaRebalanceAndTopic, and Pod: my-cluster-da91577a-kafka-0
2022-03-28 10:55:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-470
2022-03-28 10:55:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:56 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-472
2022-03-28 10:55:57 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-6 describe pod my-cluster-da91577a-kafka-0
2022-03-28 10:55:57 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=b9a4429e-22d8-41a5-b7e5-723851ceb10a, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:55:57 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:55:57 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:55:57 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-2 hasn't rolled
2022-03-28 10:55:57 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1692519ms till timeout)
2022-03-28 10:55:57 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (298798ms till timeout)
2022-03-28 10:55:57 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-6 describe pod my-cluster-da91577a-kafka-0
2022-03-28 10:55:57 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:57 [ForkJoinPool-1-worker-15] DEBUG [LogCollector:208] Collecting logs for TestCase: testPauseReconciliationInKafkaRebalanceAndTopic, and Pod: my-cluster-da91577a-kafka-1
2022-03-28 10:55:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-472
2022-03-28 10:55:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:57 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-475
2022-03-28 10:55:57 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-6 describe pod my-cluster-da91577a-kafka-1
2022-03-28 10:55:57 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-f50d425d-cruise-control-8546d57b59-c8rgt not ready: cruise-control)
2022-03-28 10:55:57 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-f50d425d-cruise-control-8546d57b59-c8rgt not ready: tls-sidecar)
2022-03-28 10:55:57 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-f50d425d-cruise-control-8546d57b59-c8rgt are ready
2022-03-28 10:55:57 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-f50d425d, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-f50d425d-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590195ms till timeout)
2022-03-28 10:55:57 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1282630ms till timeout)
2022-03-28 10:55:58 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-b6310693-cruise-control-7c74956698-lqfpx=dddfd390-e0dd-4449-a1ae-af7eeeb85c68}
2022-03-28 10:55:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-475
2022-03-28 10:55:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:58 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-476
2022-03-28 10:55:58 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-6 describe pod my-cluster-da91577a-kafka-1
2022-03-28 10:55:58 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:58 [ForkJoinPool-1-worker-15] DEBUG [LogCollector:208] Collecting logs for TestCase: testPauseReconciliationInKafkaRebalanceAndTopic, and Pod: my-cluster-da91577a-kafka-2
2022-03-28 10:55:58 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-b6310693-cruise-control-7c74956698-lqfpx=dddfd390-e0dd-4449-a1ae-af7eeeb85c68}
2022-03-28 10:55:58 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-b6310693-cruise-control-7c74956698-lqfpx=dddfd390-e0dd-4449-a1ae-af7eeeb85c68}
2022-03-28 10:55:58 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Deployment my-cluster-b6310693-cruise-control rolling update in namespace:namespace-5 not ready, will try again in 5000 ms (589594ms till timeout)
2022-03-28 10:55:58 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (297644ms till timeout)
2022-03-28 10:55:58 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-6 describe pod my-cluster-da91577a-kafka-2
2022-03-28 10:55:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-476
2022-03-28 10:55:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:58 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-477
2022-03-28 10:55:58 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-6 describe pod my-cluster-da91577a-kafka-2
2022-03-28 10:55:58 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:58 [ForkJoinPool-1-worker-15] DEBUG [LogCollector:208] Collecting logs for TestCase: testPauseReconciliationInKafkaRebalanceAndTopic, and Pod: my-cluster-da91577a-zookeeper-0
2022-03-28 10:55:58 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-f50d425d-cruise-control-8546d57b59-c8rgt not ready: cruise-control)
2022-03-28 10:55:58 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-f50d425d-cruise-control-8546d57b59-c8rgt not ready: tls-sidecar)
2022-03-28 10:55:58 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-f50d425d-cruise-control-8546d57b59-c8rgt are ready
2022-03-28 10:55:58 [ForkJoinPool-1-worker-3] INFO  [DeploymentUtils:141] Deployment my-cluster-f50d425d-cruise-control rolling update finished
2022-03-28 10:55:58 [ForkJoinPool-1-worker-3] INFO  [CruiseControlConfigurationST:171] Verifying that Kafka pods did not roll
2022-03-28 10:55:58 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Waiting for stability of rolling update will be not triggered
2022-03-28 10:55:58 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:55:58 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-6 describe pod my-cluster-da91577a-zookeeper-0
2022-03-28 10:55:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-477
2022-03-28 10:55:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:58 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-48
2022-03-28 10:55:58 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:55:58 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:55:58 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:55:58 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 50
2022-03-28 10:55:58 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (299901ms till timeout)
2022-03-28 10:55:59 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1281386ms till timeout)
2022-03-28 10:55:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-48
2022-03-28 10:55:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:59 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-480
2022-03-28 10:55:59 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (296477ms till timeout)
2022-03-28 10:55:59 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-6 describe pod my-cluster-da91577a-zookeeper-0
2022-03-28 10:55:59 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:59 [ForkJoinPool-1-worker-15] DEBUG [LogCollector:208] Collecting logs for TestCase: testPauseReconciliationInKafkaRebalanceAndTopic, and Pod: my-cluster-da91577a-zookeeper-1
2022-03-28 10:55:59 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-6 describe pod my-cluster-da91577a-zookeeper-1
2022-03-28 10:55:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-480
2022-03-28 10:55:59 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:55:59 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-486
2022-03-28 10:55:59 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:00 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:00 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:00 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:56:00 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 49
2022-03-28 10:56:00 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (298853ms till timeout)
2022-03-28 10:56:00 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-6 describe pod my-cluster-da91577a-zookeeper-1
2022-03-28 10:56:00 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:56:00 [ForkJoinPool-1-worker-15] DEBUG [LogCollector:208] Collecting logs for TestCase: testPauseReconciliationInKafkaRebalanceAndTopic, and Pod: my-cluster-da91577a-zookeeper-2
2022-03-28 10:56:00 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1280306ms till timeout)
2022-03-28 10:56:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-486
2022-03-28 10:56:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:56:00 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-487
2022-03-28 10:56:00 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-6 describe pod my-cluster-da91577a-zookeeper-2
2022-03-28 10:56:00 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (295418ms till timeout)
2022-03-28 10:56:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-487
2022-03-28 10:56:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:56:00 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-51
2022-03-28 10:56:00 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-6 describe pod my-cluster-da91577a-zookeeper-2
2022-03-28 10:56:00 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:56:00 [ForkJoinPool-1-worker-15] INFO  [LogCollector:266] Collecting Deployments in Namespace namespace-6
2022-03-28 10:56:00 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Deployment -o yaml
2022-03-28 10:56:00 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testScramUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:56:00 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Deployment -o yaml
2022-03-28 10:56:00 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:56:00 [ForkJoinPool-1-worker-15] INFO  [LogCollector:271] Collecting StatefulSets in Namespace namespace-6
2022-03-28 10:56:00 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-6 get StatefulSet -o yaml
2022-03-28 10:56:01 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-51
2022-03-28 10:56:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:56:01 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-54
2022-03-28 10:56:01 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:01 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:01 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:56:01 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 48
2022-03-28 10:56:01 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (297792ms till timeout)
2022-03-28 10:56:01 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1279220ms till timeout)
2022-03-28 10:56:01 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-6 get StatefulSet -o yaml
2022-03-28 10:56:01 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:56:01 [ForkJoinPool-1-worker-15] INFO  [LogCollector:276] Collecting ReplicaSets in Namespace namespace-6
2022-03-28 10:56:01 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-6 get replicaset -o yaml
2022-03-28 10:56:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-54
2022-03-28 10:56:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:56:01 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-56
2022-03-28 10:56:01 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (294289ms till timeout)
2022-03-28 10:56:01 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-6 get replicaset -o yaml
2022-03-28 10:56:01 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:56:01 [ForkJoinPool-1-worker-15] INFO  [LogCollector:281] Collecting Strimzi in Namespace namespace-6
2022-03-28 10:56:01 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc get strimzi -o yaml -n namespace-6
2022-03-28 10:56:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-56
2022-03-28 10:56:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:56:01 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-59
2022-03-28 10:56:02 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:02 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=b9a4429e-22d8-41a5-b7e5-723851ceb10a, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:56:02 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:02 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:02 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:56:02 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 47
2022-03-28 10:56:02 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (296677ms till timeout)
2022-03-28 10:56:02 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:56:02 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:56:02 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-2 hasn't rolled
2022-03-28 10:56:02 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1687444ms till timeout)
2022-03-28 10:56:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1278169ms till timeout)
2022-03-28 10:56:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-59
2022-03-28 10:56:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:56:02 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-6
2022-03-28 10:56:02 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (293230ms till timeout)
2022-03-28 10:56:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-6
2022-03-28 10:56:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:56:02 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-60
2022-03-28 10:56:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-60
2022-03-28 10:56:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:56:03 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-65
2022-03-28 10:56:03 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc get strimzi -o yaml -n namespace-6
2022-03-28 10:56:03 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:56:03 [ForkJoinPool-1-worker-15] INFO  [LogCollector:287] Collecting cluster status
2022-03-28 10:56:03 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc describe nodes
2022-03-28 10:56:03 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-b6310693-cruise-control-7c74956698-lqfpx=dddfd390-e0dd-4449-a1ae-af7eeeb85c68}
2022-03-28 10:56:03 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:03 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:03 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:03 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:56:03 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 46
2022-03-28 10:56:03 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (295637ms till timeout)
2022-03-28 10:56:03 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-b6310693-cruise-control-7c74956698-lqfpx=dddfd390-e0dd-4449-a1ae-af7eeeb85c68}
2022-03-28 10:56:03 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-b6310693-cruise-control-7c74956698-lqfpx=dddfd390-e0dd-4449-a1ae-af7eeeb85c68}
2022-03-28 10:56:03 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Deployment my-cluster-b6310693-cruise-control rolling update in namespace:namespace-5 not ready, will try again in 5000 ms (584514ms till timeout)
2022-03-28 10:56:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1277130ms till timeout)
2022-03-28 10:56:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-65
2022-03-28 10:56:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:56:03 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-73
2022-03-28 10:56:03 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (292061ms till timeout)
2022-03-28 10:56:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-73
2022-03-28 10:56:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:56:03 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-75
2022-03-28 10:56:04 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:04 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:04 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:04 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:56:04 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 45
2022-03-28 10:56:04 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (294550ms till timeout)
2022-03-28 10:56:04 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1276076ms till timeout)
2022-03-28 10:56:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-75
2022-03-28 10:56:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:56:04 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-8
2022-03-28 10:56:05 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (290933ms till timeout)
2022-03-28 10:56:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-8
2022-03-28 10:56:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:56:05 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-80
2022-03-28 10:56:05 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:05 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:05 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:05 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:56:05 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 44
2022-03-28 10:56:05 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (293500ms till timeout)
2022-03-28 10:56:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-80
2022-03-28 10:56:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:56:05 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-81
2022-03-28 10:56:05 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1274973ms till timeout)
2022-03-28 10:56:05 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testScramUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:56:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-81
2022-03-28 10:56:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:56:05 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-83
2022-03-28 10:56:06 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc describe nodes
2022-03-28 10:56:06 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:56:06 [ForkJoinPool-1-worker-15] INFO  [LogCollector:252] Collecting events in Namespace reconciliation-st
2022-03-28 10:56:06 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace reconciliation-st get events
2022-03-28 10:56:06 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (289814ms till timeout)
2022-03-28 10:56:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-83
2022-03-28 10:56:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:56:06 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-84
2022-03-28 10:56:06 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace reconciliation-st get events
2022-03-28 10:56:06 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:56:06 [ForkJoinPool-1-worker-15] INFO  [LogCollector:259] Collecting ConfigMaps in Namespace reconciliation-st
2022-03-28 10:56:06 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:06 [ForkJoinPool-1-worker-15] INFO  [LogCollector:217] Collecting logs for Pod(s) in Namespace reconciliation-st
2022-03-28 10:56:06 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:06 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:06 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:56:06 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 43
2022-03-28 10:56:06 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (292436ms till timeout)
2022-03-28 10:56:06 [ForkJoinPool-1-worker-15] INFO  [LogCollector:266] Collecting Deployments in Namespace reconciliation-st
2022-03-28 10:56:06 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace reconciliation-st get Deployment -o yaml
2022-03-28 10:56:06 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1273932ms till timeout)
2022-03-28 10:56:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-84
2022-03-28 10:56:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:56:06 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-86
2022-03-28 10:56:06 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace reconciliation-st get Deployment -o yaml
2022-03-28 10:56:06 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:56:06 [ForkJoinPool-1-worker-15] INFO  [LogCollector:271] Collecting StatefulSets in Namespace reconciliation-st
2022-03-28 10:56:06 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace reconciliation-st get StatefulSet -o yaml
2022-03-28 10:56:06 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace reconciliation-st get StatefulSet -o yaml
2022-03-28 10:56:06 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:56:06 [ForkJoinPool-1-worker-15] INFO  [LogCollector:276] Collecting ReplicaSets in Namespace reconciliation-st
2022-03-28 10:56:06 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace reconciliation-st get replicaset -o yaml
2022-03-28 10:56:07 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace reconciliation-st get replicaset -o yaml
2022-03-28 10:56:07 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:56:07 [ForkJoinPool-1-worker-15] INFO  [LogCollector:281] Collecting Strimzi in Namespace reconciliation-st
2022-03-28 10:56:07 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc get strimzi -o yaml -n reconciliation-st
2022-03-28 10:56:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-86
2022-03-28 10:56:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:56:07 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-87
2022-03-28 10:56:07 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=b9a4429e-22d8-41a5-b7e5-723851ceb10a, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:56:07 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (288671ms till timeout)
2022-03-28 10:56:07 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:56:07 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:56:07 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-2 hasn't rolled
2022-03-28 10:56:07 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1682362ms till timeout)
2022-03-28 10:56:07 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:07 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:07 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:07 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:56:07 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 42
2022-03-28 10:56:07 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (291323ms till timeout)
2022-03-28 10:56:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-87
2022-03-28 10:56:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:56:07 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-9
2022-03-28 10:56:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1272878ms till timeout)
2022-03-28 10:56:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-9
2022-03-28 10:56:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:56:07 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-92
2022-03-28 10:56:08 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc get strimzi -o yaml -n reconciliation-st
2022-03-28 10:56:08 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:56:08 [ForkJoinPool-1-worker-15] INFO  [LogCollector:287] Collecting cluster status
2022-03-28 10:56:08 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc describe nodes
2022-03-28 10:56:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-92
2022-03-28 10:56:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:56:08 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-94
2022-03-28 10:56:08 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-b6310693-cruise-control-7c74956698-lqfpx=dddfd390-e0dd-4449-a1ae-af7eeeb85c68}
2022-03-28 10:56:08 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (287617ms till timeout)
2022-03-28 10:56:08 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-b6310693-cruise-control-585bd649db-wtrrk=29886af2-1f73-4814-8a71-ff2a461ef4a8, my-cluster-b6310693-cruise-control-7c74956698-lqfpx=dddfd390-e0dd-4449-a1ae-af7eeeb85c68}
2022-03-28 10:56:08 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-b6310693-cruise-control-7c74956698-lqfpx=dddfd390-e0dd-4449-a1ae-af7eeeb85c68}
2022-03-28 10:56:08 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Deployment my-cluster-b6310693-cruise-control rolling update in namespace:namespace-5 not ready, will try again in 5000 ms (579439ms till timeout)
2022-03-28 10:56:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-94
2022-03-28 10:56:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:56:08 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-97
2022-03-28 10:56:08 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:08 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:08 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:08 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:56:08 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 41
2022-03-28 10:56:08 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (290205ms till timeout)
2022-03-28 10:56:08 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1271829ms till timeout)
2022-03-28 10:56:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-97
2022-03-28 10:56:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:56:08 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-99
2022-03-28 10:56:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-99
2022-03-28 10:56:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 10:56:09 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:674] ============================================================================
2022-03-28 10:56:09 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:675] [operators.topic.ThrottlingQuotaST - After Each] - Clean up after test
2022-03-28 10:56:09 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:344] ############################################################################
2022-03-28 10:56:09 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:348] Delete all resources for testThrottlingQuotasCreateTopic
2022-03-28 10:56:09 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:241] Delete of Job create-admin-my-cluster-83f3f4b8-kafka-clients in namespace throttling-quota-st
2022-03-28 10:56:09 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:create-admin-my-cluster-83f3f4b8-kafka-clients
2022-03-28 10:56:09 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:241] Delete of KafkaUser my-user-1131806865-947217230 in namespace throttling-quota-st
2022-03-28 10:56:09 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1131806865-947217230
2022-03-28 10:56:09 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (286544ms till timeout)
2022-03-28 10:56:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1131806865-947217230 not ready, will try again in 10000 ms (179854ms till timeout)
2022-03-28 10:56:09 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:09 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:09 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:09 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:56:09 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 40
2022-03-28 10:56:09 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (289104ms till timeout)
2022-03-28 10:56:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1270743ms till timeout)
2022-03-28 10:56:10 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc describe nodes
2022-03-28 10:56:10 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:56:10 [ForkJoinPool-1-worker-15] DEBUG [AbstractST:674] ============================================================================
2022-03-28 10:56:10 [ForkJoinPool-1-worker-15] DEBUG [AbstractST:675] [operators.ReconciliationST - After Each] - Clean up after test
2022-03-28 10:56:10 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:344] ############################################################################
2022-03-28 10:56:10 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:348] Delete all resources for testPauseReconciliationInKafkaRebalanceAndTopic
2022-03-28 10:56:10 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:241] Delete of KafkaTopic my-topic-1674392683-1637010870 in namespace namespace-6
2022-03-28 10:56:10 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1674392683-1637010870
2022-03-28 10:56:10 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1674392683-1637010870 not ready, will try again in 10000 ms (179906ms till timeout)
2022-03-28 10:56:10 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (285383ms till timeout)
2022-03-28 10:56:10 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testScramUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:56:10 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:10 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:10 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:10 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:56:10 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 39
2022-03-28 10:56:10 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (287989ms till timeout)
2022-03-28 10:56:10 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1269620ms till timeout)
2022-03-28 10:56:11 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (284335ms till timeout)
2022-03-28 10:56:11 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:11 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1268602ms till timeout)
2022-03-28 10:56:11 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:11 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:11 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:56:11 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 38
2022-03-28 10:56:11 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (286954ms till timeout)
2022-03-28 10:56:12 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=b9a4429e-22d8-41a5-b7e5-723851ceb10a, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:56:12 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:56:12 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:56:12 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-2 hasn't rolled
2022-03-28 10:56:12 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1677280ms till timeout)
2022-03-28 10:56:12 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (283302ms till timeout)
2022-03-28 10:56:12 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1267542ms till timeout)
2022-03-28 10:56:13 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:13 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:13 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:56:13 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 37
2022-03-28 10:56:13 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (285847ms till timeout)
2022-03-28 10:56:13 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-b6310693-cruise-control-7c74956698-lqfpx=dddfd390-e0dd-4449-a1ae-af7eeeb85c68}
2022-03-28 10:56:13 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-b6310693-cruise-control-585bd649db-wtrrk=29886af2-1f73-4814-8a71-ff2a461ef4a8, my-cluster-b6310693-cruise-control-7c74956698-lqfpx=dddfd390-e0dd-4449-a1ae-af7eeeb85c68}
2022-03-28 10:56:13 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-b6310693-cruise-control-7c74956698-lqfpx=dddfd390-e0dd-4449-a1ae-af7eeeb85c68}
2022-03-28 10:56:13 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Deployment my-cluster-b6310693-cruise-control rolling update in namespace:namespace-5 not ready, will try again in 5000 ms (574296ms till timeout)
2022-03-28 10:56:13 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (282258ms till timeout)
2022-03-28 10:56:14 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1266432ms till timeout)
2022-03-28 10:56:14 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:14 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:14 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:56:14 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 36
2022-03-28 10:56:14 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (284736ms till timeout)
2022-03-28 10:56:14 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (281116ms till timeout)
2022-03-28 10:56:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1265382ms till timeout)
2022-03-28 10:56:15 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:15 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:15 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:15 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:56:15 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 35
2022-03-28 10:56:15 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (283691ms till timeout)
2022-03-28 10:56:15 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testScramUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:56:15 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (279991ms till timeout)
2022-03-28 10:56:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1264335ms till timeout)
2022-03-28 10:56:16 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:16 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:16 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:16 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:56:16 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 34
2022-03-28 10:56:16 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (282598ms till timeout)
2022-03-28 10:56:17 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (278898ms till timeout)
2022-03-28 10:56:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1263239ms till timeout)
2022-03-28 10:56:17 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:17 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:17 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:17 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:56:17 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 33
2022-03-28 10:56:17 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (281552ms till timeout)
2022-03-28 10:56:17 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=b9a4429e-22d8-41a5-b7e5-723851ceb10a, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:56:17 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:56:17 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:56:17 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-2 hasn't rolled
2022-03-28 10:56:17 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1672226ms till timeout)
2022-03-28 10:56:18 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (277846ms till timeout)
2022-03-28 10:56:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1262206ms till timeout)
2022-03-28 10:56:18 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:18 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:18 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:18 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:56:18 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 32
2022-03-28 10:56:18 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (280455ms till timeout)
2022-03-28 10:56:18 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-b6310693-cruise-control-7c74956698-lqfpx=dddfd390-e0dd-4449-a1ae-af7eeeb85c68}
2022-03-28 10:56:18 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-b6310693-cruise-control-585bd649db-wtrrk=29886af2-1f73-4814-8a71-ff2a461ef4a8, my-cluster-b6310693-cruise-control-7c74956698-lqfpx=dddfd390-e0dd-4449-a1ae-af7eeeb85c68}
2022-03-28 10:56:18 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-b6310693-cruise-control-7c74956698-lqfpx=dddfd390-e0dd-4449-a1ae-af7eeeb85c68}
2022-03-28 10:56:18 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Deployment my-cluster-b6310693-cruise-control rolling update in namespace:namespace-5 not ready, will try again in 5000 ms (569166ms till timeout)
2022-03-28 10:56:19 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (276783ms till timeout)
2022-03-28 10:56:19 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1261166ms till timeout)
2022-03-28 10:56:19 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:19 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:369] ############################################################################
2022-03-28 10:56:19 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:267] testThrottlingQuotasCreateTopic - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTls, testSendSimpleMessageTlsScramSha, testConfigurationFileIsCreated, testReceiveSimpleMessageTls, testDeployAndUnDeployCruiseControl, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testConfigurationPerformanceOptions, testScramUserWithQuotas] to and randomly select one to start execution
2022-03-28 10:56:19 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:93] [operators.topic.ThrottlingQuotaST] - Removing parallel test: testThrottlingQuotasCreateTopic
2022-03-28 10:56:19 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:97] [operators.topic.ThrottlingQuotaST] - Parallel test count: 7
2022-03-28 10:56:19 [ForkJoinPool-1-worker-9] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testThrottlingQuotasCreateTopic-FINISHED
2022-03-28 10:56:19 [ForkJoinPool-1-worker-9] INFO  [TestSeparator:30] ############################################################################
2022-03-28 10:56:19 [ForkJoinPool-1-worker-9] INFO  [TestSeparator:23] ############################################################################
2022-03-28 10:56:19 [ForkJoinPool-1-worker-9] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testThrottlingQuotasDeleteTopic-STARTED
2022-03-28 10:56:19 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:658] ============================================================================
2022-03-28 10:56:19 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:659] [operators.topic.ThrottlingQuotaST - Before Each] - Setup test case environment
2022-03-28 10:56:19 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:77] [operators.topic.ThrottlingQuotaST] - Adding parallel test: testThrottlingQuotasDeleteTopic
2022-03-28 10:56:19 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:81] [operators.topic.ThrottlingQuotaST] - Parallel test count: 8
2022-03-28 10:56:19 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:205] [testThrottlingQuotasDeleteTopic] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:56:19 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:210] testThrottlingQuotasDeleteTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:56:19 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:19 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:19 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:56:19 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 31
2022-03-28 10:56:19 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (279385ms till timeout)
2022-03-28 10:56:20 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (275723ms till timeout)
2022-03-28 10:56:20 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1260121ms till timeout)
2022-03-28 10:56:20 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:241] Delete of Kafka my-cluster-da91577a in namespace namespace-6
2022-03-28 10:56:20 [ForkJoinPool-1-worker-15] INFO  [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-6, for cruise control Kafka cluster my-cluster-da91577a
2022-03-28 10:56:20 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:20 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:20 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:20 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:56:20 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 30
2022-03-28 10:56:20 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (278322ms till timeout)
2022-03-28 10:56:20 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:230] testScramUserWithQuotas test now can proceed its execution
2022-03-28 10:56:20 [ForkJoinPool-1-worker-13] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 10:56:20 [ForkJoinPool-1-worker-13] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testScramUserWithQuotas=my-cluster-4941482a, testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383, testConfigurationFileIsCreated=my-cluster-e0fac774, testThrottlingQuotasCreateAlterPartitions=my-cluster-28d9dd2e, testSendSimpleMessageTls=my-cluster-27ddd6ba, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c, testConfigurationReflection=my-cluster-d1e2168e, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-da91577a, testDeployAndUnDeployCruiseControl=my-cluster-c0830cbe, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8, testCapacityFile=my-cluster-fd0fb61a, testReceiveSimpleMessageTls=my-cluster-c6a15963, testConfigurationPerformanceOptions=my-cluster-b6310693, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-f50d425d, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5}
2022-03-28 10:56:20 [ForkJoinPool-1-worker-13] TRACE [AbstractST:607] USERS_NAME_MAP: {testScramUserWithQuotas=my-user-797280497-2138800976, testReceiveSimpleMessageTlsScramSha=my-user-1915916223-1265293973, testConfigurationFileIsCreated=my-user-180611180-50530491, testThrottlingQuotasCreateAlterPartitions=my-user-1933547616-1024431140, testSendSimpleMessageTls=my-user-738229993-1095121647, testCreatingUsersWithSecretPrefix=my-user-1450786684-1808961062, testConfigurationReflection=my-user-1445653023-93200027, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-1260379615-1050949907, testDeployAndUnDeployCruiseControl=my-user-1988711145-439861161, testThrottlingQuotasCreateTopic=my-user-1131806865-947217230, testCapacityFile=my-user-937015144-1959439396, testReceiveSimpleMessageTls=my-user-6088058-1142028780, testConfigurationPerformanceOptions=my-user-930975101-640523779, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-745974575-1852384454, testSendSimpleMessageTlsScramSha=my-user-1499386234-1249154328}
2022-03-28 10:56:20 [ForkJoinPool-1-worker-13] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testScramUserWithQuotas=my-topic-2004840350-265526537, testReceiveSimpleMessageTlsScramSha=my-topic-1834944466-2094449929, testConfigurationFileIsCreated=my-topic-1296407588-313880708, testThrottlingQuotasCreateAlterPartitions=my-topic-609849288-1810034325, testSendSimpleMessageTls=my-topic-563023482-1697359353, testCreatingUsersWithSecretPrefix=my-topic-1479520843-618891199, testConfigurationReflection=my-topic-640815392-1092406112, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-1674392683-1637010870, testDeployAndUnDeployCruiseControl=my-topic-1511079187-95055629, testThrottlingQuotasCreateTopic=my-topic-728672490-1194703402, testCapacityFile=my-topic-1155641705-902584520, testReceiveSimpleMessageTls=my-topic-478514536-2132782608, testConfigurationPerformanceOptions=my-topic-213912446-1165431808, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-254848724-189700675, testSendSimpleMessageTlsScramSha=my-topic-1028951580-1656411053}
2022-03-28 10:56:20 [ForkJoinPool-1-worker-13] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testScramUserWithQuotas=my-cluster-4941482a-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383-kafka-clients, testConfigurationFileIsCreated=my-cluster-e0fac774-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-28d9dd2e-kafka-clients, testSendSimpleMessageTls=my-cluster-27ddd6ba-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c-kafka-clients, testConfigurationReflection=my-cluster-d1e2168e-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-da91577a-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-c0830cbe-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8-kafka-clients, testCapacityFile=my-cluster-fd0fb61a-kafka-clients, testReceiveSimpleMessageTls=my-cluster-c6a15963-kafka-clients, testConfigurationPerformanceOptions=my-cluster-b6310693-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-f50d425d-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5-kafka-clients}
2022-03-28 10:56:20 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update KafkaUser scramed-arnost in namespace user-st
2022-03-28 10:56:20 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:scramed-arnost
2022-03-28 10:56:21 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:433] Wait for KafkaUser: scramed-arnost will have desired state: Ready
2022-03-28 10:56:21 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaUser: scramed-arnost will have desired state: Ready
2022-03-28 10:56:21 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] KafkaUser: scramed-arnost will have desired state: Ready not ready, will try again in 1000 ms (179908ms till timeout)
2022-03-28 10:56:21 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-da91577a
2022-03-28 10:56:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-da91577a not ready, will try again in 10000 ms (839890ms till timeout)
2022-03-28 10:56:21 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (274650ms till timeout)
2022-03-28 10:56:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1259000ms till timeout)
2022-03-28 10:56:21 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:21 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:21 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:21 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:56:21 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 29
2022-03-28 10:56:21 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (277236ms till timeout)
2022-03-28 10:56:22 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] KafkaUser: scramed-arnost will have desired state: Ready not ready, will try again in 1000 ms (178853ms till timeout)
2022-03-28 10:56:22 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=b9a4429e-22d8-41a5-b7e5-723851ceb10a, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:56:22 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (273335ms till timeout)
2022-03-28 10:56:22 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:22 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:56:22 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:56:22 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-2 hasn't rolled
2022-03-28 10:56:22 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1666944ms till timeout)
2022-03-28 10:56:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1257794ms till timeout)
2022-03-28 10:56:22 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:22 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:22 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:56:22 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 28
2022-03-28 10:56:22 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (276152ms till timeout)
2022-03-28 10:56:23 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:444] KafkaUser: scramed-arnost is in desired state: Ready
2022-03-28 10:56:23 [ForkJoinPool-1-worker-13] DEBUG [UserST:274] Command for kafka-configs.sh bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user scramed-arnost
2022-03-28 10:56:23 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user scramed-arnost
2022-03-28 10:56:23 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-b6310693-cruise-control-7c74956698-lqfpx=dddfd390-e0dd-4449-a1ae-af7eeeb85c68}
2022-03-28 10:56:23 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (272290ms till timeout)
2022-03-28 10:56:23 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:23 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-b6310693-cruise-control-585bd649db-wtrrk=29886af2-1f73-4814-8a71-ff2a461ef4a8, my-cluster-b6310693-cruise-control-7c74956698-lqfpx=dddfd390-e0dd-4449-a1ae-af7eeeb85c68}
2022-03-28 10:56:23 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-b6310693-cruise-control-7c74956698-lqfpx=dddfd390-e0dd-4449-a1ae-af7eeeb85c68}
2022-03-28 10:56:23 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Deployment my-cluster-b6310693-cruise-control rolling update in namespace:namespace-5 not ready, will try again in 5000 ms (564045ms till timeout)
2022-03-28 10:56:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1256730ms till timeout)
2022-03-28 10:56:23 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:23 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:23 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:56:23 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 27
2022-03-28 10:56:23 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (275088ms till timeout)
2022-03-28 10:56:24 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:210] testThrottlingQuotasDeleteTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:56:24 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (271248ms till timeout)
2022-03-28 10:56:24 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1255681ms till timeout)
2022-03-28 10:56:24 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:24 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:24 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:56:24 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 26
2022-03-28 10:56:24 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (273986ms till timeout)
2022-03-28 10:56:25 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:25 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (270001ms till timeout)
2022-03-28 10:56:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1254467ms till timeout)
2022-03-28 10:56:26 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:26 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:26 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:56:26 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 25
2022-03-28 10:56:26 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (272802ms till timeout)
2022-03-28 10:56:27 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:27 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (268868ms till timeout)
2022-03-28 10:56:27 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1253419ms till timeout)
2022-03-28 10:56:27 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:27 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:27 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:56:27 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 24
2022-03-28 10:56:27 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (271762ms till timeout)
2022-03-28 10:56:27 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=b9a4429e-22d8-41a5-b7e5-723851ceb10a, my-cluster-c0830cbe-kafka-1=c3130ccf-e289-47c4-891b-61eb070ab137, my-cluster-c0830cbe-kafka-2=eb46446a-904f-4a79-8ad3-c9914e0e3bf0}
2022-03-28 10:56:27 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 10:56:27 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 10:56:27 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:55] All pods seem to have rolled
2022-03-28 10:56:27 [ForkJoinPool-1-worker-1] INFO  [RollingUpdateUtils:86] Component with name: my-cluster-c0830cbe-kafka has been successfully rolled
2022-03-28 10:56:27 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:87] Component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={}) successfully rolled
2022-03-28 10:56:27 [ForkJoinPool-1-worker-1] INFO  [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-c0830cbe-kafka to be ready
2022-03-28 10:56:27 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready
2022-03-28 10:56:27 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 10:56:27 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-1 not ready: kafka)
2022-03-28 10:56:27 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-c0830cbe-kafka-2)
2022-03-28 10:56:27 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1799902ms till timeout)
2022-03-28 10:56:28 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:28 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (267808ms till timeout)
2022-03-28 10:56:28 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1252348ms till timeout)
2022-03-28 10:56:28 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:28 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:28 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:56:28 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 23
2022-03-28 10:56:28 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (270700ms till timeout)
2022-03-28 10:56:28 [ForkJoinPool-1-worker-13] INFO  [Exec:417] Command: oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user scramed-arnost
2022-03-28 10:56:28 [ForkJoinPool-1-worker-13] INFO  [Exec:417] Return code: 0
2022-03-28 10:56:28 [ForkJoinPool-1-worker-13] INFO  [KafkaUserUtils:62] Waiting for KafkaUser deletion scramed-arnost
2022-03-28 10:56:28 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaUser deletion scramed-arnost
2022-03-28 10:56:28 [ForkJoinPool-1-worker-13] WARN  [KafkaUserUtils:68] KafkaUser scramed-arnost is not deleted yet! Triggering force delete by cmd client!
2022-03-28 10:56:28 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-7 delete KafkaUser scramed-arnost
2022-03-28 10:56:28 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-7 delete KafkaUser scramed-arnost
2022-03-28 10:56:28 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 1
2022-03-28 10:56:28 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 10:56:28 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Error from server (NotFound): kafkausers.kafka.strimzi.io "scramed-arnost" not found
2022-03-28 10:56:28 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] ======STDERR END======
2022-03-28 10:56:28 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] KafkaUser deletion scramed-arnost not ready, will try again in 1000 ms (179630ms till timeout)
2022-03-28 10:56:28 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-b6310693-cruise-control-7c74956698-lqfpx=dddfd390-e0dd-4449-a1ae-af7eeeb85c68}
2022-03-28 10:56:28 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-b6310693-cruise-control-585bd649db-wtrrk=29886af2-1f73-4814-8a71-ff2a461ef4a8, my-cluster-b6310693-cruise-control-7c74956698-lqfpx=dddfd390-e0dd-4449-a1ae-af7eeeb85c68}
2022-03-28 10:56:28 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-b6310693-cruise-control-7c74956698-lqfpx=dddfd390-e0dd-4449-a1ae-af7eeeb85c68}
2022-03-28 10:56:28 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Deployment my-cluster-b6310693-cruise-control rolling update in namespace:namespace-5 not ready, will try again in 5000 ms (558972ms till timeout)
2022-03-28 10:56:28 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 10:56:28 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-1 not ready: kafka)
2022-03-28 10:56:28 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-c0830cbe-kafka-2)
2022-03-28 10:56:28 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1798861ms till timeout)
2022-03-28 10:56:29 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:29 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (266701ms till timeout)
2022-03-28 10:56:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1251270ms till timeout)
2022-03-28 10:56:29 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:29 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:29 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:56:29 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 22
2022-03-28 10:56:29 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (269624ms till timeout)
2022-03-28 10:56:29 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:210] testThrottlingQuotasDeleteTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:56:29 [ForkJoinPool-1-worker-13] INFO  [KafkaUserUtils:75] KafkaUser scramed-arnost deleted
2022-03-28 10:56:29 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for all KafkaUser scramed-arnost attributes will be cleaned
2022-03-28 10:56:29 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user scramed-arnost
2022-03-28 10:56:30 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 10:56:30 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-1 not ready: kafka)
2022-03-28 10:56:30 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-c0830cbe-kafka-2)
2022-03-28 10:56:30 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1797825ms till timeout)
2022-03-28 10:56:30 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:30 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1250210ms till timeout)
2022-03-28 10:56:30 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (265636ms till timeout)
2022-03-28 10:56:30 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:30 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:30 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:56:30 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 21
2022-03-28 10:56:30 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (268561ms till timeout)
2022-03-28 10:56:31 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 10:56:31 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-1 not ready: kafka)
2022-03-28 10:56:31 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-c0830cbe-kafka-2)
2022-03-28 10:56:31 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1796723ms till timeout)
2022-03-28 10:56:31 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:369] ############################################################################
2022-03-28 10:56:31 [ForkJoinPool-1-worker-15] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-6 for test case:testPauseReconciliationInKafkaRebalanceAndTopic
2022-03-28 10:56:31 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:31 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1249167ms till timeout)
2022-03-28 10:56:31 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (264569ms till timeout)
2022-03-28 10:56:31 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:31 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:31 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:56:31 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 20
2022-03-28 10:56:31 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (267497ms till timeout)
2022-03-28 10:56:31 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Namespace namespace-6 removal
2022-03-28 10:56:31 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:56:32 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 10:56:32 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-1 not ready: kafka)
2022-03-28 10:56:32 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-c0830cbe-kafka-2)
2022-03-28 10:56:32 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1795554ms till timeout)
2022-03-28 10:56:32 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:32 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1248130ms till timeout)
2022-03-28 10:56:32 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:32 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:32 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:56:32 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 19
2022-03-28 10:56:32 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (266439ms till timeout)
2022-03-28 10:56:32 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (263471ms till timeout)
2022-03-28 10:56:33 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 10:56:33 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-1 not ready: kafka)
2022-03-28 10:56:33 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-c0830cbe-kafka-2)
2022-03-28 10:56:33 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1794465ms till timeout)
2022-03-28 10:56:33 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:33 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1247068ms till timeout)
2022-03-28 10:56:33 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:33 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:33 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:56:33 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 18
2022-03-28 10:56:33 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (265357ms till timeout)
2022-03-28 10:56:33 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (262406ms till timeout)
2022-03-28 10:56:33 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-b6310693-cruise-control-7c74956698-lqfpx=dddfd390-e0dd-4449-a1ae-af7eeeb85c68}
2022-03-28 10:56:33 [ForkJoinPool-1-worker-13] INFO  [Exec:417] Command: oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user scramed-arnost
2022-03-28 10:56:33 [ForkJoinPool-1-worker-13] INFO  [Exec:417] Return code: 0
2022-03-28 10:56:33 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:674] ============================================================================
2022-03-28 10:56:33 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:675] [operators.user.UserST - After Each] - Clean up after test
2022-03-28 10:56:33 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:344] ############################################################################
2022-03-28 10:56:33 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:348] Delete all resources for testScramUserWithQuotas
2022-03-28 10:56:33 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of KafkaUser scramed-arnost in namespace user-st
2022-03-28 10:56:34 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-b6310693-cruise-control-585bd649db-wtrrk=29886af2-1f73-4814-8a71-ff2a461ef4a8, my-cluster-b6310693-cruise-control-7c74956698-lqfpx=dddfd390-e0dd-4449-a1ae-af7eeeb85c68}
2022-03-28 10:56:34 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-b6310693-cruise-control-7c74956698-lqfpx=dddfd390-e0dd-4449-a1ae-af7eeeb85c68}
2022-03-28 10:56:34 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Deployment my-cluster-b6310693-cruise-control rolling update in namespace:namespace-5 not ready, will try again in 5000 ms (553776ms till timeout)
2022-03-28 10:56:34 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:scramed-arnost
2022-03-28 10:56:34 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:369] ############################################################################
2022-03-28 10:56:34 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:267] testScramUserWithQuotas - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTls, testSendSimpleMessageTlsScramSha, testConfigurationFileIsCreated, testReceiveSimpleMessageTls, testDeployAndUnDeployCruiseControl, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testConfigurationPerformanceOptions, testThrottlingQuotasDeleteTopic] to and randomly select one to start execution
2022-03-28 10:56:34 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:93] [operators.user.UserST] - Removing parallel test: testScramUserWithQuotas
2022-03-28 10:56:34 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:97] [operators.user.UserST] - Parallel test count: 7
2022-03-28 10:56:34 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testScramUserWithQuotas-FINISHED
2022-03-28 10:56:34 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:30] ############################################################################
2022-03-28 10:56:34 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:23] ############################################################################
2022-03-28 10:56:34 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testTlsUserWithQuotas-STARTED
2022-03-28 10:56:34 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:658] ============================================================================
2022-03-28 10:56:34 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:659] [operators.user.UserST - Before Each] - Setup test case environment
2022-03-28 10:56:34 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:77] [operators.user.UserST] - Adding parallel test: testTlsUserWithQuotas
2022-03-28 10:56:34 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:81] [operators.user.UserST] - Parallel test count: 8
2022-03-28 10:56:34 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:205] [testTlsUserWithQuotas] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:56:34 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testTlsUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:56:34 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:230] testThrottlingQuotasDeleteTopic test now can proceed its execution
2022-03-28 10:56:34 [ForkJoinPool-1-worker-9] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 10:56:34 [ForkJoinPool-1-worker-9] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testScramUserWithQuotas=my-cluster-4941482a, testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383, testConfigurationFileIsCreated=my-cluster-e0fac774, testThrottlingQuotasCreateAlterPartitions=my-cluster-28d9dd2e, testSendSimpleMessageTls=my-cluster-27ddd6ba, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c, testThrottlingQuotasDeleteTopic=my-cluster-8f262de5, testConfigurationReflection=my-cluster-d1e2168e, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-da91577a, testDeployAndUnDeployCruiseControl=my-cluster-c0830cbe, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8, testCapacityFile=my-cluster-fd0fb61a, testReceiveSimpleMessageTls=my-cluster-c6a15963, testConfigurationPerformanceOptions=my-cluster-b6310693, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-f50d425d, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5}
2022-03-28 10:56:34 [ForkJoinPool-1-worker-9] TRACE [AbstractST:607] USERS_NAME_MAP: {testScramUserWithQuotas=my-user-797280497-2138800976, testReceiveSimpleMessageTlsScramSha=my-user-1915916223-1265293973, testConfigurationFileIsCreated=my-user-180611180-50530491, testThrottlingQuotasCreateAlterPartitions=my-user-1933547616-1024431140, testSendSimpleMessageTls=my-user-738229993-1095121647, testCreatingUsersWithSecretPrefix=my-user-1450786684-1808961062, testThrottlingQuotasDeleteTopic=my-user-1558977140-777236615, testConfigurationReflection=my-user-1445653023-93200027, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-1260379615-1050949907, testDeployAndUnDeployCruiseControl=my-user-1988711145-439861161, testThrottlingQuotasCreateTopic=my-user-1131806865-947217230, testCapacityFile=my-user-937015144-1959439396, testReceiveSimpleMessageTls=my-user-6088058-1142028780, testConfigurationPerformanceOptions=my-user-930975101-640523779, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-745974575-1852384454, testSendSimpleMessageTlsScramSha=my-user-1499386234-1249154328}
2022-03-28 10:56:34 [ForkJoinPool-1-worker-9] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testScramUserWithQuotas=my-topic-2004840350-265526537, testReceiveSimpleMessageTlsScramSha=my-topic-1834944466-2094449929, testConfigurationFileIsCreated=my-topic-1296407588-313880708, testThrottlingQuotasCreateAlterPartitions=my-topic-609849288-1810034325, testSendSimpleMessageTls=my-topic-563023482-1697359353, testCreatingUsersWithSecretPrefix=my-topic-1479520843-618891199, testThrottlingQuotasDeleteTopic=my-topic-2128658681-1688111273, testConfigurationReflection=my-topic-640815392-1092406112, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-1674392683-1637010870, testDeployAndUnDeployCruiseControl=my-topic-1511079187-95055629, testThrottlingQuotasCreateTopic=my-topic-728672490-1194703402, testCapacityFile=my-topic-1155641705-902584520, testReceiveSimpleMessageTls=my-topic-478514536-2132782608, testConfigurationPerformanceOptions=my-topic-213912446-1165431808, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-254848724-189700675, testSendSimpleMessageTlsScramSha=my-topic-1028951580-1656411053}
2022-03-28 10:56:34 [ForkJoinPool-1-worker-9] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testScramUserWithQuotas=my-cluster-4941482a-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383-kafka-clients, testConfigurationFileIsCreated=my-cluster-e0fac774-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-28d9dd2e-kafka-clients, testSendSimpleMessageTls=my-cluster-27ddd6ba-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-8f262de5-kafka-clients, testConfigurationReflection=my-cluster-d1e2168e-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-da91577a-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-c0830cbe-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8-kafka-clients, testCapacityFile=my-cluster-fd0fb61a-kafka-clients, testReceiveSimpleMessageTls=my-cluster-c6a15963-kafka-clients, testConfigurationPerformanceOptions=my-cluster-b6310693-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-f50d425d-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5-kafka-clients}
2022-03-28 10:56:34 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update KafkaUser my-user-1558977140-777236615 in namespace throttling-quota-st
2022-03-28 10:56:34 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 10:56:34 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-1 not ready: kafka)
2022-03-28 10:56:34 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-c0830cbe-kafka-2)
2022-03-28 10:56:34 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1793384ms till timeout)
2022-03-28 10:56:34 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:34 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1245990ms till timeout)
2022-03-28 10:56:34 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:34 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:34 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:56:34 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 17
2022-03-28 10:56:34 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (264316ms till timeout)
2022-03-28 10:56:34 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-1558977140-777236615
2022-03-28 10:56:34 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-1558977140-777236615 will have desired state: Ready
2022-03-28 10:56:34 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-1558977140-777236615 will have desired state: Ready
2022-03-28 10:56:34 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (261241ms till timeout)
2022-03-28 10:56:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] KafkaUser: my-user-1558977140-777236615 will have desired state: Ready not ready, will try again in 1000 ms (179973ms till timeout)
2022-03-28 10:56:35 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 10:56:35 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-1 not ready: kafka)
2022-03-28 10:56:35 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-c0830cbe-kafka-2)
2022-03-28 10:56:35 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1792311ms till timeout)
2022-03-28 10:56:35 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1244957ms till timeout)
2022-03-28 10:56:35 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:35 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:35 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:35 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:56:35 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 16
2022-03-28 10:56:35 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (263249ms till timeout)
2022-03-28 10:56:35 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:444] KafkaUser: my-user-1558977140-777236615 is in desired state: Ready
2022-03-28 10:56:35 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (260119ms till timeout)
2022-03-28 10:56:35 [ForkJoinPool-1-worker-9] INFO  [ThrottlingQuotaST:112] Executing 1/5 iteration.
2022-03-28 10:56:35 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st
2022-03-28 10:56:35 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:create-admin-my-cluster-8f262de5-kafka-clients
2022-03-28 10:56:35 [ForkJoinPool-1-worker-9] INFO  [JobUtils:81] Waiting for job: create-admin-my-cluster-8f262de5-kafka-clients will be in active state
2022-03-28 10:56:35 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 10:56:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job active not ready, will try again in 1000 ms (179922ms till timeout)
2022-03-28 10:56:36 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1243940ms till timeout)
2022-03-28 10:56:36 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 10:56:36 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-1 not ready: kafka)
2022-03-28 10:56:36 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-c0830cbe-kafka-2)
2022-03-28 10:56:36 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1791282ms till timeout)
2022-03-28 10:56:36 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:36 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:36 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:36 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:56:36 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 15
2022-03-28 10:56:36 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (262196ms till timeout)
2022-03-28 10:56:36 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:56:36 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:56:36 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (474694ms till timeout)
2022-03-28 10:56:36 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (259069ms till timeout)
2022-03-28 10:56:37 [ForkJoinPool-1-worker-9] INFO  [ClientUtils:76] Waiting for producer/consumer:create-admin-my-cluster-8f262de5-kafka-clients to finished
2022-03-28 10:56:37 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 10:56:37 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:56:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (219948ms till timeout)
2022-03-28 10:56:37 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1242890ms till timeout)
2022-03-28 10:56:37 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 10:56:37 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-1 not ready: kafka)
2022-03-28 10:56:37 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-c0830cbe-kafka-2)
2022-03-28 10:56:37 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1790179ms till timeout)
2022-03-28 10:56:37 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:37 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:56:37 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:37 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:37 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:56:37 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 14
2022-03-28 10:56:37 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (261068ms till timeout)
2022-03-28 10:56:37 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (258010ms till timeout)
2022-03-28 10:56:38 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:56:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (218851ms till timeout)
2022-03-28 10:56:38 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1241865ms till timeout)
2022-03-28 10:56:38 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 10:56:38 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-1 not ready: kafka)
2022-03-28 10:56:38 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-c0830cbe-kafka-2)
2022-03-28 10:56:38 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1789136ms till timeout)
2022-03-28 10:56:38 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:38 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:38 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:38 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:56:38 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 13
2022-03-28 10:56:38 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (260016ms till timeout)
2022-03-28 10:56:38 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (256957ms till timeout)
2022-03-28 10:56:39 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-b6310693-cruise-control-7c74956698-lqfpx=dddfd390-e0dd-4449-a1ae-af7eeeb85c68}
2022-03-28 10:56:39 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-b6310693-cruise-control-585bd649db-wtrrk=29886af2-1f73-4814-8a71-ff2a461ef4a8, my-cluster-b6310693-cruise-control-7c74956698-lqfpx=dddfd390-e0dd-4449-a1ae-af7eeeb85c68}
2022-03-28 10:56:39 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-b6310693-cruise-control-7c74956698-lqfpx=dddfd390-e0dd-4449-a1ae-af7eeeb85c68}
2022-03-28 10:56:39 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Deployment my-cluster-b6310693-cruise-control rolling update in namespace:namespace-5 not ready, will try again in 5000 ms (548661ms till timeout)
2022-03-28 10:56:39 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testTlsUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:56:39 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:56:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (217784ms till timeout)
2022-03-28 10:56:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1240837ms till timeout)
2022-03-28 10:56:39 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 10:56:39 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-1 not ready: kafka)
2022-03-28 10:56:39 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-c0830cbe-kafka-2)
2022-03-28 10:56:39 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1788113ms till timeout)
2022-03-28 10:56:39 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:39 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:39 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:39 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:56:39 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 12
2022-03-28 10:56:39 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (258994ms till timeout)
2022-03-28 10:56:40 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (255888ms till timeout)
2022-03-28 10:56:40 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:56:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (216701ms till timeout)
2022-03-28 10:56:40 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1239794ms till timeout)
2022-03-28 10:56:40 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 10:56:40 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-1 not ready: kafka)
2022-03-28 10:56:40 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-c0830cbe-kafka-2)
2022-03-28 10:56:40 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1787095ms till timeout)
2022-03-28 10:56:40 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:40 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:40 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:40 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:56:40 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 11
2022-03-28 10:56:40 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (257948ms till timeout)
2022-03-28 10:56:41 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (254835ms till timeout)
2022-03-28 10:56:41 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:56:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (215603ms till timeout)
2022-03-28 10:56:41 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1238777ms till timeout)
2022-03-28 10:56:41 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 10:56:41 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-1 not ready: kafka)
2022-03-28 10:56:41 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-c0830cbe-kafka-2)
2022-03-28 10:56:41 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1786080ms till timeout)
2022-03-28 10:56:41 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:41 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:41 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:41 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:56:41 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 10
2022-03-28 10:56:41 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (256933ms till timeout)
2022-03-28 10:56:42 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (253805ms till timeout)
2022-03-28 10:56:42 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:56:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (214508ms till timeout)
2022-03-28 10:56:42 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1237753ms till timeout)
2022-03-28 10:56:42 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 10:56:42 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-1 not ready: kafka)
2022-03-28 10:56:42 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-c0830cbe-kafka-2)
2022-03-28 10:56:42 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1785031ms till timeout)
2022-03-28 10:56:42 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:43 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:43 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:43 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:56:43 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 9
2022-03-28 10:56:43 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (255859ms till timeout)
2022-03-28 10:56:43 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:56:43 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:56:43 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (468326ms till timeout)
2022-03-28 10:56:43 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (252717ms till timeout)
2022-03-28 10:56:43 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:56:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (213339ms till timeout)
2022-03-28 10:56:43 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1236711ms till timeout)
2022-03-28 10:56:43 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 10:56:43 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-1 not ready: kafka)
2022-03-28 10:56:43 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-c0830cbe-kafka-2)
2022-03-28 10:56:43 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1783981ms till timeout)
2022-03-28 10:56:44 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:44 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:44 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:44 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:56:44 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 8
2022-03-28 10:56:44 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (254839ms till timeout)
2022-03-28 10:56:44 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:56:44 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-b6310693-cruise-control-7c74956698-lqfpx=dddfd390-e0dd-4449-a1ae-af7eeeb85c68}
2022-03-28 10:56:44 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testTlsUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:56:44 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-b6310693-cruise-control-585bd649db-wtrrk=29886af2-1f73-4814-8a71-ff2a461ef4a8}
2022-03-28 10:56:44 [ForkJoinPool-1-worker-5] DEBUG [DeploymentUtils:119] All pods seem to have rolled
2022-03-28 10:56:44 [ForkJoinPool-1-worker-5] INFO  [DeploymentUtils:161] Wait for Deployment: my-cluster-b6310693-cruise-control will be ready
2022-03-28 10:56:44 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Wait for Deployment: my-cluster-b6310693-cruise-control will be ready
2022-03-28 10:56:44 [ForkJoinPool-1-worker-5] INFO  [DeploymentUtils:168] Deployment: my-cluster-b6310693-cruise-control is ready
2022-03-28 10:56:44 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (251634ms till timeout)
2022-03-28 10:56:44 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-b6310693, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-b6310693-cruise-control}, additionalProperties={})to be ready
2022-03-28 10:56:44 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b6310693-cruise-control-585bd649db-wtrrk not ready: cruise-control)
2022-03-28 10:56:44 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b6310693-cruise-control-585bd649db-wtrrk not ready: tls-sidecar)
2022-03-28 10:56:44 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:106] Pods my-cluster-b6310693-cruise-control-585bd649db-wtrrk are ready
2022-03-28 10:56:44 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-b6310693, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-b6310693-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599980ms till timeout)
2022-03-28 10:56:44 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:56:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (212310ms till timeout)
2022-03-28 10:56:44 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1235683ms till timeout)
2022-03-28 10:56:44 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 10:56:44 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-1 not ready: kafka)
2022-03-28 10:56:44 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-c0830cbe-kafka-2)
2022-03-28 10:56:44 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1782942ms till timeout)
2022-03-28 10:56:45 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:45 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:45 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:45 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:56:45 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 7
2022-03-28 10:56:45 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (253819ms till timeout)
2022-03-28 10:56:45 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (250588ms till timeout)
2022-03-28 10:56:45 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b6310693-cruise-control-585bd649db-wtrrk not ready: cruise-control)
2022-03-28 10:56:45 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b6310693-cruise-control-585bd649db-wtrrk not ready: tls-sidecar)
2022-03-28 10:56:45 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:106] Pods my-cluster-b6310693-cruise-control-585bd649db-wtrrk are ready
2022-03-28 10:56:45 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-b6310693, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-b6310693-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598940ms till timeout)
2022-03-28 10:56:45 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:56:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (211259ms till timeout)
2022-03-28 10:56:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1234671ms till timeout)
2022-03-28 10:56:45 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 10:56:45 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-1 not ready: kafka)
2022-03-28 10:56:45 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-c0830cbe-kafka-2)
2022-03-28 10:56:45 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1781906ms till timeout)
2022-03-28 10:56:46 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:46 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:46 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:46 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:56:46 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 6
2022-03-28 10:56:46 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (252756ms till timeout)
2022-03-28 10:56:46 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (249509ms till timeout)
2022-03-28 10:56:46 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b6310693-cruise-control-585bd649db-wtrrk not ready: cruise-control)
2022-03-28 10:56:46 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b6310693-cruise-control-585bd649db-wtrrk not ready: tls-sidecar)
2022-03-28 10:56:46 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:106] Pods my-cluster-b6310693-cruise-control-585bd649db-wtrrk are ready
2022-03-28 10:56:46 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-b6310693, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-b6310693-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597847ms till timeout)
2022-03-28 10:56:46 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:56:46 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1233585ms till timeout)
2022-03-28 10:56:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (210128ms till timeout)
2022-03-28 10:56:47 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 10:56:47 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-1 not ready: kafka)
2022-03-28 10:56:47 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-c0830cbe-kafka-2)
2022-03-28 10:56:47 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1780816ms till timeout)
2022-03-28 10:56:47 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:47 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:47 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:47 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:56:47 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 5
2022-03-28 10:56:47 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (251704ms till timeout)
2022-03-28 10:56:47 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (248439ms till timeout)
2022-03-28 10:56:47 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b6310693-cruise-control-585bd649db-wtrrk not ready: cruise-control)
2022-03-28 10:56:47 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b6310693-cruise-control-585bd649db-wtrrk not ready: tls-sidecar)
2022-03-28 10:56:47 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:106] Pods my-cluster-b6310693-cruise-control-585bd649db-wtrrk are ready
2022-03-28 10:56:47 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-b6310693, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-b6310693-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596793ms till timeout)
2022-03-28 10:56:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1232538ms till timeout)
2022-03-28 10:56:47 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:56:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (209026ms till timeout)
2022-03-28 10:56:48 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 10:56:48 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-1 not ready: kafka)
2022-03-28 10:56:48 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-c0830cbe-kafka-2)
2022-03-28 10:56:48 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1779704ms till timeout)
2022-03-28 10:56:48 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:48 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:48 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:48 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:56:48 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 4
2022-03-28 10:56:48 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (250639ms till timeout)
2022-03-28 10:56:48 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b6310693-cruise-control-585bd649db-wtrrk not ready: cruise-control)
2022-03-28 10:56:48 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b6310693-cruise-control-585bd649db-wtrrk not ready: tls-sidecar)
2022-03-28 10:56:48 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:106] Pods my-cluster-b6310693-cruise-control-585bd649db-wtrrk are ready
2022-03-28 10:56:48 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-b6310693, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-b6310693-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595753ms till timeout)
2022-03-28 10:56:48 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (247365ms till timeout)
2022-03-28 10:56:49 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1231503ms till timeout)
2022-03-28 10:56:49 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:56:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (207946ms till timeout)
2022-03-28 10:56:49 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testTlsUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:56:49 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:49 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 10:56:49 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-1 not ready: kafka)
2022-03-28 10:56:49 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-c0830cbe-kafka-2)
2022-03-28 10:56:49 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1778604ms till timeout)
2022-03-28 10:56:49 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:49 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:49 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:56:49 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 3
2022-03-28 10:56:49 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (249582ms till timeout)
2022-03-28 10:56:49 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:56:49 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:56:49 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (462066ms till timeout)
2022-03-28 10:56:49 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b6310693-cruise-control-585bd649db-wtrrk not ready: cruise-control)
2022-03-28 10:56:49 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b6310693-cruise-control-585bd649db-wtrrk not ready: tls-sidecar)
2022-03-28 10:56:49 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:106] Pods my-cluster-b6310693-cruise-control-585bd649db-wtrrk are ready
2022-03-28 10:56:49 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-b6310693, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-b6310693-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594719ms till timeout)
2022-03-28 10:56:49 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (246305ms till timeout)
2022-03-28 10:56:50 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1230480ms till timeout)
2022-03-28 10:56:50 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:56:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (206877ms till timeout)
2022-03-28 10:56:50 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:50 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 10:56:50 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-1 not ready: kafka)
2022-03-28 10:56:50 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-c0830cbe-kafka-2)
2022-03-28 10:56:50 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1777549ms till timeout)
2022-03-28 10:56:50 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:50 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:50 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:56:50 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 2
2022-03-28 10:56:50 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (248551ms till timeout)
2022-03-28 10:56:50 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:56:50 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b6310693-cruise-control-585bd649db-wtrrk not ready: cruise-control)
2022-03-28 10:56:50 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b6310693-cruise-control-585bd649db-wtrrk not ready: tls-sidecar)
2022-03-28 10:56:50 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:106] Pods my-cluster-b6310693-cruise-control-585bd649db-wtrrk are ready
2022-03-28 10:56:50 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-b6310693, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-b6310693-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593633ms till timeout)
2022-03-28 10:56:50 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (245183ms till timeout)
2022-03-28 10:56:51 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1229443ms till timeout)
2022-03-28 10:56:51 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:56:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (205844ms till timeout)
2022-03-28 10:56:51 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:51 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 10:56:51 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-1 not ready: kafka)
2022-03-28 10:56:51 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-c0830cbe-kafka-2)
2022-03-28 10:56:51 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1776502ms till timeout)
2022-03-28 10:56:51 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:51 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:51 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:56:51 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 1
2022-03-28 10:56:51 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (247487ms till timeout)
2022-03-28 10:56:51 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b6310693-cruise-control-585bd649db-wtrrk not ready: cruise-control)
2022-03-28 10:56:51 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b6310693-cruise-control-585bd649db-wtrrk not ready: tls-sidecar)
2022-03-28 10:56:51 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:106] Pods my-cluster-b6310693-cruise-control-585bd649db-wtrrk are ready
2022-03-28 10:56:51 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-b6310693, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-b6310693-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592593ms till timeout)
2022-03-28 10:56:51 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (244059ms till timeout)
2022-03-28 10:56:52 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1228384ms till timeout)
2022-03-28 10:56:52 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:56:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (204759ms till timeout)
2022-03-28 10:56:52 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:52 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 10:56:52 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-1 not ready: kafka)
2022-03-28 10:56:52 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-c0830cbe-kafka-2)
2022-03-28 10:56:52 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1775450ms till timeout)
2022-03-28 10:56:52 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:52 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104}
2022-03-28 10:56:52 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-f50d425d-kafka-2 hasn't rolled
2022-03-28 10:56:52 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-f50d425d-kafka-2=725fb552-f9ae-480a-a6c8-fac53ca84104, my-cluster-f50d425d-kafka-1=6afc64a3-e6ee-4d7d-aceb-bbc0c7039a64, my-cluster-f50d425d-kafka-0=bcdde426-faeb-4d54-940a-71b1230b9539} pods didn't roll. Remaining seconds for stability: 0
2022-03-28 10:56:52 [ForkJoinPool-1-worker-3] INFO  [CruiseControlConfigurationST:174] Verifying new configuration in the Kafka CR
2022-03-28 10:56:52 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Verify that kafka contains cruise control topics with related configuration.
2022-03-28 10:56:52 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Verify that kafka contains cruise control topics with related configuration.
2022-03-28 10:56:52 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b6310693-cruise-control-585bd649db-wtrrk not ready: cruise-control)
2022-03-28 10:56:52 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b6310693-cruise-control-585bd649db-wtrrk not ready: tls-sidecar)
2022-03-28 10:56:52 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:106] Pods my-cluster-b6310693-cruise-control-585bd649db-wtrrk are ready
2022-03-28 10:56:52 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-b6310693, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-b6310693-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591539ms till timeout)
2022-03-28 10:56:52 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:674] ============================================================================
2022-03-28 10:56:52 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:675] [cruisecontrol.CruiseControlConfigurationST - After Each] - Clean up after test
2022-03-28 10:56:52 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:344] ############################################################################
2022-03-28 10:56:52 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:348] Delete all resources for testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods
2022-03-28 10:56:52 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:241] Delete of Kafka my-cluster-f50d425d in namespace namespace-4
2022-03-28 10:56:52 [ForkJoinPool-1-worker-3] INFO  [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-4, for cruise control Kafka cluster my-cluster-f50d425d
2022-03-28 10:56:52 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (242992ms till timeout)
2022-03-28 10:56:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1227221ms till timeout)
2022-03-28 10:56:53 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:56:53 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 10:56:53 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-1 not ready: kafka)
2022-03-28 10:56:53 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-2 not ready: kafka)
2022-03-28 10:56:53 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:106] Pods my-cluster-c0830cbe-kafka-0, my-cluster-c0830cbe-kafka-1, my-cluster-c0830cbe-kafka-2 are ready
2022-03-28 10:56:53 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1774305ms till timeout)
2022-03-28 10:56:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (203483ms till timeout)
2022-03-28 10:56:53 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-f50d425d
2022-03-28 10:56:53 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-f50d425d not ready, will try again in 10000 ms (839889ms till timeout)
2022-03-28 10:56:53 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b6310693-cruise-control-585bd649db-wtrrk not ready: cruise-control)
2022-03-28 10:56:53 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b6310693-cruise-control-585bd649db-wtrrk not ready: tls-sidecar)
2022-03-28 10:56:53 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:106] Pods my-cluster-b6310693-cruise-control-585bd649db-wtrrk are ready
2022-03-28 10:56:53 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-b6310693, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-b6310693-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590441ms till timeout)
2022-03-28 10:56:54 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (241871ms till timeout)
2022-03-28 10:56:54 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testTlsUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:56:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1226137ms till timeout)
2022-03-28 10:56:54 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:56:54 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 10:56:54 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-1 not ready: kafka)
2022-03-28 10:56:54 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-2 not ready: kafka)
2022-03-28 10:56:54 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:106] Pods my-cluster-c0830cbe-kafka-0, my-cluster-c0830cbe-kafka-1, my-cluster-c0830cbe-kafka-2 are ready
2022-03-28 10:56:54 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1773178ms till timeout)
2022-03-28 10:56:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (202335ms till timeout)
2022-03-28 10:56:55 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b6310693-cruise-control-585bd649db-wtrrk not ready: cruise-control)
2022-03-28 10:56:55 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-b6310693-cruise-control-585bd649db-wtrrk not ready: tls-sidecar)
2022-03-28 10:56:55 [ForkJoinPool-1-worker-5] DEBUG [PodUtils:106] Pods my-cluster-b6310693-cruise-control-585bd649db-wtrrk are ready
2022-03-28 10:56:55 [ForkJoinPool-1-worker-5] INFO  [DeploymentUtils:141] Deployment my-cluster-b6310693-cruise-control rolling update finished
2022-03-28 10:56:55 [ForkJoinPool-1-worker-5] INFO  [CruiseControlConfigurationST:280] Verifying that Kafka pods did not roll
2022-03-28 10:56:55 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Waiting for stability of rolling update will be not triggered
2022-03-28 10:56:55 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:56:55 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (240671ms till timeout)
2022-03-28 10:56:55 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:56:55 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:56:55 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:56:55 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 50
2022-03-28 10:56:55 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (299868ms till timeout)
2022-03-28 10:56:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1225098ms till timeout)
2022-03-28 10:56:55 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 10:56:55 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-1 not ready: kafka)
2022-03-28 10:56:55 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-2 not ready: kafka)
2022-03-28 10:56:55 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:106] Pods my-cluster-c0830cbe-kafka-0, my-cluster-c0830cbe-kafka-1, my-cluster-c0830cbe-kafka-2 are ready
2022-03-28 10:56:55 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1772088ms till timeout)
2022-03-28 10:56:55 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:56:55 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:56:55 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:56:55 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (455517ms till timeout)
2022-03-28 10:56:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (201135ms till timeout)
2022-03-28 10:56:56 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (239573ms till timeout)
2022-03-28 10:56:56 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:56:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1224014ms till timeout)
2022-03-28 10:56:56 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:56:56 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:56:56 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:56:56 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 49
2022-03-28 10:56:56 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (298749ms till timeout)
2022-03-28 10:56:56 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 10:56:56 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-1 not ready: kafka)
2022-03-28 10:56:56 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-2 not ready: kafka)
2022-03-28 10:56:56 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:106] Pods my-cluster-c0830cbe-kafka-0, my-cluster-c0830cbe-kafka-1, my-cluster-c0830cbe-kafka-2 are ready
2022-03-28 10:56:56 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1771011ms till timeout)
2022-03-28 10:56:56 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:56:57 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:56:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (199959ms till timeout)
2022-03-28 10:56:57 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:56:57 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:56:57 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (454143ms till timeout)
2022-03-28 10:56:57 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (238511ms till timeout)
2022-03-28 10:56:57 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:56:57 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1222950ms till timeout)
2022-03-28 10:56:57 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:56:57 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:56:57 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:56:57 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 48
2022-03-28 10:56:57 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (297686ms till timeout)
2022-03-28 10:56:57 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 10:56:57 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-1 not ready: kafka)
2022-03-28 10:56:57 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-2 not ready: kafka)
2022-03-28 10:56:57 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:106] Pods my-cluster-c0830cbe-kafka-0, my-cluster-c0830cbe-kafka-1, my-cluster-c0830cbe-kafka-2 are ready
2022-03-28 10:56:57 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1769913ms till timeout)
2022-03-28 10:56:58 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:56:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (198879ms till timeout)
2022-03-28 10:56:58 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:56:58 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:56:58 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:56:58 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:56:58 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (452813ms till timeout)
2022-03-28 10:56:58 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1221922ms till timeout)
2022-03-28 10:56:58 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (237310ms till timeout)
2022-03-28 10:56:58 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:56:58 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:56:58 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:56:58 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 47
2022-03-28 10:56:58 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (296615ms till timeout)
2022-03-28 10:56:59 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 10:56:59 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-1 not ready: kafka)
2022-03-28 10:56:59 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-2 not ready: kafka)
2022-03-28 10:56:59 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:106] Pods my-cluster-c0830cbe-kafka-0, my-cluster-c0830cbe-kafka-1, my-cluster-c0830cbe-kafka-2 are ready
2022-03-28 10:56:59 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1768820ms till timeout)
2022-03-28 10:56:59 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testTlsUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:56:59 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:56:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (197777ms till timeout)
2022-03-28 10:56:59 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:56:59 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:56:59 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1220863ms till timeout)
2022-03-28 10:56:59 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:56:59 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:56:59 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:56:59 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 46
2022-03-28 10:56:59 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (295523ms till timeout)
2022-03-28 10:56:59 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (236169ms till timeout)
2022-03-28 10:56:59 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:56:59 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:56:59 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (451522ms till timeout)
2022-03-28 10:57:00 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 10:57:00 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-1 not ready: kafka)
2022-03-28 10:57:00 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-2 not ready: kafka)
2022-03-28 10:57:00 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:106] Pods my-cluster-c0830cbe-kafka-0, my-cluster-c0830cbe-kafka-1, my-cluster-c0830cbe-kafka-2 are ready
2022-03-28 10:57:00 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1767761ms till timeout)
2022-03-28 10:57:00 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (196683ms till timeout)
2022-03-28 10:57:00 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1219837ms till timeout)
2022-03-28 10:57:00 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:00 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:00 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:00 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:57:00 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 45
2022-03-28 10:57:00 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (294484ms till timeout)
2022-03-28 10:57:00 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (235128ms till timeout)
2022-03-28 10:57:00 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:57:01 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:57:01 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:01 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (450303ms till timeout)
2022-03-28 10:57:01 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 10:57:01 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-1 not ready: kafka)
2022-03-28 10:57:01 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-2 not ready: kafka)
2022-03-28 10:57:01 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:106] Pods my-cluster-c0830cbe-kafka-0, my-cluster-c0830cbe-kafka-1, my-cluster-c0830cbe-kafka-2 are ready
2022-03-28 10:57:01 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1766732ms till timeout)
2022-03-28 10:57:01 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (195576ms till timeout)
2022-03-28 10:57:01 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1218822ms till timeout)
2022-03-28 10:57:01 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:01 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:01 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:01 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:57:01 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 44
2022-03-28 10:57:01 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (293433ms till timeout)
2022-03-28 10:57:01 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (234045ms till timeout)
2022-03-28 10:57:02 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:57:02 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 10:57:02 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-1 not ready: kafka)
2022-03-28 10:57:02 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-2 not ready: kafka)
2022-03-28 10:57:02 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:106] Pods my-cluster-c0830cbe-kafka-0, my-cluster-c0830cbe-kafka-1, my-cluster-c0830cbe-kafka-2 are ready
2022-03-28 10:57:02 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1765573ms till timeout)
2022-03-28 10:57:02 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:57:02 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:02 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (448958ms till timeout)
2022-03-28 10:57:02 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (194509ms till timeout)
2022-03-28 10:57:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1217757ms till timeout)
2022-03-28 10:57:02 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:02 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:02 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:02 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:57:02 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 43
2022-03-28 10:57:02 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (292306ms till timeout)
2022-03-28 10:57:03 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (232937ms till timeout)
2022-03-28 10:57:03 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 10:57:03 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-1 not ready: kafka)
2022-03-28 10:57:03 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-2 not ready: kafka)
2022-03-28 10:57:03 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:106] Pods my-cluster-c0830cbe-kafka-0, my-cluster-c0830cbe-kafka-1, my-cluster-c0830cbe-kafka-2 are ready
2022-03-28 10:57:03 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1764533ms till timeout)
2022-03-28 10:57:03 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:57:03 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:03 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:57:03 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:03 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (447656ms till timeout)
2022-03-28 10:57:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (193310ms till timeout)
2022-03-28 10:57:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1216731ms till timeout)
2022-03-28 10:57:03 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:04 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testTlsUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:57:04 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:369] ############################################################################
2022-03-28 10:57:04 [ForkJoinPool-1-worker-3] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-4 for test case:testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods
2022-03-28 10:57:04 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:04 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:04 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:57:04 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 42
2022-03-28 10:57:04 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (291059ms till timeout)
2022-03-28 10:57:04 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (231610ms till timeout)
2022-03-28 10:57:04 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Namespace namespace-4 removal
2022-03-28 10:57:04 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:04 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 10:57:04 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-1 not ready: kafka)
2022-03-28 10:57:04 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-2 not ready: kafka)
2022-03-28 10:57:04 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:106] Pods my-cluster-c0830cbe-kafka-0, my-cluster-c0830cbe-kafka-1, my-cluster-c0830cbe-kafka-2 are ready
2022-03-28 10:57:04 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-c0830cbe will have desired state: Ready
2022-03-28 10:57:04 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-c0830cbe will have desired state: Ready
2022-03-28 10:57:04 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:444] Kafka: my-cluster-c0830cbe is in desired state: Ready
2022-03-28 10:57:04 [ForkJoinPool-1-worker-1] INFO  [RollingUpdateUtils:132] Kafka: my-cluster-c0830cbe is ready
2022-03-28 10:57:04 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:04 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:04 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (479763ms till timeout)
2022-03-28 10:57:04 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:57:04 [ForkJoinPool-1-worker-1] INFO  [CruiseControlConfigurationST:117] Verifying that in Cruise Control is not present in the Kafka cluster
2022-03-28 10:57:04 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:04 [ForkJoinPool-1-worker-1] INFO  [CruiseControlConfigurationST:120] Verifying that my-cluster-c0830cbe-cruise-control- pod is not present
2022-03-28 10:57:04 [ForkJoinPool-1-worker-1] INFO  [PodUtils:209] Wait until Pod my-cluster-c0830cbe-cruise-control- will have stable 0 replicas
2022-03-28 10:57:04 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for  Podmy-cluster-c0830cbe-cruise-control- will have 0 replicas
2022-03-28 10:57:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (192218ms till timeout)
2022-03-28 10:57:04 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1215674ms till timeout)
2022-03-28 10:57:04 [ForkJoinPool-1-worker-1] INFO  [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-03-28 10:57:04 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176]  Podmy-cluster-c0830cbe-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (179835ms till timeout)
2022-03-28 10:57:05 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:57:05 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:05 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (446269ms till timeout)
2022-03-28 10:57:05 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:05 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:05 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:05 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:57:05 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 41
2022-03-28 10:57:05 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (289912ms till timeout)
2022-03-28 10:57:05 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (230479ms till timeout)
2022-03-28 10:57:05 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:05 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1214599ms till timeout)
2022-03-28 10:57:05 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (191087ms till timeout)
2022-03-28 10:57:06 [ForkJoinPool-1-worker-1] INFO  [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-03-28 10:57:06 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176]  Podmy-cluster-c0830cbe-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (178763ms till timeout)
2022-03-28 10:57:06 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:06 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:06 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (478392ms till timeout)
2022-03-28 10:57:06 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:57:06 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:06 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:06 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:06 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:57:06 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 40
2022-03-28 10:57:06 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (288797ms till timeout)
2022-03-28 10:57:06 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:57:06 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:06 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (444942ms till timeout)
2022-03-28 10:57:06 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (229379ms till timeout)
2022-03-28 10:57:06 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1213553ms till timeout)
2022-03-28 10:57:07 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:07 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (189909ms till timeout)
2022-03-28 10:57:07 [ForkJoinPool-1-worker-1] INFO  [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-03-28 10:57:07 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176]  Podmy-cluster-c0830cbe-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (177637ms till timeout)
2022-03-28 10:57:07 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:07 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:07 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (477156ms till timeout)
2022-03-28 10:57:07 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:07 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:57:07 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:07 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:07 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:57:07 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 39
2022-03-28 10:57:07 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (287748ms till timeout)
2022-03-28 10:57:07 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (228311ms till timeout)
2022-03-28 10:57:07 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:57:07 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:07 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (443672ms till timeout)
2022-03-28 10:57:08 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1212505ms till timeout)
2022-03-28 10:57:08 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:08 [ForkJoinPool-1-worker-1] INFO  [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-03-28 10:57:08 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176]  Podmy-cluster-c0830cbe-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (176527ms till timeout)
2022-03-28 10:57:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (188756ms till timeout)
2022-03-28 10:57:08 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:08 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:08 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:08 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:08 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:57:08 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 38
2022-03-28 10:57:08 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (286707ms till timeout)
2022-03-28 10:57:08 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:08 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:08 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (475903ms till timeout)
2022-03-28 10:57:08 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (227236ms till timeout)
2022-03-28 10:57:08 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:57:08 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:57:08 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:08 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (442403ms till timeout)
2022-03-28 10:57:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1211426ms till timeout)
2022-03-28 10:57:09 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testTlsUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:57:09 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:09 [ForkJoinPool-1-worker-1] INFO  [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-03-28 10:57:09 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176]  Podmy-cluster-c0830cbe-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (175454ms till timeout)
2022-03-28 10:57:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (187679ms till timeout)
2022-03-28 10:57:09 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:09 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:09 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:09 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:57:09 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 37
2022-03-28 10:57:09 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (285687ms till timeout)
2022-03-28 10:57:09 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:09 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:09 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:09 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (474679ms till timeout)
2022-03-28 10:57:09 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (226143ms till timeout)
2022-03-28 10:57:09 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:57:10 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1210388ms till timeout)
2022-03-28 10:57:10 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:57:10 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:10 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (441226ms till timeout)
2022-03-28 10:57:10 [ForkJoinPool-1-worker-1] INFO  [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-03-28 10:57:10 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176]  Podmy-cluster-c0830cbe-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (174413ms till timeout)
2022-03-28 10:57:10 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (186615ms till timeout)
2022-03-28 10:57:10 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:10 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:10 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:10 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:57:10 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 36
2022-03-28 10:57:10 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (284626ms till timeout)
2022-03-28 10:57:10 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:10 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (225070ms till timeout)
2022-03-28 10:57:11 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:11 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:11 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (473456ms till timeout)
2022-03-28 10:57:11 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1209346ms till timeout)
2022-03-28 10:57:11 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:57:11 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:57:11 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:11 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (440012ms till timeout)
2022-03-28 10:57:11 [ForkJoinPool-1-worker-1] INFO  [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-03-28 10:57:11 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176]  Podmy-cluster-c0830cbe-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (173381ms till timeout)
2022-03-28 10:57:11 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (185527ms till timeout)
2022-03-28 10:57:11 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:11 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:11 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:11 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:57:11 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 35
2022-03-28 10:57:11 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (283560ms till timeout)
2022-03-28 10:57:11 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (224020ms till timeout)
2022-03-28 10:57:12 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1208327ms till timeout)
2022-03-28 10:57:12 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:12 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:12 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (472259ms till timeout)
2022-03-28 10:57:12 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:57:12 [ForkJoinPool-1-worker-1] INFO  [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-03-28 10:57:12 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176]  Podmy-cluster-c0830cbe-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (172289ms till timeout)
2022-03-28 10:57:12 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:12 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:57:12 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:12 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (438765ms till timeout)
2022-03-28 10:57:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (184403ms till timeout)
2022-03-28 10:57:12 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:12 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:12 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:12 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:57:12 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 34
2022-03-28 10:57:12 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (282506ms till timeout)
2022-03-28 10:57:13 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (222941ms till timeout)
2022-03-28 10:57:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1207301ms till timeout)
2022-03-28 10:57:13 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:13 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:13 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:13 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (471026ms till timeout)
2022-03-28 10:57:13 [ForkJoinPool-1-worker-1] INFO  [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-03-28 10:57:13 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176]  Podmy-cluster-c0830cbe-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (171198ms till timeout)
2022-03-28 10:57:13 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:57:13 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (183334ms till timeout)
2022-03-28 10:57:13 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:13 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:13 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:13 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:57:13 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 33
2022-03-28 10:57:13 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (281469ms till timeout)
2022-03-28 10:57:13 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:57:13 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:13 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (437480ms till timeout)
2022-03-28 10:57:14 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (221837ms till timeout)
2022-03-28 10:57:14 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testTlsUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:57:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1206131ms till timeout)
2022-03-28 10:57:14 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:14 [ForkJoinPool-1-worker-1] INFO  [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-03-28 10:57:14 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176]  Podmy-cluster-c0830cbe-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (170142ms till timeout)
2022-03-28 10:57:14 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:14 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:14 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (469801ms till timeout)
2022-03-28 10:57:14 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:14 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (182209ms till timeout)
2022-03-28 10:57:14 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:14 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:14 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:57:14 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 32
2022-03-28 10:57:14 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (280393ms till timeout)
2022-03-28 10:57:14 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:57:15 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:57:15 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:15 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (436273ms till timeout)
2022-03-28 10:57:15 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (220782ms till timeout)
2022-03-28 10:57:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1205071ms till timeout)
2022-03-28 10:57:15 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:15 [ForkJoinPool-1-worker-1] INFO  [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-03-28 10:57:15 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176]  Podmy-cluster-c0830cbe-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (169082ms till timeout)
2022-03-28 10:57:15 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:15 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:16 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:16 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:16 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:57:16 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 31
2022-03-28 10:57:16 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (279253ms till timeout)
2022-03-28 10:57:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (181019ms till timeout)
2022-03-28 10:57:16 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:57:16 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:16 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:16 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (468342ms till timeout)
2022-03-28 10:57:16 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (219676ms till timeout)
2022-03-28 10:57:16 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:57:16 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:16 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (434990ms till timeout)
2022-03-28 10:57:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1204005ms till timeout)
2022-03-28 10:57:16 [ForkJoinPool-1-worker-1] INFO  [PodUtils:221] Pod replicas are not stable. Going to set the counter to zero.
2022-03-28 10:57:16 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176]  Podmy-cluster-c0830cbe-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (167967ms till timeout)
2022-03-28 10:57:17 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:17 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:17 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:17 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:57:17 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 30
2022-03-28 10:57:17 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (278161ms till timeout)
2022-03-28 10:57:17 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:17 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (179858ms till timeout)
2022-03-28 10:57:17 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (218562ms till timeout)
2022-03-28 10:57:17 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:17 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:17 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (467070ms till timeout)
2022-03-28 10:57:17 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:57:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1202932ms till timeout)
2022-03-28 10:57:17 [ForkJoinPool-1-worker-1] INFO  [PodUtils:225] Pod replicas gonna be stable in 19 polls
2022-03-28 10:57:17 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176]  Podmy-cluster-c0830cbe-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (166881ms till timeout)
2022-03-28 10:57:17 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:57:17 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:17 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (433459ms till timeout)
2022-03-28 10:57:18 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:18 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:18 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:18 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:57:18 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 29
2022-03-28 10:57:18 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (277091ms till timeout)
2022-03-28 10:57:18 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (178730ms till timeout)
2022-03-28 10:57:18 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:18 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (217455ms till timeout)
2022-03-28 10:57:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1201912ms till timeout)
2022-03-28 10:57:18 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:18 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:18 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (465848ms till timeout)
2022-03-28 10:57:18 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:57:19 [ForkJoinPool-1-worker-1] INFO  [PodUtils:225] Pod replicas gonna be stable in 18 polls
2022-03-28 10:57:19 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176]  Podmy-cluster-c0830cbe-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (165766ms till timeout)
2022-03-28 10:57:19 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:19 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testTlsUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:57:19 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:57:19 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:19 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (432153ms till timeout)
2022-03-28 10:57:19 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:19 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:19 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:57:19 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 28
2022-03-28 10:57:19 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (275932ms till timeout)
2022-03-28 10:57:19 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (177562ms till timeout)
2022-03-28 10:57:19 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:19 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1200867ms till timeout)
2022-03-28 10:57:19 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (216298ms till timeout)
2022-03-28 10:57:19 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:19 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:19 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (464521ms till timeout)
2022-03-28 10:57:20 [ForkJoinPool-1-worker-1] INFO  [PodUtils:225] Pod replicas gonna be stable in 17 polls
2022-03-28 10:57:20 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176]  Podmy-cluster-c0830cbe-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (164653ms till timeout)
2022-03-28 10:57:20 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:57:20 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:20 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:20 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:20 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:57:20 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 27
2022-03-28 10:57:20 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (274885ms till timeout)
2022-03-28 10:57:20 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:57:20 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:20 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (430939ms till timeout)
2022-03-28 10:57:20 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (176488ms till timeout)
2022-03-28 10:57:20 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1199778ms till timeout)
2022-03-28 10:57:20 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (215182ms till timeout)
2022-03-28 10:57:20 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:21 [ForkJoinPool-1-worker-1] INFO  [PodUtils:225] Pod replicas gonna be stable in 16 polls
2022-03-28 10:57:21 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176]  Podmy-cluster-c0830cbe-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (163560ms till timeout)
2022-03-28 10:57:21 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:21 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:21 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (463238ms till timeout)
2022-03-28 10:57:21 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:21 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:21 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:21 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:57:21 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 26
2022-03-28 10:57:21 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (273835ms till timeout)
2022-03-28 10:57:21 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:57:21 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (175358ms till timeout)
2022-03-28 10:57:21 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:57:21 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (429688ms till timeout)
2022-03-28 10:57:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1198725ms till timeout)
2022-03-28 10:57:21 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (214111ms till timeout)
2022-03-28 10:57:22 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:22 [ForkJoinPool-1-worker-1] INFO  [PodUtils:225] Pod replicas gonna be stable in 15 polls
2022-03-28 10:57:22 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176]  Podmy-cluster-c0830cbe-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (162518ms till timeout)
2022-03-28 10:57:22 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:22 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:22 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:22 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:57:22 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 25
2022-03-28 10:57:22 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (272782ms till timeout)
2022-03-28 10:57:22 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:22 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:22 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (462001ms till timeout)
2022-03-28 10:57:22 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:57:22 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (174239ms till timeout)
2022-03-28 10:57:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1197680ms till timeout)
2022-03-28 10:57:22 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (213071ms till timeout)
2022-03-28 10:57:22 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:57:22 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:22 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (428479ms till timeout)
2022-03-28 10:57:23 [ForkJoinPool-1-worker-1] INFO  [PodUtils:225] Pod replicas gonna be stable in 14 polls
2022-03-28 10:57:23 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176]  Podmy-cluster-c0830cbe-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (161451ms till timeout)
2022-03-28 10:57:23 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:23 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:23 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:23 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:23 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:57:23 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 24
2022-03-28 10:57:23 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (271704ms till timeout)
2022-03-28 10:57:23 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:23 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:23 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (460742ms till timeout)
2022-03-28 10:57:23 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:23 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:57:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1196579ms till timeout)
2022-03-28 10:57:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (173122ms till timeout)
2022-03-28 10:57:23 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (211960ms till timeout)
2022-03-28 10:57:24 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:57:24 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:24 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (427258ms till timeout)
2022-03-28 10:57:24 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testTlsUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:57:24 [ForkJoinPool-1-worker-1] INFO  [PodUtils:225] Pod replicas gonna be stable in 13 polls
2022-03-28 10:57:24 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176]  Podmy-cluster-c0830cbe-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (160356ms till timeout)
2022-03-28 10:57:24 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:24 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:24 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:24 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:57:24 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 23
2022-03-28 10:57:24 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (270658ms till timeout)
2022-03-28 10:57:24 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:24 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:24 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:24 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (459514ms till timeout)
2022-03-28 10:57:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1195534ms till timeout)
2022-03-28 10:57:25 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:25 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (210840ms till timeout)
2022-03-28 10:57:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (171952ms till timeout)
2022-03-28 10:57:25 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:57:25 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:57:25 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:25 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (426020ms till timeout)
2022-03-28 10:57:25 [ForkJoinPool-1-worker-1] INFO  [PodUtils:225] Pod replicas gonna be stable in 12 polls
2022-03-28 10:57:25 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176]  Podmy-cluster-c0830cbe-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (159308ms till timeout)
2022-03-28 10:57:25 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:25 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:25 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:25 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:57:25 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 22
2022-03-28 10:57:25 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (269625ms till timeout)
2022-03-28 10:57:25 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1194506ms till timeout)
2022-03-28 10:57:26 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:26 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:26 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (458328ms till timeout)
2022-03-28 10:57:26 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (209770ms till timeout)
2022-03-28 10:57:26 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (170838ms till timeout)
2022-03-28 10:57:26 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:57:26 [ForkJoinPool-1-worker-1] INFO  [PodUtils:225] Pod replicas gonna be stable in 11 polls
2022-03-28 10:57:26 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176]  Podmy-cluster-c0830cbe-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (158206ms till timeout)
2022-03-28 10:57:26 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:26 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-7 get Namespace namespace-6 -o yaml
2022-03-28 10:57:26 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 1
2022-03-28 10:57:26 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 10:57:26 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-6" not found
2022-03-28 10:57:26 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] ======STDERR END======
2022-03-28 10:57:26 [ForkJoinPool-1-worker-15] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[namespace-7], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@f851b6c3=[namespace-5], io.strimzi.test.logs.CollectorElement@3881d5f2=[namespace-4], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[infra-namespace, reconciliation-st], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-3], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 10:57:26 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:267] testPauseReconciliationInKafkaRebalanceAndTopic - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTls, testSendSimpleMessageTlsScramSha, testConfigurationFileIsCreated, testReceiveSimpleMessageTls, testDeployAndUnDeployCruiseControl, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testConfigurationPerformanceOptions, testTlsUserWithQuotas] to and randomly select one to start execution
2022-03-28 10:57:26 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:93] [operators.ReconciliationST] - Removing parallel test: testPauseReconciliationInKafkaRebalanceAndTopic
2022-03-28 10:57:26 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:97] [operators.ReconciliationST] - Parallel test count: 7
2022-03-28 10:57:26 [ForkJoinPool-1-worker-15] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.ReconciliationST.testPauseReconciliationInKafkaRebalanceAndTopic-FINISHED
2022-03-28 10:57:26 [ForkJoinPool-1-worker-15] INFO  [TestSeparator:30] ############################################################################
2022-03-28 10:57:26 [ForkJoinPool-1-worker-15] INFO  [TestSeparator:23] ############################################################################
2022-03-28 10:57:26 [ForkJoinPool-1-worker-15] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.ReconciliationST.testPauseReconciliationInKafkaAndKafkaConnectWithConnector-STARTED
2022-03-28 10:57:26 [ForkJoinPool-1-worker-15] DEBUG [AbstractST:658] ============================================================================
2022-03-28 10:57:26 [ForkJoinPool-1-worker-15] DEBUG [AbstractST:659] [operators.ReconciliationST - Before Each] - Setup test case environment
2022-03-28 10:57:26 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:77] [operators.ReconciliationST] - Adding parallel test: testPauseReconciliationInKafkaAndKafkaConnectWithConnector
2022-03-28 10:57:26 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:81] [operators.ReconciliationST] - Parallel test count: 8
2022-03-28 10:57:26 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:205] [testPauseReconciliationInKafkaAndKafkaConnectWithConnector] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:57:26 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:210] testPauseReconciliationInKafkaAndKafkaConnectWithConnector is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:57:26 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:26 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:26 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:57:26 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 21
2022-03-28 10:57:26 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (268533ms till timeout)
2022-03-28 10:57:27 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1193406ms till timeout)
2022-03-28 10:57:27 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:27 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:27 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (208664ms till timeout)
2022-03-28 10:57:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (169725ms till timeout)
2022-03-28 10:57:27 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:27 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:27 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (456999ms till timeout)
2022-03-28 10:57:27 [ForkJoinPool-1-worker-1] INFO  [PodUtils:225] Pod replicas gonna be stable in 10 polls
2022-03-28 10:57:27 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176]  Podmy-cluster-c0830cbe-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (157155ms till timeout)
2022-03-28 10:57:27 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:27 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:27 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:27 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:57:27 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 20
2022-03-28 10:57:27 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (267500ms till timeout)
2022-03-28 10:57:28 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1192298ms till timeout)
2022-03-28 10:57:28 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (207610ms till timeout)
2022-03-28 10:57:28 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (168651ms till timeout)
2022-03-28 10:57:28 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:28 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:28 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:28 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (455766ms till timeout)
2022-03-28 10:57:28 [ForkJoinPool-1-worker-1] INFO  [PodUtils:225] Pod replicas gonna be stable in 9 polls
2022-03-28 10:57:28 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176]  Podmy-cluster-c0830cbe-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (156068ms till timeout)
2022-03-28 10:57:28 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:28 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:28 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:28 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:57:28 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 19
2022-03-28 10:57:28 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (266464ms till timeout)
2022-03-28 10:57:29 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:230] testTlsUserWithQuotas test now can proceed its execution
2022-03-28 10:57:29 [ForkJoinPool-1-worker-13] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 10:57:29 [ForkJoinPool-1-worker-13] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testScramUserWithQuotas=my-cluster-4941482a, testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383, testConfigurationFileIsCreated=my-cluster-e0fac774, testThrottlingQuotasCreateAlterPartitions=my-cluster-28d9dd2e, testSendSimpleMessageTls=my-cluster-27ddd6ba, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c, testThrottlingQuotasDeleteTopic=my-cluster-8f262de5, testConfigurationReflection=my-cluster-d1e2168e, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-da91577a, testDeployAndUnDeployCruiseControl=my-cluster-c0830cbe, testTlsUserWithQuotas=my-cluster-d49e42e8, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8, testCapacityFile=my-cluster-fd0fb61a, testReceiveSimpleMessageTls=my-cluster-c6a15963, testConfigurationPerformanceOptions=my-cluster-b6310693, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-f50d425d, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5}
2022-03-28 10:57:29 [ForkJoinPool-1-worker-13] TRACE [AbstractST:607] USERS_NAME_MAP: {testScramUserWithQuotas=my-user-797280497-2138800976, testReceiveSimpleMessageTlsScramSha=my-user-1915916223-1265293973, testConfigurationFileIsCreated=my-user-180611180-50530491, testThrottlingQuotasCreateAlterPartitions=my-user-1933547616-1024431140, testSendSimpleMessageTls=my-user-738229993-1095121647, testCreatingUsersWithSecretPrefix=my-user-1450786684-1808961062, testThrottlingQuotasDeleteTopic=my-user-1558977140-777236615, testConfigurationReflection=my-user-1445653023-93200027, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-1260379615-1050949907, testDeployAndUnDeployCruiseControl=my-user-1988711145-439861161, testTlsUserWithQuotas=my-user-811912745-1441809840, testThrottlingQuotasCreateTopic=my-user-1131806865-947217230, testCapacityFile=my-user-937015144-1959439396, testReceiveSimpleMessageTls=my-user-6088058-1142028780, testConfigurationPerformanceOptions=my-user-930975101-640523779, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-745974575-1852384454, testSendSimpleMessageTlsScramSha=my-user-1499386234-1249154328}
2022-03-28 10:57:29 [ForkJoinPool-1-worker-13] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testScramUserWithQuotas=my-topic-2004840350-265526537, testReceiveSimpleMessageTlsScramSha=my-topic-1834944466-2094449929, testConfigurationFileIsCreated=my-topic-1296407588-313880708, testThrottlingQuotasCreateAlterPartitions=my-topic-609849288-1810034325, testSendSimpleMessageTls=my-topic-563023482-1697359353, testCreatingUsersWithSecretPrefix=my-topic-1479520843-618891199, testThrottlingQuotasDeleteTopic=my-topic-2128658681-1688111273, testConfigurationReflection=my-topic-640815392-1092406112, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-1674392683-1637010870, testDeployAndUnDeployCruiseControl=my-topic-1511079187-95055629, testTlsUserWithQuotas=my-topic-418284173-1689090259, testThrottlingQuotasCreateTopic=my-topic-728672490-1194703402, testCapacityFile=my-topic-1155641705-902584520, testReceiveSimpleMessageTls=my-topic-478514536-2132782608, testConfigurationPerformanceOptions=my-topic-213912446-1165431808, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-254848724-189700675, testSendSimpleMessageTlsScramSha=my-topic-1028951580-1656411053}
2022-03-28 10:57:29 [ForkJoinPool-1-worker-13] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testScramUserWithQuotas=my-cluster-4941482a-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383-kafka-clients, testConfigurationFileIsCreated=my-cluster-e0fac774-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-28d9dd2e-kafka-clients, testSendSimpleMessageTls=my-cluster-27ddd6ba-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-8f262de5-kafka-clients, testConfigurationReflection=my-cluster-d1e2168e-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-da91577a-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-c0830cbe-kafka-clients, testTlsUserWithQuotas=my-cluster-d49e42e8-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8-kafka-clients, testCapacityFile=my-cluster-fd0fb61a-kafka-clients, testReceiveSimpleMessageTls=my-cluster-c6a15963-kafka-clients, testConfigurationPerformanceOptions=my-cluster-b6310693-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-f50d425d-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5-kafka-clients}
2022-03-28 10:57:29 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update KafkaUser encrypted-arnost in namespace user-st
2022-03-28 10:57:29 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:encrypted-arnost
2022-03-28 10:57:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1191223ms till timeout)
2022-03-28 10:57:29 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:433] Wait for KafkaUser: encrypted-arnost will have desired state: Ready
2022-03-28 10:57:29 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaUser: encrypted-arnost will have desired state: Ready
2022-03-28 10:57:29 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] KafkaUser: encrypted-arnost will have desired state: Ready not ready, will try again in 1000 ms (179931ms till timeout)
2022-03-28 10:57:29 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (206504ms till timeout)
2022-03-28 10:57:29 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (167559ms till timeout)
2022-03-28 10:57:29 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:29 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:29 [ForkJoinPool-1-worker-1] INFO  [PodUtils:225] Pod replicas gonna be stable in 8 polls
2022-03-28 10:57:29 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176]  Podmy-cluster-c0830cbe-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (154944ms till timeout)
2022-03-28 10:57:29 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:29 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:29 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:57:29 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 18
2022-03-28 10:57:29 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (265402ms till timeout)
2022-03-28 10:57:29 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:29 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:29 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (454507ms till timeout)
2022-03-28 10:57:30 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-d1e2168e will have desired state: Ready not ready, will try again in 1000 ms (1190091ms till timeout)
2022-03-28 10:57:30 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:444] KafkaUser: encrypted-arnost is in desired state: Ready
2022-03-28 10:57:30 [ForkJoinPool-1-worker-13] DEBUG [UserST:274] Command for kafka-configs.sh bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=encrypted-arnost
2022-03-28 10:57:30 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=encrypted-arnost
2022-03-28 10:57:30 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:30 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (205289ms till timeout)
2022-03-28 10:57:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (166364ms till timeout)
2022-03-28 10:57:30 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:30 [ForkJoinPool-1-worker-1] INFO  [PodUtils:225] Pod replicas gonna be stable in 7 polls
2022-03-28 10:57:30 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176]  Podmy-cluster-c0830cbe-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (153890ms till timeout)
2022-03-28 10:57:30 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:30 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:30 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:57:30 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 17
2022-03-28 10:57:30 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (264344ms till timeout)
2022-03-28 10:57:30 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:31 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:31 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:31 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (453261ms till timeout)
2022-03-28 10:57:31 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:444] Kafka: my-cluster-d1e2168e is in desired state: Ready
2022-03-28 10:57:31 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-7 exec my-cluster-d1e2168e-cruise-control-64d864f895-pqv8w -- /bin/bash -c cat /tmp/cruisecontrol.properties
2022-03-28 10:57:31 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:210] testPauseReconciliationInKafkaAndKafkaConnectWithConnector is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:57:31 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (204212ms till timeout)
2022-03-28 10:57:31 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (165271ms till timeout)
2022-03-28 10:57:31 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:32 [ForkJoinPool-1-worker-1] INFO  [PodUtils:225] Pod replicas gonna be stable in 6 polls
2022-03-28 10:57:32 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176]  Podmy-cluster-c0830cbe-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (152753ms till timeout)
2022-03-28 10:57:32 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:32 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:32 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:57:32 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 16
2022-03-28 10:57:32 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (263157ms till timeout)
2022-03-28 10:57:32 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:32 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:32 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:32 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (452054ms till timeout)
2022-03-28 10:57:32 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Command: oc --namespace namespace-7 exec my-cluster-d1e2168e-cruise-control-64d864f895-pqv8w -- /bin/bash -c cat /tmp/cruisecontrol.properties
2022-03-28 10:57:32 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Return code: 0
2022-03-28 10:57:32 [ForkJoinPool-1-worker-7] INFO  [CruiseControlConfigurationST:221] Verifying that all configuration in the cruise control container matching the cruise control file /tmp/cruisecontrol.properties properties
2022-03-28 10:57:32 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:674] ============================================================================
2022-03-28 10:57:32 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:675] [cruisecontrol.CruiseControlConfigurationST - After Each] - Clean up after test
2022-03-28 10:57:32 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:344] ############################################################################
2022-03-28 10:57:32 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:348] Delete all resources for testConfigurationReflection
2022-03-28 10:57:32 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of Kafka my-cluster-d1e2168e in namespace namespace-7
2022-03-28 10:57:32 [ForkJoinPool-1-worker-7] INFO  [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-7, for cruise control Kafka cluster my-cluster-d1e2168e
2022-03-28 10:57:32 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:32 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (203098ms till timeout)
2022-03-28 10:57:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (164206ms till timeout)
2022-03-28 10:57:33 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-d1e2168e
2022-03-28 10:57:33 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-d1e2168e not ready, will try again in 10000 ms (839942ms till timeout)
2022-03-28 10:57:33 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:33 [ForkJoinPool-1-worker-1] INFO  [PodUtils:225] Pod replicas gonna be stable in 5 polls
2022-03-28 10:57:33 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176]  Podmy-cluster-c0830cbe-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (151687ms till timeout)
2022-03-28 10:57:33 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:33 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:33 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:57:33 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 15
2022-03-28 10:57:33 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (262062ms till timeout)
2022-03-28 10:57:33 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:33 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:33 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:33 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (450665ms till timeout)
2022-03-28 10:57:34 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:34 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (201881ms till timeout)
2022-03-28 10:57:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (162941ms till timeout)
2022-03-28 10:57:34 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:34 [ForkJoinPool-1-worker-1] INFO  [PodUtils:225] Pod replicas gonna be stable in 4 polls
2022-03-28 10:57:34 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176]  Podmy-cluster-c0830cbe-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (150595ms till timeout)
2022-03-28 10:57:34 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:34 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:34 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:57:34 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 14
2022-03-28 10:57:34 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (260982ms till timeout)
2022-03-28 10:57:34 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:35 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:35 [ForkJoinPool-1-worker-13] INFO  [Exec:417] Command: oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=encrypted-arnost
2022-03-28 10:57:35 [ForkJoinPool-1-worker-13] INFO  [Exec:417] Return code: 0
2022-03-28 10:57:35 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:35 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:35 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:35 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (449175ms till timeout)
2022-03-28 10:57:35 [ForkJoinPool-1-worker-1] INFO  [PodUtils:225] Pod replicas gonna be stable in 3 polls
2022-03-28 10:57:35 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176]  Podmy-cluster-c0830cbe-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (149490ms till timeout)
2022-03-28 10:57:35 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:35 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:35 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:57:35 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 13
2022-03-28 10:57:35 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (259947ms till timeout)
2022-03-28 10:57:35 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (200578ms till timeout)
2022-03-28 10:57:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (161655ms till timeout)
2022-03-28 10:57:35 [ForkJoinPool-1-worker-13] INFO  [KafkaUserUtils:62] Waiting for KafkaUser deletion encrypted-arnost
2022-03-28 10:57:35 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaUser deletion encrypted-arnost
2022-03-28 10:57:35 [ForkJoinPool-1-worker-13] WARN  [KafkaUserUtils:68] KafkaUser encrypted-arnost is not deleted yet! Triggering force delete by cmd client!
2022-03-28 10:57:35 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-7 delete KafkaUser encrypted-arnost
2022-03-28 10:57:35 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-7 delete KafkaUser encrypted-arnost
2022-03-28 10:57:35 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 1
2022-03-28 10:57:35 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 10:57:35 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Error from server (NotFound): kafkausers.kafka.strimzi.io "encrypted-arnost" not found
2022-03-28 10:57:35 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] ======STDERR END======
2022-03-28 10:57:35 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] KafkaUser deletion encrypted-arnost not ready, will try again in 1000 ms (179595ms till timeout)
2022-03-28 10:57:36 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:36 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:36 [ForkJoinPool-1-worker-1] INFO  [PodUtils:225] Pod replicas gonna be stable in 2 polls
2022-03-28 10:57:36 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176]  Podmy-cluster-c0830cbe-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (148425ms till timeout)
2022-03-28 10:57:36 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:36 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:36 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:57:36 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 12
2022-03-28 10:57:36 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (258837ms till timeout)
2022-03-28 10:57:36 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:36 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (199411ms till timeout)
2022-03-28 10:57:36 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:36 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:36 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (447881ms till timeout)
2022-03-28 10:57:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (160461ms till timeout)
2022-03-28 10:57:36 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:210] testPauseReconciliationInKafkaAndKafkaConnectWithConnector is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:57:36 [ForkJoinPool-1-worker-13] INFO  [KafkaUserUtils:75] KafkaUser encrypted-arnost deleted
2022-03-28 10:57:36 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for all KafkaUser CN=encrypted-arnost attributes will be cleaned
2022-03-28 10:57:36 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=encrypted-arnost
2022-03-28 10:57:37 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:37 [ForkJoinPool-1-worker-1] INFO  [PodUtils:225] Pod replicas gonna be stable in 1 polls
2022-03-28 10:57:37 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176]  Podmy-cluster-c0830cbe-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (147368ms till timeout)
2022-03-28 10:57:37 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:37 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:37 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:57:37 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 11
2022-03-28 10:57:37 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (257768ms till timeout)
2022-03-28 10:57:37 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (198358ms till timeout)
2022-03-28 10:57:37 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:37 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (159351ms till timeout)
2022-03-28 10:57:37 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:37 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:37 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (446662ms till timeout)
2022-03-28 10:57:38 [ForkJoinPool-1-worker-1] INFO  [PodUtils:217] Pod replicas are stable for 20 polls intervals
2022-03-28 10:57:38 [ForkJoinPool-1-worker-1] INFO  [PodUtils:228] Pod my-cluster-c0830cbe-cruise-control- has 0 replicas
2022-03-28 10:57:38 [ForkJoinPool-1-worker-1] INFO  [CruiseControlConfigurationST:123] Verifying that in Kafka config map there is no configuration to cruise control metric reporter
2022-03-28 10:57:38 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:38 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties
2022-03-28 10:57:38 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (120000ms till timeout)
2022-03-28 10:57:38 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:38 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:38 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:57:38 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 10
2022-03-28 10:57:38 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (256729ms till timeout)
2022-03-28 10:57:38 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (197299ms till timeout)
2022-03-28 10:57:38 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:38 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (158238ms till timeout)
2022-03-28 10:57:38 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:38 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:38 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (445486ms till timeout)
2022-03-28 10:57:39 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (119000ms till timeout)
2022-03-28 10:57:39 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:39 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:39 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:39 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:57:39 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 9
2022-03-28 10:57:39 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (255669ms till timeout)
2022-03-28 10:57:39 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (196203ms till timeout)
2022-03-28 10:57:39 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (157201ms till timeout)
2022-03-28 10:57:39 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:40 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:40 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:40 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (444258ms till timeout)
2022-03-28 10:57:40 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (117999ms till timeout)
2022-03-28 10:57:40 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:40 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:40 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:40 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:57:40 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 8
2022-03-28 10:57:40 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (254651ms till timeout)
2022-03-28 10:57:40 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (195157ms till timeout)
2022-03-28 10:57:40 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (156148ms till timeout)
2022-03-28 10:57:41 [ForkJoinPool-1-worker-13] INFO  [Exec:417] Command: oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=encrypted-arnost
2022-03-28 10:57:41 [ForkJoinPool-1-worker-13] INFO  [Exec:417] Return code: 0
2022-03-28 10:57:41 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:674] ============================================================================
2022-03-28 10:57:41 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:675] [operators.user.UserST - After Each] - Clean up after test
2022-03-28 10:57:41 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:344] ############################################################################
2022-03-28 10:57:41 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:348] Delete all resources for testTlsUserWithQuotas
2022-03-28 10:57:41 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of KafkaUser encrypted-arnost in namespace user-st
2022-03-28 10:57:41 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:encrypted-arnost
2022-03-28 10:57:41 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:369] ############################################################################
2022-03-28 10:57:41 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:267] testTlsUserWithQuotas - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTls, testSendSimpleMessageTlsScramSha, testConfigurationFileIsCreated, testReceiveSimpleMessageTls, testDeployAndUnDeployCruiseControl, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testConfigurationPerformanceOptions, testPauseReconciliationInKafkaAndKafkaConnectWithConnector] to and randomly select one to start execution
2022-03-28 10:57:41 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:93] [operators.user.UserST] - Removing parallel test: testTlsUserWithQuotas
2022-03-28 10:57:41 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:97] [operators.user.UserST] - Parallel test count: 7
2022-03-28 10:57:41 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testTlsUserWithQuotas-FINISHED
2022-03-28 10:57:41 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:30] ############################################################################
2022-03-28 10:57:41 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:23] ############################################################################
2022-03-28 10:57:41 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testUserWithNameMoreThan64Chars-STARTED
2022-03-28 10:57:41 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:658] ============================================================================
2022-03-28 10:57:41 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:659] [operators.user.UserST - Before Each] - Setup test case environment
2022-03-28 10:57:41 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:77] [operators.user.UserST] - Adding parallel test: testUserWithNameMoreThan64Chars
2022-03-28 10:57:41 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:81] [operators.user.UserST] - Parallel test count: 8
2022-03-28 10:57:41 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:205] [testUserWithNameMoreThan64Chars] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:57:41 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUserWithNameMoreThan64Chars is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:57:41 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:41 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 10:57:41 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 1
2022-03-28 10:57:41 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 10:57:41 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-4" not found
2022-03-28 10:57:41 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] ======STDERR END======
2022-03-28 10:57:41 [ForkJoinPool-1-worker-3] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[namespace-7], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@f851b6c3=[namespace-5], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[infra-namespace, reconciliation-st], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-3], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 10:57:41 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:267] testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTls, testSendSimpleMessageTlsScramSha, testConfigurationFileIsCreated, testReceiveSimpleMessageTls, testDeployAndUnDeployCruiseControl, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testConfigurationPerformanceOptions, testPauseReconciliationInKafkaAndKafkaConnectWithConnector, testUserWithNameMoreThan64Chars] to and randomly select one to start execution
2022-03-28 10:57:41 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:93] [cruisecontrol.CruiseControlConfigurationST] - Removing parallel test: testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods
2022-03-28 10:57:41 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:97] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 7
2022-03-28 10:57:41 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods-FINISHED
2022-03-28 10:57:41 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:30] ############################################################################
2022-03-28 10:57:41 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:23] ############################################################################
2022-03-28 10:57:41 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.topic.TopicST.testTopicModificationOfReplicationFactor-STARTED
2022-03-28 10:57:41 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:658] ============================================================================
2022-03-28 10:57:41 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:659] [operators.topic.TopicST - Before Each] - Setup test case environment
2022-03-28 10:57:41 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:77] [operators.topic.TopicST] - Adding parallel test: testTopicModificationOfReplicationFactor
2022-03-28 10:57:41 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:81] [operators.topic.TopicST] - Parallel test count: 8
2022-03-28 10:57:41 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:205] [testTopicModificationOfReplicationFactor] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:57:41 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTopicModificationOfReplicationFactor is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:57:41 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (116998ms till timeout)
2022-03-28 10:57:41 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:41 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:41 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:41 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:57:41 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 7
2022-03-28 10:57:41 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (253568ms till timeout)
2022-03-28 10:57:41 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:230] testPauseReconciliationInKafkaAndKafkaConnectWithConnector test now can proceed its execution
2022-03-28 10:57:41 [ForkJoinPool-1-worker-15] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 10:57:41 [ForkJoinPool-1-worker-15] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testScramUserWithQuotas=my-cluster-4941482a, testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383, testConfigurationFileIsCreated=my-cluster-e0fac774, testThrottlingQuotasCreateAlterPartitions=my-cluster-28d9dd2e, testSendSimpleMessageTls=my-cluster-27ddd6ba, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c, testThrottlingQuotasDeleteTopic=my-cluster-8f262de5, testConfigurationReflection=my-cluster-d1e2168e, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-da91577a, testDeployAndUnDeployCruiseControl=my-cluster-c0830cbe, testTlsUserWithQuotas=my-cluster-d49e42e8, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8, testCapacityFile=my-cluster-fd0fb61a, testReceiveSimpleMessageTls=my-cluster-c6a15963, testConfigurationPerformanceOptions=my-cluster-b6310693, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-66f23d43, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-f50d425d, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5}
2022-03-28 10:57:41 [ForkJoinPool-1-worker-15] TRACE [AbstractST:607] USERS_NAME_MAP: {testScramUserWithQuotas=my-user-797280497-2138800976, testReceiveSimpleMessageTlsScramSha=my-user-1915916223-1265293973, testConfigurationFileIsCreated=my-user-180611180-50530491, testThrottlingQuotasCreateAlterPartitions=my-user-1933547616-1024431140, testSendSimpleMessageTls=my-user-738229993-1095121647, testCreatingUsersWithSecretPrefix=my-user-1450786684-1808961062, testThrottlingQuotasDeleteTopic=my-user-1558977140-777236615, testConfigurationReflection=my-user-1445653023-93200027, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-1260379615-1050949907, testDeployAndUnDeployCruiseControl=my-user-1988711145-439861161, testTlsUserWithQuotas=my-user-811912745-1441809840, testThrottlingQuotasCreateTopic=my-user-1131806865-947217230, testCapacityFile=my-user-937015144-1959439396, testReceiveSimpleMessageTls=my-user-6088058-1142028780, testConfigurationPerformanceOptions=my-user-930975101-640523779, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-1150765500-2076224625, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-745974575-1852384454, testSendSimpleMessageTlsScramSha=my-user-1499386234-1249154328}
2022-03-28 10:57:41 [ForkJoinPool-1-worker-15] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testScramUserWithQuotas=my-topic-2004840350-265526537, testReceiveSimpleMessageTlsScramSha=my-topic-1834944466-2094449929, testConfigurationFileIsCreated=my-topic-1296407588-313880708, testThrottlingQuotasCreateAlterPartitions=my-topic-609849288-1810034325, testSendSimpleMessageTls=my-topic-563023482-1697359353, testCreatingUsersWithSecretPrefix=my-topic-1479520843-618891199, testThrottlingQuotasDeleteTopic=my-topic-2128658681-1688111273, testConfigurationReflection=my-topic-640815392-1092406112, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-1674392683-1637010870, testDeployAndUnDeployCruiseControl=my-topic-1511079187-95055629, testTlsUserWithQuotas=my-topic-418284173-1689090259, testThrottlingQuotasCreateTopic=my-topic-728672490-1194703402, testCapacityFile=my-topic-1155641705-902584520, testReceiveSimpleMessageTls=my-topic-478514536-2132782608, testConfigurationPerformanceOptions=my-topic-213912446-1165431808, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-842198221-1227962115, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-254848724-189700675, testSendSimpleMessageTlsScramSha=my-topic-1028951580-1656411053}
2022-03-28 10:57:41 [ForkJoinPool-1-worker-15] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testScramUserWithQuotas=my-cluster-4941482a-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383-kafka-clients, testConfigurationFileIsCreated=my-cluster-e0fac774-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-28d9dd2e-kafka-clients, testSendSimpleMessageTls=my-cluster-27ddd6ba-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-8f262de5-kafka-clients, testConfigurationReflection=my-cluster-d1e2168e-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-da91577a-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-c0830cbe-kafka-clients, testTlsUserWithQuotas=my-cluster-d49e42e8-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8-kafka-clients, testCapacityFile=my-cluster-fd0fb61a-kafka-clients, testReceiveSimpleMessageTls=my-cluster-c6a15963-kafka-clients, testConfigurationPerformanceOptions=my-cluster-b6310693-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-66f23d43-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-f50d425d-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5-kafka-clients}
2022-03-28 10:57:41 [ForkJoinPool-1-worker-15] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-8 for test case:testPauseReconciliationInKafkaAndKafkaConnectWithConnector
2022-03-28 10:57:41 [ForkJoinPool-1-worker-15] INFO  [KubeClusterResource:156] Creating Namespace: namespace-8
2022-03-28 10:57:41 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Namespace namespace-8
2022-03-28 10:57:41 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-8 -o json
2022-03-28 10:57:41 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (194131ms till timeout)
2022-03-28 10:57:41 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:41 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-8 -o json
2022-03-28 10:57:41 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:41 [ForkJoinPool-1-worker-15] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c28,c7",
            "openshift.io/sa.scc.supplemental-groups": "1000770000/10000",
            "openshift.io/sa.scc.uid-range": "1000770000/10000"
        },
        "creationTimestamp": "2022-03-28T10:57:39Z",
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T10:57:09Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:status": {
                        "f:phase": {}
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T10:57:39Z"
            }
        ],
        "name": "namespace-8",
        "resourceVersion": "1875116",
        "selfLink": "/api/v1/namespaces/namespace-8",
        "uid": "660ffbb4-255b-4e2d-b0a2-9a01181ece54"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 10:57:41 [ForkJoinPool-1-worker-15] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[namespace-7], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[namespace-8], io.strimzi.test.logs.CollectorElement@f851b6c3=[namespace-5], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[infra-namespace, reconciliation-st], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-3], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 10:57:41 [ForkJoinPool-1-worker-15] INFO  [KubeClusterResource:82] Client use Namespace: namespace-8
2022-03-28 10:57:41 [ForkJoinPool-1-worker-15] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-8, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 10:57:41 [ForkJoinPool-1-worker-15] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-8
2022-03-28 10:57:41 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-66f23d43 in namespace namespace-8
2022-03-28 10:57:41 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:164] Using Namespace: namespace-8
2022-03-28 10:57:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (155068ms till timeout)
2022-03-28 10:57:42 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-66f23d43
2022-03-28 10:57:42 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-66f23d43 will have desired state: Ready
2022-03-28 10:57:42 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-66f23d43 will have desired state: Ready
2022-03-28 10:57:42 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (839969ms till timeout)
2022-03-28 10:57:42 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (115998ms till timeout)
2022-03-28 10:57:42 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:42 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:42 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:42 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:57:42 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 6
2022-03-28 10:57:42 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (252427ms till timeout)
2022-03-28 10:57:42 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (192962ms till timeout)
2022-03-28 10:57:43 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (153956ms till timeout)
2022-03-28 10:57:43 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:369] ############################################################################
2022-03-28 10:57:43 [ForkJoinPool-1-worker-7] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-7 for test case:testConfigurationReflection
2022-03-28 10:57:43 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (838914ms till timeout)
2022-03-28 10:57:43 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Namespace namespace-7 removal
2022-03-28 10:57:43 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 10:57:43 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 10:57:43 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:43 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (479739ms till timeout)
2022-03-28 10:57:43 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (114997ms till timeout)
2022-03-28 10:57:43 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:43 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:43 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:43 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:57:43 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 5
2022-03-28 10:57:43 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (251358ms till timeout)
2022-03-28 10:57:44 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (191924ms till timeout)
2022-03-28 10:57:44 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (152873ms till timeout)
2022-03-28 10:57:44 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (837895ms till timeout)
2022-03-28 10:57:44 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 10:57:44 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (113997ms till timeout)
2022-03-28 10:57:44 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 10:57:44 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:44 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (478499ms till timeout)
2022-03-28 10:57:44 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:45 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:45 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:45 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:57:45 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 4
2022-03-28 10:57:45 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (250219ms till timeout)
2022-03-28 10:57:45 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (190746ms till timeout)
2022-03-28 10:57:45 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:45 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (836848ms till timeout)
2022-03-28 10:57:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (151687ms till timeout)
2022-03-28 10:57:45 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (112996ms till timeout)
2022-03-28 10:57:45 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 10:57:45 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 10:57:45 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (477240ms till timeout)
2022-03-28 10:57:46 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:46 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUserWithNameMoreThan64Chars is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:57:46 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:46 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:46 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:57:46 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 3
2022-03-28 10:57:46 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (249102ms till timeout)
2022-03-28 10:57:46 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (835755ms till timeout)
2022-03-28 10:57:46 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (189545ms till timeout)
2022-03-28 10:57:46 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:46 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTopicModificationOfReplicationFactor is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:57:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (150581ms till timeout)
2022-03-28 10:57:46 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (111995ms till timeout)
2022-03-28 10:57:46 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 10:57:47 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:47 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:47 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:47 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:57:47 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 2
2022-03-28 10:57:47 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (247959ms till timeout)
2022-03-28 10:57:47 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 10:57:47 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (475911ms till timeout)
2022-03-28 10:57:47 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (834617ms till timeout)
2022-03-28 10:57:47 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (110995ms till timeout)
2022-03-28 10:57:47 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:47 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (188354ms till timeout)
2022-03-28 10:57:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (149363ms till timeout)
2022-03-28 10:57:48 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:48 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 10:57:48 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:48 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:48 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:57:48 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 1
2022-03-28 10:57:48 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (246873ms till timeout)
2022-03-28 10:57:48 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (109994ms till timeout)
2022-03-28 10:57:48 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (833559ms till timeout)
2022-03-28 10:57:48 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 10:57:48 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (474548ms till timeout)
2022-03-28 10:57:48 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (187266ms till timeout)
2022-03-28 10:57:48 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (148285ms till timeout)
2022-03-28 10:57:49 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:49 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:49 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d, my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec}
2022-03-28 10:57:49 [ForkJoinPool-1-worker-5] DEBUG [RollingUpdateUtils:50] At least my-cluster-b6310693-kafka-2 hasn't rolled
2022-03-28 10:57:49 [ForkJoinPool-1-worker-5] INFO  [RollingUpdateUtils:143] {my-cluster-b6310693-kafka-2=c97dacea-64e5-4835-855e-c04b5c9083ec, my-cluster-b6310693-kafka-0=b6ea7e83-3de6-46ee-86d8-31148066aabb, my-cluster-b6310693-kafka-1=44e18a1b-1e99-4b48-958e-b58b954e674d} pods didn't roll. Remaining seconds for stability: 0
2022-03-28 10:57:49 [ForkJoinPool-1-worker-5] INFO  [CruiseControlConfigurationST:283] Verifying new configuration in the Kafka CR
2022-03-28 10:57:49 [ForkJoinPool-1-worker-5] INFO  [CruiseControlConfigurationST:300] Verifying Cruise control performance options are set in Kafka CR
2022-03-28 10:57:49 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:674] ============================================================================
2022-03-28 10:57:49 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:675] [cruisecontrol.CruiseControlConfigurationST - After Each] - Clean up after test
2022-03-28 10:57:49 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:344] ############################################################################
2022-03-28 10:57:49 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:348] Delete all resources for testConfigurationPerformanceOptions
2022-03-28 10:57:49 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of Kafka my-cluster-b6310693 in namespace namespace-5
2022-03-28 10:57:49 [ForkJoinPool-1-worker-5] INFO  [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-5, for cruise control Kafka cluster my-cluster-b6310693
2022-03-28 10:57:49 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (108994ms till timeout)
2022-03-28 10:57:49 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (832520ms till timeout)
2022-03-28 10:57:49 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 10:57:49 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (186160ms till timeout)
2022-03-28 10:57:49 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (147194ms till timeout)
2022-03-28 10:57:50 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-b6310693
2022-03-28 10:57:50 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-b6310693 not ready, will try again in 10000 ms (839950ms till timeout)
2022-03-28 10:57:50 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (107993ms till timeout)
2022-03-28 10:57:50 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (831419ms till timeout)
2022-03-28 10:57:50 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:51 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (184881ms till timeout)
2022-03-28 10:57:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (145951ms till timeout)
2022-03-28 10:57:51 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUserWithNameMoreThan64Chars is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:57:51 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTopicModificationOfReplicationFactor is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:57:51 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (106992ms till timeout)
2022-03-28 10:57:51 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (830350ms till timeout)
2022-03-28 10:57:52 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:52 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (183716ms till timeout)
2022-03-28 10:57:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (144778ms till timeout)
2022-03-28 10:57:52 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (105992ms till timeout)
2022-03-28 10:57:52 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (829265ms till timeout)
2022-03-28 10:57:53 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (182664ms till timeout)
2022-03-28 10:57:53 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (143674ms till timeout)
2022-03-28 10:57:53 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (104991ms till timeout)
2022-03-28 10:57:53 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (828179ms till timeout)
2022-03-28 10:57:54 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (181606ms till timeout)
2022-03-28 10:57:54 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (142567ms till timeout)
2022-03-28 10:57:54 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (103991ms till timeout)
2022-03-28 10:57:54 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (827154ms till timeout)
2022-03-28 10:57:55 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 10:57:55 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (468174ms till timeout)
2022-03-28 10:57:55 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (180575ms till timeout)
2022-03-28 10:57:55 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (102990ms till timeout)
2022-03-28 10:57:55 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (141401ms till timeout)
2022-03-28 10:57:56 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (826069ms till timeout)
2022-03-28 10:57:56 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 10:57:56 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUserWithNameMoreThan64Chars is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:57:56 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 10:57:56 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (466832ms till timeout)
2022-03-28 10:57:56 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (179525ms till timeout)
2022-03-28 10:57:56 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTopicModificationOfReplicationFactor is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:57:56 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (101990ms till timeout)
2022-03-28 10:57:56 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (140328ms till timeout)
2022-03-28 10:57:57 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (825040ms till timeout)
2022-03-28 10:57:57 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 10:57:57 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (178457ms till timeout)
2022-03-28 10:57:57 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (100989ms till timeout)
2022-03-28 10:57:57 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 10:57:57 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:57 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (465547ms till timeout)
2022-03-28 10:57:57 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (139195ms till timeout)
2022-03-28 10:57:58 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (823957ms till timeout)
2022-03-28 10:57:58 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (99989ms till timeout)
2022-03-28 10:57:58 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (177339ms till timeout)
2022-03-28 10:57:58 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 10:57:58 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:57:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (138115ms till timeout)
2022-03-28 10:57:59 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 10:57:59 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 10:57:59 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (464102ms till timeout)
2022-03-28 10:57:59 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (822874ms till timeout)
2022-03-28 10:57:59 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (98988ms till timeout)
2022-03-28 10:57:59 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (176236ms till timeout)
2022-03-28 10:57:59 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:58:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (137037ms till timeout)
2022-03-28 10:58:00 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 10:58:00 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:369] ############################################################################
2022-03-28 10:58:00 [ForkJoinPool-1-worker-5] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-5 for test case:testConfigurationPerformanceOptions
2022-03-28 10:58:00 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Namespace namespace-5 removal
2022-03-28 10:58:00 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:00 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (821821ms till timeout)
2022-03-28 10:58:00 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 10:58:00 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 10:58:00 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (462822ms till timeout)
2022-03-28 10:58:00 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (97988ms till timeout)
2022-03-28 10:58:00 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:00 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 10:58:00 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (479715ms till timeout)
2022-03-28 10:58:00 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (175195ms till timeout)
2022-03-28 10:58:01 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:58:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (135983ms till timeout)
2022-03-28 10:58:01 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUserWithNameMoreThan64Chars is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:58:01 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (820774ms till timeout)
2022-03-28 10:58:01 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 10:58:01 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTopicModificationOfReplicationFactor is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:58:01 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (96987ms till timeout)
2022-03-28 10:58:01 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:01 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 10:58:01 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 10:58:01 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (461538ms till timeout)
2022-03-28 10:58:01 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (174149ms till timeout)
2022-03-28 10:58:01 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:01 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 10:58:01 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (478390ms till timeout)
2022-03-28 10:58:02 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:58:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (134838ms till timeout)
2022-03-28 10:58:02 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (819734ms till timeout)
2022-03-28 10:58:02 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (95987ms till timeout)
2022-03-28 10:58:02 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 10:58:02 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (173123ms till timeout)
2022-03-28 10:58:02 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:02 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 10:58:02 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 10:58:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (460239ms till timeout)
2022-03-28 10:58:03 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:03 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 10:58:03 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (477049ms till timeout)
2022-03-28 10:58:03 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:58:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (133756ms till timeout)
2022-03-28 10:58:03 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (818643ms till timeout)
2022-03-28 10:58:03 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (94986ms till timeout)
2022-03-28 10:58:03 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (172024ms till timeout)
2022-03-28 10:58:03 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 10:58:04 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:04 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 10:58:04 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 10:58:04 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (458843ms till timeout)
2022-03-28 10:58:04 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:58:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (132627ms till timeout)
2022-03-28 10:58:04 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (817619ms till timeout)
2022-03-28 10:58:04 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:04 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 10:58:04 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (475751ms till timeout)
2022-03-28 10:58:04 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (93986ms till timeout)
2022-03-28 10:58:05 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (170915ms till timeout)
2022-03-28 10:58:05 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 10:58:05 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:58:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (131602ms till timeout)
2022-03-28 10:58:05 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (816594ms till timeout)
2022-03-28 10:58:05 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:05 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (92985ms till timeout)
2022-03-28 10:58:05 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 10:58:05 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 10:58:05 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (457527ms till timeout)
2022-03-28 10:58:06 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUserWithNameMoreThan64Chars is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:58:06 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (169786ms till timeout)
2022-03-28 10:58:06 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTopicModificationOfReplicationFactor is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:58:06 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:58:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (130545ms till timeout)
2022-03-28 10:58:06 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (91985ms till timeout)
2022-03-28 10:58:06 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (815537ms till timeout)
2022-03-28 10:58:06 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 10:58:07 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (168762ms till timeout)
2022-03-28 10:58:07 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (90983ms till timeout)
2022-03-28 10:58:07 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:58:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129491ms till timeout)
2022-03-28 10:58:07 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (814486ms till timeout)
2022-03-28 10:58:08 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (167645ms till timeout)
2022-03-28 10:58:08 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (89982ms till timeout)
2022-03-28 10:58:08 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:58:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (128405ms till timeout)
2022-03-28 10:58:08 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (813410ms till timeout)
2022-03-28 10:58:09 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (166563ms till timeout)
2022-03-28 10:58:09 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (88982ms till timeout)
2022-03-28 10:58:09 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:58:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (127331ms till timeout)
2022-03-28 10:58:09 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (812344ms till timeout)
2022-03-28 10:58:10 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (165519ms till timeout)
2022-03-28 10:58:10 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (87981ms till timeout)
2022-03-28 10:58:10 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T10:58:07Z, conditions=[JobCondition(lastProbeTime=2022-03-28T10:58:07Z, lastTransitionTime=2022-03-28T10:58:07Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T10:56:33Z, succeeded=1, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:58:10 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:10 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 10:58:10 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (469407ms till timeout)
2022-03-28 10:58:10 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (811246ms till timeout)
2022-03-28 10:58:10 [ForkJoinPool-1-worker-9] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 10:58:10 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 10:58:11 [ForkJoinPool-1-worker-9] INFO  [PodUtils:189] Message All topics created found in create-admin-my-cluster-8f262de5-kafka-clients-m8l9n log
2022-03-28 10:58:11 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUserWithNameMoreThan64Chars is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:58:11 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment create-admin-my-cluster-8f262de5-kafka-clients deletion
2022-03-28 10:58:11 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for ReplicaSet create-admin-my-cluster-8f262de5-kafka-clients to be deleted
2022-03-28 10:58:11 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTopicModificationOfReplicationFactor is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:58:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] ReplicaSet create-admin-my-cluster-8f262de5-kafka-clients to be deleted not ready, will try again in 5000 ms (179933ms till timeout)
2022-03-28 10:58:11 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (86981ms till timeout)
2022-03-28 10:58:11 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (164379ms till timeout)
2022-03-28 10:58:11 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:11 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (810231ms till timeout)
2022-03-28 10:58:11 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 10:58:11 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 10:58:11 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (451269ms till timeout)
2022-03-28 10:58:12 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:12 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 10:58:12 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (468130ms till timeout)
2022-03-28 10:58:12 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (85980ms till timeout)
2022-03-28 10:58:12 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (163264ms till timeout)
2022-03-28 10:58:12 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (809182ms till timeout)
2022-03-28 10:58:12 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 10:58:13 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:13 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 10:58:13 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 10:58:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (449981ms till timeout)
2022-03-28 10:58:13 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (84980ms till timeout)
2022-03-28 10:58:13 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (162193ms till timeout)
2022-03-28 10:58:13 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (808138ms till timeout)
2022-03-28 10:58:14 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 10:58:14 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (83979ms till timeout)
2022-03-28 10:58:14 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 10:58:14 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 10:58:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (448613ms till timeout)
2022-03-28 10:58:14 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (161158ms till timeout)
2022-03-28 10:58:15 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (807080ms till timeout)
2022-03-28 10:58:15 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (82979ms till timeout)
2022-03-28 10:58:15 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 10:58:15 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (160104ms till timeout)
2022-03-28 10:58:15 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 10:58:15 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 10:58:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (447282ms till timeout)
2022-03-28 10:58:16 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (806010ms till timeout)
2022-03-28 10:58:16 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUserWithNameMoreThan64Chars is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:58:16 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTopicModificationOfReplicationFactor is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:58:16 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (81978ms till timeout)
2022-03-28 10:58:16 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:40] Job create-admin-my-cluster-8f262de5-kafka-clients was deleted
2022-03-28 10:58:16 [ForkJoinPool-1-worker-9] INFO  [ThrottlingQuotaST:112] Executing 2/5 iteration.
2022-03-28 10:58:16 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st
2022-03-28 10:58:16 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:create-admin-my-cluster-8f262de5-kafka-clients
2022-03-28 10:58:16 [ForkJoinPool-1-worker-9] INFO  [JobUtils:81] Waiting for job: create-admin-my-cluster-8f262de5-kafka-clients will be in active state
2022-03-28 10:58:16 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 10:58:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job active not ready, will try again in 1000 ms (179969ms till timeout)
2022-03-28 10:58:16 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (159055ms till timeout)
2022-03-28 10:58:16 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 10:58:17 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (804954ms till timeout)
2022-03-28 10:58:17 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 10:58:17 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 10:58:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (445945ms till timeout)
2022-03-28 10:58:17 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (80978ms till timeout)
2022-03-28 10:58:17 [ForkJoinPool-1-worker-9] INFO  [ClientUtils:76] Waiting for producer/consumer:create-admin-my-cluster-8f262de5-kafka-clients to finished
2022-03-28 10:58:17 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 10:58:17 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:58:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (219975ms till timeout)
2022-03-28 10:58:17 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (158028ms till timeout)
2022-03-28 10:58:18 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (803944ms till timeout)
2022-03-28 10:58:18 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 10:58:18 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:18 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 10:58:18 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (461807ms till timeout)
2022-03-28 10:58:18 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (79977ms till timeout)
2022-03-28 10:58:18 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 10:58:18 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 10:58:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (444599ms till timeout)
2022-03-28 10:58:18 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:58:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (218913ms till timeout)
2022-03-28 10:58:18 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (156993ms till timeout)
2022-03-28 10:58:19 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (802879ms till timeout)
2022-03-28 10:58:19 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:19 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (78977ms till timeout)
2022-03-28 10:58:19 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 10:58:19 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:19 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 10:58:19 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (460589ms till timeout)
2022-03-28 10:58:19 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:58:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (217855ms till timeout)
2022-03-28 10:58:19 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-8 get Namespace namespace-7 -o yaml
2022-03-28 10:58:19 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 1
2022-03-28 10:58:19 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 10:58:19 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-7" not found
2022-03-28 10:58:19 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] ======STDERR END======
2022-03-28 10:58:19 [ForkJoinPool-1-worker-7] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[namespace-8], io.strimzi.test.logs.CollectorElement@f851b6c3=[namespace-5], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[infra-namespace, reconciliation-st], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-3], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 10:58:19 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:267] testConfigurationReflection - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTls, testSendSimpleMessageTlsScramSha, testConfigurationFileIsCreated, testReceiveSimpleMessageTls, testDeployAndUnDeployCruiseControl, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testConfigurationPerformanceOptions, testUserWithNameMoreThan64Chars, testTopicModificationOfReplicationFactor] to and randomly select one to start execution
2022-03-28 10:58:19 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:93] [cruisecontrol.CruiseControlConfigurationST] - Removing parallel test: testConfigurationReflection
2022-03-28 10:58:19 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:97] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 7
2022-03-28 10:58:19 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationReflection-FINISHED
2022-03-28 10:58:19 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:30] ############################################################################
2022-03-28 10:58:19 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:23] ############################################################################
2022-03-28 10:58:19 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testKafkaAdminTopicOperations-STARTED
2022-03-28 10:58:19 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:658] ============================================================================
2022-03-28 10:58:19 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:659] [operators.topic.ThrottlingQuotaST - Before Each] - Setup test case environment
2022-03-28 10:58:19 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:77] [operators.topic.ThrottlingQuotaST] - Adding parallel test: testKafkaAdminTopicOperations
2022-03-28 10:58:19 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:81] [operators.topic.ThrottlingQuotaST] - Parallel test count: 8
2022-03-28 10:58:19 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:205] [testKafkaAdminTopicOperations] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:58:19 [ForkJoinPool-1-worker-7] TRACE [SuiteThreadController:210] testKafkaAdminTopicOperations is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:58:20 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (155896ms till timeout)
2022-03-28 10:58:20 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (801862ms till timeout)
2022-03-28 10:58:20 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (77976ms till timeout)
2022-03-28 10:58:20 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:20 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:58:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (216820ms till timeout)
2022-03-28 10:58:20 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:20 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 10:58:20 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (459278ms till timeout)
2022-03-28 10:58:21 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (154863ms till timeout)
2022-03-28 10:58:21 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:230] testUserWithNameMoreThan64Chars test now can proceed its execution
2022-03-28 10:58:21 [ForkJoinPool-1-worker-13] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 10:58:21 [ForkJoinPool-1-worker-13] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testScramUserWithQuotas=my-cluster-4941482a, testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383, testConfigurationFileIsCreated=my-cluster-e0fac774, testThrottlingQuotasCreateAlterPartitions=my-cluster-28d9dd2e, testSendSimpleMessageTls=my-cluster-27ddd6ba, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c, testThrottlingQuotasDeleteTopic=my-cluster-8f262de5, testConfigurationReflection=my-cluster-d1e2168e, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-da91577a, testUserWithNameMoreThan64Chars=my-cluster-35e66bba, testDeployAndUnDeployCruiseControl=my-cluster-c0830cbe, testTlsUserWithQuotas=my-cluster-d49e42e8, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8, testCapacityFile=my-cluster-fd0fb61a, testReceiveSimpleMessageTls=my-cluster-c6a15963, testConfigurationPerformanceOptions=my-cluster-b6310693, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-66f23d43, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-f50d425d, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5}
2022-03-28 10:58:21 [ForkJoinPool-1-worker-13] TRACE [AbstractST:607] USERS_NAME_MAP: {testScramUserWithQuotas=my-user-797280497-2138800976, testReceiveSimpleMessageTlsScramSha=my-user-1915916223-1265293973, testConfigurationFileIsCreated=my-user-180611180-50530491, testThrottlingQuotasCreateAlterPartitions=my-user-1933547616-1024431140, testSendSimpleMessageTls=my-user-738229993-1095121647, testCreatingUsersWithSecretPrefix=my-user-1450786684-1808961062, testThrottlingQuotasDeleteTopic=my-user-1558977140-777236615, testConfigurationReflection=my-user-1445653023-93200027, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-1260379615-1050949907, testUserWithNameMoreThan64Chars=my-user-1703808150-314159031, testDeployAndUnDeployCruiseControl=my-user-1988711145-439861161, testTlsUserWithQuotas=my-user-811912745-1441809840, testThrottlingQuotasCreateTopic=my-user-1131806865-947217230, testCapacityFile=my-user-937015144-1959439396, testReceiveSimpleMessageTls=my-user-6088058-1142028780, testConfigurationPerformanceOptions=my-user-930975101-640523779, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-1150765500-2076224625, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-745974575-1852384454, testSendSimpleMessageTlsScramSha=my-user-1499386234-1249154328}
2022-03-28 10:58:21 [ForkJoinPool-1-worker-13] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testScramUserWithQuotas=my-topic-2004840350-265526537, testReceiveSimpleMessageTlsScramSha=my-topic-1834944466-2094449929, testConfigurationFileIsCreated=my-topic-1296407588-313880708, testThrottlingQuotasCreateAlterPartitions=my-topic-609849288-1810034325, testSendSimpleMessageTls=my-topic-563023482-1697359353, testCreatingUsersWithSecretPrefix=my-topic-1479520843-618891199, testThrottlingQuotasDeleteTopic=my-topic-2128658681-1688111273, testConfigurationReflection=my-topic-640815392-1092406112, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-1674392683-1637010870, testUserWithNameMoreThan64Chars=my-topic-2021464661-1313035472, testDeployAndUnDeployCruiseControl=my-topic-1511079187-95055629, testTlsUserWithQuotas=my-topic-418284173-1689090259, testThrottlingQuotasCreateTopic=my-topic-728672490-1194703402, testCapacityFile=my-topic-1155641705-902584520, testReceiveSimpleMessageTls=my-topic-478514536-2132782608, testConfigurationPerformanceOptions=my-topic-213912446-1165431808, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-842198221-1227962115, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-254848724-189700675, testSendSimpleMessageTlsScramSha=my-topic-1028951580-1656411053}
2022-03-28 10:58:21 [ForkJoinPool-1-worker-13] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testScramUserWithQuotas=my-cluster-4941482a-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383-kafka-clients, testConfigurationFileIsCreated=my-cluster-e0fac774-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-28d9dd2e-kafka-clients, testSendSimpleMessageTls=my-cluster-27ddd6ba-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-8f262de5-kafka-clients, testConfigurationReflection=my-cluster-d1e2168e-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-da91577a-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-35e66bba-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-c0830cbe-kafka-clients, testTlsUserWithQuotas=my-cluster-d49e42e8-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8-kafka-clients, testCapacityFile=my-cluster-fd0fb61a-kafka-clients, testReceiveSimpleMessageTls=my-cluster-c6a15963-kafka-clients, testConfigurationPerformanceOptions=my-cluster-b6310693-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-66f23d43-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-f50d425d-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5-kafka-clients}
2022-03-28 10:58:21 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update KafkaUser user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq in namespace user-st
2022-03-28 10:58:21 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq
2022-03-28 10:58:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (800816ms till timeout)
2022-03-28 10:58:21 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:433] Wait for KafkaUser: user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq will have desired state: Ready
2022-03-28 10:58:21 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaUser: user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq will have desired state: Ready
2022-03-28 10:58:21 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] KafkaUser: user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq will have desired state: Ready not ready, will try again in 1000 ms (179971ms till timeout)
2022-03-28 10:58:21 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTopicModificationOfReplicationFactor is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:58:21 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (76976ms till timeout)
2022-03-28 10:58:21 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:58:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (215745ms till timeout)
2022-03-28 10:58:21 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:22 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (153736ms till timeout)
2022-03-28 10:58:22 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:22 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 10:58:22 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (457986ms till timeout)
2022-03-28 10:58:22 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (799743ms till timeout)
2022-03-28 10:58:22 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:444] KafkaUser: user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq is in desired state: Ready
2022-03-28 10:58:22 [ForkJoinPool-1-worker-13] INFO  [KafkaUserUtils:90] Wait until KafkaUser user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq status is available
2022-03-28 10:58:22 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaUser user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq status is available
2022-03-28 10:58:22 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (75975ms till timeout)
2022-03-28 10:58:22 [ForkJoinPool-1-worker-13] INFO  [KafkaUserUtils:95] KafkaUser user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq status is available
2022-03-28 10:58:22 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update KafkaUser sasl-userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef in namespace user-st
2022-03-28 10:58:22 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:sasl-userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef
2022-03-28 10:58:22 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:433] Wait for KafkaUser: sasl-userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef will have desired state: Ready
2022-03-28 10:58:22 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaUser: sasl-userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef will have desired state: Ready
2022-03-28 10:58:22 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] KafkaUser: sasl-userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef will have desired state: Ready not ready, will try again in 1000 ms (179974ms till timeout)
2022-03-28 10:58:22 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:58:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (214703ms till timeout)
2022-03-28 10:58:23 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:23 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (152580ms till timeout)
2022-03-28 10:58:23 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (798673ms till timeout)
2022-03-28 10:58:23 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:23 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 10:58:23 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (456743ms till timeout)
2022-03-28 10:58:23 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (74975ms till timeout)
2022-03-28 10:58:23 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:444] KafkaUser: sasl-userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef is in desired state: Ready
2022-03-28 10:58:23 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update KafkaUser userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdefghijk in namespace user-st
2022-03-28 10:58:23 [ForkJoinPool-1-worker-13] INFO  [KafkaUserUtils:90] Wait until KafkaUser userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdefghijk status is available
2022-03-28 10:58:23 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaUser userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdefghijk status is available
2022-03-28 10:58:23 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] KafkaUser userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdefghijk status is available not ready, will try again in 1000 ms (299961ms till timeout)
2022-03-28 10:58:23 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:58:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (213651ms till timeout)
2022-03-28 10:58:24 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (797619ms till timeout)
2022-03-28 10:58:24 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:24 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (73974ms till timeout)
2022-03-28 10:58:24 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (151405ms till timeout)
2022-03-28 10:58:24 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:24 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 10:58:24 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (455428ms till timeout)
2022-03-28 10:58:24 [ForkJoinPool-1-worker-7] TRACE [SuiteThreadController:210] testKafkaAdminTopicOperations is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:58:25 [ForkJoinPool-1-worker-13] INFO  [KafkaUserUtils:95] KafkaUser userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdefghijk status is available
2022-03-28 10:58:25 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:58:25 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:674] ============================================================================
2022-03-28 10:58:25 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:675] [operators.user.UserST - After Each] - Clean up after test
2022-03-28 10:58:25 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:344] ############################################################################
2022-03-28 10:58:25 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:348] Delete all resources for testUserWithNameMoreThan64Chars
2022-03-28 10:58:25 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of KafkaUser sasl-userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef in namespace user-st
2022-03-28 10:58:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (212592ms till timeout)
2022-03-28 10:58:25 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:sasl-userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef
2022-03-28 10:58:25 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:sasl-userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef not ready, will try again in 10000 ms (179815ms till timeout)
2022-03-28 10:58:25 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (72974ms till timeout)
2022-03-28 10:58:25 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (796565ms till timeout)
2022-03-28 10:58:25 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (150298ms till timeout)
2022-03-28 10:58:25 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:26 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:26 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 10:58:26 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (454178ms till timeout)
2022-03-28 10:58:26 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:58:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (211488ms till timeout)
2022-03-28 10:58:26 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTopicModificationOfReplicationFactor is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:58:26 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (71973ms till timeout)
2022-03-28 10:58:26 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (795410ms till timeout)
2022-03-28 10:58:26 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (149214ms till timeout)
2022-03-28 10:58:27 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:27 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:58:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (210401ms till timeout)
2022-03-28 10:58:27 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:27 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 10:58:27 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (452976ms till timeout)
2022-03-28 10:58:27 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (70973ms till timeout)
2022-03-28 10:58:27 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (794366ms till timeout)
2022-03-28 10:58:27 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (148163ms till timeout)
2022-03-28 10:58:28 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:28 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:58:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (209307ms till timeout)
2022-03-28 10:58:28 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (69972ms till timeout)
2022-03-28 10:58:28 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:28 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 10:58:28 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (451727ms till timeout)
2022-03-28 10:58:28 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (793318ms till timeout)
2022-03-28 10:58:29 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (146877ms till timeout)
2022-03-28 10:58:29 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:58:29 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (68972ms till timeout)
2022-03-28 10:58:29 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (208119ms till timeout)
2022-03-28 10:58:29 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:29 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 10:58:29 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (450486ms till timeout)
2022-03-28 10:58:29 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (792285ms till timeout)
2022-03-28 10:58:29 [ForkJoinPool-1-worker-7] TRACE [SuiteThreadController:210] testKafkaAdminTopicOperations is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:58:30 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (145773ms till timeout)
2022-03-28 10:58:30 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (67971ms till timeout)
2022-03-28 10:58:30 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:58:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (206994ms till timeout)
2022-03-28 10:58:30 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:30 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (791214ms till timeout)
2022-03-28 10:58:31 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:31 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 10:58:31 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (449105ms till timeout)
2022-03-28 10:58:31 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (144590ms till timeout)
2022-03-28 10:58:31 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTopicModificationOfReplicationFactor is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:58:31 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (66971ms till timeout)
2022-03-28 10:58:31 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:58:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (205700ms till timeout)
2022-03-28 10:58:31 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (790113ms till timeout)
2022-03-28 10:58:32 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:32 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (143420ms till timeout)
2022-03-28 10:58:32 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (65970ms till timeout)
2022-03-28 10:58:32 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:32 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 10:58:32 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (447583ms till timeout)
2022-03-28 10:58:33 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:58:33 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (789042ms till timeout)
2022-03-28 10:58:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (204598ms till timeout)
2022-03-28 10:58:33 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (64969ms till timeout)
2022-03-28 10:58:33 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (142344ms till timeout)
2022-03-28 10:58:33 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:33 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:33 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 10:58:33 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (446381ms till timeout)
2022-03-28 10:58:34 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (787971ms till timeout)
2022-03-28 10:58:34 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:58:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (203491ms till timeout)
2022-03-28 10:58:34 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (63969ms till timeout)
2022-03-28 10:58:34 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (141245ms till timeout)
2022-03-28 10:58:34 [ForkJoinPool-1-worker-7] TRACE [SuiteThreadController:210] testKafkaAdminTopicOperations is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:58:34 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:35 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:35 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 10:58:35 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (445130ms till timeout)
2022-03-28 10:58:35 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (786948ms till timeout)
2022-03-28 10:58:35 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:58:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (202424ms till timeout)
2022-03-28 10:58:35 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of KafkaUser userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdefghijk in namespace user-st
2022-03-28 10:58:35 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdefghijk
2022-03-28 10:58:35 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdefghijk not ready, will try again in 10000 ms (179880ms till timeout)
2022-03-28 10:58:35 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (62968ms till timeout)
2022-03-28 10:58:35 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (140127ms till timeout)
2022-03-28 10:58:36 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:36 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (785885ms till timeout)
2022-03-28 10:58:36 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:58:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (201344ms till timeout)
2022-03-28 10:58:36 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:36 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 10:58:36 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (443884ms till timeout)
2022-03-28 10:58:36 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTopicModificationOfReplicationFactor is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:58:36 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (61968ms till timeout)
2022-03-28 10:58:36 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (139008ms till timeout)
2022-03-28 10:58:37 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (784804ms till timeout)
2022-03-28 10:58:37 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:37 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:58:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (200258ms till timeout)
2022-03-28 10:58:37 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (60968ms till timeout)
2022-03-28 10:58:37 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:37 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 10:58:37 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (442686ms till timeout)
2022-03-28 10:58:37 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (137979ms till timeout)
2022-03-28 10:58:38 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (783773ms till timeout)
2022-03-28 10:58:38 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:58:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (199191ms till timeout)
2022-03-28 10:58:38 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (59967ms till timeout)
2022-03-28 10:58:38 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:38 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:38 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 10:58:38 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (441425ms till timeout)
2022-03-28 10:58:39 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (136932ms till timeout)
2022-03-28 10:58:39 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (782719ms till timeout)
2022-03-28 10:58:39 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:58:39 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (58967ms till timeout)
2022-03-28 10:58:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (198125ms till timeout)
2022-03-28 10:58:39 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:39 [ForkJoinPool-1-worker-7] TRACE [SuiteThreadController:210] testKafkaAdminTopicOperations is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:58:40 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:40 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 10:58:40 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (440216ms till timeout)
2022-03-28 10:58:40 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (135827ms till timeout)
2022-03-28 10:58:40 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (781704ms till timeout)
2022-03-28 10:58:40 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (57966ms till timeout)
2022-03-28 10:58:40 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:58:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (197044ms till timeout)
2022-03-28 10:58:41 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:41 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (134765ms till timeout)
2022-03-28 10:58:41 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:41 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 10:58:41 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (438947ms till timeout)
2022-03-28 10:58:41 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTopicModificationOfReplicationFactor is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:58:41 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (780665ms till timeout)
2022-03-28 10:58:41 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (56965ms till timeout)
2022-03-28 10:58:41 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:58:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (195963ms till timeout)
2022-03-28 10:58:42 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (133712ms till timeout)
2022-03-28 10:58:42 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:42 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (779622ms till timeout)
2022-03-28 10:58:42 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (55965ms till timeout)
2022-03-28 10:58:42 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:42 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 10:58:42 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (437638ms till timeout)
2022-03-28 10:58:42 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:58:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (194882ms till timeout)
2022-03-28 10:58:43 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (132633ms till timeout)
2022-03-28 10:58:43 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (778588ms till timeout)
2022-03-28 10:58:43 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (54964ms till timeout)
2022-03-28 10:58:43 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:43 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:58:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (193778ms till timeout)
2022-03-28 10:58:43 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 10:58:43 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 1
2022-03-28 10:58:43 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 10:58:43 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-5" not found
2022-03-28 10:58:43 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] ======STDERR END======
2022-03-28 10:58:43 [ForkJoinPool-1-worker-5] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[namespace-8], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[infra-namespace, reconciliation-st], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-3], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 10:58:43 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:267] testConfigurationPerformanceOptions - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTls, testSendSimpleMessageTlsScramSha, testConfigurationFileIsCreated, testReceiveSimpleMessageTls, testDeployAndUnDeployCruiseControl, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testConfigurationPerformanceOptions, testTopicModificationOfReplicationFactor, testKafkaAdminTopicOperations] to and randomly select one to start execution
2022-03-28 10:58:43 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:93] [cruisecontrol.CruiseControlConfigurationST] - Removing parallel test: testConfigurationPerformanceOptions
2022-03-28 10:58:43 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:97] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 7
2022-03-28 10:58:43 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationPerformanceOptions-FINISHED
2022-03-28 10:58:43 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:30] ############################################################################
2022-03-28 10:58:43 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:23] ############################################################################
2022-03-28 10:58:43 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.topic.TopicST.testSendingMessagesToNonExistingTopic-STARTED
2022-03-28 10:58:43 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:658] ============================================================================
2022-03-28 10:58:43 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:659] [operators.topic.TopicST - Before Each] - Setup test case environment
2022-03-28 10:58:43 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:77] [operators.topic.TopicST] - Adding parallel test: testSendingMessagesToNonExistingTopic
2022-03-28 10:58:43 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:81] [operators.topic.TopicST] - Parallel test count: 8
2022-03-28 10:58:43 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:205] [testSendingMessagesToNonExistingTopic] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:58:43 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testSendingMessagesToNonExistingTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:58:44 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (131462ms till timeout)
2022-03-28 10:58:44 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (53963ms till timeout)
2022-03-28 10:58:44 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (777522ms till timeout)
2022-03-28 10:58:44 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:230] testKafkaAdminTopicOperations test now can proceed its execution
2022-03-28 10:58:44 [ForkJoinPool-1-worker-7] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 10:58:44 [ForkJoinPool-1-worker-7] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testScramUserWithQuotas=my-cluster-4941482a, testKafkaAdminTopicOperations=my-cluster-5a0bfe14, testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383, testConfigurationFileIsCreated=my-cluster-e0fac774, testThrottlingQuotasCreateAlterPartitions=my-cluster-28d9dd2e, testSendSimpleMessageTls=my-cluster-27ddd6ba, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c, testThrottlingQuotasDeleteTopic=my-cluster-8f262de5, testConfigurationReflection=my-cluster-d1e2168e, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-da91577a, testUserWithNameMoreThan64Chars=my-cluster-35e66bba, testDeployAndUnDeployCruiseControl=my-cluster-c0830cbe, testTlsUserWithQuotas=my-cluster-d49e42e8, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8, testCapacityFile=my-cluster-fd0fb61a, testReceiveSimpleMessageTls=my-cluster-c6a15963, testConfigurationPerformanceOptions=my-cluster-b6310693, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-66f23d43, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-f50d425d, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5}
2022-03-28 10:58:44 [ForkJoinPool-1-worker-7] TRACE [AbstractST:607] USERS_NAME_MAP: {testScramUserWithQuotas=my-user-797280497-2138800976, testKafkaAdminTopicOperations=my-user-1708656731-1087276375, testReceiveSimpleMessageTlsScramSha=my-user-1915916223-1265293973, testConfigurationFileIsCreated=my-user-180611180-50530491, testThrottlingQuotasCreateAlterPartitions=my-user-1933547616-1024431140, testSendSimpleMessageTls=my-user-738229993-1095121647, testCreatingUsersWithSecretPrefix=my-user-1450786684-1808961062, testThrottlingQuotasDeleteTopic=my-user-1558977140-777236615, testConfigurationReflection=my-user-1445653023-93200027, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-1260379615-1050949907, testUserWithNameMoreThan64Chars=my-user-1703808150-314159031, testDeployAndUnDeployCruiseControl=my-user-1988711145-439861161, testTlsUserWithQuotas=my-user-811912745-1441809840, testThrottlingQuotasCreateTopic=my-user-1131806865-947217230, testCapacityFile=my-user-937015144-1959439396, testReceiveSimpleMessageTls=my-user-6088058-1142028780, testConfigurationPerformanceOptions=my-user-930975101-640523779, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-1150765500-2076224625, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-745974575-1852384454, testSendSimpleMessageTlsScramSha=my-user-1499386234-1249154328}
2022-03-28 10:58:44 [ForkJoinPool-1-worker-7] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testScramUserWithQuotas=my-topic-2004840350-265526537, testKafkaAdminTopicOperations=my-topic-1564170108-1493323202, testReceiveSimpleMessageTlsScramSha=my-topic-1834944466-2094449929, testConfigurationFileIsCreated=my-topic-1296407588-313880708, testThrottlingQuotasCreateAlterPartitions=my-topic-609849288-1810034325, testSendSimpleMessageTls=my-topic-563023482-1697359353, testCreatingUsersWithSecretPrefix=my-topic-1479520843-618891199, testThrottlingQuotasDeleteTopic=my-topic-2128658681-1688111273, testConfigurationReflection=my-topic-640815392-1092406112, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-1674392683-1637010870, testUserWithNameMoreThan64Chars=my-topic-2021464661-1313035472, testDeployAndUnDeployCruiseControl=my-topic-1511079187-95055629, testTlsUserWithQuotas=my-topic-418284173-1689090259, testThrottlingQuotasCreateTopic=my-topic-728672490-1194703402, testCapacityFile=my-topic-1155641705-902584520, testReceiveSimpleMessageTls=my-topic-478514536-2132782608, testConfigurationPerformanceOptions=my-topic-213912446-1165431808, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-842198221-1227962115, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-254848724-189700675, testSendSimpleMessageTlsScramSha=my-topic-1028951580-1656411053}
2022-03-28 10:58:44 [ForkJoinPool-1-worker-7] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testScramUserWithQuotas=my-cluster-4941482a-kafka-clients, testKafkaAdminTopicOperations=my-cluster-5a0bfe14-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383-kafka-clients, testConfigurationFileIsCreated=my-cluster-e0fac774-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-28d9dd2e-kafka-clients, testSendSimpleMessageTls=my-cluster-27ddd6ba-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-8f262de5-kafka-clients, testConfigurationReflection=my-cluster-d1e2168e-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-da91577a-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-35e66bba-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-c0830cbe-kafka-clients, testTlsUserWithQuotas=my-cluster-d49e42e8-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8-kafka-clients, testCapacityFile=my-cluster-fd0fb61a-kafka-clients, testReceiveSimpleMessageTls=my-cluster-c6a15963-kafka-clients, testConfigurationPerformanceOptions=my-cluster-b6310693-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-66f23d43-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-f50d425d-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5-kafka-clients}
2022-03-28 10:58:44 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update KafkaUser my-user-1708656731-1087276375 in namespace throttling-quota-st
2022-03-28 10:58:44 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:58:44 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-1708656731-1087276375
2022-03-28 10:58:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (192625ms till timeout)
2022-03-28 10:58:45 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-1708656731-1087276375 will have desired state: Ready
2022-03-28 10:58:45 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-1708656731-1087276375 will have desired state: Ready
2022-03-28 10:58:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaUser: my-user-1708656731-1087276375 will have desired state: Ready not ready, will try again in 1000 ms (179972ms till timeout)
2022-03-28 10:58:45 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of KafkaUser user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq in namespace user-st
2022-03-28 10:58:45 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (52962ms till timeout)
2022-03-28 10:58:45 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (130383ms till timeout)
2022-03-28 10:58:45 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq
2022-03-28 10:58:45 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (776453ms till timeout)
2022-03-28 10:58:45 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq not ready, will try again in 10000 ms (179874ms till timeout)
2022-03-28 10:58:46 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:58:46 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:444] KafkaUser: my-user-1708656731-1087276375 is in desired state: Ready
2022-03-28 10:58:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (191524ms till timeout)
2022-03-28 10:58:46 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update Job create-admin-my-cluster-5a0bfe14-kafka-clients in namespace throttling-quota-st
2022-03-28 10:58:46 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:create-admin-my-cluster-5a0bfe14-kafka-clients
2022-03-28 10:58:46 [ForkJoinPool-1-worker-7] INFO  [JobUtils:81] Waiting for job: create-admin-my-cluster-5a0bfe14-kafka-clients will be in active state
2022-03-28 10:58:46 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 10:58:46 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job active not ready, will try again in 1000 ms (179970ms till timeout)
2022-03-28 10:58:46 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTopicModificationOfReplicationFactor is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:58:46 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (51961ms till timeout)
2022-03-28 10:58:46 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (129311ms till timeout)
2022-03-28 10:58:46 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (775413ms till timeout)
2022-03-28 10:58:47 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:58:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (190379ms till timeout)
2022-03-28 10:58:47 [ForkJoinPool-1-worker-7] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 10:58:47 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 10:58:47 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (50960ms till timeout)
2022-03-28 10:58:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (119896ms till timeout)
2022-03-28 10:58:47 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (774385ms till timeout)
2022-03-28 10:58:47 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (128212ms till timeout)
2022-03-28 10:58:48 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:58:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (189305ms till timeout)
2022-03-28 10:58:48 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (49960ms till timeout)
2022-03-28 10:58:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (118826ms till timeout)
2022-03-28 10:58:48 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (773306ms till timeout)
2022-03-28 10:58:48 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (127130ms till timeout)
2022-03-28 10:58:48 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testSendingMessagesToNonExistingTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:58:49 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:58:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (188174ms till timeout)
2022-03-28 10:58:49 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (48959ms till timeout)
2022-03-28 10:58:49 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (117679ms till timeout)
2022-03-28 10:58:49 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (772221ms till timeout)
2022-03-28 10:58:49 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (126016ms till timeout)
2022-03-28 10:58:50 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:58:50 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (47958ms till timeout)
2022-03-28 10:58:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (187121ms till timeout)
2022-03-28 10:58:50 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (116601ms till timeout)
2022-03-28 10:58:50 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (771196ms till timeout)
2022-03-28 10:58:50 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (124972ms till timeout)
2022-03-28 10:58:51 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTopicModificationOfReplicationFactor is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:58:51 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (46958ms till timeout)
2022-03-28 10:58:51 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:58:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (186004ms till timeout)
2022-03-28 10:58:51 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (115567ms till timeout)
2022-03-28 10:58:51 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (770162ms till timeout)
2022-03-28 10:58:52 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (123930ms till timeout)
2022-03-28 10:58:52 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (45957ms till timeout)
2022-03-28 10:58:52 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:58:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (184905ms till timeout)
2022-03-28 10:58:52 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (114539ms till timeout)
2022-03-28 10:58:52 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (769148ms till timeout)
2022-03-28 10:58:53 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (122903ms till timeout)
2022-03-28 10:58:53 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (44957ms till timeout)
2022-03-28 10:58:53 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:58:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (183779ms till timeout)
2022-03-28 10:58:53 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testSendingMessagesToNonExistingTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:58:54 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (768098ms till timeout)
2022-03-28 10:58:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (113429ms till timeout)
2022-03-28 10:58:54 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (121844ms till timeout)
2022-03-28 10:58:54 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (43956ms till timeout)
2022-03-28 10:58:54 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:58:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (182680ms till timeout)
2022-03-28 10:58:55 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (767060ms till timeout)
2022-03-28 10:58:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (112377ms till timeout)
2022-03-28 10:58:55 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (120796ms till timeout)
2022-03-28 10:58:55 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (42956ms till timeout)
2022-03-28 10:58:55 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:369] ############################################################################
2022-03-28 10:58:55 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:267] testUserWithNameMoreThan64Chars - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTls, testSendSimpleMessageTlsScramSha, testConfigurationFileIsCreated, testReceiveSimpleMessageTls, testDeployAndUnDeployCruiseControl, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testConfigurationPerformanceOptions, testTopicModificationOfReplicationFactor, testSendingMessagesToNonExistingTopic] to and randomly select one to start execution
2022-03-28 10:58:55 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:93] [operators.user.UserST] - Removing parallel test: testUserWithNameMoreThan64Chars
2022-03-28 10:58:55 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:97] [operators.user.UserST] - Parallel test count: 7
2022-03-28 10:58:55 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testUserWithNameMoreThan64Chars-FINISHED
2022-03-28 10:58:55 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:30] ############################################################################
2022-03-28 10:58:55 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:23] ############################################################################
2022-03-28 10:58:55 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testTlsExternalUser-STARTED
2022-03-28 10:58:55 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:658] ============================================================================
2022-03-28 10:58:55 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:659] [operators.user.UserST - Before Each] - Setup test case environment
2022-03-28 10:58:55 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:77] [operators.user.UserST] - Adding parallel test: testTlsExternalUser
2022-03-28 10:58:55 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:81] [operators.user.UserST] - Parallel test count: 8
2022-03-28 10:58:55 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:205] [testTlsExternalUser] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:58:55 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testTlsExternalUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:58:56 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:58:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (181565ms till timeout)
2022-03-28 10:58:56 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (765984ms till timeout)
2022-03-28 10:58:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (111237ms till timeout)
2022-03-28 10:58:56 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (119717ms till timeout)
2022-03-28 10:58:56 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:230] testTopicModificationOfReplicationFactor test now can proceed its execution
2022-03-28 10:58:56 [ForkJoinPool-1-worker-3] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 10:58:56 [ForkJoinPool-1-worker-3] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testScramUserWithQuotas=my-cluster-4941482a, testTopicModificationOfReplicationFactor=my-cluster-38e659b2, testKafkaAdminTopicOperations=my-cluster-5a0bfe14, testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383, testConfigurationFileIsCreated=my-cluster-e0fac774, testThrottlingQuotasCreateAlterPartitions=my-cluster-28d9dd2e, testSendSimpleMessageTls=my-cluster-27ddd6ba, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c, testThrottlingQuotasDeleteTopic=my-cluster-8f262de5, testConfigurationReflection=my-cluster-d1e2168e, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-da91577a, testUserWithNameMoreThan64Chars=my-cluster-35e66bba, testDeployAndUnDeployCruiseControl=my-cluster-c0830cbe, testTlsUserWithQuotas=my-cluster-d49e42e8, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8, testCapacityFile=my-cluster-fd0fb61a, testReceiveSimpleMessageTls=my-cluster-c6a15963, testConfigurationPerformanceOptions=my-cluster-b6310693, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-66f23d43, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-f50d425d, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5}
2022-03-28 10:58:56 [ForkJoinPool-1-worker-3] TRACE [AbstractST:607] USERS_NAME_MAP: {testScramUserWithQuotas=my-user-797280497-2138800976, testTopicModificationOfReplicationFactor=my-user-2003504850-304348854, testKafkaAdminTopicOperations=my-user-1708656731-1087276375, testReceiveSimpleMessageTlsScramSha=my-user-1915916223-1265293973, testConfigurationFileIsCreated=my-user-180611180-50530491, testThrottlingQuotasCreateAlterPartitions=my-user-1933547616-1024431140, testSendSimpleMessageTls=my-user-738229993-1095121647, testCreatingUsersWithSecretPrefix=my-user-1450786684-1808961062, testThrottlingQuotasDeleteTopic=my-user-1558977140-777236615, testConfigurationReflection=my-user-1445653023-93200027, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-1260379615-1050949907, testUserWithNameMoreThan64Chars=my-user-1703808150-314159031, testDeployAndUnDeployCruiseControl=my-user-1988711145-439861161, testTlsUserWithQuotas=my-user-811912745-1441809840, testThrottlingQuotasCreateTopic=my-user-1131806865-947217230, testCapacityFile=my-user-937015144-1959439396, testReceiveSimpleMessageTls=my-user-6088058-1142028780, testConfigurationPerformanceOptions=my-user-930975101-640523779, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-1150765500-2076224625, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-745974575-1852384454, testSendSimpleMessageTlsScramSha=my-user-1499386234-1249154328}
2022-03-28 10:58:56 [ForkJoinPool-1-worker-3] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testScramUserWithQuotas=my-topic-2004840350-265526537, testTopicModificationOfReplicationFactor=my-topic-1789870897-1466152225, testKafkaAdminTopicOperations=my-topic-1564170108-1493323202, testReceiveSimpleMessageTlsScramSha=my-topic-1834944466-2094449929, testConfigurationFileIsCreated=my-topic-1296407588-313880708, testThrottlingQuotasCreateAlterPartitions=my-topic-609849288-1810034325, testSendSimpleMessageTls=my-topic-563023482-1697359353, testCreatingUsersWithSecretPrefix=my-topic-1479520843-618891199, testThrottlingQuotasDeleteTopic=my-topic-2128658681-1688111273, testConfigurationReflection=my-topic-640815392-1092406112, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-1674392683-1637010870, testUserWithNameMoreThan64Chars=my-topic-2021464661-1313035472, testDeployAndUnDeployCruiseControl=my-topic-1511079187-95055629, testTlsUserWithQuotas=my-topic-418284173-1689090259, testThrottlingQuotasCreateTopic=my-topic-728672490-1194703402, testCapacityFile=my-topic-1155641705-902584520, testReceiveSimpleMessageTls=my-topic-478514536-2132782608, testConfigurationPerformanceOptions=my-topic-213912446-1165431808, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-842198221-1227962115, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-254848724-189700675, testSendSimpleMessageTlsScramSha=my-topic-1028951580-1656411053}
2022-03-28 10:58:56 [ForkJoinPool-1-worker-3] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testScramUserWithQuotas=my-cluster-4941482a-kafka-clients, testTopicModificationOfReplicationFactor=my-cluster-38e659b2-kafka-clients, testKafkaAdminTopicOperations=my-cluster-5a0bfe14-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383-kafka-clients, testConfigurationFileIsCreated=my-cluster-e0fac774-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-28d9dd2e-kafka-clients, testSendSimpleMessageTls=my-cluster-27ddd6ba-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-8f262de5-kafka-clients, testConfigurationReflection=my-cluster-d1e2168e-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-da91577a-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-35e66bba-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-c0830cbe-kafka-clients, testTlsUserWithQuotas=my-cluster-d49e42e8-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8-kafka-clients, testCapacityFile=my-cluster-fd0fb61a-kafka-clients, testReceiveSimpleMessageTls=my-cluster-c6a15963-kafka-clients, testConfigurationPerformanceOptions=my-cluster-b6310693-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-66f23d43-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-f50d425d-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5-kafka-clients}
2022-03-28 10:58:56 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:155] Create/Update KafkaTopic my-topic-1789870897-1466152225 in namespace topic-st
2022-03-28 10:58:56 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (41955ms till timeout)
2022-03-28 10:58:56 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1789870897-1466152225
2022-03-28 10:58:56 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-1789870897-1466152225 will have desired state: Ready
2022-03-28 10:58:56 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-1789870897-1466152225 will have desired state: Ready
2022-03-28 10:58:56 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] KafkaTopic: my-topic-1789870897-1466152225 will have desired state: Ready not ready, will try again in 1000 ms (179956ms till timeout)
2022-03-28 10:58:57 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (764927ms till timeout)
2022-03-28 10:58:57 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:58:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (180448ms till timeout)
2022-03-28 10:58:57 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (118648ms till timeout)
2022-03-28 10:58:57 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (110118ms till timeout)
2022-03-28 10:58:57 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (40954ms till timeout)
2022-03-28 10:58:57 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:444] KafkaTopic: my-topic-1789870897-1466152225 is in desired state: Ready
2022-03-28 10:58:57 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-1789870897-1466152225 will have desired state: NotReady
2022-03-28 10:58:57 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-1789870897-1466152225 will have desired state: NotReady
2022-03-28 10:58:57 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] KafkaTopic: my-topic-1789870897-1466152225 will have desired state: NotReady not ready, will try again in 1000 ms (179907ms till timeout)
2022-03-28 10:58:58 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (763876ms till timeout)
2022-03-28 10:58:58 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:58:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (179327ms till timeout)
2022-03-28 10:58:58 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (117502ms till timeout)
2022-03-28 10:58:58 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (108995ms till timeout)
2022-03-28 10:58:58 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (39954ms till timeout)
2022-03-28 10:58:58 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testSendingMessagesToNonExistingTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:58:58 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:444] KafkaTopic: my-topic-1789870897-1466152225 is in desired state: NotReady
2022-03-28 10:58:59 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace topic-st delete kafkatopic my-topic-1789870897-1466152225
2022-03-28 10:58:59 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (762775ms till timeout)
2022-03-28 10:58:59 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:58:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (178242ms till timeout)
2022-03-28 10:58:59 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace topic-st delete kafkatopic my-topic-1789870897-1466152225
2022-03-28 10:58:59 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 10:58:59 [ForkJoinPool-1-worker-3] INFO  [KafkaTopicUtils:104] Waiting for KafkaTopic my-topic-1789870897-1466152225 deletion
2022-03-28 10:58:59 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for KafkaTopic deletion my-topic-1789870897-1466152225
2022-03-28 10:58:59 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:674] ============================================================================
2022-03-28 10:58:59 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:675] [operators.topic.TopicST - After Each] - Clean up after test
2022-03-28 10:58:59 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:344] ############################################################################
2022-03-28 10:58:59 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:348] Delete all resources for testTopicModificationOfReplicationFactor
2022-03-28 10:58:59 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:241] Delete of KafkaTopic my-topic-1789870897-1466152225 in namespace topic-st
2022-03-28 10:58:59 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (116401ms till timeout)
2022-03-28 10:58:59 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (107899ms till timeout)
2022-03-28 10:58:59 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (38953ms till timeout)
2022-03-28 10:58:59 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1789870897-1466152225
2022-03-28 10:58:59 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:369] ############################################################################
2022-03-28 10:58:59 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:267] testTopicModificationOfReplicationFactor - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTls, testSendSimpleMessageTlsScramSha, testConfigurationFileIsCreated, testReceiveSimpleMessageTls, testDeployAndUnDeployCruiseControl, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testConfigurationPerformanceOptions, testSendingMessagesToNonExistingTopic, testTlsExternalUser] to and randomly select one to start execution
2022-03-28 10:58:59 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:93] [operators.topic.TopicST] - Removing parallel test: testTopicModificationOfReplicationFactor
2022-03-28 10:58:59 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:97] [operators.topic.TopicST] - Parallel test count: 7
2022-03-28 10:58:59 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.topic.TopicST.testTopicModificationOfReplicationFactor-FINISHED
2022-03-28 10:58:59 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:30] ############################################################################
2022-03-28 10:58:59 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:23] ############################################################################
2022-03-28 10:58:59 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testTlsExternalUserWithQuotas-STARTED
2022-03-28 10:58:59 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:658] ============================================================================
2022-03-28 10:58:59 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:659] [operators.user.UserST - Before Each] - Setup test case environment
2022-03-28 10:58:59 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:77] [operators.user.UserST] - Adding parallel test: testTlsExternalUserWithQuotas
2022-03-28 10:58:59 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:81] [operators.user.UserST] - Parallel test count: 8
2022-03-28 10:58:59 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:205] [testTlsExternalUserWithQuotas] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:58:59 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:59:00 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (761746ms till timeout)
2022-03-28 10:59:00 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:59:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (177161ms till timeout)
2022-03-28 10:59:00 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (37953ms till timeout)
2022-03-28 10:59:00 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (115347ms till timeout)
2022-03-28 10:59:00 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (106771ms till timeout)
2022-03-28 10:59:00 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:230] testTlsExternalUser test now can proceed its execution
2022-03-28 10:59:00 [ForkJoinPool-1-worker-13] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 10:59:00 [ForkJoinPool-1-worker-13] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testScramUserWithQuotas=my-cluster-4941482a, testTopicModificationOfReplicationFactor=my-cluster-38e659b2, testKafkaAdminTopicOperations=my-cluster-5a0bfe14, testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383, testConfigurationFileIsCreated=my-cluster-e0fac774, testThrottlingQuotasCreateAlterPartitions=my-cluster-28d9dd2e, testSendSimpleMessageTls=my-cluster-27ddd6ba, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c, testThrottlingQuotasDeleteTopic=my-cluster-8f262de5, testConfigurationReflection=my-cluster-d1e2168e, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-da91577a, testUserWithNameMoreThan64Chars=my-cluster-35e66bba, testDeployAndUnDeployCruiseControl=my-cluster-c0830cbe, testTlsUserWithQuotas=my-cluster-d49e42e8, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8, testCapacityFile=my-cluster-fd0fb61a, testReceiveSimpleMessageTls=my-cluster-c6a15963, testConfigurationPerformanceOptions=my-cluster-b6310693, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-66f23d43, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-f50d425d, testTlsExternalUser=my-cluster-3ba9cc5b, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5}
2022-03-28 10:59:00 [ForkJoinPool-1-worker-13] TRACE [AbstractST:607] USERS_NAME_MAP: {testScramUserWithQuotas=my-user-797280497-2138800976, testTopicModificationOfReplicationFactor=my-user-2003504850-304348854, testKafkaAdminTopicOperations=my-user-1708656731-1087276375, testReceiveSimpleMessageTlsScramSha=my-user-1915916223-1265293973, testConfigurationFileIsCreated=my-user-180611180-50530491, testThrottlingQuotasCreateAlterPartitions=my-user-1933547616-1024431140, testSendSimpleMessageTls=my-user-738229993-1095121647, testCreatingUsersWithSecretPrefix=my-user-1450786684-1808961062, testThrottlingQuotasDeleteTopic=my-user-1558977140-777236615, testConfigurationReflection=my-user-1445653023-93200027, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-1260379615-1050949907, testUserWithNameMoreThan64Chars=my-user-1703808150-314159031, testDeployAndUnDeployCruiseControl=my-user-1988711145-439861161, testTlsUserWithQuotas=my-user-811912745-1441809840, testThrottlingQuotasCreateTopic=my-user-1131806865-947217230, testCapacityFile=my-user-937015144-1959439396, testReceiveSimpleMessageTls=my-user-6088058-1142028780, testConfigurationPerformanceOptions=my-user-930975101-640523779, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-1150765500-2076224625, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-745974575-1852384454, testTlsExternalUser=my-user-1382292264-1641298587, testSendSimpleMessageTlsScramSha=my-user-1499386234-1249154328}
2022-03-28 10:59:00 [ForkJoinPool-1-worker-13] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testScramUserWithQuotas=my-topic-2004840350-265526537, testTopicModificationOfReplicationFactor=my-topic-1789870897-1466152225, testKafkaAdminTopicOperations=my-topic-1564170108-1493323202, testReceiveSimpleMessageTlsScramSha=my-topic-1834944466-2094449929, testConfigurationFileIsCreated=my-topic-1296407588-313880708, testThrottlingQuotasCreateAlterPartitions=my-topic-609849288-1810034325, testSendSimpleMessageTls=my-topic-563023482-1697359353, testCreatingUsersWithSecretPrefix=my-topic-1479520843-618891199, testThrottlingQuotasDeleteTopic=my-topic-2128658681-1688111273, testConfigurationReflection=my-topic-640815392-1092406112, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-1674392683-1637010870, testUserWithNameMoreThan64Chars=my-topic-2021464661-1313035472, testDeployAndUnDeployCruiseControl=my-topic-1511079187-95055629, testTlsUserWithQuotas=my-topic-418284173-1689090259, testThrottlingQuotasCreateTopic=my-topic-728672490-1194703402, testCapacityFile=my-topic-1155641705-902584520, testReceiveSimpleMessageTls=my-topic-478514536-2132782608, testConfigurationPerformanceOptions=my-topic-213912446-1165431808, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-842198221-1227962115, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-254848724-189700675, testTlsExternalUser=my-topic-5642373-135861890, testSendSimpleMessageTlsScramSha=my-topic-1028951580-1656411053}
2022-03-28 10:59:00 [ForkJoinPool-1-worker-13] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testScramUserWithQuotas=my-cluster-4941482a-kafka-clients, testTopicModificationOfReplicationFactor=my-cluster-38e659b2-kafka-clients, testKafkaAdminTopicOperations=my-cluster-5a0bfe14-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383-kafka-clients, testConfigurationFileIsCreated=my-cluster-e0fac774-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-28d9dd2e-kafka-clients, testSendSimpleMessageTls=my-cluster-27ddd6ba-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-8f262de5-kafka-clients, testConfigurationReflection=my-cluster-d1e2168e-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-da91577a-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-35e66bba-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-c0830cbe-kafka-clients, testTlsUserWithQuotas=my-cluster-d49e42e8-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8-kafka-clients, testCapacityFile=my-cluster-fd0fb61a-kafka-clients, testReceiveSimpleMessageTls=my-cluster-c6a15963-kafka-clients, testConfigurationPerformanceOptions=my-cluster-b6310693-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-66f23d43-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-f50d425d-kafka-clients, testTlsExternalUser=my-cluster-3ba9cc5b-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5-kafka-clients}
2022-03-28 10:59:00 [ForkJoinPool-1-worker-13] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-9 for test case:testTlsExternalUser
2022-03-28 10:59:00 [ForkJoinPool-1-worker-13] INFO  [KubeClusterResource:156] Creating Namespace: namespace-9
2022-03-28 10:59:00 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Namespace namespace-9
2022-03-28 10:59:00 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-9 -o json
2022-03-28 10:59:01 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-9 -o json
2022-03-28 10:59:01 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 10:59:01 [ForkJoinPool-1-worker-13] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c28,c12",
            "openshift.io/sa.scc.supplemental-groups": "1000780000/10000",
            "openshift.io/sa.scc.uid-range": "1000780000/10000"
        },
        "creationTimestamp": "2022-03-28T10:58:58Z",
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T10:58:28Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:status": {
                        "f:phase": {}
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T10:58:58Z"
            }
        ],
        "name": "namespace-9",
        "resourceVersion": "1876601",
        "selfLink": "/api/v1/namespaces/namespace-9",
        "uid": "de93f6e1-807b-4065-93be-4a7bbce5e326"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 10:59:01 [ForkJoinPool-1-worker-13] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[namespace-8], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@5c7379cb=[namespace-9], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[infra-namespace, reconciliation-st], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-3], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 10:59:01 [ForkJoinPool-1-worker-13] INFO  [KubeClusterResource:82] Client use Namespace: namespace-9
2022-03-28 10:59:01 [ForkJoinPool-1-worker-13] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-9, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 10:59:01 [ForkJoinPool-1-worker-13] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-9
2022-03-28 10:59:01 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-3ba9cc5b in namespace namespace-9
2022-03-28 10:59:01 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:164] Using Namespace: namespace-9
2022-03-28 10:59:01 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-3ba9cc5b
2022-03-28 10:59:01 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-3ba9cc5b will have desired state: Ready
2022-03-28 10:59:01 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-3ba9cc5b will have desired state: Ready
2022-03-28 10:59:01 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (760690ms till timeout)
2022-03-28 10:59:01 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (839930ms till timeout)
2022-03-28 10:59:01 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (36952ms till timeout)
2022-03-28 10:59:01 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:59:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (176075ms till timeout)
2022-03-28 10:59:01 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (114189ms till timeout)
2022-03-28 10:59:01 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (105662ms till timeout)
2022-03-28 10:59:02 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (759640ms till timeout)
2022-03-28 10:59:02 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (838841ms till timeout)
2022-03-28 10:59:02 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (35952ms till timeout)
2022-03-28 10:59:02 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:59:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (174929ms till timeout)
2022-03-28 10:59:02 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (113112ms till timeout)
2022-03-28 10:59:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (104615ms till timeout)
2022-03-28 10:59:03 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (758599ms till timeout)
2022-03-28 10:59:03 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (837827ms till timeout)
2022-03-28 10:59:03 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (34951ms till timeout)
2022-03-28 10:59:03 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:59:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (173844ms till timeout)
2022-03-28 10:59:03 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (112065ms till timeout)
2022-03-28 10:59:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (103549ms till timeout)
2022-03-28 10:59:03 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testSendingMessagesToNonExistingTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:59:04 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (33951ms till timeout)
2022-03-28 10:59:04 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (757541ms till timeout)
2022-03-28 10:59:04 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:59:04 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (836730ms till timeout)
2022-03-28 10:59:04 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:59:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (172761ms till timeout)
2022-03-28 10:59:04 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (111021ms till timeout)
2022-03-28 10:59:04 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (102487ms till timeout)
2022-03-28 10:59:05 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (32950ms till timeout)
2022-03-28 10:59:05 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (756509ms till timeout)
2022-03-28 10:59:05 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (835710ms till timeout)
2022-03-28 10:59:05 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:59:05 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (109961ms till timeout)
2022-03-28 10:59:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (171681ms till timeout)
2022-03-28 10:59:06 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (101422ms till timeout)
2022-03-28 10:59:06 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (31950ms till timeout)
2022-03-28 10:59:06 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (755464ms till timeout)
2022-03-28 10:59:06 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (834668ms till timeout)
2022-03-28 10:59:07 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (108901ms till timeout)
2022-03-28 10:59:07 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:59:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (100376ms till timeout)
2022-03-28 10:59:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (170585ms till timeout)
2022-03-28 10:59:07 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (30949ms till timeout)
2022-03-28 10:59:07 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (754407ms till timeout)
2022-03-28 10:59:07 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (833595ms till timeout)
2022-03-28 10:59:08 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:59:08 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (99173ms till timeout)
2022-03-28 10:59:08 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (107667ms till timeout)
2022-03-28 10:59:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (169384ms till timeout)
2022-03-28 10:59:08 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (29949ms till timeout)
2022-03-28 10:59:08 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (753374ms till timeout)
2022-03-28 10:59:08 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (832554ms till timeout)
2022-03-28 10:59:08 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testSendingMessagesToNonExistingTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:59:09 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:59:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (98060ms till timeout)
2022-03-28 10:59:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (168239ms till timeout)
2022-03-28 10:59:09 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (106493ms till timeout)
2022-03-28 10:59:09 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (28948ms till timeout)
2022-03-28 10:59:09 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:59:09 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (752318ms till timeout)
2022-03-28 10:59:09 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (831534ms till timeout)
2022-03-28 10:59:10 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (97030ms till timeout)
2022-03-28 10:59:10 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:59:10 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (105426ms till timeout)
2022-03-28 10:59:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (167150ms till timeout)
2022-03-28 10:59:10 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (27948ms till timeout)
2022-03-28 10:59:10 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (751269ms till timeout)
2022-03-28 10:59:10 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (830476ms till timeout)
2022-03-28 10:59:11 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (95954ms till timeout)
2022-03-28 10:59:11 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (26947ms till timeout)
2022-03-28 10:59:11 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:59:11 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (104360ms till timeout)
2022-03-28 10:59:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (166088ms till timeout)
2022-03-28 10:59:11 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (750244ms till timeout)
2022-03-28 10:59:11 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (829458ms till timeout)
2022-03-28 10:59:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (94908ms till timeout)
2022-03-28 10:59:12 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (25947ms till timeout)
2022-03-28 10:59:12 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:59:12 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (103288ms till timeout)
2022-03-28 10:59:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (164991ms till timeout)
2022-03-28 10:59:12 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (749171ms till timeout)
2022-03-28 10:59:12 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (828406ms till timeout)
2022-03-28 10:59:13 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (24946ms till timeout)
2022-03-28 10:59:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (93820ms till timeout)
2022-03-28 10:59:13 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:59:13 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (102191ms till timeout)
2022-03-28 10:59:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (163879ms till timeout)
2022-03-28 10:59:13 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testSendingMessagesToNonExistingTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:59:13 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (748116ms till timeout)
2022-03-28 10:59:14 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (827335ms till timeout)
2022-03-28 10:59:14 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (23946ms till timeout)
2022-03-28 10:59:14 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:59:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (92748ms till timeout)
2022-03-28 10:59:14 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (101140ms till timeout)
2022-03-28 10:59:14 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:59:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (162819ms till timeout)
2022-03-28 10:59:15 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (747076ms till timeout)
2022-03-28 10:59:15 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (826283ms till timeout)
2022-03-28 10:59:15 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (22945ms till timeout)
2022-03-28 10:59:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (91649ms till timeout)
2022-03-28 10:59:15 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (100087ms till timeout)
2022-03-28 10:59:15 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:59:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (161728ms till timeout)
2022-03-28 10:59:16 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (746033ms till timeout)
2022-03-28 10:59:16 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (825238ms till timeout)
2022-03-28 10:59:16 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (21944ms till timeout)
2022-03-28 10:59:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (90580ms till timeout)
2022-03-28 10:59:16 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (98958ms till timeout)
2022-03-28 10:59:17 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:59:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (160588ms till timeout)
2022-03-28 10:59:17 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (745005ms till timeout)
2022-03-28 10:59:17 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (824206ms till timeout)
2022-03-28 10:59:17 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (20944ms till timeout)
2022-03-28 10:59:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (89462ms till timeout)
2022-03-28 10:59:18 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:59:18 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (97826ms till timeout)
2022-03-28 10:59:18 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (743971ms till timeout)
2022-03-28 10:59:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (159515ms till timeout)
2022-03-28 10:59:18 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (823166ms till timeout)
2022-03-28 10:59:18 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (19943ms till timeout)
2022-03-28 10:59:18 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testSendingMessagesToNonExistingTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:59:19 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (88362ms till timeout)
2022-03-28 10:59:19 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (96762ms till timeout)
2022-03-28 10:59:19 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (742920ms till timeout)
2022-03-28 10:59:19 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:59:19 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (822127ms till timeout)
2022-03-28 10:59:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (158444ms till timeout)
2022-03-28 10:59:19 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (18943ms till timeout)
2022-03-28 10:59:19 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:59:20 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (821090ms till timeout)
2022-03-28 10:59:20 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (87180ms till timeout)
2022-03-28 10:59:20 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (95675ms till timeout)
2022-03-28 10:59:20 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (741803ms till timeout)
2022-03-28 10:59:20 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:59:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (157305ms till timeout)
2022-03-28 10:59:20 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (17942ms till timeout)
2022-03-28 10:59:21 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (820019ms till timeout)
2022-03-28 10:59:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (740710ms till timeout)
2022-03-28 10:59:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (86034ms till timeout)
2022-03-28 10:59:21 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (94529ms till timeout)
2022-03-28 10:59:21 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:59:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (156132ms till timeout)
2022-03-28 10:59:21 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (16941ms till timeout)
2022-03-28 10:59:22 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (818955ms till timeout)
2022-03-28 10:59:22 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (739660ms till timeout)
2022-03-28 10:59:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (84950ms till timeout)
2022-03-28 10:59:22 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (93415ms till timeout)
2022-03-28 10:59:22 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (15941ms till timeout)
2022-03-28 10:59:22 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:59:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (154997ms till timeout)
2022-03-28 10:59:23 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (817936ms till timeout)
2022-03-28 10:59:23 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (738625ms till timeout)
2022-03-28 10:59:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (83884ms till timeout)
2022-03-28 10:59:23 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (14941ms till timeout)
2022-03-28 10:59:23 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (92336ms till timeout)
2022-03-28 10:59:23 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:59:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (153934ms till timeout)
2022-03-28 10:59:23 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testSendingMessagesToNonExistingTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:59:24 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (816871ms till timeout)
2022-03-28 10:59:24 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (737553ms till timeout)
2022-03-28 10:59:24 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (13940ms till timeout)
2022-03-28 10:59:24 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:59:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (82818ms till timeout)
2022-03-28 10:59:24 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (91277ms till timeout)
2022-03-28 10:59:24 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:59:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (152861ms till timeout)
2022-03-28 10:59:25 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (815847ms till timeout)
2022-03-28 10:59:25 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (12940ms till timeout)
2022-03-28 10:59:25 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (736436ms till timeout)
2022-03-28 10:59:25 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (81755ms till timeout)
2022-03-28 10:59:25 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (90061ms till timeout)
2022-03-28 10:59:25 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:59:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (151750ms till timeout)
2022-03-28 10:59:26 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (814787ms till timeout)
2022-03-28 10:59:26 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (11939ms till timeout)
2022-03-28 10:59:26 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (735362ms till timeout)
2022-03-28 10:59:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (80528ms till timeout)
2022-03-28 10:59:26 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:59:27 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (88899ms till timeout)
2022-03-28 10:59:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (150592ms till timeout)
2022-03-28 10:59:27 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (10939ms till timeout)
2022-03-28 10:59:27 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (813666ms till timeout)
2022-03-28 10:59:27 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (734291ms till timeout)
2022-03-28 10:59:28 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (79401ms till timeout)
2022-03-28 10:59:28 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (87796ms till timeout)
2022-03-28 10:59:28 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:59:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (149446ms till timeout)
2022-03-28 10:59:28 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (9938ms till timeout)
2022-03-28 10:59:28 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (812645ms till timeout)
2022-03-28 10:59:28 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (733229ms till timeout)
2022-03-28 10:59:28 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testSendingMessagesToNonExistingTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:59:29 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:59:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (78141ms till timeout)
2022-03-28 10:59:29 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (86635ms till timeout)
2022-03-28 10:59:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (148326ms till timeout)
2022-03-28 10:59:29 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (8937ms till timeout)
2022-03-28 10:59:29 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:59:29 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (811558ms till timeout)
2022-03-28 10:59:29 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (732186ms till timeout)
2022-03-28 10:59:30 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (77026ms till timeout)
2022-03-28 10:59:30 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:59:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (147210ms till timeout)
2022-03-28 10:59:30 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (85453ms till timeout)
2022-03-28 10:59:30 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (7937ms till timeout)
2022-03-28 10:59:30 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (810510ms till timeout)
2022-03-28 10:59:31 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (731108ms till timeout)
2022-03-28 10:59:31 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (75927ms till timeout)
2022-03-28 10:59:31 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:59:31 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (84403ms till timeout)
2022-03-28 10:59:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (146123ms till timeout)
2022-03-28 10:59:31 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (6936ms till timeout)
2022-03-28 10:59:31 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (809471ms till timeout)
2022-03-28 10:59:32 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (730074ms till timeout)
2022-03-28 10:59:32 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (5936ms till timeout)
2022-03-28 10:59:32 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (74866ms till timeout)
2022-03-28 10:59:32 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:59:32 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (83318ms till timeout)
2022-03-28 10:59:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (145013ms till timeout)
2022-03-28 10:59:32 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (808440ms till timeout)
2022-03-28 10:59:33 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (728999ms till timeout)
2022-03-28 10:59:33 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (4935ms till timeout)
2022-03-28 10:59:33 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (73763ms till timeout)
2022-03-28 10:59:33 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (82207ms till timeout)
2022-03-28 10:59:33 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:59:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (143862ms till timeout)
2022-03-28 10:59:33 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testSendingMessagesToNonExistingTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:59:33 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (807356ms till timeout)
2022-03-28 10:59:34 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (727961ms till timeout)
2022-03-28 10:59:34 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (3935ms till timeout)
2022-03-28 10:59:34 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:59:34 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (72701ms till timeout)
2022-03-28 10:59:34 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (81125ms till timeout)
2022-03-28 10:59:34 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:59:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (142771ms till timeout)
2022-03-28 10:59:35 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (806289ms till timeout)
2022-03-28 10:59:35 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (726906ms till timeout)
2022-03-28 10:59:35 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (2934ms till timeout)
2022-03-28 10:59:35 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (71573ms till timeout)
2022-03-28 10:59:35 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (80044ms till timeout)
2022-03-28 10:59:35 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:59:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (141708ms till timeout)
2022-03-28 10:59:36 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (805260ms till timeout)
2022-03-28 10:59:36 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (725866ms till timeout)
2022-03-28 10:59:36 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (1934ms till timeout)
2022-03-28 10:59:36 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (70510ms till timeout)
2022-03-28 10:59:36 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (78950ms till timeout)
2022-03-28 10:59:37 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:59:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (140657ms till timeout)
2022-03-28 10:59:37 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (804234ms till timeout)
2022-03-28 10:59:37 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (724792ms till timeout)
2022-03-28 10:59:37 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties not ready, will try again in 933 ms (933ms till timeout)
2022-03-28 10:59:38 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (69417ms till timeout)
2022-03-28 10:59:38 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (77872ms till timeout)
2022-03-28 10:59:38 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:59:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (139557ms till timeout)
2022-03-28 10:59:38 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (803170ms till timeout)
2022-03-28 10:59:38 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (723700ms till timeout)
2022-03-28 10:59:38 [ForkJoinPool-1-worker-1] ERROR [TestUtils:162] Exception waiting for Verify that kafka configuration {cluster-name=my-cluster-c0830cbe} has correct cruise control metric reporter properties, null
2022-03-28 10:59:38 [ForkJoinPool-1-worker-1] INFO  [CruiseControlConfigurationST:126] Cruise Control topics will not be deleted and will stay in the Kafka cluster
2022-03-28 10:59:38 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Verify that kafka contains cruise control topics with related configuration.
2022-03-28 10:59:38 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Verify that kafka contains cruise control topics with related configuration.
2022-03-28 10:59:38 [ForkJoinPool-1-worker-1] INFO  [CruiseControlConfigurationST:130] Adding Cruise Control to the classic Kafka.
2022-03-28 10:59:38 [ForkJoinPool-1-worker-1] INFO  [RollingUpdateUtils:73] Waiting for component with name: my-cluster-c0830cbe-kafka rolling update
2022-03-28 10:59:38 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:74] Waiting for rolling update of component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})
2022-03-28 10:59:38 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for component with name my-cluster-c0830cbe-kafka rolling update
2022-03-28 10:59:38 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 10:59:38 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 10:59:38 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 10:59:38 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-2 hasn't rolled
2022-03-28 10:59:38 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1799976ms till timeout)
2022-03-28 10:59:38 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testSendingMessagesToNonExistingTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:59:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (68377ms till timeout)
2022-03-28 10:59:39 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (76812ms till timeout)
2022-03-28 10:59:39 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:59:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (138506ms till timeout)
2022-03-28 10:59:39 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (802152ms till timeout)
2022-03-28 10:59:39 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (722620ms till timeout)
2022-03-28 10:59:39 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:59:40 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (67335ms till timeout)
2022-03-28 10:59:40 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (75757ms till timeout)
2022-03-28 10:59:40 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:59:40 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (801133ms till timeout)
2022-03-28 10:59:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (137460ms till timeout)
2022-03-28 10:59:40 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (721580ms till timeout)
2022-03-28 10:59:41 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (66256ms till timeout)
2022-03-28 10:59:41 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (74711ms till timeout)
2022-03-28 10:59:41 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:59:41 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (800078ms till timeout)
2022-03-28 10:59:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (136379ms till timeout)
2022-03-28 10:59:41 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (720507ms till timeout)
2022-03-28 10:59:42 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (65204ms till timeout)
2022-03-28 10:59:42 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (73633ms till timeout)
2022-03-28 10:59:42 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (799023ms till timeout)
2022-03-28 10:59:42 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:59:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (135296ms till timeout)
2022-03-28 10:59:42 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (719489ms till timeout)
2022-03-28 10:59:43 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (64100ms till timeout)
2022-03-28 10:59:43 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (797903ms till timeout)
2022-03-28 10:59:43 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:59:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (134162ms till timeout)
2022-03-28 10:59:43 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (72408ms till timeout)
2022-03-28 10:59:43 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (718429ms till timeout)
2022-03-28 10:59:43 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 10:59:43 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 10:59:43 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 10:59:43 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-2 hasn't rolled
2022-03-28 10:59:43 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1794884ms till timeout)
2022-03-28 10:59:43 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testSendingMessagesToNonExistingTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:59:44 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (63021ms till timeout)
2022-03-28 10:59:44 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (796806ms till timeout)
2022-03-28 10:59:44 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:59:44 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:59:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (133054ms till timeout)
2022-03-28 10:59:44 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (71284ms till timeout)
2022-03-28 10:59:44 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (717358ms till timeout)
2022-03-28 10:59:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (61988ms till timeout)
2022-03-28 10:59:45 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (795774ms till timeout)
2022-03-28 10:59:45 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:59:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (131998ms till timeout)
2022-03-28 10:59:45 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (70193ms till timeout)
2022-03-28 10:59:45 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (716336ms till timeout)
2022-03-28 10:59:46 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (60905ms till timeout)
2022-03-28 10:59:46 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (794727ms till timeout)
2022-03-28 10:59:46 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:59:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (130963ms till timeout)
2022-03-28 10:59:46 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (69121ms till timeout)
2022-03-28 10:59:46 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:444] Kafka: my-cluster-66f23d43 is in desired state: Ready
2022-03-28 10:59:46 [ForkJoinPool-1-worker-15] INFO  [ReconciliationST:80] Adding pause annotation into Kafka resource and also scaling replicas to 4, new pod should not appear
2022-03-28 10:59:47 [ForkJoinPool-1-worker-15] INFO  [ReconciliationST:86] Kafka should contain status with ReconciliationPaused
2022-03-28 10:59:47 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-66f23d43 will have desired state: ReconciliationPaused
2022-03-28 10:59:47 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-66f23d43 will have desired state: ReconciliationPaused
2022-03-28 10:59:47 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: ReconciliationPaused not ready, will try again in 1000 ms (839957ms till timeout)
2022-03-28 10:59:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (59851ms till timeout)
2022-03-28 10:59:47 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (793667ms till timeout)
2022-03-28 10:59:47 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:59:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129884ms till timeout)
2022-03-28 10:59:47 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (68036ms till timeout)
2022-03-28 10:59:48 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: ReconciliationPaused not ready, will try again in 1000 ms (838935ms till timeout)
2022-03-28 10:59:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (58776ms till timeout)
2022-03-28 10:59:48 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (792634ms till timeout)
2022-03-28 10:59:48 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 10:59:48 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:59:48 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 10:59:48 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 10:59:48 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-2 hasn't rolled
2022-03-28 10:59:48 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1789844ms till timeout)
2022-03-28 10:59:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (128833ms till timeout)
2022-03-28 10:59:48 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (66990ms till timeout)
2022-03-28 10:59:48 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testSendingMessagesToNonExistingTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:59:49 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: ReconciliationPaused not ready, will try again in 1000 ms (837746ms till timeout)
2022-03-28 10:59:49 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:59:49 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (791527ms till timeout)
2022-03-28 10:59:49 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (57544ms till timeout)
2022-03-28 10:59:49 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:59:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (127758ms till timeout)
2022-03-28 10:59:50 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (65877ms till timeout)
2022-03-28 10:59:50 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: ReconciliationPaused not ready, will try again in 1000 ms (836715ms till timeout)
2022-03-28 10:59:50 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (790486ms till timeout)
2022-03-28 10:59:50 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (56517ms till timeout)
2022-03-28 10:59:50 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:59:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (126687ms till timeout)
2022-03-28 10:59:51 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (64812ms till timeout)
2022-03-28 10:59:51 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: ReconciliationPaused not ready, will try again in 1000 ms (835673ms till timeout)
2022-03-28 10:59:51 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (789456ms till timeout)
2022-03-28 10:59:52 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (55447ms till timeout)
2022-03-28 10:59:52 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T10:59:49Z, conditions=[JobCondition(lastProbeTime=2022-03-28T10:59:49Z, lastTransitionTime=2022-03-28T10:59:49Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T10:58:14Z, succeeded=1, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:59:52 [ForkJoinPool-1-worker-9] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 10:59:52 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 10:59:52 [ForkJoinPool-1-worker-9] INFO  [PodUtils:189] Message All topics created found in create-admin-my-cluster-8f262de5-kafka-clients-cnsbn log
2022-03-28 10:59:52 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (63711ms till timeout)
2022-03-28 10:59:52 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment create-admin-my-cluster-8f262de5-kafka-clients deletion
2022-03-28 10:59:52 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for ReplicaSet create-admin-my-cluster-8f262de5-kafka-clients to be deleted
2022-03-28 10:59:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] ReplicaSet create-admin-my-cluster-8f262de5-kafka-clients to be deleted not ready, will try again in 5000 ms (179975ms till timeout)
2022-03-28 10:59:52 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: ReconciliationPaused not ready, will try again in 1000 ms (834643ms till timeout)
2022-03-28 10:59:52 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (788409ms till timeout)
2022-03-28 10:59:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (54364ms till timeout)
2022-03-28 10:59:53 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (62633ms till timeout)
2022-03-28 10:59:53 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-66f23d43 will have desired state: ReconciliationPaused not ready, will try again in 1000 ms (833611ms till timeout)
2022-03-28 10:59:53 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 10:59:53 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 10:59:53 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 10:59:53 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-2 hasn't rolled
2022-03-28 10:59:53 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1784800ms till timeout)
2022-03-28 10:59:53 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testSendingMessagesToNonExistingTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:59:53 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (787370ms till timeout)
2022-03-28 10:59:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (53284ms till timeout)
2022-03-28 10:59:54 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (61548ms till timeout)
2022-03-28 10:59:54 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:444] Kafka: my-cluster-66f23d43 is in desired state: ReconciliationPaused
2022-03-28 10:59:54 [ForkJoinPool-1-worker-15] INFO  [PodUtils:209] Wait until Pod my-cluster-66f23d43-kafka will have stable 3 replicas
2022-03-28 10:59:54 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for  Podmy-cluster-66f23d43-kafka will have 3 replicas
2022-03-28 10:59:54 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 19 polls
2022-03-28 10:59:54 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-66f23d43-kafka will have 3 replicas not ready, will try again in 1000 ms (179933ms till timeout)
2022-03-28 10:59:54 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:59:55 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (786305ms till timeout)
2022-03-28 10:59:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (52235ms till timeout)
2022-03-28 10:59:55 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (60468ms till timeout)
2022-03-28 10:59:55 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 18 polls
2022-03-28 10:59:55 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-66f23d43-kafka will have 3 replicas not ready, will try again in 1000 ms (178875ms till timeout)
2022-03-28 10:59:56 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (785249ms till timeout)
2022-03-28 10:59:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (51111ms till timeout)
2022-03-28 10:59:56 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (59366ms till timeout)
2022-03-28 10:59:56 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 17 polls
2022-03-28 10:59:56 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-66f23d43-kafka will have 3 replicas not ready, will try again in 1000 ms (177809ms till timeout)
2022-03-28 10:59:57 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (784180ms till timeout)
2022-03-28 10:59:57 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:40] Job create-admin-my-cluster-8f262de5-kafka-clients was deleted
2022-03-28 10:59:57 [ForkJoinPool-1-worker-9] INFO  [ThrottlingQuotaST:112] Executing 3/5 iteration.
2022-03-28 10:59:57 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st
2022-03-28 10:59:57 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:create-admin-my-cluster-8f262de5-kafka-clients
2022-03-28 10:59:57 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (50073ms till timeout)
2022-03-28 10:59:57 [ForkJoinPool-1-worker-9] INFO  [JobUtils:81] Waiting for job: create-admin-my-cluster-8f262de5-kafka-clients will be in active state
2022-03-28 10:59:57 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 10:59:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job active not ready, will try again in 1000 ms (179994ms till timeout)
2022-03-28 10:59:57 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (58305ms till timeout)
2022-03-28 10:59:57 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 16 polls
2022-03-28 10:59:57 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-66f23d43-kafka will have 3 replicas not ready, will try again in 1000 ms (176749ms till timeout)
2022-03-28 10:59:58 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (783139ms till timeout)
2022-03-28 10:59:58 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (49030ms till timeout)
2022-03-28 10:59:58 [ForkJoinPool-1-worker-9] INFO  [ClientUtils:76] Waiting for producer/consumer:create-admin-my-cluster-8f262de5-kafka-clients to finished
2022-03-28 10:59:58 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 10:59:58 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:59:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (219922ms till timeout)
2022-03-28 10:59:58 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (57177ms till timeout)
2022-03-28 10:59:58 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 15 polls
2022-03-28 10:59:58 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-66f23d43-kafka will have 3 replicas not ready, will try again in 1000 ms (175647ms till timeout)
2022-03-28 10:59:58 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 10:59:58 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testSendingMessagesToNonExistingTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:59:58 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=5505da1a-a039-4a47-aceb-373e93b4c7b2, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 10:59:58 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=5505da1a-a039-4a47-aceb-373e93b4c7b2, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 10:59:58 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-2 hasn't rolled
2022-03-28 10:59:58 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1779725ms till timeout)
2022-03-28 10:59:59 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (782009ms till timeout)
2022-03-28 10:59:59 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (47948ms till timeout)
2022-03-28 10:59:59 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 10:59:59 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 10:59:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (218827ms till timeout)
2022-03-28 10:59:59 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (56097ms till timeout)
2022-03-28 11:00:00 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 14 polls
2022-03-28 11:00:00 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-66f23d43-kafka will have 3 replicas not ready, will try again in 1000 ms (174481ms till timeout)
2022-03-28 11:00:00 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (780966ms till timeout)
2022-03-28 11:00:00 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (46819ms till timeout)
2022-03-28 11:00:00 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (217767ms till timeout)
2022-03-28 11:00:00 [ForkJoinPool-1-worker-17] INFO  [PodUtils:189] Message org.apache.kafka.common.errors.ThrottlingQuotaExceededException: The throttling quota has been exceeded. found in create-admin-my-cluster-28d9dd2e-kafka-clients-jkdbs log
2022-03-28 11:00:00 [ForkJoinPool-1-worker-17] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment create-admin-my-cluster-28d9dd2e-kafka-clients deletion
2022-03-28 11:00:00 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for ReplicaSet create-admin-my-cluster-28d9dd2e-kafka-clients to be deleted
2022-03-28 11:00:01 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] ReplicaSet create-admin-my-cluster-28d9dd2e-kafka-clients to be deleted not ready, will try again in 5000 ms (179955ms till timeout)
2022-03-28 11:00:01 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 13 polls
2022-03-28 11:00:01 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-66f23d43-kafka will have 3 replicas not ready, will try again in 1000 ms (173430ms till timeout)
2022-03-28 11:00:01 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (779922ms till timeout)
2022-03-28 11:00:01 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:01 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (45704ms till timeout)
2022-03-28 11:00:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (216706ms till timeout)
2022-03-28 11:00:02 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 12 polls
2022-03-28 11:00:02 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-66f23d43-kafka will have 3 replicas not ready, will try again in 1000 ms (172354ms till timeout)
2022-03-28 11:00:02 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (778872ms till timeout)
2022-03-28 11:00:02 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (44610ms till timeout)
2022-03-28 11:00:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (215593ms till timeout)
2022-03-28 11:00:03 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 11 polls
2022-03-28 11:00:03 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-66f23d43-kafka will have 3 replicas not ready, will try again in 1000 ms (171265ms till timeout)
2022-03-28 11:00:03 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (777860ms till timeout)
2022-03-28 11:00:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (43585ms till timeout)
2022-03-28 11:00:03 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (214565ms till timeout)
2022-03-28 11:00:03 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testSendingMessagesToNonExistingTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:00:03 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 11:00:04 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=5505da1a-a039-4a47-aceb-373e93b4c7b2, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 11:00:04 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=5505da1a-a039-4a47-aceb-373e93b4c7b2, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 11:00:04 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-2 hasn't rolled
2022-03-28 11:00:04 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1774692ms till timeout)
2022-03-28 11:00:04 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 10 polls
2022-03-28 11:00:04 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-66f23d43-kafka will have 3 replicas not ready, will try again in 1000 ms (170223ms till timeout)
2022-03-28 11:00:04 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (776816ms till timeout)
2022-03-28 11:00:04 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:00:04 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (42495ms till timeout)
2022-03-28 11:00:04 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (213404ms till timeout)
2022-03-28 11:00:05 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 9 polls
2022-03-28 11:00:05 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-66f23d43-kafka will have 3 replicas not ready, will try again in 1000 ms (169035ms till timeout)
2022-03-28 11:00:05 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (775739ms till timeout)
2022-03-28 11:00:06 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (41377ms till timeout)
2022-03-28 11:00:06 [ForkJoinPool-1-worker-17] DEBUG [JobUtils:40] Job create-admin-my-cluster-28d9dd2e-kafka-clients was deleted
2022-03-28 11:00:06 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (212106ms till timeout)
2022-03-28 11:00:06 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-30
2022-03-28 11:00:06 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 8 polls
2022-03-28 11:00:06 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-66f23d43-kafka will have 3 replicas not ready, will try again in 1000 ms (167936ms till timeout)
2022-03-28 11:00:06 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (774654ms till timeout)
2022-03-28 11:00:06 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-30
2022-03-28 11:00:06 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 11:00:06 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-31
2022-03-28 11:00:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (40229ms till timeout)
2022-03-28 11:00:07 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (210820ms till timeout)
2022-03-28 11:00:07 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-31
2022-03-28 11:00:07 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 11:00:07 [ForkJoinPool-1-worker-17] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-32
2022-03-28 11:00:07 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 7 polls
2022-03-28 11:00:07 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-66f23d43-kafka will have 3 replicas not ready, will try again in 1000 ms (166716ms till timeout)
2022-03-28 11:00:07 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (773518ms till timeout)
2022-03-28 11:00:08 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (39070ms till timeout)
2022-03-28 11:00:08 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-32
2022-03-28 11:00:08 [ForkJoinPool-1-worker-17] DEBUG [Exec:419] Return code: 0
2022-03-28 11:00:08 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:155] Create/Update Job create-admin-my-cluster-28d9dd2e-kafka-clients in namespace throttling-quota-st
2022-03-28 11:00:08 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:create-admin-my-cluster-28d9dd2e-kafka-clients
2022-03-28 11:00:08 [ForkJoinPool-1-worker-17] INFO  [JobUtils:81] Waiting for job: create-admin-my-cluster-28d9dd2e-kafka-clients will be in active state
2022-03-28 11:00:08 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 11:00:08 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job active not ready, will try again in 1000 ms (179899ms till timeout)
2022-03-28 11:00:08 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (209593ms till timeout)
2022-03-28 11:00:08 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (772460ms till timeout)
2022-03-28 11:00:08 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 6 polls
2022-03-28 11:00:08 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-66f23d43-kafka will have 3 replicas not ready, will try again in 1000 ms (165635ms till timeout)
2022-03-28 11:00:08 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testSendingMessagesToNonExistingTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:00:09 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 11:00:09 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=5505da1a-a039-4a47-aceb-373e93b4c7b2, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 11:00:09 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=5505da1a-a039-4a47-aceb-373e93b4c7b2, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 11:00:09 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-2 hasn't rolled
2022-03-28 11:00:09 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1769644ms till timeout)
2022-03-28 11:00:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (37895ms till timeout)
2022-03-28 11:00:09 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:00:09 [ForkJoinPool-1-worker-17] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 11:00:09 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 11:00:09 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (771370ms till timeout)
2022-03-28 11:00:10 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:10 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (299819ms till timeout)
2022-03-28 11:00:10 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 5 polls
2022-03-28 11:00:10 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-66f23d43-kafka will have 3 replicas not ready, will try again in 1000 ms (164455ms till timeout)
2022-03-28 11:00:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (208275ms till timeout)
2022-03-28 11:00:10 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (36833ms till timeout)
2022-03-28 11:00:11 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (770294ms till timeout)
2022-03-28 11:00:11 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 4 polls
2022-03-28 11:00:11 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-66f23d43-kafka will have 3 replicas not ready, will try again in 1000 ms (163354ms till timeout)
2022-03-28 11:00:11 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:11 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (298595ms till timeout)
2022-03-28 11:00:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (207115ms till timeout)
2022-03-28 11:00:11 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (35709ms till timeout)
2022-03-28 11:00:12 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (769191ms till timeout)
2022-03-28 11:00:12 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 3 polls
2022-03-28 11:00:12 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-66f23d43-kafka will have 3 replicas not ready, will try again in 1000 ms (162254ms till timeout)
2022-03-28 11:00:12 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (297505ms till timeout)
2022-03-28 11:00:12 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (206029ms till timeout)
2022-03-28 11:00:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (34527ms till timeout)
2022-03-28 11:00:13 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (768129ms till timeout)
2022-03-28 11:00:13 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 2 polls
2022-03-28 11:00:13 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-66f23d43-kafka will have 3 replicas not ready, will try again in 1000 ms (161192ms till timeout)
2022-03-28 11:00:13 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:13 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (296367ms till timeout)
2022-03-28 11:00:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (204881ms till timeout)
2022-03-28 11:00:13 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testSendingMessagesToNonExistingTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:00:14 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 11:00:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (33366ms till timeout)
2022-03-28 11:00:14 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=5505da1a-a039-4a47-aceb-373e93b4c7b2, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 11:00:14 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=5505da1a-a039-4a47-aceb-373e93b4c7b2, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 11:00:14 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-2 hasn't rolled
2022-03-28 11:00:14 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1764579ms till timeout)
2022-03-28 11:00:14 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (766996ms till timeout)
2022-03-28 11:00:14 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 1 polls
2022-03-28 11:00:14 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-66f23d43-kafka will have 3 replicas not ready, will try again in 1000 ms (160108ms till timeout)
2022-03-28 11:00:14 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:00:14 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:14 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (295234ms till timeout)
2022-03-28 11:00:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (203714ms till timeout)
2022-03-28 11:00:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (32211ms till timeout)
2022-03-28 11:00:15 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (765920ms till timeout)
2022-03-28 11:00:15 [ForkJoinPool-1-worker-15] INFO  [PodUtils:217] Pod replicas are stable for 20 polls intervals
2022-03-28 11:00:15 [ForkJoinPool-1-worker-15] INFO  [PodUtils:228] Pod my-cluster-66f23d43-kafka has 3 replicas
2022-03-28 11:00:15 [ForkJoinPool-1-worker-15] INFO  [ReconciliationST:90] Setting annotation to "false", Kafka should be scaled to 4
2022-03-28 11:00:15 [ForkJoinPool-1-worker-15] INFO  [RollingUpdateUtils:127] Waiting for 4 Pod(s) of my-cluster-66f23d43-kafka to be ready
2022-03-28 11:00:15 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready
2022-03-28 11:00:15 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:00:15 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2399945ms till timeout)
2022-03-28 11:00:15 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (294086ms till timeout)
2022-03-28 11:00:15 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (202647ms till timeout)
2022-03-28 11:00:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (30937ms till timeout)
2022-03-28 11:00:16 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (764782ms till timeout)
2022-03-28 11:00:16 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:00:16 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2398856ms till timeout)
2022-03-28 11:00:16 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (292924ms till timeout)
2022-03-28 11:00:16 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (201450ms till timeout)
2022-03-28 11:00:17 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (763684ms till timeout)
2022-03-28 11:00:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (29741ms till timeout)
2022-03-28 11:00:18 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:00:18 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2397640ms till timeout)
2022-03-28 11:00:18 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (291782ms till timeout)
2022-03-28 11:00:18 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (200230ms till timeout)
2022-03-28 11:00:18 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (762607ms till timeout)
2022-03-28 11:00:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (28661ms till timeout)
2022-03-28 11:00:18 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testSendingMessagesToNonExistingTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:00:19 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:00:19 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2396605ms till timeout)
2022-03-28 11:00:19 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 11:00:19 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=5505da1a-a039-4a47-aceb-373e93b4c7b2, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 11:00:19 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=5505da1a-a039-4a47-aceb-373e93b4c7b2, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 11:00:19 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-2 hasn't rolled
2022-03-28 11:00:19 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1759549ms till timeout)
2022-03-28 11:00:19 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (290662ms till timeout)
2022-03-28 11:00:19 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (199152ms till timeout)
2022-03-28 11:00:19 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:00:19 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (761572ms till timeout)
2022-03-28 11:00:19 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (27546ms till timeout)
2022-03-28 11:00:20 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:00:20 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2395469ms till timeout)
2022-03-28 11:00:20 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (198046ms till timeout)
2022-03-28 11:00:20 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (289433ms till timeout)
2022-03-28 11:00:20 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (760537ms till timeout)
2022-03-28 11:00:21 [ForkJoinPool-1-worker-7] INFO  [PodUtils:189] Message All topics created found in create-admin-my-cluster-5a0bfe14-kafka-clients-69znx log
2022-03-28 11:00:21 [ForkJoinPool-1-worker-7] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment create-admin-my-cluster-5a0bfe14-kafka-clients deletion
2022-03-28 11:00:21 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for ReplicaSet create-admin-my-cluster-5a0bfe14-kafka-clients to be deleted
2022-03-28 11:00:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] ReplicaSet create-admin-my-cluster-5a0bfe14-kafka-clients to be deleted not ready, will try again in 5000 ms (179895ms till timeout)
2022-03-28 11:00:21 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:00:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2394379ms till timeout)
2022-03-28 11:00:21 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (196848ms till timeout)
2022-03-28 11:00:21 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (288216ms till timeout)
2022-03-28 11:00:21 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (759491ms till timeout)
2022-03-28 11:00:22 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:00:22 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2393197ms till timeout)
2022-03-28 11:00:22 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:22 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (287079ms till timeout)
2022-03-28 11:00:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (195670ms till timeout)
2022-03-28 11:00:22 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (758405ms till timeout)
2022-03-28 11:00:23 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:00:23 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2392158ms till timeout)
2022-03-28 11:00:23 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testSendingMessagesToNonExistingTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:00:23 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:23 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (285871ms till timeout)
2022-03-28 11:00:24 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (757321ms till timeout)
2022-03-28 11:00:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (194411ms till timeout)
2022-03-28 11:00:24 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 11:00:24 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=5505da1a-a039-4a47-aceb-373e93b4c7b2, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 11:00:24 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=5505da1a-a039-4a47-aceb-373e93b4c7b2, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 11:00:24 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-2 hasn't rolled
2022-03-28 11:00:24 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1754416ms till timeout)
2022-03-28 11:00:24 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:00:24 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2391085ms till timeout)
2022-03-28 11:00:24 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:00:25 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:25 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (756219ms till timeout)
2022-03-28 11:00:25 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (284692ms till timeout)
2022-03-28 11:00:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (193290ms till timeout)
2022-03-28 11:00:25 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:00:25 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2389970ms till timeout)
2022-03-28 11:00:26 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (755060ms till timeout)
2022-03-28 11:00:26 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (192105ms till timeout)
2022-03-28 11:00:26 [ForkJoinPool-1-worker-7] DEBUG [JobUtils:40] Job create-admin-my-cluster-5a0bfe14-kafka-clients was deleted
2022-03-28 11:00:26 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (283470ms till timeout)
2022-03-28 11:00:26 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update Job list-admin-my-cluster-5a0bfe14-kafka-clients in namespace throttling-quota-st
2022-03-28 11:00:26 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:list-admin-my-cluster-5a0bfe14-kafka-clients
2022-03-28 11:00:26 [ForkJoinPool-1-worker-7] INFO  [JobUtils:81] Waiting for job: list-admin-my-cluster-5a0bfe14-kafka-clients will be in active state
2022-03-28 11:00:26 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 11:00:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job active not ready, will try again in 1000 ms (179913ms till timeout)
2022-03-28 11:00:26 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:00:26 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2388927ms till timeout)
2022-03-28 11:00:27 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (754001ms till timeout)
2022-03-28 11:00:27 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (191016ms till timeout)
2022-03-28 11:00:27 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (282395ms till timeout)
2022-03-28 11:00:27 [ForkJoinPool-1-worker-7] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 11:00:27 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 11:00:27 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:00:27 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2387785ms till timeout)
2022-03-28 11:00:27 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (119832ms till timeout)
2022-03-28 11:00:28 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (752915ms till timeout)
2022-03-28 11:00:28 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (189914ms till timeout)
2022-03-28 11:00:28 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (281300ms till timeout)
2022-03-28 11:00:28 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:00:28 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2386747ms till timeout)
2022-03-28 11:00:28 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testSendingMessagesToNonExistingTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:00:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (118725ms till timeout)
2022-03-28 11:00:29 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 11:00:29 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=5505da1a-a039-4a47-aceb-373e93b4c7b2, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 11:00:29 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=5505da1a-a039-4a47-aceb-373e93b4c7b2, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 11:00:29 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-2 hasn't rolled
2022-03-28 11:00:29 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1749277ms till timeout)
2022-03-28 11:00:29 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-3ba9cc5b will have desired state: Ready not ready, will try again in 1000 ms (751802ms till timeout)
2022-03-28 11:00:29 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:00:29 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:29 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (280197ms till timeout)
2022-03-28 11:00:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (188702ms till timeout)
2022-03-28 11:00:30 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:00:30 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2385643ms till timeout)
2022-03-28 11:00:30 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (117584ms till timeout)
2022-03-28 11:00:30 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:444] Kafka: my-cluster-3ba9cc5b is in desired state: Ready
2022-03-28 11:00:30 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update KafkaUser my-user-1382292264-1641298587 in namespace namespace-9
2022-03-28 11:00:30 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:164] Using Namespace: namespace-9
2022-03-28 11:00:30 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (279114ms till timeout)
2022-03-28 11:00:30 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:30 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-1382292264-1641298587
2022-03-28 11:00:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (187485ms till timeout)
2022-03-28 11:00:30 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-1382292264-1641298587 will have desired state: Ready
2022-03-28 11:00:30 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-1382292264-1641298587 will have desired state: Ready
2022-03-28 11:00:31 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] KafkaUser: my-user-1382292264-1641298587 will have desired state: Ready not ready, will try again in 1000 ms (179938ms till timeout)
2022-03-28 11:00:31 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:00:31 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2384581ms till timeout)
2022-03-28 11:00:31 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (116507ms till timeout)
2022-03-28 11:00:31 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (278028ms till timeout)
2022-03-28 11:00:32 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (186409ms till timeout)
2022-03-28 11:00:32 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:444] KafkaUser: my-user-1382292264-1641298587 is in desired state: Ready
2022-03-28 11:00:32 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:00:32 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2383529ms till timeout)
2022-03-28 11:00:32 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-1382292264-1641298587 will have desired state: Ready
2022-03-28 11:00:32 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-1382292264-1641298587 will have desired state: Ready
2022-03-28 11:00:32 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:444] KafkaUser: my-user-1382292264-1641298587 is in desired state: Ready
2022-03-28 11:00:32 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (115332ms till timeout)
2022-03-28 11:00:32 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:674] ============================================================================
2022-03-28 11:00:32 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:675] [operators.user.UserST - After Each] - Clean up after test
2022-03-28 11:00:32 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:344] ############################################################################
2022-03-28 11:00:32 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:348] Delete all resources for testTlsExternalUser
2022-03-28 11:00:32 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of KafkaUser my-user-1382292264-1641298587 in namespace namespace-9
2022-03-28 11:00:32 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1382292264-1641298587
2022-03-28 11:00:32 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1382292264-1641298587 not ready, will try again in 10000 ms (179912ms till timeout)
2022-03-28 11:00:32 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (276904ms till timeout)
2022-03-28 11:00:33 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (185200ms till timeout)
2022-03-28 11:00:33 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:00:33 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2382416ms till timeout)
2022-03-28 11:00:33 [ForkJoinPool-1-worker-7] INFO  [PodUtils:189] Message quota-topic-test-simple-99 found in list-admin-my-cluster-5a0bfe14-kafka-clients-44lsv log
2022-03-28 11:00:33 [ForkJoinPool-1-worker-7] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment list-admin-my-cluster-5a0bfe14-kafka-clients deletion
2022-03-28 11:00:33 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for ReplicaSet list-admin-my-cluster-5a0bfe14-kafka-clients to be deleted
2022-03-28 11:00:33 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] ReplicaSet list-admin-my-cluster-5a0bfe14-kafka-clients to be deleted not ready, will try again in 5000 ms (179806ms till timeout)
2022-03-28 11:00:33 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testSendingMessagesToNonExistingTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:00:34 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (275745ms till timeout)
2022-03-28 11:00:34 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:34 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:00:34 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2381297ms till timeout)
2022-03-28 11:00:34 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 11:00:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (184017ms till timeout)
2022-03-28 11:00:34 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=5505da1a-a039-4a47-aceb-373e93b4c7b2, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 11:00:34 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=5505da1a-a039-4a47-aceb-373e93b4c7b2, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 11:00:34 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-2 hasn't rolled
2022-03-28 11:00:34 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1744222ms till timeout)
2022-03-28 11:00:34 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:00:35 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (274637ms till timeout)
2022-03-28 11:00:35 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:00:35 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2380204ms till timeout)
2022-03-28 11:00:35 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (182914ms till timeout)
2022-03-28 11:00:36 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (273490ms till timeout)
2022-03-28 11:00:36 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:00:36 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2379107ms till timeout)
2022-03-28 11:00:36 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (181667ms till timeout)
2022-03-28 11:00:37 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (272379ms till timeout)
2022-03-28 11:00:37 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:00:37 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2378078ms till timeout)
2022-03-28 11:00:37 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (180579ms till timeout)
2022-03-28 11:00:38 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (271215ms till timeout)
2022-03-28 11:00:38 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:00:38 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2376890ms till timeout)
2022-03-28 11:00:38 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:38 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testSendingMessagesToNonExistingTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:00:39 [ForkJoinPool-1-worker-7] DEBUG [JobUtils:40] Job list-admin-my-cluster-5a0bfe14-kafka-clients was deleted
2022-03-28 11:00:39 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update Job delete-admin-my-cluster-5a0bfe14-kafka-clients in namespace throttling-quota-st
2022-03-28 11:00:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (179411ms till timeout)
2022-03-28 11:00:39 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:delete-admin-my-cluster-5a0bfe14-kafka-clients
2022-03-28 11:00:39 [ForkJoinPool-1-worker-7] INFO  [JobUtils:81] Waiting for job: delete-admin-my-cluster-5a0bfe14-kafka-clients will be in active state
2022-03-28 11:00:39 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 11:00:39 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 11:00:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job active not ready, will try again in 1000 ms (179772ms till timeout)
2022-03-28 11:00:39 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:00:39 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=5505da1a-a039-4a47-aceb-373e93b4c7b2, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=723994bf-c11d-4bc9-8a65-9864eb44e356}
2022-03-28 11:00:39 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=5505da1a-a039-4a47-aceb-373e93b4c7b2, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=723994bf-c11d-4bc9-8a65-9864eb44e356}
2022-03-28 11:00:39 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-1 hasn't rolled
2022-03-28 11:00:39 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1738996ms till timeout)
2022-03-28 11:00:39 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (269949ms till timeout)
2022-03-28 11:00:39 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:00:39 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2375748ms till timeout)
2022-03-28 11:00:40 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (178257ms till timeout)
2022-03-28 11:00:40 [ForkJoinPool-1-worker-7] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 11:00:40 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 11:00:41 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (268841ms till timeout)
2022-03-28 11:00:41 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (119838ms till timeout)
2022-03-28 11:00:41 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:00:41 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2374620ms till timeout)
2022-03-28 11:00:41 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (177077ms till timeout)
2022-03-28 11:00:42 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (267792ms till timeout)
2022-03-28 11:00:42 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (118776ms till timeout)
2022-03-28 11:00:42 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:00:42 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2373583ms till timeout)
2022-03-28 11:00:42 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (175910ms till timeout)
2022-03-28 11:00:42 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of Kafka my-cluster-3ba9cc5b in namespace namespace-9
2022-03-28 11:00:42 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-3ba9cc5b
2022-03-28 11:00:42 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-3ba9cc5b not ready, will try again in 10000 ms (839919ms till timeout)
2022-03-28 11:00:43 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:00:43 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2372410ms till timeout)
2022-03-28 11:00:43 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (117544ms till timeout)
2022-03-28 11:00:43 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (266546ms till timeout)
2022-03-28 11:00:43 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (174750ms till timeout)
2022-03-28 11:00:43 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testSendingMessagesToNonExistingTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:00:44 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:00:44 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2371204ms till timeout)
2022-03-28 11:00:44 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (265256ms till timeout)
2022-03-28 11:00:44 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (116253ms till timeout)
2022-03-28 11:00:44 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:00:44 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 11:00:44 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=5505da1a-a039-4a47-aceb-373e93b4c7b2, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=723994bf-c11d-4bc9-8a65-9864eb44e356}
2022-03-28 11:00:44 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=5505da1a-a039-4a47-aceb-373e93b4c7b2, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=723994bf-c11d-4bc9-8a65-9864eb44e356}
2022-03-28 11:00:44 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-1 hasn't rolled
2022-03-28 11:00:44 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1733847ms till timeout)
2022-03-28 11:00:44 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (173474ms till timeout)
2022-03-28 11:00:45 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:00:45 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2370090ms till timeout)
2022-03-28 11:00:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (115122ms till timeout)
2022-03-28 11:00:45 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (264115ms till timeout)
2022-03-28 11:00:46 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (172249ms till timeout)
2022-03-28 11:00:46 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:00:46 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2368972ms till timeout)
2022-03-28 11:00:46 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (262929ms till timeout)
2022-03-28 11:00:46 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (113927ms till timeout)
2022-03-28 11:00:47 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (171026ms till timeout)
2022-03-28 11:00:47 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:00:47 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2367885ms till timeout)
2022-03-28 11:00:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (112810ms till timeout)
2022-03-28 11:00:48 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (261787ms till timeout)
2022-03-28 11:00:48 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (169877ms till timeout)
2022-03-28 11:00:48 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:00:48 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2366766ms till timeout)
2022-03-28 11:00:48 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testSendingMessagesToNonExistingTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:00:49 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (111692ms till timeout)
2022-03-28 11:00:49 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (260667ms till timeout)
2022-03-28 11:00:49 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:00:49 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (168675ms till timeout)
2022-03-28 11:00:49 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 11:00:49 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=5505da1a-a039-4a47-aceb-373e93b4c7b2, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=723994bf-c11d-4bc9-8a65-9864eb44e356}
2022-03-28 11:00:49 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=5505da1a-a039-4a47-aceb-373e93b4c7b2, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=723994bf-c11d-4bc9-8a65-9864eb44e356}
2022-03-28 11:00:49 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-1 hasn't rolled
2022-03-28 11:00:49 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1728766ms till timeout)
2022-03-28 11:00:50 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:00:50 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2365653ms till timeout)
2022-03-28 11:00:50 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (110589ms till timeout)
2022-03-28 11:00:50 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (259571ms till timeout)
2022-03-28 11:00:50 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (167465ms till timeout)
2022-03-28 11:00:51 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:00:51 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2364614ms till timeout)
2022-03-28 11:00:51 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (109506ms till timeout)
2022-03-28 11:00:51 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (258414ms till timeout)
2022-03-28 11:00:52 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (166429ms till timeout)
2022-03-28 11:00:52 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:00:52 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2363487ms till timeout)
2022-03-28 11:00:52 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (108389ms till timeout)
2022-03-28 11:00:52 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (257281ms till timeout)
2022-03-28 11:00:53 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:369] ############################################################################
2022-03-28 11:00:53 [ForkJoinPool-1-worker-13] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-9 for test case:testTlsExternalUser
2022-03-28 11:00:53 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:53 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Namespace namespace-9 removal
2022-03-28 11:00:53 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 11:00:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (165265ms till timeout)
2022-03-28 11:00:53 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:00:53 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2362398ms till timeout)
2022-03-28 11:00:53 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 11:00:53 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 11:00:53 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (479622ms till timeout)
2022-03-28 11:00:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (107296ms till timeout)
2022-03-28 11:00:53 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (256189ms till timeout)
2022-03-28 11:00:53 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testSendingMessagesToNonExistingTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:00:54 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (164123ms till timeout)
2022-03-28 11:00:54 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:00:54 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2361326ms till timeout)
2022-03-28 11:00:54 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 11:00:54 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:00:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (106191ms till timeout)
2022-03-28 11:00:54 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (255072ms till timeout)
2022-03-28 11:00:54 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 11:00:55 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=5505da1a-a039-4a47-aceb-373e93b4c7b2, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=723994bf-c11d-4bc9-8a65-9864eb44e356}
2022-03-28 11:00:55 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=5505da1a-a039-4a47-aceb-373e93b4c7b2, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=723994bf-c11d-4bc9-8a65-9864eb44e356}
2022-03-28 11:00:55 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-1 hasn't rolled
2022-03-28 11:00:55 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1723648ms till timeout)
2022-03-28 11:00:55 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (162921ms till timeout)
2022-03-28 11:00:55 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:00:55 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2360158ms till timeout)
2022-03-28 11:00:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (104805ms till timeout)
2022-03-28 11:00:56 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (253699ms till timeout)
2022-03-28 11:00:56 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:56 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:00:56 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2359005ms till timeout)
2022-03-28 11:00:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (161730ms till timeout)
2022-03-28 11:00:57 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (103487ms till timeout)
2022-03-28 11:00:57 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (252489ms till timeout)
2022-03-28 11:00:57 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:57 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:00:57 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2357849ms till timeout)
2022-03-28 11:00:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (160502ms till timeout)
2022-03-28 11:00:58 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (102310ms till timeout)
2022-03-28 11:00:58 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (251226ms till timeout)
2022-03-28 11:00:58 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:00:58 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2356757ms till timeout)
2022-03-28 11:00:58 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testSendingMessagesToNonExistingTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:00:59 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:00:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (159387ms till timeout)
2022-03-28 11:00:59 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (101254ms till timeout)
2022-03-28 11:00:59 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:00:59 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (250093ms till timeout)
2022-03-28 11:00:59 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 11:00:59 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 11:00:59 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (473272ms till timeout)
2022-03-28 11:01:00 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:00 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2355676ms till timeout)
2022-03-28 11:01:00 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 11:01:00 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:00 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=5505da1a-a039-4a47-aceb-373e93b4c7b2, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=723994bf-c11d-4bc9-8a65-9864eb44e356}
2022-03-28 11:01:00 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=5505da1a-a039-4a47-aceb-373e93b4c7b2, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=723994bf-c11d-4bc9-8a65-9864eb44e356}
2022-03-28 11:01:00 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-1 hasn't rolled
2022-03-28 11:01:00 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1718469ms till timeout)
2022-03-28 11:01:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (158187ms till timeout)
2022-03-28 11:01:00 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (100092ms till timeout)
2022-03-28 11:01:00 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 11:01:01 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (248854ms till timeout)
2022-03-28 11:01:01 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:01 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2354458ms till timeout)
2022-03-28 11:01:01 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 11:01:01 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 11:01:01 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (471864ms till timeout)
2022-03-28 11:01:01 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (157063ms till timeout)
2022-03-28 11:01:01 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (98966ms till timeout)
2022-03-28 11:01:02 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (247620ms till timeout)
2022-03-28 11:01:02 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 11:01:02 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:02 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2353322ms till timeout)
2022-03-28 11:01:02 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (155849ms till timeout)
2022-03-28 11:01:02 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 11:01:02 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 11:01:02 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (470421ms till timeout)
2022-03-28 11:01:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (97813ms till timeout)
2022-03-28 11:01:03 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (246392ms till timeout)
2022-03-28 11:01:03 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:03 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2352137ms till timeout)
2022-03-28 11:01:03 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (154712ms till timeout)
2022-03-28 11:01:03 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 11:01:03 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testSendingMessagesToNonExistingTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:01:04 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (96636ms till timeout)
2022-03-28 11:01:04 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (245265ms till timeout)
2022-03-28 11:01:04 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 11:01:04 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 11:01:04 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (468585ms till timeout)
2022-03-28 11:01:04 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:01:04 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:04 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2351060ms till timeout)
2022-03-28 11:01:04 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (153574ms till timeout)
2022-03-28 11:01:05 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 11:01:05 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=5505da1a-a039-4a47-aceb-373e93b4c7b2, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=723994bf-c11d-4bc9-8a65-9864eb44e356}
2022-03-28 11:01:05 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=5505da1a-a039-4a47-aceb-373e93b4c7b2, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=723994bf-c11d-4bc9-8a65-9864eb44e356}
2022-03-28 11:01:05 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-1 hasn't rolled
2022-03-28 11:01:05 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1713365ms till timeout)
2022-03-28 11:01:05 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (95502ms till timeout)
2022-03-28 11:01:05 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 11:01:05 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (244163ms till timeout)
2022-03-28 11:01:05 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:05 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2349998ms till timeout)
2022-03-28 11:01:05 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 11:01:05 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 11:01:05 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (467214ms till timeout)
2022-03-28 11:01:06 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (152328ms till timeout)
2022-03-28 11:01:06 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (94423ms till timeout)
2022-03-28 11:01:06 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (243029ms till timeout)
2022-03-28 11:01:06 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:06 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2348857ms till timeout)
2022-03-28 11:01:06 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 11:01:07 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (151235ms till timeout)
2022-03-28 11:01:07 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 11:01:07 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 11:01:07 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (465836ms till timeout)
2022-03-28 11:01:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (93342ms till timeout)
2022-03-28 11:01:07 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:07 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2347806ms till timeout)
2022-03-28 11:01:07 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (241963ms till timeout)
2022-03-28 11:01:08 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:08 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 11:01:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (150079ms till timeout)
2022-03-28 11:01:08 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 11:01:08 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 11:01:08 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (464521ms till timeout)
2022-03-28 11:01:08 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (92153ms till timeout)
2022-03-28 11:01:08 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:08 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2346730ms till timeout)
2022-03-28 11:01:08 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testSendingMessagesToNonExistingTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:01:09 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (240853ms till timeout)
2022-03-28 11:01:09 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (148923ms till timeout)
2022-03-28 11:01:09 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:01:09 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 11:01:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (90989ms till timeout)
2022-03-28 11:01:09 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 11:01:09 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 11:01:09 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (463289ms till timeout)
2022-03-28 11:01:10 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:10 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2345628ms till timeout)
2022-03-28 11:01:10 [ForkJoinPool-1-worker-17] INFO  [PodUtils:189] Message All topics created found in create-admin-my-cluster-28d9dd2e-kafka-clients-wtnln log
2022-03-28 11:01:10 [ForkJoinPool-1-worker-17] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment create-admin-my-cluster-28d9dd2e-kafka-clients deletion
2022-03-28 11:01:10 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for ReplicaSet create-admin-my-cluster-28d9dd2e-kafka-clients to be deleted
2022-03-28 11:01:10 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 11:01:10 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] ReplicaSet create-admin-my-cluster-28d9dd2e-kafka-clients to be deleted not ready, will try again in 5000 ms (179940ms till timeout)
2022-03-28 11:01:10 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=5505da1a-a039-4a47-aceb-373e93b4c7b2, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=723994bf-c11d-4bc9-8a65-9864eb44e356}
2022-03-28 11:01:10 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=5505da1a-a039-4a47-aceb-373e93b4c7b2, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=723994bf-c11d-4bc9-8a65-9864eb44e356}
2022-03-28 11:01:10 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-1 hasn't rolled
2022-03-28 11:01:10 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1708291ms till timeout)
2022-03-28 11:01:10 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (147673ms till timeout)
2022-03-28 11:01:10 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 11:01:11 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (89835ms till timeout)
2022-03-28 11:01:11 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:11 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2344516ms till timeout)
2022-03-28 11:01:11 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 11:01:11 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 11:01:11 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (461988ms till timeout)
2022-03-28 11:01:12 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (146408ms till timeout)
2022-03-28 11:01:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (88774ms till timeout)
2022-03-28 11:01:12 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 11:01:12 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:12 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2343417ms till timeout)
2022-03-28 11:01:12 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 11:01:12 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 11:01:12 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (460673ms till timeout)
2022-03-28 11:01:13 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (145275ms till timeout)
2022-03-28 11:01:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (87655ms till timeout)
2022-03-28 11:01:13 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:13 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2342365ms till timeout)
2022-03-28 11:01:13 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 11:01:13 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 11:01:13 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 11:01:13 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (459377ms till timeout)
2022-03-28 11:01:13 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testSendingMessagesToNonExistingTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:01:14 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (144172ms till timeout)
2022-03-28 11:01:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (86559ms till timeout)
2022-03-28 11:01:14 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:14 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2341267ms till timeout)
2022-03-28 11:01:14 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:01:14 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 11:01:15 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 11:01:15 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 11:01:15 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (458098ms till timeout)
2022-03-28 11:01:15 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:15 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 11:01:15 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] ReplicaSet create-admin-my-cluster-28d9dd2e-kafka-clients to be deleted not ready, will try again in 5000 ms (174843ms till timeout)
2022-03-28 11:01:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (143012ms till timeout)
2022-03-28 11:01:15 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=5505da1a-a039-4a47-aceb-373e93b4c7b2, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=723994bf-c11d-4bc9-8a65-9864eb44e356}
2022-03-28 11:01:15 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=5505da1a-a039-4a47-aceb-373e93b4c7b2, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=723994bf-c11d-4bc9-8a65-9864eb44e356}
2022-03-28 11:01:15 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-1 hasn't rolled
2022-03-28 11:01:15 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1703172ms till timeout)
2022-03-28 11:01:15 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:15 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2340170ms till timeout)
2022-03-28 11:01:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (85328ms till timeout)
2022-03-28 11:01:16 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 11:01:16 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 11:01:16 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 11:01:16 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (456848ms till timeout)
2022-03-28 11:01:16 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:16 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:16 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2339058ms till timeout)
2022-03-28 11:01:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (141805ms till timeout)
2022-03-28 11:01:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (84179ms till timeout)
2022-03-28 11:01:17 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 11:01:17 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 11:01:17 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 11:01:17 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (455590ms till timeout)
2022-03-28 11:01:17 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:17 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2338009ms till timeout)
2022-03-28 11:01:17 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (83071ms till timeout)
2022-03-28 11:01:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (140671ms till timeout)
2022-03-28 11:01:18 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 11:01:18 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:18 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2336958ms till timeout)
2022-03-28 11:01:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (82020ms till timeout)
2022-03-28 11:01:18 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (139534ms till timeout)
2022-03-28 11:01:18 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 11:01:18 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 1
2022-03-28 11:01:18 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 11:01:18 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-9" not found
2022-03-28 11:01:18 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] ======STDERR END======
2022-03-28 11:01:18 [ForkJoinPool-1-worker-13] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[namespace-8], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[infra-namespace, reconciliation-st], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-3], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 11:01:18 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:267] testTlsExternalUser - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTls, testSendSimpleMessageTlsScramSha, testConfigurationFileIsCreated, testReceiveSimpleMessageTls, testDeployAndUnDeployCruiseControl, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testConfigurationPerformanceOptions, testSendingMessagesToNonExistingTopic, testTlsExternalUserWithQuotas] to and randomly select one to start execution
2022-03-28 11:01:18 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:93] [operators.user.UserST] - Removing parallel test: testTlsExternalUser
2022-03-28 11:01:18 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:97] [operators.user.UserST] - Parallel test count: 7
2022-03-28 11:01:18 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testTlsExternalUser-FINISHED
2022-03-28 11:01:18 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:30] ############################################################################
2022-03-28 11:01:18 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:23] ############################################################################
2022-03-28 11:01:18 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testUpdateUser-STARTED
2022-03-28 11:01:18 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:658] ============================================================================
2022-03-28 11:01:18 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:659] [operators.user.UserST - Before Each] - Setup test case environment
2022-03-28 11:01:18 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:77] [operators.user.UserST] - Adding parallel test: testUpdateUser
2022-03-28 11:01:18 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:81] [operators.user.UserST] - Parallel test count: 8
2022-03-28 11:01:18 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:205] [testUpdateUser] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:01:18 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUpdateUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:01:18 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:230] testSendingMessagesToNonExistingTopic test now can proceed its execution
2022-03-28 11:01:18 [ForkJoinPool-1-worker-5] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 11:01:18 [ForkJoinPool-1-worker-5] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testScramUserWithQuotas=my-cluster-4941482a, testTopicModificationOfReplicationFactor=my-cluster-38e659b2, testKafkaAdminTopicOperations=my-cluster-5a0bfe14, testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383, testConfigurationFileIsCreated=my-cluster-e0fac774, testThrottlingQuotasCreateAlterPartitions=my-cluster-28d9dd2e, testSendSimpleMessageTls=my-cluster-27ddd6ba, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c, testThrottlingQuotasDeleteTopic=my-cluster-8f262de5, testConfigurationReflection=my-cluster-d1e2168e, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-da91577a, testUserWithNameMoreThan64Chars=my-cluster-35e66bba, testDeployAndUnDeployCruiseControl=my-cluster-c0830cbe, testTlsUserWithQuotas=my-cluster-d49e42e8, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8, testSendingMessagesToNonExistingTopic=my-cluster-241cc7c0, testCapacityFile=my-cluster-fd0fb61a, testReceiveSimpleMessageTls=my-cluster-c6a15963, testConfigurationPerformanceOptions=my-cluster-b6310693, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-66f23d43, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-f50d425d, testTlsExternalUser=my-cluster-3ba9cc5b, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5}
2022-03-28 11:01:18 [ForkJoinPool-1-worker-5] TRACE [AbstractST:607] USERS_NAME_MAP: {testScramUserWithQuotas=my-user-797280497-2138800976, testTopicModificationOfReplicationFactor=my-user-2003504850-304348854, testKafkaAdminTopicOperations=my-user-1708656731-1087276375, testReceiveSimpleMessageTlsScramSha=my-user-1915916223-1265293973, testConfigurationFileIsCreated=my-user-180611180-50530491, testThrottlingQuotasCreateAlterPartitions=my-user-1933547616-1024431140, testSendSimpleMessageTls=my-user-738229993-1095121647, testCreatingUsersWithSecretPrefix=my-user-1450786684-1808961062, testThrottlingQuotasDeleteTopic=my-user-1558977140-777236615, testConfigurationReflection=my-user-1445653023-93200027, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-1260379615-1050949907, testUserWithNameMoreThan64Chars=my-user-1703808150-314159031, testDeployAndUnDeployCruiseControl=my-user-1988711145-439861161, testTlsUserWithQuotas=my-user-811912745-1441809840, testThrottlingQuotasCreateTopic=my-user-1131806865-947217230, testSendingMessagesToNonExistingTopic=my-user-927451395-640848463, testCapacityFile=my-user-937015144-1959439396, testReceiveSimpleMessageTls=my-user-6088058-1142028780, testConfigurationPerformanceOptions=my-user-930975101-640523779, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-1150765500-2076224625, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-745974575-1852384454, testTlsExternalUser=my-user-1382292264-1641298587, testSendSimpleMessageTlsScramSha=my-user-1499386234-1249154328}
2022-03-28 11:01:18 [ForkJoinPool-1-worker-5] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testScramUserWithQuotas=my-topic-2004840350-265526537, testTopicModificationOfReplicationFactor=my-topic-1789870897-1466152225, testKafkaAdminTopicOperations=my-topic-1564170108-1493323202, testReceiveSimpleMessageTlsScramSha=my-topic-1834944466-2094449929, testConfigurationFileIsCreated=my-topic-1296407588-313880708, testThrottlingQuotasCreateAlterPartitions=my-topic-609849288-1810034325, testSendSimpleMessageTls=my-topic-563023482-1697359353, testCreatingUsersWithSecretPrefix=my-topic-1479520843-618891199, testThrottlingQuotasDeleteTopic=my-topic-2128658681-1688111273, testConfigurationReflection=my-topic-640815392-1092406112, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-1674392683-1637010870, testUserWithNameMoreThan64Chars=my-topic-2021464661-1313035472, testDeployAndUnDeployCruiseControl=my-topic-1511079187-95055629, testTlsUserWithQuotas=my-topic-418284173-1689090259, testThrottlingQuotasCreateTopic=my-topic-728672490-1194703402, testSendingMessagesToNonExistingTopic=my-topic-645675602-646682485, testCapacityFile=my-topic-1155641705-902584520, testReceiveSimpleMessageTls=my-topic-478514536-2132782608, testConfigurationPerformanceOptions=my-topic-213912446-1165431808, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-842198221-1227962115, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-254848724-189700675, testTlsExternalUser=my-topic-5642373-135861890, testSendSimpleMessageTlsScramSha=my-topic-1028951580-1656411053}
2022-03-28 11:01:18 [ForkJoinPool-1-worker-5] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testScramUserWithQuotas=my-cluster-4941482a-kafka-clients, testTopicModificationOfReplicationFactor=my-cluster-38e659b2-kafka-clients, testKafkaAdminTopicOperations=my-cluster-5a0bfe14-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383-kafka-clients, testConfigurationFileIsCreated=my-cluster-e0fac774-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-28d9dd2e-kafka-clients, testSendSimpleMessageTls=my-cluster-27ddd6ba-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-8f262de5-kafka-clients, testConfigurationReflection=my-cluster-d1e2168e-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-da91577a-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-35e66bba-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-c0830cbe-kafka-clients, testTlsUserWithQuotas=my-cluster-d49e42e8-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8-kafka-clients, testSendingMessagesToNonExistingTopic=my-cluster-241cc7c0-kafka-clients, testCapacityFile=my-cluster-fd0fb61a-kafka-clients, testReceiveSimpleMessageTls=my-cluster-c6a15963-kafka-clients, testConfigurationPerformanceOptions=my-cluster-b6310693-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-66f23d43-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-f50d425d-kafka-clients, testTlsExternalUser=my-cluster-3ba9cc5b-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5-kafka-clients}
2022-03-28 11:01:18 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update Deployment topic-cluster-name-kafka-clients in namespace topic-st
2022-03-28 11:01:19 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:topic-cluster-name-kafka-clients
2022-03-28 11:01:19 [ForkJoinPool-1-worker-5] INFO  [DeploymentUtils:161] Wait for Deployment: topic-cluster-name-kafka-clients will be ready
2022-03-28 11:01:19 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Wait for Deployment: topic-cluster-name-kafka-clients will be ready
2022-03-28 11:01:19 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Wait for Deployment: topic-cluster-name-kafka-clients will be ready not ready, will try again in 1000 ms (479934ms till timeout)
2022-03-28 11:01:19 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:01:19 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:19 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2335919ms till timeout)
2022-03-28 11:01:19 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (80938ms till timeout)
2022-03-28 11:01:19 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (138464ms till timeout)
2022-03-28 11:01:20 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Wait for Deployment: topic-cluster-name-kafka-clients will be ready not ready, will try again in 1000 ms (478864ms till timeout)
2022-03-28 11:01:20 [ForkJoinPool-1-worker-17] DEBUG [JobUtils:40] Job create-admin-my-cluster-28d9dd2e-kafka-clients was deleted
2022-03-28 11:01:20 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:155] Create/Update Job alter-admin-my-cluster-28d9dd2e-kafka-clients in namespace throttling-quota-st
2022-03-28 11:01:20 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 11:01:20 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:alter-admin-my-cluster-28d9dd2e-kafka-clients
2022-03-28 11:01:20 [ForkJoinPool-1-worker-17] INFO  [JobUtils:81] Waiting for job: alter-admin-my-cluster-28d9dd2e-kafka-clients will be in active state
2022-03-28 11:01:20 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 11:01:20 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=5505da1a-a039-4a47-aceb-373e93b4c7b2, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=723994bf-c11d-4bc9-8a65-9864eb44e356}
2022-03-28 11:01:20 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=5505da1a-a039-4a47-aceb-373e93b4c7b2, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=723994bf-c11d-4bc9-8a65-9864eb44e356}
2022-03-28 11:01:20 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-1 hasn't rolled
2022-03-28 11:01:20 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1698151ms till timeout)
2022-03-28 11:01:20 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job active not ready, will try again in 1000 ms (179955ms till timeout)
2022-03-28 11:01:20 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:20 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2334856ms till timeout)
2022-03-28 11:01:20 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (79880ms till timeout)
2022-03-28 11:01:21 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (137398ms till timeout)
2022-03-28 11:01:21 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Wait for Deployment: topic-cluster-name-kafka-clients will be ready not ready, will try again in 1000 ms (477760ms till timeout)
2022-03-28 11:01:21 [ForkJoinPool-1-worker-17] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 11:01:21 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 11:01:21 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (299873ms till timeout)
2022-03-28 11:01:21 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2333801ms till timeout)
2022-03-28 11:01:22 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (78661ms till timeout)
2022-03-28 11:01:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (136253ms till timeout)
2022-03-28 11:01:22 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Wait for Deployment: topic-cluster-name-kafka-clients will be ready not ready, will try again in 1000 ms (476720ms till timeout)
2022-03-28 11:01:23 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (298686ms till timeout)
2022-03-28 11:01:23 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:23 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2332614ms till timeout)
2022-03-28 11:01:23 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (77562ms till timeout)
2022-03-28 11:01:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (135159ms till timeout)
2022-03-28 11:01:23 [ForkJoinPool-1-worker-5] INFO  [DeploymentUtils:168] Deployment: topic-cluster-name-kafka-clients is ready
2022-03-28 11:01:23 [ForkJoinPool-1-worker-5] INFO  [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-03-28 11:01:23 [ForkJoinPool-1-worker-5] INFO  [TopicST:320] Checking if my-topic-645675602-646682485 is on topic list
2022-03-28 11:01:23 [ForkJoinPool-1-worker-5] INFO  [TopicST:456] Checking topic my-topic-645675602-646682485 in Kafka
2022-03-28 11:01:23 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 11:01:23 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUpdateUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:01:24 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (297484ms till timeout)
2022-03-28 11:01:24 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:24 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2331405ms till timeout)
2022-03-28 11:01:24 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (76433ms till timeout)
2022-03-28 11:01:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (133965ms till timeout)
2022-03-28 11:01:24 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:01:25 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (296386ms till timeout)
2022-03-28 11:01:25 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:25 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2330273ms till timeout)
2022-03-28 11:01:25 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 11:01:25 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:25 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (75252ms till timeout)
2022-03-28 11:01:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (132846ms till timeout)
2022-03-28 11:01:25 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=5505da1a-a039-4a47-aceb-373e93b4c7b2, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=723994bf-c11d-4bc9-8a65-9864eb44e356}
2022-03-28 11:01:25 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=5505da1a-a039-4a47-aceb-373e93b4c7b2, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=723994bf-c11d-4bc9-8a65-9864eb44e356}
2022-03-28 11:01:25 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-1 hasn't rolled
2022-03-28 11:01:25 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1693074ms till timeout)
2022-03-28 11:01:26 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:26 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2329142ms till timeout)
2022-03-28 11:01:26 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (295172ms till timeout)
2022-03-28 11:01:26 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (74071ms till timeout)
2022-03-28 11:01:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (131663ms till timeout)
2022-03-28 11:01:27 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:27 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2327992ms till timeout)
2022-03-28 11:01:27 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (293991ms till timeout)
2022-03-28 11:01:27 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:27 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (72954ms till timeout)
2022-03-28 11:01:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (130554ms till timeout)
2022-03-28 11:01:28 [ForkJoinPool-1-worker-5] INFO  [Exec:417] Command: oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 11:01:28 [ForkJoinPool-1-worker-5] INFO  [Exec:417] Return code: 0
2022-03-28 11:01:28 [ForkJoinPool-1-worker-5] INFO  [TopicST:323] Topic with name my-topic-645675602-646682485 is not created yet
2022-03-28 11:01:28 [ForkJoinPool-1-worker-5] INFO  [TopicST:325] Trying to send messages to non-existing topic my-topic-645675602-646682485
2022-03-28 11:01:28 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUpdateUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:01:29 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:29 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:29 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2326677ms till timeout)
2022-03-28 11:01:29 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (292743ms till timeout)
2022-03-28 11:01:29 [ForkJoinPool-1-worker-5] DEBUG [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@484869f1, which are set.
2022-03-28 11:01:29 [ForkJoinPool-1-worker-5] INFO  [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@52507a27, messages=[], arguments=[--bootstrap-server, topic-cluster-name-kafka-bootstrap.topic-st.svc:9092, --max-messages, 100, --topic, my-topic-645675602-646682485], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='topic-cluster-name-kafka-clients-55b5b77685-c5c78', podNamespace='topic-st', bootstrapServer='topic-cluster-name-kafka-bootstrap.topic-st.svc:9092', topicName='my-topic-645675602-646682485', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@484869f1}
2022-03-28 11:01:29 [ForkJoinPool-1-worker-5] INFO  [InternalKafkaClient:94] Producing 100 messages to topic-cluster-name-kafka-bootstrap.topic-st.svc:9092:my-topic-645675602-646682485 from pod topic-cluster-name-kafka-clients-55b5b77685-c5c78
2022-03-28 11:01:29 [ForkJoinPool-1-worker-5] INFO  [VerifiableClient:192] Client command: oc exec topic-cluster-name-kafka-clients-55b5b77685-c5c78 -n topic-st -- /opt/kafka/producer.sh --bootstrap-server topic-cluster-name-kafka-bootstrap.topic-st.svc:9092 --max-messages 100 --topic my-topic-645675602-646682485
2022-03-28 11:01:29 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc exec topic-cluster-name-kafka-clients-55b5b77685-c5c78 -n topic-st -- /opt/kafka/producer.sh --bootstrap-server topic-cluster-name-kafka-bootstrap.topic-st.svc:9092 --max-messages 100 --topic my-topic-645675602-646682485
2022-03-28 11:01:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129377ms till timeout)
2022-03-28 11:01:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (71772ms till timeout)
2022-03-28 11:01:29 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:01:30 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:30 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2325586ms till timeout)
2022-03-28 11:01:30 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (128273ms till timeout)
2022-03-28 11:01:30 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (70626ms till timeout)
2022-03-28 11:01:30 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (291535ms till timeout)
2022-03-28 11:01:30 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 11:01:30 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=5505da1a-a039-4a47-aceb-373e93b4c7b2, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=723994bf-c11d-4bc9-8a65-9864eb44e356}
2022-03-28 11:01:30 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=5505da1a-a039-4a47-aceb-373e93b4c7b2, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=723994bf-c11d-4bc9-8a65-9864eb44e356}
2022-03-28 11:01:30 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-1 hasn't rolled
2022-03-28 11:01:30 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1687952ms till timeout)
2022-03-28 11:01:31 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:31 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2324451ms till timeout)
2022-03-28 11:01:31 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (127132ms till timeout)
2022-03-28 11:01:31 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (290407ms till timeout)
2022-03-28 11:01:31 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (69498ms till timeout)
2022-03-28 11:01:32 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:32 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:32 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2323286ms till timeout)
2022-03-28 11:01:32 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (289352ms till timeout)
2022-03-28 11:01:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (126011ms till timeout)
2022-03-28 11:01:32 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (68353ms till timeout)
2022-03-28 11:01:33 [ForkJoinPool-1-worker-5] INFO  [InternalKafkaClient:97] Producer finished correctly: true
2022-03-28 11:01:33 [ForkJoinPool-1-worker-5] INFO  [InternalKafkaClient:101] Producer produced 100 messages
2022-03-28 11:01:33 [ForkJoinPool-1-worker-5] DEBUG [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@7a7d4cf1, which are set.
2022-03-28 11:01:33 [ForkJoinPool-1-worker-5] INFO  [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@49fe4df6, messages=[], arguments=[--bootstrap-server, topic-cluster-name-kafka-bootstrap.topic-st.svc:9092, --group-id, my-consumer-group-1719478451, --max-messages, 100, --group-instance-id, instance2065025138, --topic, my-topic-645675602-646682485], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='topic-cluster-name-kafka-clients-55b5b77685-c5c78', podNamespace='topic-st', bootstrapServer='topic-cluster-name-kafka-bootstrap.topic-st.svc:9092', topicName='my-topic-645675602-646682485', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-1719478451', consumerInstanceId='instance2065025138', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@7a7d4cf1}
2022-03-28 11:01:33 [ForkJoinPool-1-worker-5] INFO  [InternalKafkaClient:157] Consuming 100 messages from topic-cluster-name-kafka-bootstrap.topic-st.svc:9092#my-topic-645675602-646682485 from pod topic-cluster-name-kafka-clients-55b5b77685-c5c78
2022-03-28 11:01:33 [ForkJoinPool-1-worker-5] INFO  [VerifiableClient:192] Client command: oc exec topic-cluster-name-kafka-clients-55b5b77685-c5c78 -n topic-st -- /opt/kafka/consumer.sh --bootstrap-server topic-cluster-name-kafka-bootstrap.topic-st.svc:9092 --group-id my-consumer-group-1719478451 --max-messages 100 --group-instance-id instance2065025138 --topic my-topic-645675602-646682485
2022-03-28 11:01:33 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc exec topic-cluster-name-kafka-clients-55b5b77685-c5c78 -n topic-st -- /opt/kafka/consumer.sh --bootstrap-server topic-cluster-name-kafka-bootstrap.topic-st.svc:9092 --group-id my-consumer-group-1719478451 --max-messages 100 --group-instance-id instance2065025138 --topic my-topic-645675602-646682485
2022-03-28 11:01:33 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:33 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:33 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2322157ms till timeout)
2022-03-28 11:01:33 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (288193ms till timeout)
2022-03-28 11:01:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (124884ms till timeout)
2022-03-28 11:01:33 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (67267ms till timeout)
2022-03-28 11:01:33 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUpdateUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:01:34 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:34 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2321097ms till timeout)
2022-03-28 11:01:34 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:01:34 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (123687ms till timeout)
2022-03-28 11:01:34 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (286994ms till timeout)
2022-03-28 11:01:34 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (66085ms till timeout)
2022-03-28 11:01:35 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:35 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2320025ms till timeout)
2022-03-28 11:01:35 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 11:01:35 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=5505da1a-a039-4a47-aceb-373e93b4c7b2, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=723994bf-c11d-4bc9-8a65-9864eb44e356}
2022-03-28 11:01:35 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=5505da1a-a039-4a47-aceb-373e93b4c7b2, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=723994bf-c11d-4bc9-8a65-9864eb44e356}
2022-03-28 11:01:35 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-1 hasn't rolled
2022-03-28 11:01:35 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1682876ms till timeout)
2022-03-28 11:01:35 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:35 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (285886ms till timeout)
2022-03-28 11:01:35 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (64970ms till timeout)
2022-03-28 11:01:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (122551ms till timeout)
2022-03-28 11:01:36 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:36 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2318962ms till timeout)
2022-03-28 11:01:36 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (284829ms till timeout)
2022-03-28 11:01:36 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (63919ms till timeout)
2022-03-28 11:01:36 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (121482ms till timeout)
2022-03-28 11:01:37 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:37 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2317910ms till timeout)
2022-03-28 11:01:38 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (283759ms till timeout)
2022-03-28 11:01:38 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:38 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (62809ms till timeout)
2022-03-28 11:01:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (120401ms till timeout)
2022-03-28 11:01:38 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:38 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2316813ms till timeout)
2022-03-28 11:01:38 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUpdateUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:01:39 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (282660ms till timeout)
2022-03-28 11:01:39 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (61728ms till timeout)
2022-03-28 11:01:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (119328ms till timeout)
2022-03-28 11:01:39 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:01:39 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:39 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2315741ms till timeout)
2022-03-28 11:01:40 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (281544ms till timeout)
2022-03-28 11:01:40 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:40 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (60582ms till timeout)
2022-03-28 11:01:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (118175ms till timeout)
2022-03-28 11:01:40 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 11:01:40 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=5505da1a-a039-4a47-aceb-373e93b4c7b2, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=723994bf-c11d-4bc9-8a65-9864eb44e356}
2022-03-28 11:01:40 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=5505da1a-a039-4a47-aceb-373e93b4c7b2, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=723994bf-c11d-4bc9-8a65-9864eb44e356}
2022-03-28 11:01:40 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-1 hasn't rolled
2022-03-28 11:01:40 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1677837ms till timeout)
2022-03-28 11:01:40 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:40 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2314701ms till timeout)
2022-03-28 11:01:41 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (280482ms till timeout)
2022-03-28 11:01:41 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:41 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (59542ms till timeout)
2022-03-28 11:01:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (117113ms till timeout)
2022-03-28 11:01:42 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:42 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2313667ms till timeout)
2022-03-28 11:01:42 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (279415ms till timeout)
2022-03-28 11:01:42 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:42 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (58275ms till timeout)
2022-03-28 11:01:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (115849ms till timeout)
2022-03-28 11:01:43 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:43 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2312567ms till timeout)
2022-03-28 11:01:43 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (278296ms till timeout)
2022-03-28 11:01:43 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:43 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (57142ms till timeout)
2022-03-28 11:01:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (114725ms till timeout)
2022-03-28 11:01:43 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUpdateUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:01:44 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:44 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2311496ms till timeout)
2022-03-28 11:01:44 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:01:44 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (277132ms till timeout)
2022-03-28 11:01:44 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (56033ms till timeout)
2022-03-28 11:01:44 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (113606ms till timeout)
2022-03-28 11:01:45 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:45 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2310425ms till timeout)
2022-03-28 11:01:45 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (276074ms till timeout)
2022-03-28 11:01:45 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 11:01:45 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (112518ms till timeout)
2022-03-28 11:01:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (54905ms till timeout)
2022-03-28 11:01:45 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=5505da1a-a039-4a47-aceb-373e93b4c7b2, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=723994bf-c11d-4bc9-8a65-9864eb44e356}
2022-03-28 11:01:45 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=5505da1a-a039-4a47-aceb-373e93b4c7b2, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=723994bf-c11d-4bc9-8a65-9864eb44e356}
2022-03-28 11:01:45 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-1 hasn't rolled
2022-03-28 11:01:45 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1672734ms till timeout)
2022-03-28 11:01:46 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:46 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2309391ms till timeout)
2022-03-28 11:01:46 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (274852ms till timeout)
2022-03-28 11:01:47 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (53776ms till timeout)
2022-03-28 11:01:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (111331ms till timeout)
2022-03-28 11:01:47 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:47 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2308288ms till timeout)
2022-03-28 11:01:48 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (273760ms till timeout)
2022-03-28 11:01:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (52698ms till timeout)
2022-03-28 11:01:48 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (110198ms till timeout)
2022-03-28 11:01:48 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:48 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2307236ms till timeout)
2022-03-28 11:01:48 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUpdateUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:01:49 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (272648ms till timeout)
2022-03-28 11:01:49 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (51591ms till timeout)
2022-03-28 11:01:49 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (109059ms till timeout)
2022-03-28 11:01:49 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:49 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2306186ms till timeout)
2022-03-28 11:01:49 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:01:50 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (271602ms till timeout)
2022-03-28 11:01:50 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (50533ms till timeout)
2022-03-28 11:01:50 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (107908ms till timeout)
2022-03-28 11:01:50 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:50 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2305134ms till timeout)
2022-03-28 11:01:50 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 11:01:51 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=5505da1a-a039-4a47-aceb-373e93b4c7b2, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=723994bf-c11d-4bc9-8a65-9864eb44e356}
2022-03-28 11:01:51 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=5505da1a-a039-4a47-aceb-373e93b4c7b2, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=723994bf-c11d-4bc9-8a65-9864eb44e356}
2022-03-28 11:01:51 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-c0830cbe-kafka-1 hasn't rolled
2022-03-28 11:01:51 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-c0830cbe-kafka rolling update not ready, will try again in 5000 ms (1667646ms till timeout)
2022-03-28 11:01:51 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (270534ms till timeout)
2022-03-28 11:01:51 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (49421ms till timeout)
2022-03-28 11:01:51 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:51 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:51 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2304060ms till timeout)
2022-03-28 11:01:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (106774ms till timeout)
2022-03-28 11:01:51 [ForkJoinPool-1-worker-5] INFO  [InternalKafkaClient:160] Consumer finished correctly: true
2022-03-28 11:01:51 [ForkJoinPool-1-worker-5] INFO  [InternalKafkaClient:163] Consumer consumed 100 messages
2022-03-28 11:01:51 [ForkJoinPool-1-worker-5] INFO  [TopicST:341] Checking if my-topic-645675602-646682485 is on topic list
2022-03-28 11:01:51 [ForkJoinPool-1-worker-5] INFO  [TopicST:456] Checking topic my-topic-645675602-646682485 in Kafka
2022-03-28 11:01:51 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 11:01:52 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (269420ms till timeout)
2022-03-28 11:01:52 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (48233ms till timeout)
2022-03-28 11:01:52 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:52 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:52 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2302977ms till timeout)
2022-03-28 11:01:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (105730ms till timeout)
2022-03-28 11:01:53 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (268336ms till timeout)
2022-03-28 11:01:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (47168ms till timeout)
2022-03-28 11:01:53 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:53 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:53 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2301938ms till timeout)
2022-03-28 11:01:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (104693ms till timeout)
2022-03-28 11:01:53 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUpdateUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:01:54 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (267204ms till timeout)
2022-03-28 11:01:54 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:01:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (46055ms till timeout)
2022-03-28 11:01:54 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:54 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:54 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2300891ms till timeout)
2022-03-28 11:01:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (103610ms till timeout)
2022-03-28 11:01:55 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (266090ms till timeout)
2022-03-28 11:01:56 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-c0830cbe-kafka-0=07691c02-752a-471f-9593-926114c656de, my-cluster-c0830cbe-kafka-1=ca5721ed-e5af-4fc2-a918-f0c2a534cd62, my-cluster-c0830cbe-kafka-2=a6516b5d-0be0-4b4a-b4cf-6b7c62c2f9e2}
2022-03-28 11:01:56 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:56 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2299606ms till timeout)
2022-03-28 11:01:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (44737ms till timeout)
2022-03-28 11:01:56 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:56 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-c0830cbe-kafka-0=5505da1a-a039-4a47-aceb-373e93b4c7b2, my-cluster-c0830cbe-kafka-2=723994bf-c11d-4bc9-8a65-9864eb44e356}
2022-03-28 11:01:56 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-c0830cbe-kafka-0=5505da1a-a039-4a47-aceb-373e93b4c7b2, my-cluster-c0830cbe-kafka-2=723994bf-c11d-4bc9-8a65-9864eb44e356}
2022-03-28 11:01:56 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:55] All pods seem to have rolled
2022-03-28 11:01:56 [ForkJoinPool-1-worker-1] INFO  [RollingUpdateUtils:86] Component with name: my-cluster-c0830cbe-kafka has been successfully rolled
2022-03-28 11:01:56 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:87] Component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={}) successfully rolled
2022-03-28 11:01:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (102284ms till timeout)
2022-03-28 11:01:56 [ForkJoinPool-1-worker-1] INFO  [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-c0830cbe-kafka to be ready
2022-03-28 11:01:56 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready
2022-03-28 11:01:56 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:56 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1799941ms till timeout)
2022-03-28 11:01:56 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (265016ms till timeout)
2022-03-28 11:01:57 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:57 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2298561ms till timeout)
2022-03-28 11:01:57 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (43649ms till timeout)
2022-03-28 11:01:57 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (101205ms till timeout)
2022-03-28 11:01:57 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 11:01:57 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-c0830cbe-kafka-1)
2022-03-28 11:01:57 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1798888ms till timeout)
2022-03-28 11:01:57 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (263957ms till timeout)
2022-03-28 11:01:58 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:58 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2297475ms till timeout)
2022-03-28 11:01:58 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (42571ms till timeout)
2022-03-28 11:01:58 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (100138ms till timeout)
2022-03-28 11:01:58 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 11:01:58 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-c0830cbe-kafka-1)
2022-03-28 11:01:58 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1797855ms till timeout)
2022-03-28 11:01:58 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (262837ms till timeout)
2022-03-28 11:01:58 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUpdateUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:01:59 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:01:59 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2296346ms till timeout)
2022-03-28 11:01:59 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:01:59 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 11:01:59 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-c0830cbe-kafka-1)
2022-03-28 11:01:59 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1796759ms till timeout)
2022-03-28 11:01:59 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (41374ms till timeout)
2022-03-28 11:01:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (98973ms till timeout)
2022-03-28 11:01:59 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:02:00 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (261730ms till timeout)
2022-03-28 11:02:00 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:02:00 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2295296ms till timeout)
2022-03-28 11:02:00 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (40295ms till timeout)
2022-03-28 11:02:00 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T11:01:57Z, conditions=[JobCondition(lastProbeTime=2022-03-28T11:01:57Z, lastTransitionTime=2022-03-28T11:01:57Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T10:59:55Z, succeeded=1, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:00 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 11:02:00 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-c0830cbe-kafka-1)
2022-03-28 11:02:00 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1795643ms till timeout)
2022-03-28 11:02:00 [ForkJoinPool-1-worker-9] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 11:02:00 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 11:02:00 [ForkJoinPool-1-worker-9] INFO  [PodUtils:189] Message All topics created found in create-admin-my-cluster-8f262de5-kafka-clients-vdwjn log
2022-03-28 11:02:00 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment create-admin-my-cluster-8f262de5-kafka-clients deletion
2022-03-28 11:02:00 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for ReplicaSet create-admin-my-cluster-8f262de5-kafka-clients to be deleted
2022-03-28 11:02:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] ReplicaSet create-admin-my-cluster-8f262de5-kafka-clients to be deleted not ready, will try again in 5000 ms (179952ms till timeout)
2022-03-28 11:02:01 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (260609ms till timeout)
2022-03-28 11:02:01 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:02:01 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2294265ms till timeout)
2022-03-28 11:02:01 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 11:02:01 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-c0830cbe-kafka-1)
2022-03-28 11:02:01 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1794561ms till timeout)
2022-03-28 11:02:01 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (39175ms till timeout)
2022-03-28 11:02:02 [ForkJoinPool-1-worker-5] INFO  [Exec:417] Command: oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 11:02:02 [ForkJoinPool-1-worker-5] INFO  [Exec:417] Return code: 0
2022-03-28 11:02:02 [ForkJoinPool-1-worker-5] INFO  [KafkaTopicUtils:78] Waiting for KafkaTopic my-topic-645675602-646682485 creation 
2022-03-28 11:02:02 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for KafkaTopic creation my-topic-645675602-646682485
2022-03-28 11:02:02 [ForkJoinPool-1-worker-5] INFO  [TopicST:353] Topic successfully created
2022-03-28 11:02:02 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:674] ============================================================================
2022-03-28 11:02:02 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:675] [operators.topic.TopicST - After Each] - Clean up after test
2022-03-28 11:02:02 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:344] ############################################################################
2022-03-28 11:02:02 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:348] Delete all resources for testSendingMessagesToNonExistingTopic
2022-03-28 11:02:02 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of Deployment topic-cluster-name-kafka-clients in namespace topic-st
2022-03-28 11:02:02 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (259458ms till timeout)
2022-03-28 11:02:02 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:topic-cluster-name-kafka-clients
2022-03-28 11:02:02 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:02:02 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2293187ms till timeout)
2022-03-28 11:02:02 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:topic-cluster-name-kafka-clients not ready, will try again in 10000 ms (479804ms till timeout)
2022-03-28 11:02:02 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 11:02:02 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-c0830cbe-kafka-1)
2022-03-28 11:02:02 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1793487ms till timeout)
2022-03-28 11:02:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (38107ms till timeout)
2022-03-28 11:02:03 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (258289ms till timeout)
2022-03-28 11:02:03 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:02:03 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2292097ms till timeout)
2022-03-28 11:02:03 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 11:02:03 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-c0830cbe-kafka-1)
2022-03-28 11:02:03 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1792425ms till timeout)
2022-03-28 11:02:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (36941ms till timeout)
2022-03-28 11:02:03 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUpdateUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:02:04 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (257200ms till timeout)
2022-03-28 11:02:04 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:02:04 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:02:04 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2291042ms till timeout)
2022-03-28 11:02:04 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 11:02:04 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-c0830cbe-kafka-1)
2022-03-28 11:02:04 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1791366ms till timeout)
2022-03-28 11:02:04 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (35883ms till timeout)
2022-03-28 11:02:05 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (256089ms till timeout)
2022-03-28 11:02:05 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:02:05 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2289988ms till timeout)
2022-03-28 11:02:05 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 11:02:05 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-c0830cbe-kafka-1)
2022-03-28 11:02:05 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1790229ms till timeout)
2022-03-28 11:02:06 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:40] Job create-admin-my-cluster-8f262de5-kafka-clients was deleted
2022-03-28 11:02:06 [ForkJoinPool-1-worker-9] INFO  [ThrottlingQuotaST:112] Executing 4/5 iteration.
2022-03-28 11:02:06 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st
2022-03-28 11:02:06 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (34835ms till timeout)
2022-03-28 11:02:06 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:create-admin-my-cluster-8f262de5-kafka-clients
2022-03-28 11:02:06 [ForkJoinPool-1-worker-9] INFO  [JobUtils:81] Waiting for job: create-admin-my-cluster-8f262de5-kafka-clients will be in active state
2022-03-28 11:02:06 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 11:02:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job active not ready, will try again in 1000 ms (179986ms till timeout)
2022-03-28 11:02:06 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (255029ms till timeout)
2022-03-28 11:02:06 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:02:06 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2288908ms till timeout)
2022-03-28 11:02:07 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 11:02:07 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-c0830cbe-kafka-1)
2022-03-28 11:02:07 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1789155ms till timeout)
2022-03-28 11:02:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (33784ms till timeout)
2022-03-28 11:02:07 [ForkJoinPool-1-worker-9] INFO  [ClientUtils:76] Waiting for producer/consumer:create-admin-my-cluster-8f262de5-kafka-clients to finished
2022-03-28 11:02:07 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 11:02:07 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (219904ms till timeout)
2022-03-28 11:02:07 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (254002ms till timeout)
2022-03-28 11:02:07 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:02:07 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2287874ms till timeout)
2022-03-28 11:02:08 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 11:02:08 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-c0830cbe-kafka-1)
2022-03-28 11:02:08 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1788061ms till timeout)
2022-03-28 11:02:08 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:08 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (32583ms till timeout)
2022-03-28 11:02:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (218746ms till timeout)
2022-03-28 11:02:08 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (252951ms till timeout)
2022-03-28 11:02:08 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:02:08 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2286835ms till timeout)
2022-03-28 11:02:08 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUpdateUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:02:09 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 11:02:09 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-c0830cbe-kafka-1)
2022-03-28 11:02:09 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1787023ms till timeout)
2022-03-28 11:02:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (31492ms till timeout)
2022-03-28 11:02:09 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (217687ms till timeout)
2022-03-28 11:02:09 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:02:09 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (251892ms till timeout)
2022-03-28 11:02:09 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:02:09 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2285754ms till timeout)
2022-03-28 11:02:10 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 11:02:10 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-c0830cbe-kafka-1)
2022-03-28 11:02:10 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1785951ms till timeout)
2022-03-28 11:02:10 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (30441ms till timeout)
2022-03-28 11:02:10 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (216625ms till timeout)
2022-03-28 11:02:10 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (250820ms till timeout)
2022-03-28 11:02:10 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:02:10 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2284721ms till timeout)
2022-03-28 11:02:11 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 11:02:11 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-c0830cbe-kafka-1)
2022-03-28 11:02:11 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1784922ms till timeout)
2022-03-28 11:02:11 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (29376ms till timeout)
2022-03-28 11:02:11 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (215549ms till timeout)
2022-03-28 11:02:11 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (249789ms till timeout)
2022-03-28 11:02:12 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:02:12 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2283682ms till timeout)
2022-03-28 11:02:12 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 11:02:12 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-c0830cbe-kafka-1)
2022-03-28 11:02:12 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1783899ms till timeout)
2022-03-28 11:02:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (28352ms till timeout)
2022-03-28 11:02:12 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (214493ms till timeout)
2022-03-28 11:02:12 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:topic-cluster-name-kafka-clients not ready, will try again in 10000 ms (469690ms till timeout)
2022-03-28 11:02:13 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (248707ms till timeout)
2022-03-28 11:02:13 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:02:13 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2282611ms till timeout)
2022-03-28 11:02:13 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 11:02:13 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-c0830cbe-kafka-1)
2022-03-28 11:02:13 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1782783ms till timeout)
2022-03-28 11:02:13 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (27170ms till timeout)
2022-03-28 11:02:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (213423ms till timeout)
2022-03-28 11:02:13 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUpdateUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:02:14 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:02:14 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2281474ms till timeout)
2022-03-28 11:02:14 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (247530ms till timeout)
2022-03-28 11:02:14 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 11:02:14 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-c0830cbe-kafka-1)
2022-03-28 11:02:14 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1781740ms till timeout)
2022-03-28 11:02:14 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:02:14 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (26082ms till timeout)
2022-03-28 11:02:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (212303ms till timeout)
2022-03-28 11:02:15 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:02:15 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2280426ms till timeout)
2022-03-28 11:02:15 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (246408ms till timeout)
2022-03-28 11:02:15 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 11:02:15 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-c0830cbe-kafka-1)
2022-03-28 11:02:15 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1780684ms till timeout)
2022-03-28 11:02:15 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (25021ms till timeout)
2022-03-28 11:02:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (211239ms till timeout)
2022-03-28 11:02:16 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:02:16 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2279300ms till timeout)
2022-03-28 11:02:16 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (245240ms till timeout)
2022-03-28 11:02:16 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 11:02:16 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-c0830cbe-kafka-1)
2022-03-28 11:02:16 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1779614ms till timeout)
2022-03-28 11:02:17 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (23700ms till timeout)
2022-03-28 11:02:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (209858ms till timeout)
2022-03-28 11:02:17 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:02:17 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2278208ms till timeout)
2022-03-28 11:02:17 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (244152ms till timeout)
2022-03-28 11:02:17 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 11:02:17 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-c0830cbe-kafka-1)
2022-03-28 11:02:17 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1778504ms till timeout)
2022-03-28 11:02:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (22629ms till timeout)
2022-03-28 11:02:18 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (208787ms till timeout)
2022-03-28 11:02:18 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:02:18 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2277110ms till timeout)
2022-03-28 11:02:18 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (243096ms till timeout)
2022-03-28 11:02:18 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 11:02:18 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-c0830cbe-kafka-1)
2022-03-28 11:02:18 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1777461ms till timeout)
2022-03-28 11:02:18 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUpdateUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:02:19 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (21485ms till timeout)
2022-03-28 11:02:19 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (207694ms till timeout)
2022-03-28 11:02:19 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:02:19 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:02:19 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2276054ms till timeout)
2022-03-28 11:02:19 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 11:02:19 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-1 not ready: kafka)
2022-03-28 11:02:19 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-2 not ready: kafka)
2022-03-28 11:02:19 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:106] Pods my-cluster-c0830cbe-kafka-0, my-cluster-c0830cbe-kafka-1, my-cluster-c0830cbe-kafka-2 are ready
2022-03-28 11:02:19 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1776406ms till timeout)
2022-03-28 11:02:19 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (241935ms till timeout)
2022-03-28 11:02:20 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (20449ms till timeout)
2022-03-28 11:02:20 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (206636ms till timeout)
2022-03-28 11:02:20 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:02:20 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2275017ms till timeout)
2022-03-28 11:02:20 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 11:02:20 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-1 not ready: kafka)
2022-03-28 11:02:20 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-2 not ready: kafka)
2022-03-28 11:02:20 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:106] Pods my-cluster-c0830cbe-kafka-0, my-cluster-c0830cbe-kafka-1, my-cluster-c0830cbe-kafka-2 are ready
2022-03-28 11:02:20 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1775290ms till timeout)
2022-03-28 11:02:21 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (240733ms till timeout)
2022-03-28 11:02:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (19380ms till timeout)
2022-03-28 11:02:21 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (205541ms till timeout)
2022-03-28 11:02:21 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:02:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2273946ms till timeout)
2022-03-28 11:02:22 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 11:02:22 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-1 not ready: kafka)
2022-03-28 11:02:22 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-2 not ready: kafka)
2022-03-28 11:02:22 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:106] Pods my-cluster-c0830cbe-kafka-0, my-cluster-c0830cbe-kafka-1, my-cluster-c0830cbe-kafka-2 are ready
2022-03-28 11:02:22 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1774201ms till timeout)
2022-03-28 11:02:22 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (239615ms till timeout)
2022-03-28 11:02:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (18294ms till timeout)
2022-03-28 11:02:22 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (204384ms till timeout)
2022-03-28 11:02:22 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:02:22 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2272887ms till timeout)
2022-03-28 11:02:22 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:topic-cluster-name-kafka-clients not ready, will try again in 10000 ms (459489ms till timeout)
2022-03-28 11:02:23 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 11:02:23 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-1 not ready: kafka)
2022-03-28 11:02:23 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-2 not ready: kafka)
2022-03-28 11:02:23 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:106] Pods my-cluster-c0830cbe-kafka-0, my-cluster-c0830cbe-kafka-1, my-cluster-c0830cbe-kafka-2 are ready
2022-03-28 11:02:23 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1773160ms till timeout)
2022-03-28 11:02:23 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (238566ms till timeout)
2022-03-28 11:02:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (17169ms till timeout)
2022-03-28 11:02:23 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (203239ms till timeout)
2022-03-28 11:02:23 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:02:23 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2271818ms till timeout)
2022-03-28 11:02:23 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUpdateUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:02:24 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 11:02:24 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-1 not ready: kafka)
2022-03-28 11:02:24 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-2 not ready: kafka)
2022-03-28 11:02:24 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:106] Pods my-cluster-c0830cbe-kafka-0, my-cluster-c0830cbe-kafka-1, my-cluster-c0830cbe-kafka-2 are ready
2022-03-28 11:02:24 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1772104ms till timeout)
2022-03-28 11:02:24 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (237472ms till timeout)
2022-03-28 11:02:24 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:02:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (16123ms till timeout)
2022-03-28 11:02:24 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:24 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:02:24 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2270721ms till timeout)
2022-03-28 11:02:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (202089ms till timeout)
2022-03-28 11:02:25 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 11:02:25 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-1 not ready: kafka)
2022-03-28 11:02:25 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-2 not ready: kafka)
2022-03-28 11:02:25 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:106] Pods my-cluster-c0830cbe-kafka-0, my-cluster-c0830cbe-kafka-1, my-cluster-c0830cbe-kafka-2 are ready
2022-03-28 11:02:25 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1771036ms till timeout)
2022-03-28 11:02:25 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (236425ms till timeout)
2022-03-28 11:02:25 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (14926ms till timeout)
2022-03-28 11:02:26 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:02:26 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2269663ms till timeout)
2022-03-28 11:02:26 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (200971ms till timeout)
2022-03-28 11:02:26 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 11:02:26 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-1 not ready: kafka)
2022-03-28 11:02:26 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-2 not ready: kafka)
2022-03-28 11:02:26 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:106] Pods my-cluster-c0830cbe-kafka-0, my-cluster-c0830cbe-kafka-1, my-cluster-c0830cbe-kafka-2 are ready
2022-03-28 11:02:26 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1770016ms till timeout)
2022-03-28 11:02:26 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (235319ms till timeout)
2022-03-28 11:02:27 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (13822ms till timeout)
2022-03-28 11:02:27 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:02:27 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2268576ms till timeout)
2022-03-28 11:02:27 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (199882ms till timeout)
2022-03-28 11:02:27 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 11:02:27 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-1 not ready: kafka)
2022-03-28 11:02:27 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-2 not ready: kafka)
2022-03-28 11:02:27 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:106] Pods my-cluster-c0830cbe-kafka-0, my-cluster-c0830cbe-kafka-1, my-cluster-c0830cbe-kafka-2 are ready
2022-03-28 11:02:27 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1768982ms till timeout)
2022-03-28 11:02:27 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (234223ms till timeout)
2022-03-28 11:02:28 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (12767ms till timeout)
2022-03-28 11:02:28 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:02:28 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2267538ms till timeout)
2022-03-28 11:02:28 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:28 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 11:02:28 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-1 not ready: kafka)
2022-03-28 11:02:28 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-2 not ready: kafka)
2022-03-28 11:02:28 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:106] Pods my-cluster-c0830cbe-kafka-0, my-cluster-c0830cbe-kafka-1, my-cluster-c0830cbe-kafka-2 are ready
2022-03-28 11:02:28 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1767890ms till timeout)
2022-03-28 11:02:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (198730ms till timeout)
2022-03-28 11:02:28 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (233104ms till timeout)
2022-03-28 11:02:28 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUpdateUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:02:29 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:02:29 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2266430ms till timeout)
2022-03-28 11:02:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (11586ms till timeout)
2022-03-28 11:02:29 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 11:02:29 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-1 not ready: kafka)
2022-03-28 11:02:29 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-2 not ready: kafka)
2022-03-28 11:02:29 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:106] Pods my-cluster-c0830cbe-kafka-0, my-cluster-c0830cbe-kafka-1, my-cluster-c0830cbe-kafka-2 are ready
2022-03-28 11:02:29 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-c0830cbe-kafka, strimzi.io/cluster=my-cluster-c0830cbe, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1766855ms till timeout)
2022-03-28 11:02:29 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (197588ms till timeout)
2022-03-28 11:02:29 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:02:29 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (231910ms till timeout)
2022-03-28 11:02:30 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:02:30 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2265398ms till timeout)
2022-03-28 11:02:30 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (10530ms till timeout)
2022-03-28 11:02:30 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-0 not ready: kafka)
2022-03-28 11:02:30 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-1 not ready: kafka)
2022-03-28 11:02:30 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-c0830cbe-kafka-2 not ready: kafka)
2022-03-28 11:02:30 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:106] Pods my-cluster-c0830cbe-kafka-0, my-cluster-c0830cbe-kafka-1, my-cluster-c0830cbe-kafka-2 are ready
2022-03-28 11:02:30 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-c0830cbe will have desired state: Ready
2022-03-28 11:02:30 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-c0830cbe will have desired state: Ready
2022-03-28 11:02:30 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:444] Kafka: my-cluster-c0830cbe is in desired state: Ready
2022-03-28 11:02:30 [ForkJoinPool-1-worker-1] INFO  [RollingUpdateUtils:132] Kafka: my-cluster-c0830cbe is ready
2022-03-28 11:02:30 [ForkJoinPool-1-worker-1] INFO  [CruiseControlConfigurationST:136] Verifying that in Kafka config map there is configuration to cruise control metric reporter
2022-03-28 11:02:30 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Verify that kafka configuration {cruise.control.metrics.reporter.ssl.keystore.location=/tmp/kafka/cluster.keystore.p12, cruise.control.metrics.reporter.ssl.endpoint.identification.algorithm=HTTPS, cruise.control.metrics.reporter.ssl.truststore.password=${CERTS_STORE_PASSWORD}, cruise.control.metrics.reporter.ssl.truststore.type=PKCS12, cruise.control.metrics.reporter.ssl.keystore.password=${CERTS_STORE_PASSWORD}, cruise.control.metrics.topic.replication.factor=3, cluster-name=my-cluster-c0830cbe, cruise.control.metrics.reporter.security.protocol=SSL, cruise.control.metrics.reporter.ssl.keystore.type=PKCS12, cruise.control.metrics.topic.num.partitions=1, cruise.control.metrics.reporter.ssl.truststore.location=/tmp/kafka/cluster.truststore.p12, cruise.control.metrics.topic=strimzi.cruisecontrol.metrics, cruise.control.metrics.topic.min.insync.replicas=1, cruise.control.metrics.reporter.bootstrap.servers=my-cluster-c0830cbe-kafka-brokers:9091, cruise.control.metrics.topic.auto.create=true} has correct cruise control metric reporter properties
2022-03-28 11:02:30 [ForkJoinPool-1-worker-1] INFO  [CruiseControlConfigurationST:139] Verifying that Cruise Control topics are created after CC is instantiated.
2022-03-28 11:02:30 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Verify that kafka contains cruise control topics with related configuration.
2022-03-28 11:02:30 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Verify that kafka contains cruise control topics with related configuration.
2022-03-28 11:02:30 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (196435ms till timeout)
2022-03-28 11:02:30 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:674] ============================================================================
2022-03-28 11:02:30 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:675] [cruisecontrol.CruiseControlConfigurationST - After Each] - Clean up after test
2022-03-28 11:02:30 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:344] ############################################################################
2022-03-28 11:02:30 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:348] Delete all resources for testDeployAndUnDeployCruiseControl
2022-03-28 11:02:30 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:241] Delete of Kafka my-cluster-c0830cbe in namespace namespace-3
2022-03-28 11:02:30 [ForkJoinPool-1-worker-1] INFO  [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-3, for cruise control Kafka cluster my-cluster-c0830cbe
2022-03-28 11:02:30 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (230799ms till timeout)
2022-03-28 11:02:31 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:02:31 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2264317ms till timeout)
2022-03-28 11:02:31 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (9436ms till timeout)
2022-03-28 11:02:31 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-c0830cbe
2022-03-28 11:02:31 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-c0830cbe not ready, will try again in 10000 ms (839987ms till timeout)
2022-03-28 11:02:31 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (195333ms till timeout)
2022-03-28 11:02:32 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (229470ms till timeout)
2022-03-28 11:02:32 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:02:32 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2263078ms till timeout)
2022-03-28 11:02:32 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (8075ms till timeout)
2022-03-28 11:02:32 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (194120ms till timeout)
2022-03-28 11:02:33 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:topic-cluster-name-kafka-clients not ready, will try again in 10000 ms (449154ms till timeout)
2022-03-28 11:02:33 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (228251ms till timeout)
2022-03-28 11:02:33 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:02:33 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2262000ms till timeout)
2022-03-28 11:02:33 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (6920ms till timeout)
2022-03-28 11:02:33 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUpdateUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:02:34 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (193018ms till timeout)
2022-03-28 11:02:34 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:02:34 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (227134ms till timeout)
2022-03-28 11:02:34 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:02:34 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2260979ms till timeout)
2022-03-28 11:02:35 [ForkJoinPool-1-worker-7] INFO  [PodUtils:189] Message Successfully removed all 100 found in delete-admin-my-cluster-5a0bfe14-kafka-clients-46jzd log
2022-03-28 11:02:35 [ForkJoinPool-1-worker-7] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment delete-admin-my-cluster-5a0bfe14-kafka-clients deletion
2022-03-28 11:02:35 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for ReplicaSet delete-admin-my-cluster-5a0bfe14-kafka-clients to be deleted
2022-03-28 11:02:35 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (191912ms till timeout)
2022-03-28 11:02:35 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] ReplicaSet delete-admin-my-cluster-5a0bfe14-kafka-clients to be deleted not ready, will try again in 5000 ms (179928ms till timeout)
2022-03-28 11:02:35 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (226006ms till timeout)
2022-03-28 11:02:35 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:02:35 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2259915ms till timeout)
2022-03-28 11:02:36 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (190809ms till timeout)
2022-03-28 11:02:36 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (224960ms till timeout)
2022-03-28 11:02:36 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:02:36 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2258794ms till timeout)
2022-03-28 11:02:37 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (189596ms till timeout)
2022-03-28 11:02:37 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (223826ms till timeout)
2022-03-28 11:02:37 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:02:37 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2257736ms till timeout)
2022-03-28 11:02:38 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (188557ms till timeout)
2022-03-28 11:02:38 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUpdateUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:02:38 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (222772ms till timeout)
2022-03-28 11:02:39 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:02:39 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2256673ms till timeout)
2022-03-28 11:02:39 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:39 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:02:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (187448ms till timeout)
2022-03-28 11:02:40 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (221700ms till timeout)
2022-03-28 11:02:40 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:02:40 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2255619ms till timeout)
2022-03-28 11:02:40 [ForkJoinPool-1-worker-7] DEBUG [JobUtils:40] Job delete-admin-my-cluster-5a0bfe14-kafka-clients was deleted
2022-03-28 11:02:40 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update Job list-admin-my-cluster-5a0bfe14-kafka-clients in namespace throttling-quota-st
2022-03-28 11:02:40 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:list-admin-my-cluster-5a0bfe14-kafka-clients
2022-03-28 11:02:40 [ForkJoinPool-1-worker-7] INFO  [JobUtils:81] Waiting for job: list-admin-my-cluster-5a0bfe14-kafka-clients will be in active state
2022-03-28 11:02:40 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 11:02:40 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job active not ready, will try again in 1000 ms (179940ms till timeout)
2022-03-28 11:02:40 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (186273ms till timeout)
2022-03-28 11:02:41 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (220626ms till timeout)
2022-03-28 11:02:41 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:02:41 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2254543ms till timeout)
2022-03-28 11:02:41 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:369] ############################################################################
2022-03-28 11:02:41 [ForkJoinPool-1-worker-1] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-3 for test case:testDeployAndUnDeployCruiseControl
2022-03-28 11:02:41 [ForkJoinPool-1-worker-7] INFO  [ClientUtils:76] Waiting for producer/consumer:list-admin-my-cluster-5a0bfe14-kafka-clients to finished
2022-03-28 11:02:41 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 11:02:41 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job list-admin-my-cluster-5a0bfe14-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:38Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:41 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Namespace namespace-3 removal
2022-03-28 11:02:41 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 11:02:41 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (119948ms till timeout)
2022-03-28 11:02:41 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 11:02:41 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 11:02:41 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (479715ms till timeout)
2022-03-28 11:02:41 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (185199ms till timeout)
2022-03-28 11:02:42 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:02:42 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2253468ms till timeout)
2022-03-28 11:02:42 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (219536ms till timeout)
2022-03-28 11:02:42 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job list-admin-my-cluster-5a0bfe14-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:38Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:42 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (118856ms till timeout)
2022-03-28 11:02:42 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 11:02:42 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (184065ms till timeout)
2022-03-28 11:02:43 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 11:02:43 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 11:02:43 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (478245ms till timeout)
2022-03-28 11:02:43 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (218429ms till timeout)
2022-03-28 11:02:43 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:02:43 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2252357ms till timeout)
2022-03-28 11:02:43 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:topic-cluster-name-kafka-clients not ready, will try again in 10000 ms (438917ms till timeout)
2022-03-28 11:02:43 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job list-admin-my-cluster-5a0bfe14-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:38Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:43 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (117761ms till timeout)
2022-03-28 11:02:43 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUpdateUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:02:44 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (182981ms till timeout)
2022-03-28 11:02:44 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 11:02:44 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:02:44 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2251254ms till timeout)
2022-03-28 11:02:44 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (217304ms till timeout)
2022-03-28 11:02:44 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:02:44 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 11:02:44 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 11:02:44 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (476839ms till timeout)
2022-03-28 11:02:44 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job list-admin-my-cluster-5a0bfe14-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:38Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:44 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (116708ms till timeout)
2022-03-28 11:02:45 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (181929ms till timeout)
2022-03-28 11:02:45 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:02:45 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2250218ms till timeout)
2022-03-28 11:02:45 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (216222ms till timeout)
2022-03-28 11:02:45 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 11:02:45 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job list-admin-my-cluster-5a0bfe14-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:38Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (115592ms till timeout)
2022-03-28 11:02:45 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 11:02:45 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 11:02:45 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (475557ms till timeout)
2022-03-28 11:02:46 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (180774ms till timeout)
2022-03-28 11:02:46 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 11:02:46 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2249165ms till timeout)
2022-03-28 11:02:46 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (215128ms till timeout)
2022-03-28 11:02:46 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 11:02:47 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job list-admin-my-cluster-5a0bfe14-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:38Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (114377ms till timeout)
2022-03-28 11:02:47 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 11:02:47 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 11:02:47 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (474303ms till timeout)
2022-03-28 11:02:47 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (179614ms till timeout)
2022-03-28 11:02:47 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-0 not ready: kafka)
2022-03-28 11:02:47 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-1 not ready: kafka)
2022-03-28 11:02:47 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-2 not ready: kafka)
2022-03-28 11:02:47 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-66f23d43-kafka-3)
2022-03-28 11:02:47 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2248064ms till timeout)
2022-03-28 11:02:47 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (213964ms till timeout)
2022-03-28 11:02:48 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job list-admin-my-cluster-5a0bfe14-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:38Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:48 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 11:02:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (113230ms till timeout)
2022-03-28 11:02:48 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (178518ms till timeout)
2022-03-28 11:02:48 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 11:02:48 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 11:02:48 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (472794ms till timeout)
2022-03-28 11:02:48 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-0 not ready: kafka)
2022-03-28 11:02:48 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-1 not ready: kafka)
2022-03-28 11:02:48 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-2 not ready: kafka)
2022-03-28 11:02:48 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-66f23d43-kafka-3)
2022-03-28 11:02:48 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2246960ms till timeout)
2022-03-28 11:02:48 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (212882ms till timeout)
2022-03-28 11:02:48 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUpdateUser is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:02:49 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job list-admin-my-cluster-5a0bfe14-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T11:02:45Z, conditions=[JobCondition(lastProbeTime=2022-03-28T11:02:45Z, lastTransitionTime=2022-03-28T11:02:45Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T11:02:38Z, succeeded=1, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:49 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:674] ============================================================================
2022-03-28 11:02:49 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:675] [operators.topic.ThrottlingQuotaST - After Each] - Clean up after test
2022-03-28 11:02:49 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:344] ############################################################################
2022-03-28 11:02:49 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:348] Delete all resources for testKafkaAdminTopicOperations
2022-03-28 11:02:49 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of Job list-admin-my-cluster-5a0bfe14-kafka-clients in namespace throttling-quota-st
2022-03-28 11:02:49 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:02:49 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:49 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:list-admin-my-cluster-5a0bfe14-kafka-clients
2022-03-28 11:02:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (177428ms till timeout)
2022-03-28 11:02:49 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of Job list-admin-my-cluster-5a0bfe14-kafka-clients in namespace throttling-quota-st
2022-03-28 11:02:49 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 11:02:49 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:list-admin-my-cluster-5a0bfe14-kafka-clients
2022-03-28 11:02:49 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-0 not ready: kafka)
2022-03-28 11:02:49 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of Job delete-admin-my-cluster-5a0bfe14-kafka-clients in namespace throttling-quota-st
2022-03-28 11:02:49 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-1 not ready: kafka)
2022-03-28 11:02:49 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-2 not ready: kafka)
2022-03-28 11:02:49 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-66f23d43-kafka-3)
2022-03-28 11:02:49 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2245911ms till timeout)
2022-03-28 11:02:49 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:delete-admin-my-cluster-5a0bfe14-kafka-clients
2022-03-28 11:02:49 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of Job create-admin-my-cluster-5a0bfe14-kafka-clients in namespace throttling-quota-st
2022-03-28 11:02:49 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:create-admin-my-cluster-5a0bfe14-kafka-clients
2022-03-28 11:02:49 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of KafkaUser my-user-1708656731-1087276375 in namespace throttling-quota-st
2022-03-28 11:02:50 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (211769ms till timeout)
2022-03-28 11:02:50 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1708656731-1087276375
2022-03-28 11:02:50 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1708656731-1087276375 not ready, will try again in 10000 ms (179811ms till timeout)
2022-03-28 11:02:50 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (176390ms till timeout)
2022-03-28 11:02:50 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-0 not ready: kafka)
2022-03-28 11:02:50 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-1 not ready: kafka)
2022-03-28 11:02:50 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-2 not ready: kafka)
2022-03-28 11:02:50 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-66f23d43-kafka-3)
2022-03-28 11:02:50 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2244858ms till timeout)
2022-03-28 11:02:51 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (210699ms till timeout)
2022-03-28 11:02:51 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (175338ms till timeout)
2022-03-28 11:02:51 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-0 not ready: kafka)
2022-03-28 11:02:51 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-1 not ready: kafka)
2022-03-28 11:02:51 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-2 not ready: kafka)
2022-03-28 11:02:51 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-66f23d43-kafka-3)
2022-03-28 11:02:51 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2243820ms till timeout)
2022-03-28 11:02:52 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (209588ms till timeout)
2022-03-28 11:02:52 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (174295ms till timeout)
2022-03-28 11:02:52 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-0 not ready: kafka)
2022-03-28 11:02:52 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-1 not ready: kafka)
2022-03-28 11:02:52 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-2 not ready: kafka)
2022-03-28 11:02:52 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-66f23d43-kafka-3)
2022-03-28 11:02:52 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2242786ms till timeout)
2022-03-28 11:02:53 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (208499ms till timeout)
2022-03-28 11:02:53 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:369] ############################################################################
2022-03-28 11:02:53 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:267] testSendingMessagesToNonExistingTopic - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTls, testSendSimpleMessageTlsScramSha, testConfigurationFileIsCreated, testReceiveSimpleMessageTls, testDeployAndUnDeployCruiseControl, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testConfigurationPerformanceOptions, testTlsExternalUserWithQuotas, testUpdateUser] to and randomly select one to start execution
2022-03-28 11:02:53 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:93] [operators.topic.TopicST] - Removing parallel test: testSendingMessagesToNonExistingTopic
2022-03-28 11:02:53 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:97] [operators.topic.TopicST] - Parallel test count: 7
2022-03-28 11:02:53 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.topic.TopicST.testSendingMessagesToNonExistingTopic-FINISHED
2022-03-28 11:02:53 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:30] ############################################################################
2022-03-28 11:02:53 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:23] ############################################################################
2022-03-28 11:02:53 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.topic.TopicST.testMoreReplicasThanAvailableBrokers-STARTED
2022-03-28 11:02:53 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:658] ============================================================================
2022-03-28 11:02:53 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:659] [operators.topic.TopicST - Before Each] - Setup test case environment
2022-03-28 11:02:53 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:77] [operators.topic.TopicST] - Adding parallel test: testMoreReplicasThanAvailableBrokers
2022-03-28 11:02:53 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:81] [operators.topic.TopicST] - Parallel test count: 8
2022-03-28 11:02:53 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:205] [testMoreReplicasThanAvailableBrokers] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:02:53 [ForkJoinPool-1-worker-5] TRACE [SuiteThreadController:210] testMoreReplicasThanAvailableBrokers is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:02:53 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (173260ms till timeout)
2022-03-28 11:02:53 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-0 not ready: kafka)
2022-03-28 11:02:53 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-1 not ready: kafka)
2022-03-28 11:02:53 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-2 not ready: kafka)
2022-03-28 11:02:53 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-66f23d43-kafka-3)
2022-03-28 11:02:53 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2241752ms till timeout)
2022-03-28 11:02:53 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:230] testUpdateUser test now can proceed its execution
2022-03-28 11:02:53 [ForkJoinPool-1-worker-13] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 11:02:53 [ForkJoinPool-1-worker-13] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testScramUserWithQuotas=my-cluster-4941482a, testTopicModificationOfReplicationFactor=my-cluster-38e659b2, testKafkaAdminTopicOperations=my-cluster-5a0bfe14, testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383, testConfigurationFileIsCreated=my-cluster-e0fac774, testThrottlingQuotasCreateAlterPartitions=my-cluster-28d9dd2e, testSendSimpleMessageTls=my-cluster-27ddd6ba, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c, testThrottlingQuotasDeleteTopic=my-cluster-8f262de5, testUpdateUser=my-cluster-2f4b361c, testConfigurationReflection=my-cluster-d1e2168e, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-da91577a, testUserWithNameMoreThan64Chars=my-cluster-35e66bba, testDeployAndUnDeployCruiseControl=my-cluster-c0830cbe, testTlsUserWithQuotas=my-cluster-d49e42e8, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8, testSendingMessagesToNonExistingTopic=my-cluster-241cc7c0, testCapacityFile=my-cluster-fd0fb61a, testReceiveSimpleMessageTls=my-cluster-c6a15963, testConfigurationPerformanceOptions=my-cluster-b6310693, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-66f23d43, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-f50d425d, testTlsExternalUser=my-cluster-3ba9cc5b, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5}
2022-03-28 11:02:53 [ForkJoinPool-1-worker-13] TRACE [AbstractST:607] USERS_NAME_MAP: {testScramUserWithQuotas=my-user-797280497-2138800976, testTopicModificationOfReplicationFactor=my-user-2003504850-304348854, testKafkaAdminTopicOperations=my-user-1708656731-1087276375, testReceiveSimpleMessageTlsScramSha=my-user-1915916223-1265293973, testConfigurationFileIsCreated=my-user-180611180-50530491, testThrottlingQuotasCreateAlterPartitions=my-user-1933547616-1024431140, testSendSimpleMessageTls=my-user-738229993-1095121647, testCreatingUsersWithSecretPrefix=my-user-1450786684-1808961062, testThrottlingQuotasDeleteTopic=my-user-1558977140-777236615, testUpdateUser=my-user-2098198927-950610275, testConfigurationReflection=my-user-1445653023-93200027, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-1260379615-1050949907, testUserWithNameMoreThan64Chars=my-user-1703808150-314159031, testDeployAndUnDeployCruiseControl=my-user-1988711145-439861161, testTlsUserWithQuotas=my-user-811912745-1441809840, testThrottlingQuotasCreateTopic=my-user-1131806865-947217230, testSendingMessagesToNonExistingTopic=my-user-927451395-640848463, testCapacityFile=my-user-937015144-1959439396, testReceiveSimpleMessageTls=my-user-6088058-1142028780, testConfigurationPerformanceOptions=my-user-930975101-640523779, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-1150765500-2076224625, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-745974575-1852384454, testTlsExternalUser=my-user-1382292264-1641298587, testSendSimpleMessageTlsScramSha=my-user-1499386234-1249154328}
2022-03-28 11:02:53 [ForkJoinPool-1-worker-13] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testScramUserWithQuotas=my-topic-2004840350-265526537, testTopicModificationOfReplicationFactor=my-topic-1789870897-1466152225, testKafkaAdminTopicOperations=my-topic-1564170108-1493323202, testReceiveSimpleMessageTlsScramSha=my-topic-1834944466-2094449929, testConfigurationFileIsCreated=my-topic-1296407588-313880708, testThrottlingQuotasCreateAlterPartitions=my-topic-609849288-1810034325, testSendSimpleMessageTls=my-topic-563023482-1697359353, testCreatingUsersWithSecretPrefix=my-topic-1479520843-618891199, testThrottlingQuotasDeleteTopic=my-topic-2128658681-1688111273, testUpdateUser=my-topic-1741945237-916990305, testConfigurationReflection=my-topic-640815392-1092406112, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-1674392683-1637010870, testUserWithNameMoreThan64Chars=my-topic-2021464661-1313035472, testDeployAndUnDeployCruiseControl=my-topic-1511079187-95055629, testTlsUserWithQuotas=my-topic-418284173-1689090259, testThrottlingQuotasCreateTopic=my-topic-728672490-1194703402, testSendingMessagesToNonExistingTopic=my-topic-645675602-646682485, testCapacityFile=my-topic-1155641705-902584520, testReceiveSimpleMessageTls=my-topic-478514536-2132782608, testConfigurationPerformanceOptions=my-topic-213912446-1165431808, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-842198221-1227962115, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-254848724-189700675, testTlsExternalUser=my-topic-5642373-135861890, testSendSimpleMessageTlsScramSha=my-topic-1028951580-1656411053}
2022-03-28 11:02:53 [ForkJoinPool-1-worker-13] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testScramUserWithQuotas=my-cluster-4941482a-kafka-clients, testTopicModificationOfReplicationFactor=my-cluster-38e659b2-kafka-clients, testKafkaAdminTopicOperations=my-cluster-5a0bfe14-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383-kafka-clients, testConfigurationFileIsCreated=my-cluster-e0fac774-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-28d9dd2e-kafka-clients, testSendSimpleMessageTls=my-cluster-27ddd6ba-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-8f262de5-kafka-clients, testUpdateUser=my-cluster-2f4b361c-kafka-clients, testConfigurationReflection=my-cluster-d1e2168e-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-da91577a-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-35e66bba-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-c0830cbe-kafka-clients, testTlsUserWithQuotas=my-cluster-d49e42e8-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8-kafka-clients, testSendingMessagesToNonExistingTopic=my-cluster-241cc7c0-kafka-clients, testCapacityFile=my-cluster-fd0fb61a-kafka-clients, testReceiveSimpleMessageTls=my-cluster-c6a15963-kafka-clients, testConfigurationPerformanceOptions=my-cluster-b6310693-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-66f23d43-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-f50d425d-kafka-clients, testTlsExternalUser=my-cluster-3ba9cc5b-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5-kafka-clients}
2022-03-28 11:02:53 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update KafkaUser my-user-2098198927-950610275 in namespace user-st
2022-03-28 11:02:54 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-2098198927-950610275
2022-03-28 11:02:54 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-2098198927-950610275 will have desired state: Ready
2022-03-28 11:02:54 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-2098198927-950610275 will have desired state: Ready
2022-03-28 11:02:54 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] KafkaUser: my-user-2098198927-950610275 will have desired state: Ready not ready, will try again in 1000 ms (179955ms till timeout)
2022-03-28 11:02:54 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (207439ms till timeout)
2022-03-28 11:02:54 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:02:54 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (172212ms till timeout)
2022-03-28 11:02:55 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-0 not ready: kafka)
2022-03-28 11:02:55 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-1 not ready: kafka)
2022-03-28 11:02:55 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-2 not ready: kafka)
2022-03-28 11:02:55 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-66f23d43-kafka-3)
2022-03-28 11:02:55 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2240625ms till timeout)
2022-03-28 11:02:55 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 11:02:55 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 11:02:55 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (466428ms till timeout)
2022-03-28 11:02:55 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:444] KafkaUser: my-user-2098198927-950610275 is in desired state: Ready
2022-03-28 11:02:55 [ForkJoinPool-1-worker-13] DEBUG [CompiledPath:93] Evaluating path: $['data']['ca.crt']
2022-03-28 11:02:55 [ForkJoinPool-1-worker-13] DEBUG [CompiledPath:93] Evaluating path: $['data']['user.crt']
2022-03-28 11:02:55 [ForkJoinPool-1-worker-13] DEBUG [CompiledPath:93] Evaluating path: $['data']['user.key']
2022-03-28 11:02:55 [ForkJoinPool-1-worker-13] DEBUG [CompiledPath:93] Evaluating path: $['metadata']['name']
2022-03-28 11:02:55 [ForkJoinPool-1-worker-13] DEBUG [CompiledPath:93] Evaluating path: $['metadata']['namespace']
2022-03-28 11:02:55 [ForkJoinPool-1-worker-13] DEBUG [CompiledPath:93] Evaluating path: $['metadata']['name']
2022-03-28 11:02:55 [ForkJoinPool-1-worker-13] DEBUG [CompiledPath:93] Evaluating path: $['metadata']['namespace']
2022-03-28 11:02:55 [ForkJoinPool-1-worker-13] DEBUG [CompiledPath:93] Evaluating path: $['spec']['authentication']['type']
2022-03-28 11:02:55 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for increase observation generation from 1 for user my-user-2098198927-950610275
2022-03-28 11:02:55 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (206350ms till timeout)
2022-03-28 11:02:55 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] increase observation generation from 1 for user my-user-2098198927-950610275 not ready, will try again in 1000 ms (179977ms till timeout)
2022-03-28 11:02:55 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (171140ms till timeout)
2022-03-28 11:02:56 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 11:02:56 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-0 not ready: kafka)
2022-03-28 11:02:56 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-1 not ready: kafka)
2022-03-28 11:02:56 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-2 not ready: kafka)
2022-03-28 11:02:56 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-66f23d43-kafka-3)
2022-03-28 11:02:56 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2239521ms till timeout)
2022-03-28 11:02:56 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (205280ms till timeout)
2022-03-28 11:02:56 [ForkJoinPool-1-worker-13] INFO  [SecretUtils:46] Waiting for Secret my-user-2098198927-950610275
2022-03-28 11:02:56 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Expected secret my-user-2098198927-950610275 exists
2022-03-28 11:02:56 [ForkJoinPool-1-worker-13] INFO  [SecretUtils:50] Secret my-user-2098198927-950610275 created
2022-03-28 11:02:56 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-2098198927-950610275 will have desired state: Ready
2022-03-28 11:02:56 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-2098198927-950610275 will have desired state: Ready
2022-03-28 11:02:56 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 11:02:56 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 11:02:56 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (464923ms till timeout)
2022-03-28 11:02:56 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:444] KafkaUser: my-user-2098198927-950610275 is in desired state: Ready
2022-03-28 11:02:56 [ForkJoinPool-1-worker-13] DEBUG [CompiledPath:93] Evaluating path: $['data']['password']
2022-03-28 11:02:56 [ForkJoinPool-1-worker-13] DEBUG [CompiledPath:93] Evaluating path: $['metadata']['name']
2022-03-28 11:02:56 [ForkJoinPool-1-worker-13] DEBUG [CompiledPath:93] Evaluating path: $['metadata']['namespace']
2022-03-28 11:02:56 [ForkJoinPool-1-worker-13] DEBUG [CompiledPath:93] Evaluating path: $['spec']['authentication']['type']
2022-03-28 11:02:56 [ForkJoinPool-1-worker-13] INFO  [KafkaUserUtils:62] Waiting for KafkaUser deletion my-user-2098198927-950610275
2022-03-28 11:02:56 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaUser deletion my-user-2098198927-950610275
2022-03-28 11:02:56 [ForkJoinPool-1-worker-13] INFO  [KafkaUserUtils:75] KafkaUser my-user-2098198927-950610275 deleted
2022-03-28 11:02:56 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:674] ============================================================================
2022-03-28 11:02:56 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:675] [operators.user.UserST - After Each] - Clean up after test
2022-03-28 11:02:56 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:344] ############################################################################
2022-03-28 11:02:56 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:348] Delete all resources for testUpdateUser
2022-03-28 11:02:56 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of KafkaUser my-user-2098198927-950610275 in namespace user-st
2022-03-28 11:02:56 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-2098198927-950610275
2022-03-28 11:02:56 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:369] ############################################################################
2022-03-28 11:02:56 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:267] testUpdateUser - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTls, testSendSimpleMessageTlsScramSha, testConfigurationFileIsCreated, testReceiveSimpleMessageTls, testDeployAndUnDeployCruiseControl, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testConfigurationPerformanceOptions, testTlsExternalUserWithQuotas, testMoreReplicasThanAvailableBrokers] to and randomly select one to start execution
2022-03-28 11:02:56 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:93] [operators.user.UserST] - Removing parallel test: testUpdateUser
2022-03-28 11:02:56 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:97] [operators.user.UserST] - Parallel test count: 7
2022-03-28 11:02:56 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testUpdateUser-FINISHED
2022-03-28 11:02:56 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:30] ############################################################################
2022-03-28 11:02:56 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:23] ############################################################################
2022-03-28 11:02:56 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testUserTemplate-STARTED
2022-03-28 11:02:56 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:658] ============================================================================
2022-03-28 11:02:56 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:659] [operators.user.UserST - Before Each] - Setup test case environment
2022-03-28 11:02:56 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:77] [operators.user.UserST] - Adding parallel test: testUserTemplate
2022-03-28 11:02:56 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:81] [operators.user.UserST] - Parallel test count: 8
2022-03-28 11:02:56 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:205] [testUserTemplate] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:02:56 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testUserTemplate is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:02:57 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (170069ms till timeout)
2022-03-28 11:02:57 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-0 not ready: kafka)
2022-03-28 11:02:57 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-1 not ready: kafka)
2022-03-28 11:02:57 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-2 not ready: kafka)
2022-03-28 11:02:57 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-66f23d43-kafka-3)
2022-03-28 11:02:57 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2238389ms till timeout)
2022-03-28 11:02:57 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (204219ms till timeout)
2022-03-28 11:02:57 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 11:02:57 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 11:02:57 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 11:02:57 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (463556ms till timeout)
2022-03-28 11:02:58 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (168981ms till timeout)
2022-03-28 11:02:58 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-0 not ready: kafka)
2022-03-28 11:02:58 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-1 not ready: kafka)
2022-03-28 11:02:58 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-2 not ready: kafka)
2022-03-28 11:02:58 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-66f23d43-kafka-3)
2022-03-28 11:02:58 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2237303ms till timeout)
2022-03-28 11:02:58 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:230] testMoreReplicasThanAvailableBrokers test now can proceed its execution
2022-03-28 11:02:58 [ForkJoinPool-1-worker-5] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 11:02:58 [ForkJoinPool-1-worker-5] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383, testThrottlingQuotasCreateAlterPartitions=my-cluster-28d9dd2e, testSendSimpleMessageTls=my-cluster-27ddd6ba, testConfigurationReflection=my-cluster-d1e2168e, testUserWithNameMoreThan64Chars=my-cluster-35e66bba, testDeployAndUnDeployCruiseControl=my-cluster-c0830cbe, testCapacityFile=my-cluster-fd0fb61a, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-66f23d43, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-f50d425d, testTlsExternalUser=my-cluster-3ba9cc5b, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5, testScramUserWithQuotas=my-cluster-4941482a, testTopicModificationOfReplicationFactor=my-cluster-38e659b2, testKafkaAdminTopicOperations=my-cluster-5a0bfe14, testConfigurationFileIsCreated=my-cluster-e0fac774, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c, testThrottlingQuotasDeleteTopic=my-cluster-8f262de5, testUpdateUser=my-cluster-2f4b361c, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-da91577a, testTlsUserWithQuotas=my-cluster-d49e42e8, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8, testSendingMessagesToNonExistingTopic=my-cluster-241cc7c0, testReceiveSimpleMessageTls=my-cluster-c6a15963, testConfigurationPerformanceOptions=my-cluster-b6310693, testMoreReplicasThanAvailableBrokers=my-cluster-59a64014}
2022-03-28 11:02:58 [ForkJoinPool-1-worker-5] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-1915916223-1265293973, testThrottlingQuotasCreateAlterPartitions=my-user-1933547616-1024431140, testSendSimpleMessageTls=my-user-738229993-1095121647, testConfigurationReflection=my-user-1445653023-93200027, testUserWithNameMoreThan64Chars=my-user-1703808150-314159031, testDeployAndUnDeployCruiseControl=my-user-1988711145-439861161, testCapacityFile=my-user-937015144-1959439396, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-1150765500-2076224625, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-745974575-1852384454, testTlsExternalUser=my-user-1382292264-1641298587, testSendSimpleMessageTlsScramSha=my-user-1499386234-1249154328, testScramUserWithQuotas=my-user-797280497-2138800976, testTopicModificationOfReplicationFactor=my-user-2003504850-304348854, testKafkaAdminTopicOperations=my-user-1708656731-1087276375, testConfigurationFileIsCreated=my-user-180611180-50530491, testCreatingUsersWithSecretPrefix=my-user-1450786684-1808961062, testThrottlingQuotasDeleteTopic=my-user-1558977140-777236615, testUpdateUser=my-user-2098198927-950610275, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-1260379615-1050949907, testTlsUserWithQuotas=my-user-811912745-1441809840, testThrottlingQuotasCreateTopic=my-user-1131806865-947217230, testSendingMessagesToNonExistingTopic=my-user-927451395-640848463, testReceiveSimpleMessageTls=my-user-6088058-1142028780, testConfigurationPerformanceOptions=my-user-930975101-640523779, testMoreReplicasThanAvailableBrokers=my-user-1511857485-1227147676}
2022-03-28 11:02:58 [ForkJoinPool-1-worker-5] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-1834944466-2094449929, testThrottlingQuotasCreateAlterPartitions=my-topic-609849288-1810034325, testSendSimpleMessageTls=my-topic-563023482-1697359353, testConfigurationReflection=my-topic-640815392-1092406112, testUserWithNameMoreThan64Chars=my-topic-2021464661-1313035472, testDeployAndUnDeployCruiseControl=my-topic-1511079187-95055629, testCapacityFile=my-topic-1155641705-902584520, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-842198221-1227962115, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-254848724-189700675, testTlsExternalUser=my-topic-5642373-135861890, testSendSimpleMessageTlsScramSha=my-topic-1028951580-1656411053, testScramUserWithQuotas=my-topic-2004840350-265526537, testTopicModificationOfReplicationFactor=my-topic-1789870897-1466152225, testKafkaAdminTopicOperations=my-topic-1564170108-1493323202, testConfigurationFileIsCreated=my-topic-1296407588-313880708, testCreatingUsersWithSecretPrefix=my-topic-1479520843-618891199, testThrottlingQuotasDeleteTopic=my-topic-2128658681-1688111273, testUpdateUser=my-topic-1741945237-916990305, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-1674392683-1637010870, testTlsUserWithQuotas=my-topic-418284173-1689090259, testThrottlingQuotasCreateTopic=my-topic-728672490-1194703402, testSendingMessagesToNonExistingTopic=my-topic-645675602-646682485, testReceiveSimpleMessageTls=my-topic-478514536-2132782608, testConfigurationPerformanceOptions=my-topic-213912446-1165431808, testMoreReplicasThanAvailableBrokers=my-topic-1591366435-591944736}
2022-03-28 11:02:58 [ForkJoinPool-1-worker-5] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-28d9dd2e-kafka-clients, testSendSimpleMessageTls=my-cluster-27ddd6ba-kafka-clients, testConfigurationReflection=my-cluster-d1e2168e-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-35e66bba-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-c0830cbe-kafka-clients, testCapacityFile=my-cluster-fd0fb61a-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-66f23d43-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-f50d425d-kafka-clients, testTlsExternalUser=my-cluster-3ba9cc5b-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5-kafka-clients, testScramUserWithQuotas=my-cluster-4941482a-kafka-clients, testTopicModificationOfReplicationFactor=my-cluster-38e659b2-kafka-clients, testKafkaAdminTopicOperations=my-cluster-5a0bfe14-kafka-clients, testConfigurationFileIsCreated=my-cluster-e0fac774-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-8f262de5-kafka-clients, testUpdateUser=my-cluster-2f4b361c-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-da91577a-kafka-clients, testTlsUserWithQuotas=my-cluster-d49e42e8-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8-kafka-clients, testSendingMessagesToNonExistingTopic=my-cluster-241cc7c0-kafka-clients, testReceiveSimpleMessageTls=my-cluster-c6a15963-kafka-clients, testConfigurationPerformanceOptions=my-cluster-b6310693-kafka-clients, testMoreReplicasThanAvailableBrokers=my-cluster-59a64014-kafka-clients}
2022-03-28 11:02:58 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update KafkaTopic my-topic-1591366435-591944736 in namespace topic-st
2022-03-28 11:02:58 [ForkJoinPool-1-worker-5] INFO  [TopicST:461] Checking in KafkaTopic CR that topic my-topic-1591366435-591944736 exists
2022-03-28 11:02:58 [ForkJoinPool-1-worker-5] INFO  [TopicST:456] Checking topic my-topic-1591366435-591944736 in Kafka
2022-03-28 11:02:58 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 11:02:58 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (203032ms till timeout)
2022-03-28 11:02:58 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 11:02:59 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:02:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (167801ms till timeout)
2022-03-28 11:02:59 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-0 not ready: kafka)
2022-03-28 11:02:59 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-1 not ready: kafka)
2022-03-28 11:02:59 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-2 not ready: kafka)
2022-03-28 11:02:59 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-66f23d43-kafka-3)
2022-03-28 11:02:59 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2236158ms till timeout)
2022-03-28 11:02:59 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:02:59 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (201965ms till timeout)
2022-03-28 11:03:00 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:369] ############################################################################
2022-03-28 11:03:00 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:267] testKafkaAdminTopicOperations - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTls, testSendSimpleMessageTlsScramSha, testConfigurationFileIsCreated, testReceiveSimpleMessageTls, testDeployAndUnDeployCruiseControl, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testConfigurationPerformanceOptions, testTlsExternalUserWithQuotas, testUserTemplate] to and randomly select one to start execution
2022-03-28 11:03:00 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:93] [operators.topic.ThrottlingQuotaST] - Removing parallel test: testKafkaAdminTopicOperations
2022-03-28 11:03:00 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:97] [operators.topic.ThrottlingQuotaST] - Parallel test count: 7
2022-03-28 11:03:00 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testKafkaAdminTopicOperations-FINISHED
2022-03-28 11:03:00 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:30] ############################################################################
2022-03-28 11:03:00 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:23] ############################################################################
2022-03-28 11:03:00 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.topic.TopicST.testCreateTopicViaKafka-STARTED
2022-03-28 11:03:00 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:658] ============================================================================
2022-03-28 11:03:00 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:659] [operators.topic.TopicST - Before Each] - Setup test case environment
2022-03-28 11:03:00 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:77] [operators.topic.TopicST] - Adding parallel test: testCreateTopicViaKafka
2022-03-28 11:03:00 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:81] [operators.topic.TopicST] - Parallel test count: 8
2022-03-28 11:03:00 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:205] [testCreateTopicViaKafka] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:03:00 [ForkJoinPool-1-worker-7] TRACE [SuiteThreadController:210] testCreateTopicViaKafka is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:03:00 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:03:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (166730ms till timeout)
2022-03-28 11:03:00 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-0 not ready: kafka)
2022-03-28 11:03:00 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-1 not ready: kafka)
2022-03-28 11:03:00 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-2 not ready: kafka)
2022-03-28 11:03:00 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-66f23d43-kafka-3)
2022-03-28 11:03:00 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2235087ms till timeout)
2022-03-28 11:03:00 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (200884ms till timeout)
2022-03-28 11:03:01 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:03:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (165582ms till timeout)
2022-03-28 11:03:01 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-0 not ready: kafka)
2022-03-28 11:03:01 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-1 not ready: kafka)
2022-03-28 11:03:01 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-2 not ready: kafka)
2022-03-28 11:03:01 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-66f23d43-kafka-3)
2022-03-28 11:03:01 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2233945ms till timeout)
2022-03-28 11:03:01 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:230] testUserTemplate test now can proceed its execution
2022-03-28 11:03:01 [ForkJoinPool-1-worker-13] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 11:03:01 [ForkJoinPool-1-worker-13] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383, testThrottlingQuotasCreateAlterPartitions=my-cluster-28d9dd2e, testSendSimpleMessageTls=my-cluster-27ddd6ba, testConfigurationReflection=my-cluster-d1e2168e, testUserWithNameMoreThan64Chars=my-cluster-35e66bba, testDeployAndUnDeployCruiseControl=my-cluster-c0830cbe, testCapacityFile=my-cluster-fd0fb61a, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-66f23d43, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-f50d425d, testTlsExternalUser=my-cluster-3ba9cc5b, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5, testScramUserWithQuotas=my-cluster-4941482a, testTopicModificationOfReplicationFactor=my-cluster-38e659b2, testKafkaAdminTopicOperations=my-cluster-5a0bfe14, testConfigurationFileIsCreated=my-cluster-e0fac774, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c, testThrottlingQuotasDeleteTopic=my-cluster-8f262de5, testUpdateUser=my-cluster-2f4b361c, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-da91577a, testTlsUserWithQuotas=my-cluster-d49e42e8, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8, testSendingMessagesToNonExistingTopic=my-cluster-241cc7c0, testReceiveSimpleMessageTls=my-cluster-c6a15963, testConfigurationPerformanceOptions=my-cluster-b6310693, testUserTemplate=my-cluster-3189676d, testMoreReplicasThanAvailableBrokers=my-cluster-59a64014}
2022-03-28 11:03:01 [ForkJoinPool-1-worker-13] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-1915916223-1265293973, testThrottlingQuotasCreateAlterPartitions=my-user-1933547616-1024431140, testSendSimpleMessageTls=my-user-738229993-1095121647, testConfigurationReflection=my-user-1445653023-93200027, testUserWithNameMoreThan64Chars=my-user-1703808150-314159031, testDeployAndUnDeployCruiseControl=my-user-1988711145-439861161, testCapacityFile=my-user-937015144-1959439396, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-1150765500-2076224625, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-745974575-1852384454, testTlsExternalUser=my-user-1382292264-1641298587, testSendSimpleMessageTlsScramSha=my-user-1499386234-1249154328, testScramUserWithQuotas=my-user-797280497-2138800976, testTopicModificationOfReplicationFactor=my-user-2003504850-304348854, testKafkaAdminTopicOperations=my-user-1708656731-1087276375, testConfigurationFileIsCreated=my-user-180611180-50530491, testCreatingUsersWithSecretPrefix=my-user-1450786684-1808961062, testThrottlingQuotasDeleteTopic=my-user-1558977140-777236615, testUpdateUser=my-user-2098198927-950610275, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-1260379615-1050949907, testTlsUserWithQuotas=my-user-811912745-1441809840, testThrottlingQuotasCreateTopic=my-user-1131806865-947217230, testSendingMessagesToNonExistingTopic=my-user-927451395-640848463, testReceiveSimpleMessageTls=my-user-6088058-1142028780, testConfigurationPerformanceOptions=my-user-930975101-640523779, testUserTemplate=my-user-1919254789-1384584070, testMoreReplicasThanAvailableBrokers=my-user-1511857485-1227147676}
2022-03-28 11:03:01 [ForkJoinPool-1-worker-13] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-1834944466-2094449929, testThrottlingQuotasCreateAlterPartitions=my-topic-609849288-1810034325, testSendSimpleMessageTls=my-topic-563023482-1697359353, testConfigurationReflection=my-topic-640815392-1092406112, testUserWithNameMoreThan64Chars=my-topic-2021464661-1313035472, testDeployAndUnDeployCruiseControl=my-topic-1511079187-95055629, testCapacityFile=my-topic-1155641705-902584520, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-842198221-1227962115, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-254848724-189700675, testTlsExternalUser=my-topic-5642373-135861890, testSendSimpleMessageTlsScramSha=my-topic-1028951580-1656411053, testScramUserWithQuotas=my-topic-2004840350-265526537, testTopicModificationOfReplicationFactor=my-topic-1789870897-1466152225, testKafkaAdminTopicOperations=my-topic-1564170108-1493323202, testConfigurationFileIsCreated=my-topic-1296407588-313880708, testCreatingUsersWithSecretPrefix=my-topic-1479520843-618891199, testThrottlingQuotasDeleteTopic=my-topic-2128658681-1688111273, testUpdateUser=my-topic-1741945237-916990305, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-1674392683-1637010870, testTlsUserWithQuotas=my-topic-418284173-1689090259, testThrottlingQuotasCreateTopic=my-topic-728672490-1194703402, testSendingMessagesToNonExistingTopic=my-topic-645675602-646682485, testReceiveSimpleMessageTls=my-topic-478514536-2132782608, testConfigurationPerformanceOptions=my-topic-213912446-1165431808, testUserTemplate=my-topic-2114982563-185666735, testMoreReplicasThanAvailableBrokers=my-topic-1591366435-591944736}
2022-03-28 11:03:01 [ForkJoinPool-1-worker-13] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-28d9dd2e-kafka-clients, testSendSimpleMessageTls=my-cluster-27ddd6ba-kafka-clients, testConfigurationReflection=my-cluster-d1e2168e-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-35e66bba-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-c0830cbe-kafka-clients, testCapacityFile=my-cluster-fd0fb61a-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-66f23d43-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-f50d425d-kafka-clients, testTlsExternalUser=my-cluster-3ba9cc5b-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5-kafka-clients, testScramUserWithQuotas=my-cluster-4941482a-kafka-clients, testTopicModificationOfReplicationFactor=my-cluster-38e659b2-kafka-clients, testKafkaAdminTopicOperations=my-cluster-5a0bfe14-kafka-clients, testConfigurationFileIsCreated=my-cluster-e0fac774-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-8f262de5-kafka-clients, testUpdateUser=my-cluster-2f4b361c-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-da91577a-kafka-clients, testTlsUserWithQuotas=my-cluster-d49e42e8-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8-kafka-clients, testSendingMessagesToNonExistingTopic=my-cluster-241cc7c0-kafka-clients, testReceiveSimpleMessageTls=my-cluster-c6a15963-kafka-clients, testConfigurationPerformanceOptions=my-cluster-b6310693-kafka-clients, testUserTemplate=my-cluster-3189676d-kafka-clients, testMoreReplicasThanAvailableBrokers=my-cluster-59a64014-kafka-clients}
2022-03-28 11:03:01 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update KafkaUser my-user-1919254789-1384584070 in namespace user-st
2022-03-28 11:03:01 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-1919254789-1384584070
2022-03-28 11:03:01 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (199790ms till timeout)
2022-03-28 11:03:01 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-1919254789-1384584070 will have desired state: Ready
2022-03-28 11:03:01 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-1919254789-1384584070 will have desired state: Ready
2022-03-28 11:03:02 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] KafkaUser: my-user-1919254789-1384584070 will have desired state: Ready not ready, will try again in 1000 ms (179969ms till timeout)
2022-03-28 11:03:02 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:03:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (164466ms till timeout)
2022-03-28 11:03:02 [ForkJoinPool-1-worker-5] INFO  [Exec:417] Command: oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 11:03:02 [ForkJoinPool-1-worker-5] INFO  [Exec:417] Return code: 0
2022-03-28 11:03:02 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-1591366435-591944736 will have desired state: NotReady
2022-03-28 11:03:02 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-1591366435-591944736 will have desired state: NotReady
2022-03-28 11:03:02 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-0 not ready: kafka)
2022-03-28 11:03:02 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:444] KafkaTopic: my-topic-1591366435-591944736 is in desired state: NotReady
2022-03-28 11:03:02 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-1 not ready: kafka)
2022-03-28 11:03:02 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-2 not ready: kafka)
2022-03-28 11:03:02 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-66f23d43-kafka-3)
2022-03-28 11:03:02 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2232863ms till timeout)
2022-03-28 11:03:02 [ForkJoinPool-1-worker-5] INFO  [TopicST:90] Delete topic my-topic-1591366435-591944736
2022-03-28 11:03:02 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace topic-st delete kafkatopic my-topic-1591366435-591944736
2022-03-28 11:03:03 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (198739ms till timeout)
2022-03-28 11:03:03 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:444] KafkaUser: my-user-1919254789-1384584070 is in desired state: Ready
2022-03-28 11:03:03 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:674] ============================================================================
2022-03-28 11:03:03 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:675] [operators.user.UserST - After Each] - Clean up after test
2022-03-28 11:03:03 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:344] ############################################################################
2022-03-28 11:03:03 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:348] Delete all resources for testUserTemplate
2022-03-28 11:03:03 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of KafkaUser my-user-1919254789-1384584070 in namespace user-st
2022-03-28 11:03:03 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1919254789-1384584070
2022-03-28 11:03:03 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1919254789-1384584070 not ready, will try again in 10000 ms (179865ms till timeout)
2022-03-28 11:03:03 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:03:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (163280ms till timeout)
2022-03-28 11:03:03 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-0 not ready: kafka)
2022-03-28 11:03:03 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-1 not ready: kafka)
2022-03-28 11:03:03 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-2 not ready: kafka)
2022-03-28 11:03:03 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-66f23d43-kafka-3)
2022-03-28 11:03:03 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2231815ms till timeout)
2022-03-28 11:03:04 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (197691ms till timeout)
2022-03-28 11:03:04 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:03:04 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:03:04 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 11:03:04 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 11:03:04 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (456550ms till timeout)
2022-03-28 11:03:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (162116ms till timeout)
2022-03-28 11:03:05 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-0 not ready: kafka)
2022-03-28 11:03:05 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-1 not ready: kafka)
2022-03-28 11:03:05 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-2 not ready: kafka)
2022-03-28 11:03:05 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-66f23d43-kafka-3)
2022-03-28 11:03:05 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2230695ms till timeout)
2022-03-28 11:03:05 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (196644ms till timeout)
2022-03-28 11:03:05 [ForkJoinPool-1-worker-7] TRACE [SuiteThreadController:210] testCreateTopicViaKafka is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:03:05 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 11:03:06 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:03:06 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-0 not ready: kafka)
2022-03-28 11:03:06 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-1 not ready: kafka)
2022-03-28 11:03:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (161017ms till timeout)
2022-03-28 11:03:06 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-2 not ready: kafka)
2022-03-28 11:03:06 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-66f23d43-kafka-3)
2022-03-28 11:03:06 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2229601ms till timeout)
2022-03-28 11:03:06 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (195526ms till timeout)
2022-03-28 11:03:06 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 11:03:06 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 11:03:06 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (455022ms till timeout)
2022-03-28 11:03:07 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:03:07 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-0 not ready: kafka)
2022-03-28 11:03:07 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-1 not ready: kafka)
2022-03-28 11:03:07 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-2 not ready: kafka)
2022-03-28 11:03:07 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-66f23d43-kafka-3)
2022-03-28 11:03:07 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2228486ms till timeout)
2022-03-28 11:03:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (159877ms till timeout)
2022-03-28 11:03:07 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (194465ms till timeout)
2022-03-28 11:03:07 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 11:03:08 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:03:08 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-0 not ready: kafka)
2022-03-28 11:03:08 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-1 not ready: kafka)
2022-03-28 11:03:08 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-2 not ready: kafka)
2022-03-28 11:03:08 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-66f23d43-kafka-3)
2022-03-28 11:03:08 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2227412ms till timeout)
2022-03-28 11:03:08 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace topic-st delete kafkatopic my-topic-1591366435-591944736
2022-03-28 11:03:08 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 11:03:08 [ForkJoinPool-1-worker-5] INFO  [KafkaTopicUtils:104] Waiting for KafkaTopic my-topic-1591366435-591944736 deletion
2022-03-28 11:03:08 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for KafkaTopic deletion my-topic-1591366435-591944736
2022-03-28 11:03:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (158809ms till timeout)
2022-03-28 11:03:08 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update KafkaTopic topic-example-new in namespace topic-st
2022-03-28 11:03:08 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (193367ms till timeout)
2022-03-28 11:03:08 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:topic-example-new
2022-03-28 11:03:08 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:433] Wait for KafkaTopic: topic-example-new will have desired state: Ready
2022-03-28 11:03:08 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for KafkaTopic: topic-example-new will have desired state: Ready
2022-03-28 11:03:08 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaTopic: topic-example-new will have desired state: Ready not ready, will try again in 1000 ms (179940ms till timeout)
2022-03-28 11:03:09 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-0 not ready: kafka)
2022-03-28 11:03:09 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-1 not ready: kafka)
2022-03-28 11:03:09 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-2 not ready: kafka)
2022-03-28 11:03:09 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-66f23d43-kafka-3)
2022-03-28 11:03:09 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2226336ms till timeout)
2022-03-28 11:03:09 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:03:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (157694ms till timeout)
2022-03-28 11:03:09 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (192286ms till timeout)
2022-03-28 11:03:09 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:444] KafkaTopic: topic-example-new is in desired state: Ready
2022-03-28 11:03:09 [ForkJoinPool-1-worker-5] INFO  [TopicST:456] Checking topic topic-example-new in Kafka
2022-03-28 11:03:09 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 11:03:09 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTlsExternalUserWithQuotas is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:03:10 [ForkJoinPool-1-worker-7] TRACE [SuiteThreadController:210] testCreateTopicViaKafka is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (8/6)
2022-03-28 11:03:10 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-0 not ready: kafka)
2022-03-28 11:03:10 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-1 not ready: kafka)
2022-03-28 11:03:10 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-2 not ready: kafka)
2022-03-28 11:03:10 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-66f23d43-kafka-3)
2022-03-28 11:03:10 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2225253ms till timeout)
2022-03-28 11:03:10 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:03:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (156651ms till timeout)
2022-03-28 11:03:10 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (191237ms till timeout)
2022-03-28 11:03:11 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-0 not ready: kafka)
2022-03-28 11:03:11 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-1 not ready: kafka)
2022-03-28 11:03:11 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-2 not ready: kafka)
2022-03-28 11:03:11 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-66f23d43-kafka-3)
2022-03-28 11:03:11 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2224128ms till timeout)
2022-03-28 11:03:11 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:03:11 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (190168ms till timeout)
2022-03-28 11:03:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (155488ms till timeout)
2022-03-28 11:03:12 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-0 not ready: kafka)
2022-03-28 11:03:12 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-1 not ready: kafka)
2022-03-28 11:03:12 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-2 not ready: kafka)
2022-03-28 11:03:12 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-66f23d43-kafka-3)
2022-03-28 11:03:12 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2223091ms till timeout)
2022-03-28 11:03:12 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (189083ms till timeout)
2022-03-28 11:03:12 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:03:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (154378ms till timeout)
2022-03-28 11:03:12 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 11:03:12 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 11:03:12 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (448682ms till timeout)
2022-03-28 11:03:13 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:369] ############################################################################
2022-03-28 11:03:13 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:267] testUserTemplate - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTls, testSendSimpleMessageTlsScramSha, testConfigurationFileIsCreated, testReceiveSimpleMessageTls, testDeployAndUnDeployCruiseControl, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testConfigurationPerformanceOptions, testTlsExternalUserWithQuotas, testCreateTopicViaKafka] to and randomly select one to start execution
2022-03-28 11:03:13 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:93] [operators.user.UserST] - Removing parallel test: testUserTemplate
2022-03-28 11:03:13 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:97] [operators.user.UserST] - Parallel test count: 7
2022-03-28 11:03:13 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testUserTemplate-FINISHED
2022-03-28 11:03:13 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:30] ############################################################################
2022-03-28 11:03:13 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-0 not ready: kafka)
2022-03-28 11:03:13 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-1 not ready: kafka)
2022-03-28 11:03:13 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-2 not ready: kafka)
2022-03-28 11:03:13 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-66f23d43-kafka-3)
2022-03-28 11:03:13 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2222008ms till timeout)
2022-03-28 11:03:13 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (188020ms till timeout)
2022-03-28 11:03:13 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:03:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (153317ms till timeout)
2022-03-28 11:03:13 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 11:03:14 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 11:03:14 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 11:03:14 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (447321ms till timeout)
2022-03-28 11:03:14 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:230] testTlsExternalUserWithQuotas test now can proceed its execution
2022-03-28 11:03:14 [ForkJoinPool-1-worker-3] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 11:03:14 [ForkJoinPool-1-worker-3] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383, testThrottlingQuotasCreateAlterPartitions=my-cluster-28d9dd2e, testSendSimpleMessageTls=my-cluster-27ddd6ba, testConfigurationReflection=my-cluster-d1e2168e, testUserWithNameMoreThan64Chars=my-cluster-35e66bba, testDeployAndUnDeployCruiseControl=my-cluster-c0830cbe, testCapacityFile=my-cluster-fd0fb61a, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-66f23d43, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-f50d425d, testTlsExternalUser=my-cluster-3ba9cc5b, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5, testScramUserWithQuotas=my-cluster-4941482a, testTopicModificationOfReplicationFactor=my-cluster-38e659b2, testKafkaAdminTopicOperations=my-cluster-5a0bfe14, testConfigurationFileIsCreated=my-cluster-e0fac774, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c, testThrottlingQuotasDeleteTopic=my-cluster-8f262de5, testUpdateUser=my-cluster-2f4b361c, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-da91577a, testTlsExternalUserWithQuotas=my-cluster-bb24d987, testTlsUserWithQuotas=my-cluster-d49e42e8, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8, testSendingMessagesToNonExistingTopic=my-cluster-241cc7c0, testReceiveSimpleMessageTls=my-cluster-c6a15963, testConfigurationPerformanceOptions=my-cluster-b6310693, testUserTemplate=my-cluster-3189676d, testMoreReplicasThanAvailableBrokers=my-cluster-59a64014}
2022-03-28 11:03:14 [ForkJoinPool-1-worker-3] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-1915916223-1265293973, testThrottlingQuotasCreateAlterPartitions=my-user-1933547616-1024431140, testSendSimpleMessageTls=my-user-738229993-1095121647, testConfigurationReflection=my-user-1445653023-93200027, testUserWithNameMoreThan64Chars=my-user-1703808150-314159031, testDeployAndUnDeployCruiseControl=my-user-1988711145-439861161, testCapacityFile=my-user-937015144-1959439396, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-1150765500-2076224625, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-745974575-1852384454, testTlsExternalUser=my-user-1382292264-1641298587, testSendSimpleMessageTlsScramSha=my-user-1499386234-1249154328, testScramUserWithQuotas=my-user-797280497-2138800976, testTopicModificationOfReplicationFactor=my-user-2003504850-304348854, testKafkaAdminTopicOperations=my-user-1708656731-1087276375, testConfigurationFileIsCreated=my-user-180611180-50530491, testCreatingUsersWithSecretPrefix=my-user-1450786684-1808961062, testThrottlingQuotasDeleteTopic=my-user-1558977140-777236615, testUpdateUser=my-user-2098198927-950610275, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-1260379615-1050949907, testTlsExternalUserWithQuotas=my-user-656620760-372265072, testTlsUserWithQuotas=my-user-811912745-1441809840, testThrottlingQuotasCreateTopic=my-user-1131806865-947217230, testSendingMessagesToNonExistingTopic=my-user-927451395-640848463, testReceiveSimpleMessageTls=my-user-6088058-1142028780, testConfigurationPerformanceOptions=my-user-930975101-640523779, testUserTemplate=my-user-1919254789-1384584070, testMoreReplicasThanAvailableBrokers=my-user-1511857485-1227147676}
2022-03-28 11:03:14 [ForkJoinPool-1-worker-3] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-1834944466-2094449929, testThrottlingQuotasCreateAlterPartitions=my-topic-609849288-1810034325, testSendSimpleMessageTls=my-topic-563023482-1697359353, testConfigurationReflection=my-topic-640815392-1092406112, testUserWithNameMoreThan64Chars=my-topic-2021464661-1313035472, testDeployAndUnDeployCruiseControl=my-topic-1511079187-95055629, testCapacityFile=my-topic-1155641705-902584520, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-842198221-1227962115, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-254848724-189700675, testTlsExternalUser=my-topic-5642373-135861890, testSendSimpleMessageTlsScramSha=my-topic-1028951580-1656411053, testScramUserWithQuotas=my-topic-2004840350-265526537, testTopicModificationOfReplicationFactor=my-topic-1789870897-1466152225, testKafkaAdminTopicOperations=my-topic-1564170108-1493323202, testConfigurationFileIsCreated=my-topic-1296407588-313880708, testCreatingUsersWithSecretPrefix=my-topic-1479520843-618891199, testThrottlingQuotasDeleteTopic=my-topic-2128658681-1688111273, testUpdateUser=my-topic-1741945237-916990305, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-1674392683-1637010870, testTlsExternalUserWithQuotas=my-topic-1953803944-1840309972, testTlsUserWithQuotas=my-topic-418284173-1689090259, testThrottlingQuotasCreateTopic=my-topic-728672490-1194703402, testSendingMessagesToNonExistingTopic=my-topic-645675602-646682485, testReceiveSimpleMessageTls=my-topic-478514536-2132782608, testConfigurationPerformanceOptions=my-topic-213912446-1165431808, testUserTemplate=my-topic-2114982563-185666735, testMoreReplicasThanAvailableBrokers=my-topic-1591366435-591944736}
2022-03-28 11:03:14 [ForkJoinPool-1-worker-3] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-28d9dd2e-kafka-clients, testSendSimpleMessageTls=my-cluster-27ddd6ba-kafka-clients, testConfigurationReflection=my-cluster-d1e2168e-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-35e66bba-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-c0830cbe-kafka-clients, testCapacityFile=my-cluster-fd0fb61a-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-66f23d43-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-f50d425d-kafka-clients, testTlsExternalUser=my-cluster-3ba9cc5b-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5-kafka-clients, testScramUserWithQuotas=my-cluster-4941482a-kafka-clients, testTopicModificationOfReplicationFactor=my-cluster-38e659b2-kafka-clients, testKafkaAdminTopicOperations=my-cluster-5a0bfe14-kafka-clients, testConfigurationFileIsCreated=my-cluster-e0fac774-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-8f262de5-kafka-clients, testUpdateUser=my-cluster-2f4b361c-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-da91577a-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-bb24d987-kafka-clients, testTlsUserWithQuotas=my-cluster-d49e42e8-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8-kafka-clients, testSendingMessagesToNonExistingTopic=my-cluster-241cc7c0-kafka-clients, testReceiveSimpleMessageTls=my-cluster-c6a15963-kafka-clients, testConfigurationPerformanceOptions=my-cluster-b6310693-kafka-clients, testUserTemplate=my-cluster-3189676d-kafka-clients, testMoreReplicasThanAvailableBrokers=my-cluster-59a64014-kafka-clients}
2022-03-28 11:03:14 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:155] Create/Update KafkaUser my-user-656620760-372265072 in namespace user-st
2022-03-28 11:03:14 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-656620760-372265072
2022-03-28 11:03:14 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-0 not ready: kafka)
2022-03-28 11:03:14 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-1 not ready: kafka)
2022-03-28 11:03:14 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-2 not ready: kafka)
2022-03-28 11:03:14 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-66f23d43-kafka-3)
2022-03-28 11:03:14 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2220861ms till timeout)
2022-03-28 11:03:14 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-656620760-372265072 will have desired state: Ready
2022-03-28 11:03:14 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:03:14 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-656620760-372265072 will have desired state: Ready
2022-03-28 11:03:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (152203ms till timeout)
2022-03-28 11:03:14 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (186858ms till timeout)
2022-03-28 11:03:14 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] KafkaUser: my-user-656620760-372265072 will have desired state: Ready not ready, will try again in 1000 ms (179932ms till timeout)
2022-03-28 11:03:15 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 11:03:15 [ForkJoinPool-1-worker-7] TRACE [SuiteThreadController:210] testCreateTopicViaKafka is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 11:03:15 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:03:15 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-0 not ready: kafka)
2022-03-28 11:03:15 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-1 not ready: kafka)
2022-03-28 11:03:15 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-2 not ready: kafka)
2022-03-28 11:03:15 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-66f23d43-kafka-3)
2022-03-28 11:03:15 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2219740ms till timeout)
2022-03-28 11:03:16 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (185755ms till timeout)
2022-03-28 11:03:16 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:444] KafkaUser: my-user-656620760-372265072 is in desired state: Ready
2022-03-28 11:03:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (151077ms till timeout)
2022-03-28 11:03:16 [ForkJoinPool-1-worker-3] DEBUG [UserST:274] Command for kafka-configs.sh bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=my-user-656620760-372265072
2022-03-28 11:03:16 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=my-user-656620760-372265072
2022-03-28 11:03:16 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-0 not ready: kafka)
2022-03-28 11:03:16 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-1 not ready: kafka)
2022-03-28 11:03:16 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-2 not ready: kafka)
2022-03-28 11:03:16 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-66f23d43-kafka-3)
2022-03-28 11:03:16 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2218714ms till timeout)
2022-03-28 11:03:17 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:03:17 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (184594ms till timeout)
2022-03-28 11:03:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (149909ms till timeout)
2022-03-28 11:03:18 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-0 not ready: kafka)
2022-03-28 11:03:18 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-1 not ready: kafka)
2022-03-28 11:03:18 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-2 not ready: kafka)
2022-03-28 11:03:18 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-66f23d43-kafka-3)
2022-03-28 11:03:18 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2217558ms till timeout)
2022-03-28 11:03:18 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:03:18 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (183528ms till timeout)
2022-03-28 11:03:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (148851ms till timeout)
2022-03-28 11:03:19 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-0 not ready: kafka)
2022-03-28 11:03:19 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-1 not ready: kafka)
2022-03-28 11:03:19 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-2 not ready: kafka)
2022-03-28 11:03:19 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-3 not ready: kafka)
2022-03-28 11:03:19 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-66f23d43-kafka-0, my-cluster-66f23d43-kafka-1, my-cluster-66f23d43-kafka-2, my-cluster-66f23d43-kafka-3 are ready
2022-03-28 11:03:19 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2216477ms till timeout)
2022-03-28 11:03:19 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (182367ms till timeout)
2022-03-28 11:03:19 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:03:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (147632ms till timeout)
2022-03-28 11:03:19 [ForkJoinPool-1-worker-5] INFO  [Exec:417] Command: oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 11:03:19 [ForkJoinPool-1-worker-5] INFO  [Exec:417] Return code: 0
2022-03-28 11:03:19 [ForkJoinPool-1-worker-5] INFO  [TopicST:461] Checking in KafkaTopic CR that topic topic-example-new exists
2022-03-28 11:03:19 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:674] ============================================================================
2022-03-28 11:03:19 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:675] [operators.topic.TopicST - After Each] - Clean up after test
2022-03-28 11:03:19 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:344] ############################################################################
2022-03-28 11:03:19 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:348] Delete all resources for testMoreReplicasThanAvailableBrokers
2022-03-28 11:03:19 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of KafkaTopic topic-example-new in namespace topic-st
2022-03-28 11:03:19 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:topic-example-new
2022-03-28 11:03:19 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:topic-example-new not ready, will try again in 10000 ms (179817ms till timeout)
2022-03-28 11:03:20 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-0 not ready: kafka)
2022-03-28 11:03:20 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-1 not ready: kafka)
2022-03-28 11:03:20 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-2 not ready: kafka)
2022-03-28 11:03:20 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-3 not ready: kafka)
2022-03-28 11:03:20 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-66f23d43-kafka-0, my-cluster-66f23d43-kafka-1, my-cluster-66f23d43-kafka-2, my-cluster-66f23d43-kafka-3 are ready
2022-03-28 11:03:20 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2215446ms till timeout)
2022-03-28 11:03:20 [ForkJoinPool-1-worker-7] TRACE [SuiteThreadController:210] testCreateTopicViaKafka is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 11:03:20 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (181325ms till timeout)
2022-03-28 11:03:20 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:03:20 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-9 get Namespace namespace-3 -o yaml
2022-03-28 11:03:20 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 1
2022-03-28 11:03:20 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 11:03:20 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-3" not found
2022-03-28 11:03:20 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] ======STDERR END======
2022-03-28 11:03:20 [ForkJoinPool-1-worker-1] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[namespace-8], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[infra-namespace, reconciliation-st], io.strimzi.test.logs.CollectorElement@f4405b3b=[], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 11:03:20 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:267] testDeployAndUnDeployCruiseControl - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTls, testSendSimpleMessageTlsScramSha, testConfigurationFileIsCreated, testReceiveSimpleMessageTls, testDeployAndUnDeployCruiseControl, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testConfigurationPerformanceOptions, testCreateTopicViaKafka] to and randomly select one to start execution
2022-03-28 11:03:20 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:93] [cruisecontrol.CruiseControlConfigurationST] - Removing parallel test: testDeployAndUnDeployCruiseControl
2022-03-28 11:03:20 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:97] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 6
2022-03-28 11:03:20 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testDeployAndUnDeployCruiseControl-FINISHED
2022-03-28 11:03:20 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:30] ############################################################################
2022-03-28 11:03:20 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:241] Delete of KafkaTopic my-topic-1591366435-591944736 in namespace topic-st
2022-03-28 11:03:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (146577ms till timeout)
2022-03-28 11:03:20 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1591366435-591944736
2022-03-28 11:03:20 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:23] ############################################################################
2022-03-28 11:03:20 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.topic.TopicST.testCreateTopicAfterUnsupportedOperation-STARTED
2022-03-28 11:03:20 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:658] ============================================================================
2022-03-28 11:03:20 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:659] [operators.topic.TopicST - Before Each] - Setup test case environment
2022-03-28 11:03:20 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:77] [operators.topic.TopicST] - Adding parallel test: testCreateTopicAfterUnsupportedOperation
2022-03-28 11:03:20 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:81] [operators.topic.TopicST] - Parallel test count: 7
2022-03-28 11:03:20 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:205] [testCreateTopicAfterUnsupportedOperation] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 11:03:20 [ForkJoinPool-1-worker-1] TRACE [SuiteThreadController:210] testCreateTopicAfterUnsupportedOperation is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 11:03:21 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-0 not ready: kafka)
2022-03-28 11:03:21 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-1 not ready: kafka)
2022-03-28 11:03:21 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-2 not ready: kafka)
2022-03-28 11:03:21 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-3 not ready: kafka)
2022-03-28 11:03:21 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-66f23d43-kafka-0, my-cluster-66f23d43-kafka-1, my-cluster-66f23d43-kafka-2, my-cluster-66f23d43-kafka-3 are ready
2022-03-28 11:03:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2214329ms till timeout)
2022-03-28 11:03:21 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:03:21 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (180155ms till timeout)
2022-03-28 11:03:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (145414ms till timeout)
2022-03-28 11:03:22 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-0 not ready: kafka)
2022-03-28 11:03:22 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-1 not ready: kafka)
2022-03-28 11:03:22 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-2 not ready: kafka)
2022-03-28 11:03:22 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-3 not ready: kafka)
2022-03-28 11:03:22 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-66f23d43-kafka-0, my-cluster-66f23d43-kafka-1, my-cluster-66f23d43-kafka-2, my-cluster-66f23d43-kafka-3 are ready
2022-03-28 11:03:22 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2213290ms till timeout)
2022-03-28 11:03:22 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (179071ms till timeout)
2022-03-28 11:03:22 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:03:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (144340ms till timeout)
2022-03-28 11:03:23 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-0 not ready: kafka)
2022-03-28 11:03:23 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-1 not ready: kafka)
2022-03-28 11:03:23 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-2 not ready: kafka)
2022-03-28 11:03:23 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-3 not ready: kafka)
2022-03-28 11:03:23 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-66f23d43-kafka-0, my-cluster-66f23d43-kafka-1, my-cluster-66f23d43-kafka-2, my-cluster-66f23d43-kafka-3 are ready
2022-03-28 11:03:23 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2212233ms till timeout)
2022-03-28 11:03:23 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:03:23 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (177959ms till timeout)
2022-03-28 11:03:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (143292ms till timeout)
2022-03-28 11:03:24 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-0 not ready: kafka)
2022-03-28 11:03:24 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-1 not ready: kafka)
2022-03-28 11:03:24 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-2 not ready: kafka)
2022-03-28 11:03:24 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-3 not ready: kafka)
2022-03-28 11:03:24 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-66f23d43-kafka-0, my-cluster-66f23d43-kafka-1, my-cluster-66f23d43-kafka-2, my-cluster-66f23d43-kafka-3 are ready
2022-03-28 11:03:24 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2211152ms till timeout)
2022-03-28 11:03:24 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:03:24 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (176897ms till timeout)
2022-03-28 11:03:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (142193ms till timeout)
2022-03-28 11:03:25 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:230] testCreateTopicViaKafka test now can proceed its execution
2022-03-28 11:03:25 [ForkJoinPool-1-worker-7] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 11:03:25 [ForkJoinPool-1-worker-7] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383, testThrottlingQuotasCreateAlterPartitions=my-cluster-28d9dd2e, testSendSimpleMessageTls=my-cluster-27ddd6ba, testConfigurationReflection=my-cluster-d1e2168e, testUserWithNameMoreThan64Chars=my-cluster-35e66bba, testDeployAndUnDeployCruiseControl=my-cluster-c0830cbe, testCapacityFile=my-cluster-fd0fb61a, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-66f23d43, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-f50d425d, testTlsExternalUser=my-cluster-3ba9cc5b, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5, testScramUserWithQuotas=my-cluster-4941482a, testTopicModificationOfReplicationFactor=my-cluster-38e659b2, testKafkaAdminTopicOperations=my-cluster-5a0bfe14, testConfigurationFileIsCreated=my-cluster-e0fac774, testCreateTopicViaKafka=my-cluster-d73fc0b6, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c, testThrottlingQuotasDeleteTopic=my-cluster-8f262de5, testUpdateUser=my-cluster-2f4b361c, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-da91577a, testTlsExternalUserWithQuotas=my-cluster-bb24d987, testTlsUserWithQuotas=my-cluster-d49e42e8, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8, testSendingMessagesToNonExistingTopic=my-cluster-241cc7c0, testReceiveSimpleMessageTls=my-cluster-c6a15963, testConfigurationPerformanceOptions=my-cluster-b6310693, testUserTemplate=my-cluster-3189676d, testMoreReplicasThanAvailableBrokers=my-cluster-59a64014}
2022-03-28 11:03:25 [ForkJoinPool-1-worker-7] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-1915916223-1265293973, testThrottlingQuotasCreateAlterPartitions=my-user-1933547616-1024431140, testSendSimpleMessageTls=my-user-738229993-1095121647, testConfigurationReflection=my-user-1445653023-93200027, testUserWithNameMoreThan64Chars=my-user-1703808150-314159031, testDeployAndUnDeployCruiseControl=my-user-1988711145-439861161, testCapacityFile=my-user-937015144-1959439396, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-1150765500-2076224625, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-745974575-1852384454, testTlsExternalUser=my-user-1382292264-1641298587, testSendSimpleMessageTlsScramSha=my-user-1499386234-1249154328, testScramUserWithQuotas=my-user-797280497-2138800976, testTopicModificationOfReplicationFactor=my-user-2003504850-304348854, testKafkaAdminTopicOperations=my-user-1708656731-1087276375, testConfigurationFileIsCreated=my-user-180611180-50530491, testCreateTopicViaKafka=my-user-372778533-473457568, testCreatingUsersWithSecretPrefix=my-user-1450786684-1808961062, testThrottlingQuotasDeleteTopic=my-user-1558977140-777236615, testUpdateUser=my-user-2098198927-950610275, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-1260379615-1050949907, testTlsExternalUserWithQuotas=my-user-656620760-372265072, testTlsUserWithQuotas=my-user-811912745-1441809840, testThrottlingQuotasCreateTopic=my-user-1131806865-947217230, testSendingMessagesToNonExistingTopic=my-user-927451395-640848463, testReceiveSimpleMessageTls=my-user-6088058-1142028780, testConfigurationPerformanceOptions=my-user-930975101-640523779, testUserTemplate=my-user-1919254789-1384584070, testMoreReplicasThanAvailableBrokers=my-user-1511857485-1227147676}
2022-03-28 11:03:25 [ForkJoinPool-1-worker-7] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-1834944466-2094449929, testThrottlingQuotasCreateAlterPartitions=my-topic-609849288-1810034325, testSendSimpleMessageTls=my-topic-563023482-1697359353, testConfigurationReflection=my-topic-640815392-1092406112, testUserWithNameMoreThan64Chars=my-topic-2021464661-1313035472, testDeployAndUnDeployCruiseControl=my-topic-1511079187-95055629, testCapacityFile=my-topic-1155641705-902584520, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-842198221-1227962115, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-254848724-189700675, testTlsExternalUser=my-topic-5642373-135861890, testSendSimpleMessageTlsScramSha=my-topic-1028951580-1656411053, testScramUserWithQuotas=my-topic-2004840350-265526537, testTopicModificationOfReplicationFactor=my-topic-1789870897-1466152225, testKafkaAdminTopicOperations=my-topic-1564170108-1493323202, testConfigurationFileIsCreated=my-topic-1296407588-313880708, testCreateTopicViaKafka=my-topic-1960957209-999941425, testCreatingUsersWithSecretPrefix=my-topic-1479520843-618891199, testThrottlingQuotasDeleteTopic=my-topic-2128658681-1688111273, testUpdateUser=my-topic-1741945237-916990305, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-1674392683-1637010870, testTlsExternalUserWithQuotas=my-topic-1953803944-1840309972, testTlsUserWithQuotas=my-topic-418284173-1689090259, testThrottlingQuotasCreateTopic=my-topic-728672490-1194703402, testSendingMessagesToNonExistingTopic=my-topic-645675602-646682485, testReceiveSimpleMessageTls=my-topic-478514536-2132782608, testConfigurationPerformanceOptions=my-topic-213912446-1165431808, testUserTemplate=my-topic-2114982563-185666735, testMoreReplicasThanAvailableBrokers=my-topic-1591366435-591944736}
2022-03-28 11:03:25 [ForkJoinPool-1-worker-7] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-28d9dd2e-kafka-clients, testSendSimpleMessageTls=my-cluster-27ddd6ba-kafka-clients, testConfigurationReflection=my-cluster-d1e2168e-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-35e66bba-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-c0830cbe-kafka-clients, testCapacityFile=my-cluster-fd0fb61a-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-66f23d43-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-f50d425d-kafka-clients, testTlsExternalUser=my-cluster-3ba9cc5b-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5-kafka-clients, testScramUserWithQuotas=my-cluster-4941482a-kafka-clients, testTopicModificationOfReplicationFactor=my-cluster-38e659b2-kafka-clients, testKafkaAdminTopicOperations=my-cluster-5a0bfe14-kafka-clients, testConfigurationFileIsCreated=my-cluster-e0fac774-kafka-clients, testCreateTopicViaKafka=my-cluster-d73fc0b6-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-8f262de5-kafka-clients, testUpdateUser=my-cluster-2f4b361c-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-da91577a-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-bb24d987-kafka-clients, testTlsUserWithQuotas=my-cluster-d49e42e8-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8-kafka-clients, testSendingMessagesToNonExistingTopic=my-cluster-241cc7c0-kafka-clients, testReceiveSimpleMessageTls=my-cluster-c6a15963-kafka-clients, testConfigurationPerformanceOptions=my-cluster-b6310693-kafka-clients, testUserTemplate=my-cluster-3189676d-kafka-clients, testMoreReplicasThanAvailableBrokers=my-cluster-59a64014-kafka-clients}
2022-03-28 11:03:25 [ForkJoinPool-1-worker-7] DEBUG [TopicST:113] Creating topic my-topic-1960957209-999941425 with 3 replicas and 3 partitions
2022-03-28 11:03:25 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --bootstrap-server localhost:9092 --create  --topic my-topic-1960957209-999941425 --replication-factor 3 --partitions 3
2022-03-28 11:03:25 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-0 not ready: kafka)
2022-03-28 11:03:25 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-1 not ready: kafka)
2022-03-28 11:03:25 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-2 not ready: kafka)
2022-03-28 11:03:25 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-3 not ready: kafka)
2022-03-28 11:03:25 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-66f23d43-kafka-0, my-cluster-66f23d43-kafka-1, my-cluster-66f23d43-kafka-2, my-cluster-66f23d43-kafka-3 are ready
2022-03-28 11:03:25 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2210125ms till timeout)
2022-03-28 11:03:25 [ForkJoinPool-1-worker-1] TRACE [SuiteThreadController:210] testCreateTopicAfterUnsupportedOperation is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 11:03:25 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Command: oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=my-user-656620760-372265072
2022-03-28 11:03:25 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 11:03:25 [ForkJoinPool-1-worker-3] INFO  [KafkaUserUtils:62] Waiting for KafkaUser deletion my-user-656620760-372265072
2022-03-28 11:03:25 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for KafkaUser deletion my-user-656620760-372265072
2022-03-28 11:03:25 [ForkJoinPool-1-worker-3] WARN  [KafkaUserUtils:68] KafkaUser my-user-656620760-372265072 is not deleted yet! Triggering force delete by cmd client!
2022-03-28 11:03:25 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 delete KafkaUser my-user-656620760-372265072
2022-03-28 11:03:26 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (175747ms till timeout)
2022-03-28 11:03:26 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:03:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (140959ms till timeout)
2022-03-28 11:03:26 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-9 delete KafkaUser my-user-656620760-372265072
2022-03-28 11:03:26 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 1
2022-03-28 11:03:26 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 11:03:26 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Error from server (NotFound): kafkausers.kafka.strimzi.io "my-user-656620760-372265072" not found
2022-03-28 11:03:26 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] ======STDERR END======
2022-03-28 11:03:26 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] KafkaUser deletion my-user-656620760-372265072 not ready, will try again in 1000 ms (179597ms till timeout)
2022-03-28 11:03:26 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-0 not ready: kafka)
2022-03-28 11:03:26 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-1 not ready: kafka)
2022-03-28 11:03:26 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-2 not ready: kafka)
2022-03-28 11:03:26 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-3 not ready: kafka)
2022-03-28 11:03:26 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-66f23d43-kafka-0, my-cluster-66f23d43-kafka-1, my-cluster-66f23d43-kafka-2, my-cluster-66f23d43-kafka-3 are ready
2022-03-28 11:03:26 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2209090ms till timeout)
2022-03-28 11:03:27 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (174643ms till timeout)
2022-03-28 11:03:27 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:03:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (139887ms till timeout)
2022-03-28 11:03:27 [ForkJoinPool-1-worker-3] INFO  [KafkaUserUtils:75] KafkaUser my-user-656620760-372265072 deleted
2022-03-28 11:03:27 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for all KafkaUser CN=my-user-656620760-372265072 attributes will be cleaned
2022-03-28 11:03:27 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=my-user-656620760-372265072
2022-03-28 11:03:27 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-0 not ready: kafka)
2022-03-28 11:03:27 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-1 not ready: kafka)
2022-03-28 11:03:27 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-2 not ready: kafka)
2022-03-28 11:03:27 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-3 not ready: kafka)
2022-03-28 11:03:27 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-66f23d43-kafka-0, my-cluster-66f23d43-kafka-1, my-cluster-66f23d43-kafka-2, my-cluster-66f23d43-kafka-3 are ready
2022-03-28 11:03:27 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2207971ms till timeout)
2022-03-28 11:03:28 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:03:28 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (173511ms till timeout)
2022-03-28 11:03:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (138819ms till timeout)
2022-03-28 11:03:28 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-0 not ready: kafka)
2022-03-28 11:03:28 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-1 not ready: kafka)
2022-03-28 11:03:28 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-2 not ready: kafka)
2022-03-28 11:03:28 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-3 not ready: kafka)
2022-03-28 11:03:28 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-66f23d43-kafka-0, my-cluster-66f23d43-kafka-1, my-cluster-66f23d43-kafka-2, my-cluster-66f23d43-kafka-3 are ready
2022-03-28 11:03:28 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-66f23d43-kafka, strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2206916ms till timeout)
2022-03-28 11:03:29 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:03:29 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (172357ms till timeout)
2022-03-28 11:03:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (137698ms till timeout)
2022-03-28 11:03:29 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Command: oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --bootstrap-server localhost:9092 --create  --topic my-topic-1960957209-999941425 --replication-factor 3 --partitions 3
2022-03-28 11:03:29 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Return code: 0
2022-03-28 11:03:29 [ForkJoinPool-1-worker-7] INFO  [KafkaTopicUtils:78] Waiting for KafkaTopic my-topic-1960957209-999941425 creation 
2022-03-28 11:03:29 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for KafkaTopic creation my-topic-1960957209-999941425
2022-03-28 11:03:29 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:369] ############################################################################
2022-03-28 11:03:29 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:267] testMoreReplicasThanAvailableBrokers - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTls, testSendSimpleMessageTlsScramSha, testConfigurationFileIsCreated, testReceiveSimpleMessageTls, testDeployAndUnDeployCruiseControl, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testConfigurationPerformanceOptions, testCreateTopicAfterUnsupportedOperation] to and randomly select one to start execution
2022-03-28 11:03:29 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:93] [operators.topic.TopicST] - Removing parallel test: testMoreReplicasThanAvailableBrokers
2022-03-28 11:03:29 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:97] [operators.topic.TopicST] - Parallel test count: 6
2022-03-28 11:03:29 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.topic.TopicST.testMoreReplicasThanAvailableBrokers-FINISHED
2022-03-28 11:03:29 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:30] ############################################################################
2022-03-28 11:03:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic creation my-topic-1960957209-999941425 not ready, will try again in 1000 ms (179895ms till timeout)
2022-03-28 11:03:29 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-0 not ready: kafka)
2022-03-28 11:03:29 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-1 not ready: kafka)
2022-03-28 11:03:29 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-2 not ready: kafka)
2022-03-28 11:03:29 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-kafka-3 not ready: kafka)
2022-03-28 11:03:29 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-66f23d43-kafka-0, my-cluster-66f23d43-kafka-1, my-cluster-66f23d43-kafka-2, my-cluster-66f23d43-kafka-3 are ready
2022-03-28 11:03:30 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-66f23d43 will have desired state: Ready
2022-03-28 11:03:30 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-66f23d43 will have desired state: Ready
2022-03-28 11:03:30 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:444] Kafka: my-cluster-66f23d43 is in desired state: Ready
2022-03-28 11:03:30 [ForkJoinPool-1-worker-15] INFO  [RollingUpdateUtils:132] Kafka: my-cluster-66f23d43 is ready
2022-03-28 11:03:30 [ForkJoinPool-1-worker-15] INFO  [ReconciliationST:94] Deploying KafkaConnect with pause annotation from the start, no pods should appear
2022-03-28 11:03:30 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:155] Create/Update Deployment my-cluster-66f23d43-kafka-clients in namespace namespace-9
2022-03-28 11:03:30 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:164] Using Namespace: namespace-8
2022-03-28 11:03:30 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-66f23d43-kafka-clients
2022-03-28 11:03:30 [ForkJoinPool-1-worker-15] INFO  [DeploymentUtils:161] Wait for Deployment: my-cluster-66f23d43-kafka-clients will be ready
2022-03-28 11:03:30 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Wait for Deployment: my-cluster-66f23d43-kafka-clients will be ready
2022-03-28 11:03:30 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:230] testCreateTopicAfterUnsupportedOperation test now can proceed its execution
2022-03-28 11:03:30 [ForkJoinPool-1-worker-1] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 11:03:30 [ForkJoinPool-1-worker-1] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383, testThrottlingQuotasCreateAlterPartitions=my-cluster-28d9dd2e, testSendSimpleMessageTls=my-cluster-27ddd6ba, testConfigurationReflection=my-cluster-d1e2168e, testUserWithNameMoreThan64Chars=my-cluster-35e66bba, testDeployAndUnDeployCruiseControl=my-cluster-c0830cbe, testCapacityFile=my-cluster-fd0fb61a, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-66f23d43, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-f50d425d, testTlsExternalUser=my-cluster-3ba9cc5b, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5, testCreateTopicAfterUnsupportedOperation=my-cluster-b31d4894, testScramUserWithQuotas=my-cluster-4941482a, testTopicModificationOfReplicationFactor=my-cluster-38e659b2, testKafkaAdminTopicOperations=my-cluster-5a0bfe14, testConfigurationFileIsCreated=my-cluster-e0fac774, testCreateTopicViaKafka=my-cluster-d73fc0b6, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c, testThrottlingQuotasDeleteTopic=my-cluster-8f262de5, testUpdateUser=my-cluster-2f4b361c, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-da91577a, testTlsExternalUserWithQuotas=my-cluster-bb24d987, testTlsUserWithQuotas=my-cluster-d49e42e8, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8, testSendingMessagesToNonExistingTopic=my-cluster-241cc7c0, testReceiveSimpleMessageTls=my-cluster-c6a15963, testConfigurationPerformanceOptions=my-cluster-b6310693, testUserTemplate=my-cluster-3189676d, testMoreReplicasThanAvailableBrokers=my-cluster-59a64014}
2022-03-28 11:03:30 [ForkJoinPool-1-worker-1] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-1915916223-1265293973, testThrottlingQuotasCreateAlterPartitions=my-user-1933547616-1024431140, testSendSimpleMessageTls=my-user-738229993-1095121647, testConfigurationReflection=my-user-1445653023-93200027, testUserWithNameMoreThan64Chars=my-user-1703808150-314159031, testDeployAndUnDeployCruiseControl=my-user-1988711145-439861161, testCapacityFile=my-user-937015144-1959439396, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-1150765500-2076224625, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-745974575-1852384454, testTlsExternalUser=my-user-1382292264-1641298587, testSendSimpleMessageTlsScramSha=my-user-1499386234-1249154328, testCreateTopicAfterUnsupportedOperation=my-user-893702009-1709424309, testScramUserWithQuotas=my-user-797280497-2138800976, testTopicModificationOfReplicationFactor=my-user-2003504850-304348854, testKafkaAdminTopicOperations=my-user-1708656731-1087276375, testConfigurationFileIsCreated=my-user-180611180-50530491, testCreateTopicViaKafka=my-user-372778533-473457568, testCreatingUsersWithSecretPrefix=my-user-1450786684-1808961062, testThrottlingQuotasDeleteTopic=my-user-1558977140-777236615, testUpdateUser=my-user-2098198927-950610275, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-1260379615-1050949907, testTlsExternalUserWithQuotas=my-user-656620760-372265072, testTlsUserWithQuotas=my-user-811912745-1441809840, testThrottlingQuotasCreateTopic=my-user-1131806865-947217230, testSendingMessagesToNonExistingTopic=my-user-927451395-640848463, testReceiveSimpleMessageTls=my-user-6088058-1142028780, testConfigurationPerformanceOptions=my-user-930975101-640523779, testUserTemplate=my-user-1919254789-1384584070, testMoreReplicasThanAvailableBrokers=my-user-1511857485-1227147676}
2022-03-28 11:03:30 [ForkJoinPool-1-worker-1] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-1834944466-2094449929, testThrottlingQuotasCreateAlterPartitions=my-topic-609849288-1810034325, testSendSimpleMessageTls=my-topic-563023482-1697359353, testConfigurationReflection=my-topic-640815392-1092406112, testUserWithNameMoreThan64Chars=my-topic-2021464661-1313035472, testDeployAndUnDeployCruiseControl=my-topic-1511079187-95055629, testCapacityFile=my-topic-1155641705-902584520, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-842198221-1227962115, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-254848724-189700675, testTlsExternalUser=my-topic-5642373-135861890, testSendSimpleMessageTlsScramSha=my-topic-1028951580-1656411053, testCreateTopicAfterUnsupportedOperation=my-topic-1151452928-1452134654, testScramUserWithQuotas=my-topic-2004840350-265526537, testTopicModificationOfReplicationFactor=my-topic-1789870897-1466152225, testKafkaAdminTopicOperations=my-topic-1564170108-1493323202, testConfigurationFileIsCreated=my-topic-1296407588-313880708, testCreateTopicViaKafka=my-topic-1960957209-999941425, testCreatingUsersWithSecretPrefix=my-topic-1479520843-618891199, testThrottlingQuotasDeleteTopic=my-topic-2128658681-1688111273, testUpdateUser=my-topic-1741945237-916990305, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-1674392683-1637010870, testTlsExternalUserWithQuotas=my-topic-1953803944-1840309972, testTlsUserWithQuotas=my-topic-418284173-1689090259, testThrottlingQuotasCreateTopic=my-topic-728672490-1194703402, testSendingMessagesToNonExistingTopic=my-topic-645675602-646682485, testReceiveSimpleMessageTls=my-topic-478514536-2132782608, testConfigurationPerformanceOptions=my-topic-213912446-1165431808, testUserTemplate=my-topic-2114982563-185666735, testMoreReplicasThanAvailableBrokers=my-topic-1591366435-591944736}
2022-03-28 11:03:30 [ForkJoinPool-1-worker-1] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-28d9dd2e-kafka-clients, testSendSimpleMessageTls=my-cluster-27ddd6ba-kafka-clients, testConfigurationReflection=my-cluster-d1e2168e-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-35e66bba-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-c0830cbe-kafka-clients, testCapacityFile=my-cluster-fd0fb61a-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-66f23d43-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-f50d425d-kafka-clients, testTlsExternalUser=my-cluster-3ba9cc5b-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5-kafka-clients, testCreateTopicAfterUnsupportedOperation=my-cluster-b31d4894-kafka-clients, testScramUserWithQuotas=my-cluster-4941482a-kafka-clients, testTopicModificationOfReplicationFactor=my-cluster-38e659b2-kafka-clients, testKafkaAdminTopicOperations=my-cluster-5a0bfe14-kafka-clients, testConfigurationFileIsCreated=my-cluster-e0fac774-kafka-clients, testCreateTopicViaKafka=my-cluster-d73fc0b6-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-8f262de5-kafka-clients, testUpdateUser=my-cluster-2f4b361c-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-da91577a-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-bb24d987-kafka-clients, testTlsUserWithQuotas=my-cluster-d49e42e8-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8-kafka-clients, testSendingMessagesToNonExistingTopic=my-cluster-241cc7c0-kafka-clients, testReceiveSimpleMessageTls=my-cluster-c6a15963-kafka-clients, testConfigurationPerformanceOptions=my-cluster-b6310693-kafka-clients, testUserTemplate=my-cluster-3189676d-kafka-clients, testMoreReplicasThanAvailableBrokers=my-cluster-59a64014-kafka-clients}
2022-03-28 11:03:30 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:155] Create/Update KafkaTopic topic-with-replication-to-change in namespace topic-st
2022-03-28 11:03:30 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:03:30 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-kafka-clients will be ready not ready, will try again in 1000 ms (479776ms till timeout)
2022-03-28 11:03:30 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (171042ms till timeout)
2022-03-28 11:03:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (136305ms till timeout)
2022-03-28 11:03:30 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:topic-with-replication-to-change
2022-03-28 11:03:30 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:433] Wait for KafkaTopic: topic-with-replication-to-change will have desired state: Ready
2022-03-28 11:03:30 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for KafkaTopic: topic-with-replication-to-change will have desired state: Ready
2022-03-28 11:03:30 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] KafkaTopic: topic-with-replication-to-change will have desired state: Ready not ready, will try again in 1000 ms (179919ms till timeout)
2022-03-28 11:03:30 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic creation my-topic-1960957209-999941425 not ready, will try again in 1000 ms (178850ms till timeout)
2022-03-28 11:03:31 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-kafka-clients will be ready not ready, will try again in 1000 ms (478644ms till timeout)
2022-03-28 11:03:32 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:03:32 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (169672ms till timeout)
2022-03-28 11:03:32 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] KafkaTopic: topic-with-replication-to-change will have desired state: Ready not ready, will try again in 1000 ms (178723ms till timeout)
2022-03-28 11:03:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (134916ms till timeout)
2022-03-28 11:03:32 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic creation my-topic-1960957209-999941425 not ready, will try again in 1000 ms (177634ms till timeout)
2022-03-28 11:03:32 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-kafka-clients will be ready not ready, will try again in 1000 ms (477570ms till timeout)
2022-03-28 11:03:33 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (168624ms till timeout)
2022-03-28 11:03:33 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:444] KafkaTopic: topic-with-replication-to-change is in desired state: Ready
2022-03-28 11:03:33 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:03:33 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic creation my-topic-1960957209-999941425 not ready, will try again in 1000 ms (176538ms till timeout)
2022-03-28 11:03:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (133763ms till timeout)
2022-03-28 11:03:33 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:433] Wait for KafkaTopic: topic-with-replication-to-change will have desired state: NotReady
2022-03-28 11:03:33 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for KafkaTopic: topic-with-replication-to-change will have desired state: NotReady
2022-03-28 11:03:33 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] KafkaTopic: topic-with-replication-to-change will have desired state: NotReady not ready, will try again in 1000 ms (179961ms till timeout)
2022-03-28 11:03:33 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-kafka-clients will be ready not ready, will try again in 1000 ms (476540ms till timeout)
2022-03-28 11:03:34 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (167583ms till timeout)
2022-03-28 11:03:34 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic creation my-topic-1960957209-999941425 not ready, will try again in 1000 ms (175508ms till timeout)
2022-03-28 11:03:34 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:03:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (132679ms till timeout)
2022-03-28 11:03:34 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:444] KafkaTopic: topic-with-replication-to-change is in desired state: NotReady
2022-03-28 11:03:34 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:155] Create/Update KafkaTopic another-topic in namespace topic-st
2022-03-28 11:03:34 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:another-topic
2022-03-28 11:03:34 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:433] Wait for KafkaTopic: another-topic will have desired state: Ready
2022-03-28 11:03:34 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for KafkaTopic: another-topic will have desired state: Ready
2022-03-28 11:03:34 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] KafkaTopic: another-topic will have desired state: Ready not ready, will try again in 1000 ms (179982ms till timeout)
2022-03-28 11:03:34 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Command: oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=my-user-656620760-372265072
2022-03-28 11:03:34 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 11:03:34 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:674] ============================================================================
2022-03-28 11:03:34 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:675] [operators.user.UserST - After Each] - Clean up after test
2022-03-28 11:03:34 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:344] ############################################################################
2022-03-28 11:03:34 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:348] Delete all resources for testTlsExternalUserWithQuotas
2022-03-28 11:03:34 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:241] Delete of KafkaUser my-user-656620760-372265072 in namespace user-st
2022-03-28 11:03:34 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-656620760-372265072
2022-03-28 11:03:34 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:369] ############################################################################
2022-03-28 11:03:34 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:267] testTlsExternalUserWithQuotas - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTls, testSendSimpleMessageTlsScramSha, testConfigurationFileIsCreated, testReceiveSimpleMessageTls, testDeployAndUnDeployCruiseControl, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testConfigurationPerformanceOptions] to and randomly select one to start execution
2022-03-28 11:03:34 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:93] [operators.user.UserST] - Removing parallel test: testTlsExternalUserWithQuotas
2022-03-28 11:03:34 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:97] [operators.user.UserST] - Parallel test count: 5
2022-03-28 11:03:34 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testTlsExternalUserWithQuotas-FINISHED
2022-03-28 11:03:34 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:30] ############################################################################
2022-03-28 11:03:34 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:689] ============================================================================
2022-03-28 11:03:34 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:690] [operators.user.UserST - After All] - Clean up after test suite
2022-03-28 11:03:34 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:344] ############################################################################
2022-03-28 11:03:34 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:348] Delete all resources for UserST
2022-03-28 11:03:34 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of Kafka user-cluster-name in namespace user-st
2022-03-28 11:03:34 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:689] ============================================================================
2022-03-28 11:03:34 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:690] [cruisecontrol.CruiseControlConfigurationST - After All] - Clean up after test suite
2022-03-28 11:03:34 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:344] ############################################################################
2022-03-28 11:03:34 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:346] In context CruiseControlConfigurationST is everything deleted.
2022-03-28 11:03:34 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:369] ############################################################################
2022-03-28 11:03:34 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Namespace cruise-control-configuration-st removal
2022-03-28 11:03:34 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 11:03:34 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:user-cluster-name
2022-03-28 11:03:34 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:user-cluster-name not ready, will try again in 10000 ms (839954ms till timeout)
2022-03-28 11:03:34 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 11:03:34 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:03:34 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace cruise-control-configuration-st removal not ready, will try again in 1000 ms (479781ms till timeout)
2022-03-28 11:03:35 [ForkJoinPool-1-worker-15] INFO  [DeploymentUtils:168] Deployment: my-cluster-66f23d43-kafka-clients is ready
2022-03-28 11:03:35 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:155] Create/Update Deployment my-cluster-66f23d43-scraper in namespace namespace-8
2022-03-28 11:03:35 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:164] Using Namespace: namespace-8
2022-03-28 11:03:35 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-66f23d43-scraper
2022-03-28 11:03:35 [ForkJoinPool-1-worker-15] INFO  [DeploymentUtils:161] Wait for Deployment: my-cluster-66f23d43-scraper will be ready
2022-03-28 11:03:35 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Wait for Deployment: my-cluster-66f23d43-scraper will be ready
2022-03-28 11:03:35 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-scraper will be ready not ready, will try again in 1000 ms (479895ms till timeout)
2022-03-28 11:03:35 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (166433ms till timeout)
2022-03-28 11:03:35 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic creation my-topic-1960957209-999941425 not ready, will try again in 1000 ms (174480ms till timeout)
2022-03-28 11:03:35 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:03:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (131584ms till timeout)
2022-03-28 11:03:35 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:444] KafkaTopic: another-topic is in desired state: Ready
2022-03-28 11:03:35 [ForkJoinPool-1-worker-1] INFO  [TopicST:456] Checking topic topic-with-replication-to-change in Kafka
2022-03-28 11:03:35 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 11:03:35 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 11:03:36 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 11:03:36 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:03:36 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace cruise-control-configuration-st removal not ready, will try again in 1000 ms (478560ms till timeout)
2022-03-28 11:03:36 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic creation my-topic-1960957209-999941425 not ready, will try again in 1000 ms (173341ms till timeout)
2022-03-28 11:03:36 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-scraper will be ready not ready, will try again in 1000 ms (478707ms till timeout)
2022-03-28 11:03:36 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (165249ms till timeout)
2022-03-28 11:03:36 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:03:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (130535ms till timeout)
2022-03-28 11:03:37 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 11:03:37 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 11:03:37 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:03:37 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace cruise-control-configuration-st removal not ready, will try again in 1000 ms (477315ms till timeout)
2022-03-28 11:03:37 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic creation my-topic-1960957209-999941425 not ready, will try again in 1000 ms (172313ms till timeout)
2022-03-28 11:03:37 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-scraper will be ready not ready, will try again in 1000 ms (477654ms till timeout)
2022-03-28 11:03:37 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (164188ms till timeout)
2022-03-28 11:03:37 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:03:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129475ms till timeout)
2022-03-28 11:03:38 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 11:03:38 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic creation my-topic-1960957209-999941425 not ready, will try again in 1000 ms (171285ms till timeout)
2022-03-28 11:03:38 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-scraper will be ready not ready, will try again in 1000 ms (476624ms till timeout)
2022-03-28 11:03:38 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 11:03:38 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:03:38 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace cruise-control-configuration-st removal not ready, will try again in 1000 ms (476068ms till timeout)
2022-03-28 11:03:38 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:03:38 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (163072ms till timeout)
2022-03-28 11:03:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (128410ms till timeout)
2022-03-28 11:03:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic creation my-topic-1960957209-999941425 not ready, will try again in 1000 ms (170216ms till timeout)
2022-03-28 11:03:39 [ForkJoinPool-1-worker-15] INFO  [DeploymentUtils:168] Deployment: my-cluster-66f23d43-scraper is ready
2022-03-28 11:03:39 [ForkJoinPool-1-worker-15] INFO  [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-66f23d43-scraper to be ready
2022-03-28 11:03:39 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 11:03:39 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-66f23d43-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready
2022-03-28 11:03:39 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-scraper-79687dcf5d-xqgkd not ready: my-cluster-66f23d43-scraper)
2022-03-28 11:03:39 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-66f23d43-scraper-79687dcf5d-xqgkd are ready
2022-03-28 11:03:39 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-66f23d43-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599932ms till timeout)
2022-03-28 11:03:39 [ForkJoinPool-1-worker-1] INFO  [Exec:417] Command: oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 11:03:39 [ForkJoinPool-1-worker-1] INFO  [Exec:417] Return code: 0
2022-03-28 11:03:39 [ForkJoinPool-1-worker-1] INFO  [TopicST:461] Checking in KafkaTopic CR that topic topic-with-replication-to-change exists
2022-03-28 11:03:39 [ForkJoinPool-1-worker-1] INFO  [TopicST:456] Checking topic another-topic in Kafka
2022-03-28 11:03:39 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 11:03:39 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:03:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (127300ms till timeout)
2022-03-28 11:03:39 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (161944ms till timeout)
2022-03-28 11:03:39 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 11:03:39 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:03:39 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace cruise-control-configuration-st removal not ready, will try again in 1000 ms (474868ms till timeout)
2022-03-28 11:03:40 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic creation my-topic-1960957209-999941425 not ready, will try again in 1000 ms (169127ms till timeout)
2022-03-28 11:03:40 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-scraper-79687dcf5d-xqgkd not ready: my-cluster-66f23d43-scraper)
2022-03-28 11:03:40 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-66f23d43-scraper-79687dcf5d-xqgkd are ready
2022-03-28 11:03:40 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-66f23d43-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598902ms till timeout)
2022-03-28 11:03:40 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T11:03:38Z, conditions=[JobCondition(lastProbeTime=2022-03-28T11:03:38Z, lastTransitionTime=2022-03-28T11:03:38Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T11:02:03Z, succeeded=1, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:03:40 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 11:03:40 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (160863ms till timeout)
2022-03-28 11:03:40 [ForkJoinPool-1-worker-9] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 11:03:40 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 11:03:41 [ForkJoinPool-1-worker-9] INFO  [PodUtils:189] Message All topics created found in create-admin-my-cluster-8f262de5-kafka-clients-8nvfh log
2022-03-28 11:03:41 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment create-admin-my-cluster-8f262de5-kafka-clients deletion
2022-03-28 11:03:41 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for ReplicaSet create-admin-my-cluster-8f262de5-kafka-clients to be deleted
2022-03-28 11:03:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] ReplicaSet create-admin-my-cluster-8f262de5-kafka-clients to be deleted not ready, will try again in 5000 ms (179981ms till timeout)
2022-03-28 11:03:41 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 11:03:41 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:03:41 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace cruise-control-configuration-st removal not ready, will try again in 1000 ms (473578ms till timeout)
2022-03-28 11:03:41 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic creation my-topic-1960957209-999941425 not ready, will try again in 1000 ms (168064ms till timeout)
2022-03-28 11:03:41 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-scraper-79687dcf5d-xqgkd not ready: my-cluster-66f23d43-scraper)
2022-03-28 11:03:41 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-66f23d43-scraper-79687dcf5d-xqgkd are ready
2022-03-28 11:03:41 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-66f23d43-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597818ms till timeout)
2022-03-28 11:03:42 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (159772ms till timeout)
2022-03-28 11:03:42 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 11:03:42 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 11:03:42 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:03:42 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace cruise-control-configuration-st removal not ready, will try again in 1000 ms (472313ms till timeout)
2022-03-28 11:03:42 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic creation my-topic-1960957209-999941425 not ready, will try again in 1000 ms (167033ms till timeout)
2022-03-28 11:03:42 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-scraper-79687dcf5d-xqgkd not ready: my-cluster-66f23d43-scraper)
2022-03-28 11:03:42 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-66f23d43-scraper-79687dcf5d-xqgkd are ready
2022-03-28 11:03:42 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-66f23d43-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596768ms till timeout)
2022-03-28 11:03:43 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (158671ms till timeout)
2022-03-28 11:03:43 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 11:03:43 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 11:03:43 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:03:43 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace cruise-control-configuration-st removal not ready, will try again in 1000 ms (471038ms till timeout)
2022-03-28 11:03:43 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic creation my-topic-1960957209-999941425 not ready, will try again in 1000 ms (166009ms till timeout)
2022-03-28 11:03:43 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-scraper-79687dcf5d-xqgkd not ready: my-cluster-66f23d43-scraper)
2022-03-28 11:03:43 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-66f23d43-scraper-79687dcf5d-xqgkd are ready
2022-03-28 11:03:43 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-66f23d43-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595722ms till timeout)
2022-03-28 11:03:44 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (157594ms till timeout)
2022-03-28 11:03:44 [ForkJoinPool-1-worker-1] INFO  [Exec:417] Command: oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 11:03:44 [ForkJoinPool-1-worker-1] INFO  [Exec:417] Return code: 0
2022-03-28 11:03:44 [ForkJoinPool-1-worker-1] INFO  [TopicST:461] Checking in KafkaTopic CR that topic another-topic exists
2022-03-28 11:03:44 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace topic-st delete kafkatopic topic-with-replication-to-change
2022-03-28 11:03:44 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 11:03:44 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace topic-st delete kafkatopic topic-with-replication-to-change
2022-03-28 11:03:44 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 11:03:44 [ForkJoinPool-1-worker-1] INFO  [KafkaTopicUtils:104] Waiting for KafkaTopic topic-with-replication-to-change deletion
2022-03-28 11:03:44 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for KafkaTopic deletion topic-with-replication-to-change
2022-03-28 11:03:44 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace topic-st delete kafkatopic another-topic
2022-03-28 11:03:44 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:369] ############################################################################
2022-03-28 11:03:44 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Namespace user-st removal
2022-03-28 11:03:44 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 11:03:44 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic creation my-topic-1960957209-999941425 not ready, will try again in 1000 ms (164902ms till timeout)
2022-03-28 11:03:44 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 11:03:44 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:03:44 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace cruise-control-configuration-st removal not ready, will try again in 1000 ms (469752ms till timeout)
2022-03-28 11:03:45 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-scraper-79687dcf5d-xqgkd not ready: my-cluster-66f23d43-scraper)
2022-03-28 11:03:45 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-66f23d43-scraper-79687dcf5d-xqgkd are ready
2022-03-28 11:03:45 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-66f23d43-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594630ms till timeout)
2022-03-28 11:03:45 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 11:03:45 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 11:03:45 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (479658ms till timeout)
2022-03-28 11:03:45 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (156477ms till timeout)
2022-03-28 11:03:45 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace topic-st delete kafkatopic another-topic
2022-03-28 11:03:45 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 11:03:45 [ForkJoinPool-1-worker-1] INFO  [KafkaTopicUtils:104] Waiting for KafkaTopic another-topic deletion
2022-03-28 11:03:45 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for KafkaTopic deletion another-topic
2022-03-28 11:03:45 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:674] ============================================================================
2022-03-28 11:03:45 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:675] [operators.topic.TopicST - After Each] - Clean up after test
2022-03-28 11:03:45 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:344] ############################################################################
2022-03-28 11:03:45 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:348] Delete all resources for testCreateTopicAfterUnsupportedOperation
2022-03-28 11:03:45 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:241] Delete of KafkaTopic another-topic in namespace topic-st
2022-03-28 11:03:45 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:another-topic
2022-03-28 11:03:45 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:241] Delete of KafkaTopic topic-with-replication-to-change in namespace topic-st
2022-03-28 11:03:45 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:topic-with-replication-to-change
2022-03-28 11:03:45 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:369] ############################################################################
2022-03-28 11:03:45 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:267] testCreateTopicAfterUnsupportedOperation - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTls, testSendSimpleMessageTlsScramSha, testConfigurationFileIsCreated, testReceiveSimpleMessageTls, testDeployAndUnDeployCruiseControl, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testConfigurationPerformanceOptions] to and randomly select one to start execution
2022-03-28 11:03:45 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:93] [operators.topic.TopicST] - Removing parallel test: testCreateTopicAfterUnsupportedOperation
2022-03-28 11:03:45 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:97] [operators.topic.TopicST] - Parallel test count: 4
2022-03-28 11:03:45 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.topic.TopicST.testCreateTopicAfterUnsupportedOperation-FINISHED
2022-03-28 11:03:45 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:30] ############################################################################
2022-03-28 11:03:45 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 11:03:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic creation my-topic-1960957209-999941425 not ready, will try again in 1000 ms (163862ms till timeout)
2022-03-28 11:03:46 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-scraper-79687dcf5d-xqgkd not ready: my-cluster-66f23d43-scraper)
2022-03-28 11:03:46 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-66f23d43-scraper-79687dcf5d-xqgkd are ready
2022-03-28 11:03:46 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-66f23d43-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593556ms till timeout)
2022-03-28 11:03:46 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:40] Job create-admin-my-cluster-8f262de5-kafka-clients was deleted
2022-03-28 11:03:46 [ForkJoinPool-1-worker-9] INFO  [ThrottlingQuotaST:112] Executing 5/5 iteration.
2022-03-28 11:03:46 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st
2022-03-28 11:03:46 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 11:03:46 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:03:46 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace cruise-control-configuration-st removal not ready, will try again in 1000 ms (468492ms till timeout)
2022-03-28 11:03:46 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 11:03:46 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:create-admin-my-cluster-8f262de5-kafka-clients
2022-03-28 11:03:46 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (155399ms till timeout)
2022-03-28 11:03:46 [ForkJoinPool-1-worker-9] INFO  [JobUtils:81] Waiting for job: create-admin-my-cluster-8f262de5-kafka-clients will be in active state
2022-03-28 11:03:46 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 11:03:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job active not ready, will try again in 1000 ms (179951ms till timeout)
2022-03-28 11:03:46 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 11:03:46 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 11:03:46 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (478366ms till timeout)
2022-03-28 11:03:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic creation my-topic-1960957209-999941425 not ready, will try again in 1000 ms (162763ms till timeout)
2022-03-28 11:03:47 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-scraper-79687dcf5d-xqgkd not ready: my-cluster-66f23d43-scraper)
2022-03-28 11:03:47 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-66f23d43-scraper-79687dcf5d-xqgkd are ready
2022-03-28 11:03:47 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-66f23d43-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592509ms till timeout)
2022-03-28 11:03:47 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 11:03:47 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (154292ms till timeout)
2022-03-28 11:03:47 [ForkJoinPool-1-worker-9] INFO  [ClientUtils:76] Waiting for producer/consumer:create-admin-my-cluster-8f262de5-kafka-clients to finished
2022-03-28 11:03:47 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 11:03:47 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 11:03:47 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:03:47 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace cruise-control-configuration-st removal not ready, will try again in 1000 ms (467237ms till timeout)
2022-03-28 11:03:47 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 11:03:47 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:03:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (219903ms till timeout)
2022-03-28 11:03:47 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 11:03:47 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 11:03:47 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (477039ms till timeout)
2022-03-28 11:03:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic creation my-topic-1960957209-999941425 not ready, will try again in 1000 ms (161698ms till timeout)
2022-03-28 11:03:48 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-scraper-79687dcf5d-xqgkd not ready: my-cluster-66f23d43-scraper)
2022-03-28 11:03:48 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-66f23d43-scraper-79687dcf5d-xqgkd are ready
2022-03-28 11:03:48 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-66f23d43-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591444ms till timeout)
2022-03-28 11:03:48 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 11:03:48 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (153167ms till timeout)
2022-03-28 11:03:48 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:03:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (218709ms till timeout)
2022-03-28 11:03:48 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 11:03:48 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 11:03:48 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:03:48 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace cruise-control-configuration-st removal not ready, will try again in 1000 ms (465863ms till timeout)
2022-03-28 11:03:49 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 11:03:49 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 11:03:49 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (475658ms till timeout)
2022-03-28 11:03:49 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic creation my-topic-1960957209-999941425 not ready, will try again in 1000 ms (160589ms till timeout)
2022-03-28 11:03:49 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-scraper-79687dcf5d-xqgkd not ready: my-cluster-66f23d43-scraper)
2022-03-28 11:03:49 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-66f23d43-scraper-79687dcf5d-xqgkd are ready
2022-03-28 11:03:49 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-66f23d43-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590344ms till timeout)
2022-03-28 11:03:49 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (152128ms till timeout)
2022-03-28 11:03:49 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:03:49 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 11:03:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (217594ms till timeout)
2022-03-28 11:03:50 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 11:03:50 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 1
2022-03-28 11:03:50 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 11:03:50 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Error from server (NotFound): namespaces "cruise-control-configuration-st" not found
2022-03-28 11:03:50 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] ======STDERR END======
2022-03-28 11:03:50 [ForkJoinPool-1-worker-3] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[namespace-8], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[infra-namespace, reconciliation-st], io.strimzi.test.logs.CollectorElement@f4405b3b=[], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 11:03:50 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:254] CruiseControlConfigurationST - Notifies waiting test suites:[UserST, HttpBridgeTlsST, TopicST, CruiseControlConfigurationST, HttpBridgeScramShaST, ThrottlingQuotaST] to and randomly select one to start execution
2022-03-28 11:03:50 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:85] [cruisecontrol.CruiseControlConfigurationST] - Removing parallel suite: CruiseControlConfigurationST
2022-03-28 11:03:50 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:89] [cruisecontrol.CruiseControlConfigurationST] - Parallel suites count: 4
[ERROR] Tests run: 20, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 1,004.653 s <<< FAILURE! - in io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST
[ERROR] io.strimzi.systemtest.operators.ReconciliationST.testPauseReconciliationInKafkaRebalanceAndTopic(ExtensionContext)  Time elapsed: 244.383 s  <<< ERROR!
io.strimzi.test.k8s.exceptions.KubeClusterException$NotFound: 
`oc --namespace namespace-7 exec my-cluster-da91577a-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-1674392683-1637010870 --describe --bootstrap-server my-cluster-da91577a-kafka-bootstrap:9092` got status code 1 and stderr:
------
Error from server (NotFound): pods "my-cluster-da91577a-kafka-0" not found

------
and stdout:
------

------
	at io.strimzi.test.executor.Exec.exec(Exec.java:209)
	at io.strimzi.test.executor.Exec.exec(Exec.java:149)
	at io.strimzi.test.k8s.cmdClient.BaseCmdKubeClient.execInPod(BaseCmdKubeClient.java:286)
	at io.strimzi.test.k8s.cmdClient.BaseCmdKubeClient.execInPod(BaseCmdKubeClient.java:279)
	at io.strimzi.systemtest.utils.kafkaUtils.KafkaTopicUtils.describeTopicViaKafkaPod(KafkaTopicUtils.java:179)
	at io.strimzi.systemtest.utils.kafkaUtils.KafkaTopicUtils.waitForKafkaTopicSpecStability(KafkaTopicUtils.java:191)
	at io.strimzi.systemtest.operators.ReconciliationST.testPauseReconciliationInKafkaRebalanceAndTopic(ReconciliationST.java:154)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinTask.doJoin(ForkJoinTask.java:396)
	at java.base/java.util.concurrent.ForkJoinTask.join(ForkJoinTask.java:721)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.joinConcurrentTasksInReverseOrderToEnableWorkStealing(ForkJoinPoolHierarchicalTestExecutorService.java:162)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:136)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)

2022-03-28 11:03:50 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 11:03:50 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic creation my-topic-1960957209-999941425 not ready, will try again in 1000 ms (159535ms till timeout)
2022-03-28 11:03:50 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-scraper-79687dcf5d-xqgkd not ready: my-cluster-66f23d43-scraper)
2022-03-28 11:03:50 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-66f23d43-scraper-79687dcf5d-xqgkd are ready
2022-03-28 11:03:50 [ForkJoinPool-1-worker-15] INFO  [DeploymentUtils:197] Deployment my-cluster-66f23d43-scraper is ready
2022-03-28 11:03:50 [ForkJoinPool-1-worker-15] INFO  [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-66f23d43-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-03-28 11:03:50 [ForkJoinPool-1-worker-15] DEBUG [NetworkPolicyResource:227] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=my-cluster-66f23d43-allow, namespace=namespace-8, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[NetworkPolicyIngressRule(from=[NetworkPolicyPeer(ipBlock=null, namespaceSelector=null, podSelector=LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}), additionalProperties={})], ports=[NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8083, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=9404, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8080, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=9999, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={})], additionalProperties={})], podSelector=LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-66f23d43-connect}, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 11:03:50 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:155] Create/Update NetworkPolicy my-cluster-66f23d43-allow in namespace namespace-8
2022-03-28 11:03:50 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:164] Using Namespace: namespace-8
2022-03-28 11:03:50 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource NetworkPolicy:my-cluster-66f23d43-allow
2022-03-28 11:03:50 [ForkJoinPool-1-worker-15] INFO  [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-03-28 11:03:50 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:155] Create/Update KafkaConnect my-cluster-66f23d43 in namespace namespace-9
2022-03-28 11:03:50 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:164] Using Namespace: namespace-8
2022-03-28 11:03:50 [ForkJoinPool-1-worker-15] WARN  [VersionUsageUtils:60] The client is using resource type 'kafkaconnects' with unstable version 'v1beta2'
2022-03-28 11:03:50 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 11:03:50 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 11:03:50 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (474359ms till timeout)
2022-03-28 11:03:50 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:433] Wait for KafkaConnect: my-cluster-66f23d43 will have desired state: ReconciliationPaused
2022-03-28 11:03:50 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for KafkaConnect: my-cluster-66f23d43 will have desired state: ReconciliationPaused
2022-03-28 11:03:50 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaConnect: my-cluster-66f23d43 will have desired state: ReconciliationPaused not ready, will try again in 1000 ms (599937ms till timeout)
2022-03-28 11:03:50 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (151055ms till timeout)
2022-03-28 11:03:50 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:03:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (216537ms till timeout)
2022-03-28 11:03:51 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic creation my-topic-1960957209-999941425 not ready, will try again in 1000 ms (158388ms till timeout)
2022-03-28 11:03:51 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 11:03:51 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:444] KafkaConnect: my-cluster-66f23d43 is in desired state: ReconciliationPaused
2022-03-28 11:03:51 [ForkJoinPool-1-worker-15] INFO  [PodUtils:209] Wait until Pod my-cluster-66f23d43-connect will have stable 0 replicas
2022-03-28 11:03:51 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for  Podmy-cluster-66f23d43-connect will have 0 replicas
2022-03-28 11:03:51 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 11:03:51 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 11:03:51 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (473061ms till timeout)
2022-03-28 11:03:51 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 19 polls
2022-03-28 11:03:51 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-66f23d43-connect will have 0 replicas not ready, will try again in 1000 ms (179888ms till timeout)
2022-03-28 11:03:51 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (149901ms till timeout)
2022-03-28 11:03:51 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:03:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (215496ms till timeout)
2022-03-28 11:03:52 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic creation my-topic-1960957209-999941425 not ready, will try again in 1000 ms (157346ms till timeout)
2022-03-28 11:03:52 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 11:03:52 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 18 polls
2022-03-28 11:03:52 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-66f23d43-connect will have 0 replicas not ready, will try again in 1000 ms (178778ms till timeout)
2022-03-28 11:03:53 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (148751ms till timeout)
2022-03-28 11:03:53 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:03:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (214430ms till timeout)
2022-03-28 11:03:53 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 11:03:53 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 11:03:53 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (471773ms till timeout)
2022-03-28 11:03:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic creation my-topic-1960957209-999941425 not ready, will try again in 1000 ms (156317ms till timeout)
2022-03-28 11:03:54 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 17 polls
2022-03-28 11:03:54 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-66f23d43-connect will have 0 replicas not ready, will try again in 1000 ms (177711ms till timeout)
2022-03-28 11:03:54 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (147691ms till timeout)
2022-03-28 11:03:54 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 11:03:54 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:03:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (213316ms till timeout)
2022-03-28 11:03:54 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 11:03:54 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 11:03:54 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (470512ms till timeout)
2022-03-28 11:03:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic creation my-topic-1960957209-999941425 not ready, will try again in 1000 ms (155266ms till timeout)
2022-03-28 11:03:55 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 16 polls
2022-03-28 11:03:55 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-66f23d43-connect will have 0 replicas not ready, will try again in 1000 ms (176652ms till timeout)
2022-03-28 11:03:55 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (146650ms till timeout)
2022-03-28 11:03:55 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:03:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (212233ms till timeout)
2022-03-28 11:03:55 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 11:03:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic creation my-topic-1960957209-999941425 not ready, will try again in 1000 ms (154227ms till timeout)
2022-03-28 11:03:55 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 11:03:55 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 11:03:55 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (469219ms till timeout)
2022-03-28 11:03:56 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 15 polls
2022-03-28 11:03:56 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-66f23d43-connect will have 0 replicas not ready, will try again in 1000 ms (175533ms till timeout)
2022-03-28 11:03:56 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (145539ms till timeout)
2022-03-28 11:03:56 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:03:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (211153ms till timeout)
2022-03-28 11:03:56 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 11:03:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic creation my-topic-1960957209-999941425 not ready, will try again in 1000 ms (153159ms till timeout)
2022-03-28 11:03:57 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 11:03:57 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 11:03:57 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (467895ms till timeout)
2022-03-28 11:03:57 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (144393ms till timeout)
2022-03-28 11:03:57 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:03:57 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 14 polls
2022-03-28 11:03:57 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-66f23d43-connect will have 0 replicas not ready, will try again in 1000 ms (174364ms till timeout)
2022-03-28 11:03:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (210069ms till timeout)
2022-03-28 11:03:57 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic creation my-topic-1960957209-999941425 not ready, will try again in 1000 ms (152101ms till timeout)
2022-03-28 11:03:58 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 11:03:58 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 11:03:58 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 11:03:58 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (466680ms till timeout)
2022-03-28 11:03:58 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (143285ms till timeout)
2022-03-28 11:03:58 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:03:58 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 13 polls
2022-03-28 11:03:58 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-66f23d43-connect will have 0 replicas not ready, will try again in 1000 ms (173256ms till timeout)
2022-03-28 11:03:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (208961ms till timeout)
2022-03-28 11:03:58 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic creation my-topic-1960957209-999941425 not ready, will try again in 1000 ms (151064ms till timeout)
2022-03-28 11:03:59 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 11:03:59 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 11:03:59 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 11:03:59 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (465416ms till timeout)
2022-03-28 11:03:59 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:03:59 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (142175ms till timeout)
2022-03-28 11:03:59 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 12 polls
2022-03-28 11:03:59 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-66f23d43-connect will have 0 replicas not ready, will try again in 1000 ms (172135ms till timeout)
2022-03-28 11:03:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (207867ms till timeout)
2022-03-28 11:03:59 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic creation my-topic-1960957209-999941425 not ready, will try again in 1000 ms (150022ms till timeout)
2022-03-28 11:04:00 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 11:04:00 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (141118ms till timeout)
2022-03-28 11:04:00 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:00 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 11:04:00 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 11:04:00 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (464215ms till timeout)
2022-03-28 11:04:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (206778ms till timeout)
2022-03-28 11:04:00 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 11 polls
2022-03-28 11:04:00 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-66f23d43-connect will have 0 replicas not ready, will try again in 1000 ms (171028ms till timeout)
2022-03-28 11:04:00 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic creation my-topic-1960957209-999941425 not ready, will try again in 1000 ms (148977ms till timeout)
2022-03-28 11:04:01 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 11:04:01 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (140051ms till timeout)
2022-03-28 11:04:01 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:01 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 10 polls
2022-03-28 11:04:01 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-66f23d43-connect will have 0 replicas not ready, will try again in 1000 ms (169950ms till timeout)
2022-03-28 11:04:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (205683ms till timeout)
2022-03-28 11:04:01 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic creation my-topic-1960957209-999941425 not ready, will try again in 1000 ms (147922ms till timeout)
2022-03-28 11:04:01 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 11:04:01 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 11:04:01 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (462958ms till timeout)
2022-03-28 11:04:02 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (138910ms till timeout)
2022-03-28 11:04:02 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:02 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 9 polls
2022-03-28 11:04:02 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-66f23d43-connect will have 0 replicas not ready, will try again in 1000 ms (168821ms till timeout)
2022-03-28 11:04:02 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 11:04:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (204511ms till timeout)
2022-03-28 11:04:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic creation my-topic-1960957209-999941425 not ready, will try again in 1000 ms (146862ms till timeout)
2022-03-28 11:04:03 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-9 get Namespace user-st -o yaml
2022-03-28 11:04:03 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 1
2022-03-28 11:04:03 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 11:04:03 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Error from server (NotFound): namespaces "user-st" not found
2022-03-28 11:04:03 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] ======STDERR END======
2022-03-28 11:04:03 [ForkJoinPool-1-worker-13] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[namespace-8], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[infra-namespace, reconciliation-st], io.strimzi.test.logs.CollectorElement@f4405b3b=[], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 11:04:03 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:254] UserST - Notifies waiting test suites:[UserST, HttpBridgeTlsST, TopicST, CruiseControlConfigurationST, HttpBridgeScramShaST, ThrottlingQuotaST] to and randomly select one to start execution
2022-03-28 11:04:03 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:85] [operators.user.UserST] - Removing parallel suite: UserST
2022-03-28 11:04:03 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:89] [operators.user.UserST] - Parallel suites count: 3
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1,017.756 s - in io.strimzi.systemtest.operators.user.UserST
2022-03-28 11:04:03 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (137864ms till timeout)
2022-03-28 11:04:03 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 8 polls
2022-03-28 11:04:03 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-66f23d43-connect will have 0 replicas not ready, will try again in 1000 ms (167800ms till timeout)
2022-03-28 11:04:03 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic creation my-topic-1960957209-999941425 not ready, will try again in 1000 ms (145851ms till timeout)
2022-03-28 11:04:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (203478ms till timeout)
2022-03-28 11:04:04 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (136828ms till timeout)
2022-03-28 11:04:04 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 7 polls
2022-03-28 11:04:04 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-66f23d43-connect will have 0 replicas not ready, will try again in 1000 ms (166767ms till timeout)
2022-03-28 11:04:05 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic creation my-topic-1960957209-999941425 not ready, will try again in 1000 ms (144799ms till timeout)
2022-03-28 11:04:05 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (202391ms till timeout)
2022-03-28 11:04:05 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (135792ms till timeout)
2022-03-28 11:04:06 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 6 polls
2022-03-28 11:04:06 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-66f23d43-connect will have 0 replicas not ready, will try again in 1000 ms (165715ms till timeout)
2022-03-28 11:04:06 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic creation my-topic-1960957209-999941425 not ready, will try again in 1000 ms (143777ms till timeout)
2022-03-28 11:04:06 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (201346ms till timeout)
2022-03-28 11:04:07 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (134727ms till timeout)
2022-03-28 11:04:07 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 5 polls
2022-03-28 11:04:07 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-66f23d43-connect will have 0 replicas not ready, will try again in 1000 ms (164654ms till timeout)
2022-03-28 11:04:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic creation my-topic-1960957209-999941425 not ready, will try again in 1000 ms (142754ms till timeout)
2022-03-28 11:04:07 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (200273ms till timeout)
2022-03-28 11:04:08 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 4 polls
2022-03-28 11:04:08 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-66f23d43-connect will have 0 replicas not ready, will try again in 1000 ms (163583ms till timeout)
2022-03-28 11:04:08 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic creation my-topic-1960957209-999941425 not ready, will try again in 1000 ms (141689ms till timeout)
2022-03-28 11:04:08 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (133605ms till timeout)
2022-03-28 11:04:08 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (199173ms till timeout)
2022-03-28 11:04:09 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 3 polls
2022-03-28 11:04:09 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-66f23d43-connect will have 0 replicas not ready, will try again in 1000 ms (162551ms till timeout)
2022-03-28 11:04:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic creation my-topic-1960957209-999941425 not ready, will try again in 1000 ms (140657ms till timeout)
2022-03-28 11:04:09 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (132569ms till timeout)
2022-03-28 11:04:09 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (198122ms till timeout)
2022-03-28 11:04:10 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 2 polls
2022-03-28 11:04:10 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-66f23d43-connect will have 0 replicas not ready, will try again in 1000 ms (161497ms till timeout)
2022-03-28 11:04:10 [ForkJoinPool-1-worker-7] INFO  [TopicST:482] Checking in KafkaTopic CR that topic my-topic-1960957209-999941425 was created with expected settings
2022-03-28 11:04:10 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 11:04:10 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (131500ms till timeout)
2022-03-28 11:04:10 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (197092ms till timeout)
2022-03-28 11:04:11 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 1 polls
2022-03-28 11:04:11 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-66f23d43-connect will have 0 replicas not ready, will try again in 1000 ms (160409ms till timeout)
2022-03-28 11:04:11 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (130400ms till timeout)
2022-03-28 11:04:11 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (195873ms till timeout)
2022-03-28 11:04:12 [ForkJoinPool-1-worker-15] INFO  [PodUtils:217] Pod replicas are stable for 20 polls intervals
2022-03-28 11:04:12 [ForkJoinPool-1-worker-15] INFO  [PodUtils:228] Pod my-cluster-66f23d43-connect has 0 replicas
2022-03-28 11:04:12 [ForkJoinPool-1-worker-15] INFO  [ReconciliationST:108] Setting annotation to "false" and creating KafkaConnector
2022-03-28 11:04:12 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (129360ms till timeout)
2022-03-28 11:04:12 [ForkJoinPool-1-worker-15] INFO  [DeploymentUtils:161] Wait for Deployment: my-cluster-66f23d43-connect will be ready
2022-03-28 11:04:12 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Wait for Deployment: my-cluster-66f23d43-connect will be ready
2022-03-28 11:04:12 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (479961ms till timeout)
2022-03-28 11:04:12 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (194796ms till timeout)
2022-03-28 11:04:13 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (478863ms till timeout)
2022-03-28 11:04:13 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (128216ms till timeout)
2022-03-28 11:04:13 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (193703ms till timeout)
2022-03-28 11:04:14 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Command: oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 11:04:14 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Return code: 0
2022-03-28 11:04:14 [ForkJoinPool-1-worker-7] INFO  [TopicST:121] Editing topic via Kafka, settings to partitions 5
2022-03-28 11:04:14 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --bootstrap-server localhost:9092 --alter --topic my-topic-1960957209-999941425 --partitions 5
2022-03-28 11:04:14 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (477791ms till timeout)
2022-03-28 11:04:14 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (127069ms till timeout)
2022-03-28 11:04:14 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (192645ms till timeout)
2022-03-28 11:04:15 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (476735ms till timeout)
2022-03-28 11:04:15 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (125947ms till timeout)
2022-03-28 11:04:15 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (191518ms till timeout)
2022-03-28 11:04:16 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (475671ms till timeout)
2022-03-28 11:04:16 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (124901ms till timeout)
2022-03-28 11:04:17 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (190410ms till timeout)
2022-03-28 11:04:17 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (474647ms till timeout)
2022-03-28 11:04:17 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (123844ms till timeout)
2022-03-28 11:04:18 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (189318ms till timeout)
2022-03-28 11:04:18 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (473636ms till timeout)
2022-03-28 11:04:18 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (122802ms till timeout)
2022-03-28 11:04:19 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (188202ms till timeout)
2022-03-28 11:04:19 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Command: oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --bootstrap-server localhost:9092 --alter --topic my-topic-1960957209-999941425 --partitions 5
2022-03-28 11:04:19 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Return code: 0
2022-03-28 11:04:19 [ForkJoinPool-1-worker-7] DEBUG [TopicST:124] Topic my-topic-1960957209-999941425 updated from 3 to 5 partitions
2022-03-28 11:04:19 [ForkJoinPool-1-worker-7] INFO  [KafkaTopicUtils:124] Waiting for KafkaTopic change my-topic-1960957209-999941425
2022-03-28 11:04:19 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for KafkaTopic change my-topic-1960957209-999941425
2022-03-28 11:04:19 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Describing topic my-topic-1960957209-999941425 using pod CLI
2022-03-28 11:04:19 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic my-topic-1960957209-999941425
2022-03-28 11:04:19 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (472603ms till timeout)
2022-03-28 11:04:20 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (121764ms till timeout)
2022-03-28 11:04:20 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (187147ms till timeout)
2022-03-28 11:04:20 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (471570ms till timeout)
2022-03-28 11:04:21 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (120707ms till timeout)
2022-03-28 11:04:21 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (186072ms till timeout)
2022-03-28 11:04:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (470551ms till timeout)
2022-03-28 11:04:22 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (119674ms till timeout)
2022-03-28 11:04:22 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (185013ms till timeout)
2022-03-28 11:04:22 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (469477ms till timeout)
2022-03-28 11:04:23 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (118600ms till timeout)
2022-03-28 11:04:23 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (183954ms till timeout)
2022-03-28 11:04:23 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (468440ms till timeout)
2022-03-28 11:04:24 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Command: oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic my-topic-1960957209-999941425
2022-03-28 11:04:24 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Return code: 0
2022-03-28 11:04:24 [ForkJoinPool-1-worker-7] INFO  [TopicST:470] Checking topic my-topic-1960957209-999941425 in Kafka topic-cluster-name
2022-03-28 11:04:24 [ForkJoinPool-1-worker-7] DEBUG [TopicST:471] Topic my-topic-1960957209-999941425 info: [Topic:my-topic-1960957209-999941425, TopicId:MKk_3KBWRpOaHORbQlGDOQ, PartitionCount:5, ReplicationFactor:3, Configs:min.insync.replicas=2,message.format.version=3.0-IV1, Topic:my-topic-1960957209-999941425, Partition:0, Leader:2, Replicas:2,1,0, Isr:2,1,0, Topic:my-topic-1960957209-999941425, Partition:1, Leader:1, Replicas:1,0,2, Isr:1,0,2, Topic:my-topic-1960957209-999941425, Partition:2, Leader:0, Replicas:0,2,1, Isr:0,2,1, Topic:my-topic-1960957209-999941425, Partition:3, Leader:2, Replicas:2,1,0, Isr:2,1,0, Topic:my-topic-1960957209-999941425, Partition:4, Leader:0, Replicas:0,2,1, Isr:0,2,1]
2022-03-28 11:04:24 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:674] ============================================================================
2022-03-28 11:04:24 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:675] [operators.topic.TopicST - After Each] - Clean up after test
2022-03-28 11:04:24 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:344] ############################################################################
2022-03-28 11:04:24 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:346] In context testCreateTopicViaKafka is everything deleted.
2022-03-28 11:04:24 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:369] ############################################################################
2022-03-28 11:04:24 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:267] testCreateTopicViaKafka - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTls, testSendSimpleMessageTlsScramSha, testConfigurationFileIsCreated, testReceiveSimpleMessageTls, testDeployAndUnDeployCruiseControl, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testConfigurationPerformanceOptions] to and randomly select one to start execution
2022-03-28 11:04:24 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:93] [operators.topic.TopicST] - Removing parallel test: testCreateTopicViaKafka
2022-03-28 11:04:24 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:97] [operators.topic.TopicST] - Parallel test count: 3
2022-03-28 11:04:24 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.topic.TopicST.testCreateTopicViaKafka-FINISHED
2022-03-28 11:04:24 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:30] ############################################################################
2022-03-28 11:04:24 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (117505ms till timeout)
2022-03-28 11:04:24 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (182897ms till timeout)
2022-03-28 11:04:24 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (467413ms till timeout)
2022-03-28 11:04:25 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (116452ms till timeout)
2022-03-28 11:04:25 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (181745ms till timeout)
2022-03-28 11:04:26 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (466391ms till timeout)
2022-03-28 11:04:26 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (115360ms till timeout)
2022-03-28 11:04:26 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (180661ms till timeout)
2022-03-28 11:04:27 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (465362ms till timeout)
2022-03-28 11:04:27 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (114299ms till timeout)
2022-03-28 11:04:27 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (179594ms till timeout)
2022-03-28 11:04:28 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (464347ms till timeout)
2022-03-28 11:04:28 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (113168ms till timeout)
2022-03-28 11:04:28 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (178557ms till timeout)
2022-03-28 11:04:29 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (463317ms till timeout)
2022-03-28 11:04:29 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (112124ms till timeout)
2022-03-28 11:04:29 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (177486ms till timeout)
2022-03-28 11:04:30 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (462285ms till timeout)
2022-03-28 11:04:30 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (111069ms till timeout)
2022-03-28 11:04:31 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (176397ms till timeout)
2022-03-28 11:04:31 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (461262ms till timeout)
2022-03-28 11:04:31 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (110023ms till timeout)
2022-03-28 11:04:32 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (175258ms till timeout)
2022-03-28 11:04:32 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (460173ms till timeout)
2022-03-28 11:04:32 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (108974ms till timeout)
2022-03-28 11:04:33 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:33 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (459112ms till timeout)
2022-03-28 11:04:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (174184ms till timeout)
2022-03-28 11:04:33 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (107950ms till timeout)
2022-03-28 11:04:34 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (458089ms till timeout)
2022-03-28 11:04:34 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (173124ms till timeout)
2022-03-28 11:04:34 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (106895ms till timeout)
2022-03-28 11:04:35 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (457052ms till timeout)
2022-03-28 11:04:35 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (172069ms till timeout)
2022-03-28 11:04:35 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (105824ms till timeout)
2022-03-28 11:04:36 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (455993ms till timeout)
2022-03-28 11:04:36 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (170992ms till timeout)
2022-03-28 11:04:37 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (104710ms till timeout)
2022-03-28 11:04:37 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (454965ms till timeout)
2022-03-28 11:04:37 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (169935ms till timeout)
2022-03-28 11:04:38 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (103649ms till timeout)
2022-03-28 11:04:38 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (453923ms till timeout)
2022-03-28 11:04:38 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (168823ms till timeout)
2022-03-28 11:04:39 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (102592ms till timeout)
2022-03-28 11:04:39 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (452888ms till timeout)
2022-03-28 11:04:39 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (167713ms till timeout)
2022-03-28 11:04:40 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (101517ms till timeout)
2022-03-28 11:04:40 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (451874ms till timeout)
2022-03-28 11:04:40 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (166601ms till timeout)
2022-03-28 11:04:41 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (100428ms till timeout)
2022-03-28 11:04:41 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (450831ms till timeout)
2022-03-28 11:04:41 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (165547ms till timeout)
2022-03-28 11:04:42 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (99384ms till timeout)
2022-03-28 11:04:42 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (449781ms till timeout)
2022-03-28 11:04:42 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (164501ms till timeout)
2022-03-28 11:04:43 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (98263ms till timeout)
2022-03-28 11:04:43 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (448726ms till timeout)
2022-03-28 11:04:44 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (163445ms till timeout)
2022-03-28 11:04:44 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (97190ms till timeout)
2022-03-28 11:04:44 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (447712ms till timeout)
2022-03-28 11:04:45 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (162366ms till timeout)
2022-03-28 11:04:45 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (96153ms till timeout)
2022-03-28 11:04:45 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (446695ms till timeout)
2022-03-28 11:04:46 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (161273ms till timeout)
2022-03-28 11:04:46 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (95090ms till timeout)
2022-03-28 11:04:46 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (445641ms till timeout)
2022-03-28 11:04:47 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (160197ms till timeout)
2022-03-28 11:04:47 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (94058ms till timeout)
2022-03-28 11:04:47 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (444623ms till timeout)
2022-03-28 11:04:48 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (159062ms till timeout)
2022-03-28 11:04:48 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (443590ms till timeout)
2022-03-28 11:04:48 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (92935ms till timeout)
2022-03-28 11:04:49 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (158015ms till timeout)
2022-03-28 11:04:49 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (442558ms till timeout)
2022-03-28 11:04:49 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (91858ms till timeout)
2022-03-28 11:04:50 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (156956ms till timeout)
2022-03-28 11:04:50 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (441525ms till timeout)
2022-03-28 11:04:51 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (90735ms till timeout)
2022-03-28 11:04:51 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (155866ms till timeout)
2022-03-28 11:04:51 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (440419ms till timeout)
2022-03-28 11:04:52 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (89636ms till timeout)
2022-03-28 11:04:52 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (154795ms till timeout)
2022-03-28 11:04:53 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (439387ms till timeout)
2022-03-28 11:04:53 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (88599ms till timeout)
2022-03-28 11:04:53 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (153729ms till timeout)
2022-03-28 11:04:54 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (438298ms till timeout)
2022-03-28 11:04:54 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (87507ms till timeout)
2022-03-28 11:04:54 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (152669ms till timeout)
2022-03-28 11:04:55 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (437210ms till timeout)
2022-03-28 11:04:55 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (86471ms till timeout)
2022-03-28 11:04:55 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (151600ms till timeout)
2022-03-28 11:04:56 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (436151ms till timeout)
2022-03-28 11:04:56 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (85361ms till timeout)
2022-03-28 11:04:56 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (150541ms till timeout)
2022-03-28 11:04:57 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (435105ms till timeout)
2022-03-28 11:04:57 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (84287ms till timeout)
2022-03-28 11:04:58 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (149440ms till timeout)
2022-03-28 11:04:58 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (434076ms till timeout)
2022-03-28 11:04:58 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (83227ms till timeout)
2022-03-28 11:04:59 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:04:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (148206ms till timeout)
2022-03-28 11:04:59 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (433043ms till timeout)
2022-03-28 11:04:59 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (82186ms till timeout)
2022-03-28 11:05:00 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:05:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (147176ms till timeout)
2022-03-28 11:05:00 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (431995ms till timeout)
2022-03-28 11:05:00 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (81067ms till timeout)
2022-03-28 11:05:01 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:05:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (146093ms till timeout)
2022-03-28 11:05:01 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (430978ms till timeout)
2022-03-28 11:05:01 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (80012ms till timeout)
2022-03-28 11:05:02 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:05:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (145026ms till timeout)
2022-03-28 11:05:02 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (429904ms till timeout)
2022-03-28 11:05:02 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (78888ms till timeout)
2022-03-28 11:05:03 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:05:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (143876ms till timeout)
2022-03-28 11:05:03 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (428791ms till timeout)
2022-03-28 11:05:03 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (77801ms till timeout)
2022-03-28 11:05:04 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (427733ms till timeout)
2022-03-28 11:05:04 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:05:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (142770ms till timeout)
2022-03-28 11:05:05 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (76684ms till timeout)
2022-03-28 11:05:05 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (426627ms till timeout)
2022-03-28 11:05:05 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:05:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (141483ms till timeout)
2022-03-28 11:05:06 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (75517ms till timeout)
2022-03-28 11:05:06 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (425575ms till timeout)
2022-03-28 11:05:07 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:05:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (140434ms till timeout)
2022-03-28 11:05:07 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (74461ms till timeout)
2022-03-28 11:05:07 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (424529ms till timeout)
2022-03-28 11:05:08 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:05:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (139352ms till timeout)
2022-03-28 11:05:08 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (73361ms till timeout)
2022-03-28 11:05:08 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (423487ms till timeout)
2022-03-28 11:05:09 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:05:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (138197ms till timeout)
2022-03-28 11:05:09 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (72333ms till timeout)
2022-03-28 11:05:09 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (422431ms till timeout)
2022-03-28 11:05:10 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:05:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (137087ms till timeout)
2022-03-28 11:05:10 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (71310ms till timeout)
2022-03-28 11:05:11 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (421401ms till timeout)
2022-03-28 11:05:11 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:05:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (136033ms till timeout)
2022-03-28 11:05:11 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (70252ms till timeout)
2022-03-28 11:05:12 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (420373ms till timeout)
2022-03-28 11:05:12 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:05:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (134940ms till timeout)
2022-03-28 11:05:12 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (69184ms till timeout)
2022-03-28 11:05:13 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (419349ms till timeout)
2022-03-28 11:05:13 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:05:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (133748ms till timeout)
2022-03-28 11:05:13 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (67988ms till timeout)
2022-03-28 11:05:14 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (418210ms till timeout)
2022-03-28 11:05:14 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:05:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (132713ms till timeout)
2022-03-28 11:05:14 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (66933ms till timeout)
2022-03-28 11:05:15 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (417187ms till timeout)
2022-03-28 11:05:15 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:05:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (131652ms till timeout)
2022-03-28 11:05:15 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (65909ms till timeout)
2022-03-28 11:05:16 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (416121ms till timeout)
2022-03-28 11:05:16 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:05:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (130573ms till timeout)
2022-03-28 11:05:16 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (64829ms till timeout)
2022-03-28 11:05:17 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-66f23d43-connect will be ready not ready, will try again in 1000 ms (415092ms till timeout)
2022-03-28 11:05:17 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:05:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129480ms till timeout)
2022-03-28 11:05:18 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (63748ms till timeout)
2022-03-28 11:05:18 [ForkJoinPool-1-worker-15] INFO  [DeploymentUtils:168] Deployment: my-cluster-66f23d43-connect is ready
2022-03-28 11:05:18 [ForkJoinPool-1-worker-15] INFO  [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-66f23d43-connect to be ready
2022-03-28 11:05:18 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-66f23d43-connect}, additionalProperties={})to be ready
2022-03-28 11:05:18 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-connect-6f8cbd555f-8xfpj not ready: my-cluster-66f23d43-connect)
2022-03-28 11:05:18 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-66f23d43-connect-6f8cbd555f-8xfpj are ready
2022-03-28 11:05:18 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-66f23d43-connect}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599938ms till timeout)
2022-03-28 11:05:19 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:05:19 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (62668ms till timeout)
2022-03-28 11:05:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (128276ms till timeout)
2022-03-28 11:05:19 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-connect-6f8cbd555f-8xfpj not ready: my-cluster-66f23d43-connect)
2022-03-28 11:05:19 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-66f23d43-connect-6f8cbd555f-8xfpj are ready
2022-03-28 11:05:19 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-66f23d43-connect}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598795ms till timeout)
2022-03-28 11:05:20 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (61606ms till timeout)
2022-03-28 11:05:20 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:05:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (127101ms till timeout)
2022-03-28 11:05:20 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-connect-6f8cbd555f-8xfpj not ready: my-cluster-66f23d43-connect)
2022-03-28 11:05:20 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-66f23d43-connect-6f8cbd555f-8xfpj are ready
2022-03-28 11:05:20 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-66f23d43-connect}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597750ms till timeout)
2022-03-28 11:05:21 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (60526ms till timeout)
2022-03-28 11:05:21 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:05:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (126020ms till timeout)
2022-03-28 11:05:21 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-connect-6f8cbd555f-8xfpj not ready: my-cluster-66f23d43-connect)
2022-03-28 11:05:21 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-66f23d43-connect-6f8cbd555f-8xfpj are ready
2022-03-28 11:05:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-66f23d43-connect}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596684ms till timeout)
2022-03-28 11:05:22 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (59382ms till timeout)
2022-03-28 11:05:22 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T11:05:19Z, conditions=[JobCondition(lastProbeTime=2022-03-28T11:05:19Z, lastTransitionTime=2022-03-28T11:05:19Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T11:03:43Z, succeeded=1, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:05:22 [ForkJoinPool-1-worker-9] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 11:05:22 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 11:05:22 [ForkJoinPool-1-worker-9] INFO  [PodUtils:189] Message All topics created found in create-admin-my-cluster-8f262de5-kafka-clients-m6wf9 log
2022-03-28 11:05:22 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment create-admin-my-cluster-8f262de5-kafka-clients deletion
2022-03-28 11:05:22 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for ReplicaSet create-admin-my-cluster-8f262de5-kafka-clients to be deleted
2022-03-28 11:05:22 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-connect-6f8cbd555f-8xfpj not ready: my-cluster-66f23d43-connect)
2022-03-28 11:05:22 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-66f23d43-connect-6f8cbd555f-8xfpj are ready
2022-03-28 11:05:22 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-66f23d43-connect}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595651ms till timeout)
2022-03-28 11:05:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] ReplicaSet create-admin-my-cluster-8f262de5-kafka-clients to be deleted not ready, will try again in 5000 ms (179958ms till timeout)
2022-03-28 11:05:23 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (58281ms till timeout)
2022-03-28 11:05:23 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-connect-6f8cbd555f-8xfpj not ready: my-cluster-66f23d43-connect)
2022-03-28 11:05:23 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-66f23d43-connect-6f8cbd555f-8xfpj are ready
2022-03-28 11:05:23 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-66f23d43-connect}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594623ms till timeout)
2022-03-28 11:05:24 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (57176ms till timeout)
2022-03-28 11:05:24 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-connect-6f8cbd555f-8xfpj not ready: my-cluster-66f23d43-connect)
2022-03-28 11:05:24 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-66f23d43-connect-6f8cbd555f-8xfpj are ready
2022-03-28 11:05:24 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-66f23d43-connect}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593584ms till timeout)
2022-03-28 11:05:25 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (56114ms till timeout)
2022-03-28 11:05:25 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-connect-6f8cbd555f-8xfpj not ready: my-cluster-66f23d43-connect)
2022-03-28 11:05:25 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-66f23d43-connect-6f8cbd555f-8xfpj are ready
2022-03-28 11:05:25 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-66f23d43-connect}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592532ms till timeout)
2022-03-28 11:05:26 [ForkJoinPool-1-worker-17] INFO  [PodUtils:189] Message org.apache.kafka.common.errors.ThrottlingQuotaExceededException: The throttling quota has been exceeded. found in alter-admin-my-cluster-28d9dd2e-kafka-clients-npvgc log
2022-03-28 11:05:26 [ForkJoinPool-1-worker-17] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment alter-admin-my-cluster-28d9dd2e-kafka-clients deletion
2022-03-28 11:05:26 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for ReplicaSet alter-admin-my-cluster-28d9dd2e-kafka-clients to be deleted
2022-03-28 11:05:26 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] ReplicaSet alter-admin-my-cluster-28d9dd2e-kafka-clients to be deleted not ready, will try again in 5000 ms (179986ms till timeout)
2022-03-28 11:05:26 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-connect-6f8cbd555f-8xfpj not ready: my-cluster-66f23d43-connect)
2022-03-28 11:05:26 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-66f23d43-connect-6f8cbd555f-8xfpj are ready
2022-03-28 11:05:26 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-66f23d43-connect}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591481ms till timeout)
2022-03-28 11:05:27 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:40] Job create-admin-my-cluster-8f262de5-kafka-clients was deleted
2022-03-28 11:05:27 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st
2022-03-28 11:05:27 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:delete-admin-my-cluster-8f262de5-kafka-clients
2022-03-28 11:05:27 [ForkJoinPool-1-worker-9] INFO  [JobUtils:81] Waiting for job: delete-admin-my-cluster-8f262de5-kafka-clients will be in active state
2022-03-28 11:05:27 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 11:05:27 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-connect-6f8cbd555f-8xfpj not ready: my-cluster-66f23d43-connect)
2022-03-28 11:05:27 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-66f23d43-connect-6f8cbd555f-8xfpj are ready
2022-03-28 11:05:27 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-66f23d43, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-66f23d43-connect}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590433ms till timeout)
2022-03-28 11:05:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job active not ready, will try again in 1000 ms (179955ms till timeout)
2022-03-28 11:05:28 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-66f23d43-connect-6f8cbd555f-8xfpj not ready: my-cluster-66f23d43-connect)
2022-03-28 11:05:28 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-66f23d43-connect-6f8cbd555f-8xfpj are ready
2022-03-28 11:05:28 [ForkJoinPool-1-worker-15] INFO  [DeploymentUtils:197] Deployment my-cluster-66f23d43-connect is ready
2022-03-28 11:05:29 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:155] Create/Update KafkaConnector my-cluster-66f23d43 in namespace namespace-9
2022-03-28 11:05:29 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:164] Using Namespace: namespace-8
2022-03-28 11:05:29 [ForkJoinPool-1-worker-15] WARN  [VersionUsageUtils:60] The client is using resource type 'kafkaconnectors' with unstable version 'v1beta2'
2022-03-28 11:05:29 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaConnector:my-cluster-66f23d43
2022-03-28 11:05:29 [ForkJoinPool-1-worker-9] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 11:05:29 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 11:05:29 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:433] Wait for KafkaConnector: my-cluster-66f23d43 will have desired state: Ready
2022-03-28 11:05:29 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for KafkaConnector: my-cluster-66f23d43 will have desired state: Ready
2022-03-28 11:05:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (299862ms till timeout)
2022-03-28 11:05:29 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaConnector: my-cluster-66f23d43 will have desired state: Ready not ready, will try again in 1000 ms (239928ms till timeout)
2022-03-28 11:05:30 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:444] KafkaConnector: my-cluster-66f23d43 is in desired state: Ready
2022-03-28 11:05:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (298707ms till timeout)
2022-03-28 11:05:30 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 exec my-cluster-66f23d43-connect-6f8cbd555f-8xfpj -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-66f23d43
2022-03-28 11:05:31 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-8 exec my-cluster-66f23d43-connect-6f8cbd555f-8xfpj -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-66f23d43
2022-03-28 11:05:31 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 11:05:31 [ForkJoinPool-1-worker-15] INFO  [ReconciliationST:118] Adding pause annotation into the KafkaConnector and scaling taskMax to 4
2022-03-28 11:05:31 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:433] Wait for KafkaConnector: my-cluster-66f23d43 will have desired state: ReconciliationPaused
2022-03-28 11:05:31 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for KafkaConnector: my-cluster-66f23d43 will have desired state: ReconciliationPaused
2022-03-28 11:05:31 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:444] KafkaConnector: my-cluster-66f23d43 is in desired state: ReconciliationPaused
2022-03-28 11:05:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (297589ms till timeout)
2022-03-28 11:05:31 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Connector's spec will be stable
2022-03-28 11:05:31 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 exec my-cluster-66f23d43-connect-6f8cbd555f-8xfpj -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-66f23d43
2022-03-28 11:05:31 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] ReplicaSet alter-admin-my-cluster-28d9dd2e-kafka-clients to be deleted not ready, will try again in 5000 ms (174957ms till timeout)
2022-03-28 11:05:32 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-8 exec my-cluster-66f23d43-connect-6f8cbd555f-8xfpj -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-66f23d43
2022-03-28 11:05:32 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 11:05:32 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 19 polls
2022-03-28 11:05:32 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (179366ms till timeout)
2022-03-28 11:05:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (296496ms till timeout)
2022-03-28 11:05:33 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 exec my-cluster-66f23d43-connect-6f8cbd555f-8xfpj -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-66f23d43
2022-03-28 11:05:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (295368ms till timeout)
2022-03-28 11:05:33 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-8 exec my-cluster-66f23d43-connect-6f8cbd555f-8xfpj -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-66f23d43
2022-03-28 11:05:33 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 11:05:33 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 18 polls
2022-03-28 11:05:33 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (177684ms till timeout)
2022-03-28 11:05:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (294275ms till timeout)
2022-03-28 11:05:34 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 exec my-cluster-66f23d43-connect-6f8cbd555f-8xfpj -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-66f23d43
2022-03-28 11:05:35 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-8 exec my-cluster-66f23d43-connect-6f8cbd555f-8xfpj -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-66f23d43
2022-03-28 11:05:35 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 11:05:35 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 17 polls
2022-03-28 11:05:35 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (175953ms till timeout)
2022-03-28 11:05:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (293105ms till timeout)
2022-03-28 11:05:36 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 exec my-cluster-66f23d43-connect-6f8cbd555f-8xfpj -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-66f23d43
2022-03-28 11:05:36 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] ReplicaSet alter-admin-my-cluster-28d9dd2e-kafka-clients to be deleted not ready, will try again in 5000 ms (169865ms till timeout)
2022-03-28 11:05:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (291939ms till timeout)
2022-03-28 11:05:37 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-8 exec my-cluster-66f23d43-connect-6f8cbd555f-8xfpj -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-66f23d43
2022-03-28 11:05:37 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 11:05:37 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 16 polls
2022-03-28 11:05:37 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (174112ms till timeout)
2022-03-28 11:05:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (290854ms till timeout)
2022-03-28 11:05:38 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 exec my-cluster-66f23d43-connect-6f8cbd555f-8xfpj -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-66f23d43
2022-03-28 11:05:39 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-8 exec my-cluster-66f23d43-connect-6f8cbd555f-8xfpj -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-66f23d43
2022-03-28 11:05:39 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 11:05:39 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 15 polls
2022-03-28 11:05:39 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (172283ms till timeout)
2022-03-28 11:05:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (289772ms till timeout)
2022-03-28 11:05:40 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 exec my-cluster-66f23d43-connect-6f8cbd555f-8xfpj -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-66f23d43
2022-03-28 11:05:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (288700ms till timeout)
2022-03-28 11:05:41 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-8 exec my-cluster-66f23d43-connect-6f8cbd555f-8xfpj -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-66f23d43
2022-03-28 11:05:41 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 11:05:41 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 14 polls
2022-03-28 11:05:41 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (170439ms till timeout)
2022-03-28 11:05:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (287665ms till timeout)
2022-03-28 11:05:41 [ForkJoinPool-1-worker-17] DEBUG [JobUtils:40] Job alter-admin-my-cluster-28d9dd2e-kafka-clients was deleted
2022-03-28 11:05:41 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:155] Create/Update Job teardown-delete in namespace throttling-quota-st
2022-03-28 11:05:41 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:teardown-delete
2022-03-28 11:05:42 [ForkJoinPool-1-worker-17] INFO  [JobUtils:81] Waiting for job: teardown-delete will be in active state
2022-03-28 11:05:42 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 11:05:42 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job active not ready, will try again in 1000 ms (179990ms till timeout)
2022-03-28 11:05:42 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 exec my-cluster-66f23d43-connect-6f8cbd555f-8xfpj -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-66f23d43
2022-03-28 11:05:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (286576ms till timeout)
2022-03-28 11:05:43 [ForkJoinPool-1-worker-17] INFO  [ClientUtils:76] Waiting for producer/consumer:teardown-delete to finished
2022-03-28 11:05:43 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 11:05:43 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:05:43 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129926ms till timeout)
2022-03-28 11:05:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (285496ms till timeout)
2022-03-28 11:05:44 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:05:44 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (128855ms till timeout)
2022-03-28 11:05:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (284356ms till timeout)
2022-03-28 11:05:45 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:05:45 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (127822ms till timeout)
2022-03-28 11:05:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (283309ms till timeout)
2022-03-28 11:05:46 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:05:46 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (126735ms till timeout)
2022-03-28 11:05:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (282267ms till timeout)
2022-03-28 11:05:47 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:05:47 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (125612ms till timeout)
2022-03-28 11:05:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (281093ms till timeout)
2022-03-28 11:05:48 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:05:48 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (124484ms till timeout)
2022-03-28 11:05:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (280019ms till timeout)
2022-03-28 11:05:49 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:05:49 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (123319ms till timeout)
2022-03-28 11:05:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (278947ms till timeout)
2022-03-28 11:05:50 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:05:50 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (122077ms till timeout)
2022-03-28 11:05:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (277745ms till timeout)
2022-03-28 11:05:52 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:05:52 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (121019ms till timeout)
2022-03-28 11:05:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (276685ms till timeout)
2022-03-28 11:05:53 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:05:53 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (119955ms till timeout)
2022-03-28 11:05:53 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-8 exec my-cluster-66f23d43-connect-6f8cbd555f-8xfpj -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-66f23d43
2022-03-28 11:05:53 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 11:05:53 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 13 polls
2022-03-28 11:05:53 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (158036ms till timeout)
2022-03-28 11:05:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (275618ms till timeout)
2022-03-28 11:05:54 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:05:54 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (118907ms till timeout)
2022-03-28 11:05:54 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 exec my-cluster-66f23d43-connect-6f8cbd555f-8xfpj -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-66f23d43
2022-03-28 11:05:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (274544ms till timeout)
2022-03-28 11:05:55 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:05:55 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (117828ms till timeout)
2022-03-28 11:05:55 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-8 exec my-cluster-66f23d43-connect-6f8cbd555f-8xfpj -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-66f23d43
2022-03-28 11:05:55 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 11:05:55 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 12 polls
2022-03-28 11:05:55 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (156174ms till timeout)
2022-03-28 11:05:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (273428ms till timeout)
2022-03-28 11:05:56 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 exec my-cluster-66f23d43-connect-6f8cbd555f-8xfpj -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-66f23d43
2022-03-28 11:05:56 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:05:56 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (116689ms till timeout)
2022-03-28 11:05:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (272329ms till timeout)
2022-03-28 11:05:57 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:05:57 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (115641ms till timeout)
2022-03-28 11:05:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (271223ms till timeout)
2022-03-28 11:05:58 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:05:58 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (114587ms till timeout)
2022-03-28 11:05:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (270135ms till timeout)
2022-03-28 11:05:59 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:05:59 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (113507ms till timeout)
2022-03-28 11:05:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (269079ms till timeout)
2022-03-28 11:06:00 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:06:00 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (112402ms till timeout)
2022-03-28 11:06:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (267952ms till timeout)
2022-03-28 11:06:01 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:06:01 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (111340ms till timeout)
2022-03-28 11:06:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (266871ms till timeout)
2022-03-28 11:06:02 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:06:02 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (110232ms till timeout)
2022-03-28 11:06:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (265744ms till timeout)
2022-03-28 11:06:03 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:06:03 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (109177ms till timeout)
2022-03-28 11:06:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (264658ms till timeout)
2022-03-28 11:06:04 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:06:04 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (108115ms till timeout)
2022-03-28 11:06:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (263624ms till timeout)
2022-03-28 11:06:05 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:06:06 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (107059ms till timeout)
2022-03-28 11:06:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (262552ms till timeout)
2022-03-28 11:06:07 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-8 exec my-cluster-66f23d43-connect-6f8cbd555f-8xfpj -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-66f23d43
2022-03-28 11:06:07 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 11:06:07 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 11 polls
2022-03-28 11:06:07 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (144410ms till timeout)
2022-03-28 11:06:07 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:06:07 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (105986ms till timeout)
2022-03-28 11:06:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (261505ms till timeout)
2022-03-28 11:06:08 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 exec my-cluster-66f23d43-connect-6f8cbd555f-8xfpj -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-66f23d43
2022-03-28 11:06:08 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:06:08 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (104951ms till timeout)
2022-03-28 11:06:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (260459ms till timeout)
2022-03-28 11:06:09 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-8 exec my-cluster-66f23d43-connect-6f8cbd555f-8xfpj -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-66f23d43
2022-03-28 11:06:09 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 11:06:09 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 10 polls
2022-03-28 11:06:09 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (142405ms till timeout)
2022-03-28 11:06:09 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:06:09 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (103913ms till timeout)
2022-03-28 11:06:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (259390ms till timeout)
2022-03-28 11:06:10 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 exec my-cluster-66f23d43-connect-6f8cbd555f-8xfpj -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-66f23d43
2022-03-28 11:06:10 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:06:10 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (102889ms till timeout)
2022-03-28 11:06:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (258343ms till timeout)
2022-03-28 11:06:11 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:06:11 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (101824ms till timeout)
2022-03-28 11:06:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (257284ms till timeout)
2022-03-28 11:06:12 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:06:12 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (100744ms till timeout)
2022-03-28 11:06:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (256183ms till timeout)
2022-03-28 11:06:13 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:06:13 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (99674ms till timeout)
2022-03-28 11:06:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (255101ms till timeout)
2022-03-28 11:06:14 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:06:14 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (98627ms till timeout)
2022-03-28 11:06:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (253969ms till timeout)
2022-03-28 11:06:15 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:06:15 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (97533ms till timeout)
2022-03-28 11:06:15 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-8 exec my-cluster-66f23d43-connect-6f8cbd555f-8xfpj -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-66f23d43
2022-03-28 11:06:15 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 11:06:15 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 9 polls
2022-03-28 11:06:15 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (135610ms till timeout)
2022-03-28 11:06:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (252872ms till timeout)
2022-03-28 11:06:16 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:06:16 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (96429ms till timeout)
2022-03-28 11:06:16 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 exec my-cluster-66f23d43-connect-6f8cbd555f-8xfpj -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-66f23d43
2022-03-28 11:06:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (251704ms till timeout)
2022-03-28 11:06:17 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:06:17 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (95327ms till timeout)
2022-03-28 11:06:18 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-8 exec my-cluster-66f23d43-connect-6f8cbd555f-8xfpj -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-66f23d43
2022-03-28 11:06:18 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 11:06:18 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 8 polls
2022-03-28 11:06:18 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (133322ms till timeout)
2022-03-28 11:06:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (250605ms till timeout)
2022-03-28 11:06:18 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:06:18 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (94262ms till timeout)
2022-03-28 11:06:19 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 exec my-cluster-66f23d43-connect-6f8cbd555f-8xfpj -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-66f23d43
2022-03-28 11:06:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (249551ms till timeout)
2022-03-28 11:06:19 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:06:19 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (93124ms till timeout)
2022-03-28 11:06:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (248491ms till timeout)
2022-03-28 11:06:20 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:06:21 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (92055ms till timeout)
2022-03-28 11:06:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (247314ms till timeout)
2022-03-28 11:06:22 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:06:22 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (91007ms till timeout)
2022-03-28 11:06:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (246235ms till timeout)
2022-03-28 11:06:23 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:06:23 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (89968ms till timeout)
2022-03-28 11:06:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (245168ms till timeout)
2022-03-28 11:06:24 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:06:24 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (88938ms till timeout)
2022-03-28 11:06:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (244077ms till timeout)
2022-03-28 11:06:25 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:06:25 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (87814ms till timeout)
2022-03-28 11:06:25 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-8 exec my-cluster-66f23d43-connect-6f8cbd555f-8xfpj -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-66f23d43
2022-03-28 11:06:25 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 11:06:25 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 7 polls
2022-03-28 11:06:25 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (125995ms till timeout)
2022-03-28 11:06:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (242960ms till timeout)
2022-03-28 11:06:26 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:06:26 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (86733ms till timeout)
2022-03-28 11:06:26 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 exec my-cluster-66f23d43-connect-6f8cbd555f-8xfpj -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-66f23d43
2022-03-28 11:06:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (241856ms till timeout)
2022-03-28 11:06:27 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:06:27 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (85649ms till timeout)
2022-03-28 11:06:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (240720ms till timeout)
2022-03-28 11:06:28 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:06:28 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (84590ms till timeout)
2022-03-28 11:06:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (239658ms till timeout)
2022-03-28 11:06:29 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:06:29 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (83513ms till timeout)
2022-03-28 11:06:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (238579ms till timeout)
2022-03-28 11:06:30 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:06:30 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (82381ms till timeout)
2022-03-28 11:06:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (237483ms till timeout)
2022-03-28 11:06:31 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:06:31 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (81243ms till timeout)
2022-03-28 11:06:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (236428ms till timeout)
2022-03-28 11:06:32 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:06:32 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (80108ms till timeout)
2022-03-28 11:06:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (235366ms till timeout)
2022-03-28 11:06:33 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:06:34 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (79068ms till timeout)
2022-03-28 11:06:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (234299ms till timeout)
2022-03-28 11:06:35 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:06:35 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (77966ms till timeout)
2022-03-28 11:06:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (233222ms till timeout)
2022-03-28 11:06:36 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:06:36 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (76880ms till timeout)
2022-03-28 11:06:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (232150ms till timeout)
2022-03-28 11:06:37 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-8 exec my-cluster-66f23d43-connect-6f8cbd555f-8xfpj -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-66f23d43
2022-03-28 11:06:37 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 11:06:37 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 6 polls
2022-03-28 11:06:37 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (114260ms till timeout)
2022-03-28 11:06:37 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:06:37 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (75802ms till timeout)
2022-03-28 11:06:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (231092ms till timeout)
2022-03-28 11:06:38 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 exec my-cluster-66f23d43-connect-6f8cbd555f-8xfpj -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-66f23d43
2022-03-28 11:06:38 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:06:38 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (74761ms till timeout)
2022-03-28 11:06:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (230046ms till timeout)
2022-03-28 11:06:39 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:06:39 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (73701ms till timeout)
2022-03-28 11:06:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (228989ms till timeout)
2022-03-28 11:06:40 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:06:40 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (72656ms till timeout)
2022-03-28 11:06:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (227944ms till timeout)
2022-03-28 11:06:41 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:06:41 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (71611ms till timeout)
2022-03-28 11:06:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (226882ms till timeout)
2022-03-28 11:06:42 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:06:42 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (70574ms till timeout)
2022-03-28 11:06:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (225810ms till timeout)
2022-03-28 11:06:43 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:06:43 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (69473ms till timeout)
2022-03-28 11:06:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (224731ms till timeout)
2022-03-28 11:06:44 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-8 exec my-cluster-66f23d43-connect-6f8cbd555f-8xfpj -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-66f23d43
2022-03-28 11:06:44 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 11:06:44 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 5 polls
2022-03-28 11:06:44 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (106940ms till timeout)
2022-03-28 11:06:44 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:06:44 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (68388ms till timeout)
2022-03-28 11:06:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (223677ms till timeout)
2022-03-28 11:06:45 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 exec my-cluster-66f23d43-connect-6f8cbd555f-8xfpj -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-66f23d43
2022-03-28 11:06:45 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:06:45 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (67343ms till timeout)
2022-03-28 11:06:46 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-8 exec my-cluster-66f23d43-connect-6f8cbd555f-8xfpj -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-66f23d43
2022-03-28 11:06:46 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 11:06:46 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 4 polls
2022-03-28 11:06:46 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (105035ms till timeout)
2022-03-28 11:06:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (222428ms till timeout)
2022-03-28 11:06:46 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:06:46 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (66137ms till timeout)
2022-03-28 11:06:47 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 exec my-cluster-66f23d43-connect-6f8cbd555f-8xfpj -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-66f23d43
2022-03-28 11:06:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (221351ms till timeout)
2022-03-28 11:06:48 [ForkJoinPool-1-worker-17] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T11:06:44Z, conditions=[JobCondition(lastProbeTime=2022-03-28T11:06:44Z, lastTransitionTime=2022-03-28T11:06:44Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T11:05:39Z, succeeded=1, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:06:48 [ForkJoinPool-1-worker-17] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment teardown-delete deletion
2022-03-28 11:06:48 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for ReplicaSet teardown-delete to be deleted
2022-03-28 11:06:48 [ForkJoinPool-1-worker-17] TRACE [TestUtils:176] ReplicaSet teardown-delete to be deleted not ready, will try again in 5000 ms (179986ms till timeout)
2022-03-28 11:06:48 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-8 exec my-cluster-66f23d43-connect-6f8cbd555f-8xfpj -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-66f23d43
2022-03-28 11:06:48 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 11:06:48 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 3 polls
2022-03-28 11:06:48 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (103190ms till timeout)
2022-03-28 11:06:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (220283ms till timeout)
2022-03-28 11:06:49 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 exec my-cluster-66f23d43-connect-6f8cbd555f-8xfpj -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-66f23d43
2022-03-28 11:06:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (219193ms till timeout)
2022-03-28 11:06:50 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-8 exec my-cluster-66f23d43-connect-6f8cbd555f-8xfpj -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-66f23d43
2022-03-28 11:06:50 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 11:06:50 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 2 polls
2022-03-28 11:06:50 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (101477ms till timeout)
2022-03-28 11:06:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (218070ms till timeout)
2022-03-28 11:06:51 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 exec my-cluster-66f23d43-connect-6f8cbd555f-8xfpj -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-66f23d43
2022-03-28 11:06:51 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-8 exec my-cluster-66f23d43-connect-6f8cbd555f-8xfpj -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-66f23d43
2022-03-28 11:06:51 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 11:06:51 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 1 polls
2022-03-28 11:06:51 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (99768ms till timeout)
2022-03-28 11:06:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (217042ms till timeout)
2022-03-28 11:06:52 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 exec my-cluster-66f23d43-connect-6f8cbd555f-8xfpj -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-66f23d43
2022-03-28 11:06:53 [ForkJoinPool-1-worker-17] DEBUG [JobUtils:40] Job teardown-delete was deleted
2022-03-28 11:06:53 [ForkJoinPool-1-worker-17] DEBUG [AbstractST:674] ============================================================================
2022-03-28 11:06:53 [ForkJoinPool-1-worker-17] DEBUG [AbstractST:675] [operators.topic.ThrottlingQuotaST - After Each] - Clean up after test
2022-03-28 11:06:53 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:344] ############################################################################
2022-03-28 11:06:53 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:348] Delete all resources for testThrottlingQuotasCreateAlterPartitions
2022-03-28 11:06:53 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:241] Delete of Job create-admin-my-cluster-28d9dd2e-kafka-clients in namespace throttling-quota-st
2022-03-28 11:06:53 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:241] Delete of Job teardown-delete in namespace throttling-quota-st
2022-03-28 11:06:53 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of Job alter-admin-my-cluster-28d9dd2e-kafka-clients in namespace throttling-quota-st
2022-03-28 11:06:53 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of Job create-admin-my-cluster-28d9dd2e-kafka-clients in namespace throttling-quota-st
2022-03-28 11:06:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (215886ms till timeout)
2022-03-28 11:06:53 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:alter-admin-my-cluster-28d9dd2e-kafka-clients
2022-03-28 11:06:53 [ForkJoinPool-1-worker-17] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:create-admin-my-cluster-28d9dd2e-kafka-clients
2022-03-28 11:06:53 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:create-admin-my-cluster-28d9dd2e-kafka-clients
2022-03-28 11:06:53 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:teardown-delete
2022-03-28 11:06:53 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of KafkaUser my-user-1933547616-1024431140 in namespace throttling-quota-st
2022-03-28 11:06:53 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1933547616-1024431140
2022-03-28 11:06:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1933547616-1024431140 not ready, will try again in 10000 ms (179907ms till timeout)
2022-03-28 11:06:53 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-8 exec my-cluster-66f23d43-connect-6f8cbd555f-8xfpj -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-66f23d43
2022-03-28 11:06:53 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 11:06:53 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:154] Connector's spec is stable for 20 polls intervals
2022-03-28 11:06:53 [ForkJoinPool-1-worker-15] INFO  [ReconciliationST:127] Setting annotation to "false", taskMax should be increased to 4
2022-03-28 11:06:53 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Wait for KafkaConnector config will contain desired config
2022-03-28 11:06:53 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 exec my-cluster-66f23d43-connect-6f8cbd555f-8xfpj -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-66f23d43/config
2022-03-28 11:06:54 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-8 exec my-cluster-66f23d43-connect-6f8cbd555f-8xfpj -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-66f23d43/config
2022-03-28 11:06:54 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 11:06:54 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 exec my-cluster-66f23d43-connect-6f8cbd555f-8xfpj -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-66f23d43/config
2022-03-28 11:06:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (214818ms till timeout)
2022-03-28 11:06:54 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-8 exec my-cluster-66f23d43-connect-6f8cbd555f-8xfpj -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-66f23d43/config
2022-03-28 11:06:54 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 11:06:54 [ForkJoinPool-1-worker-15] DEBUG [AbstractST:674] ============================================================================
2022-03-28 11:06:54 [ForkJoinPool-1-worker-15] DEBUG [AbstractST:675] [operators.ReconciliationST - After Each] - Clean up after test
2022-03-28 11:06:54 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:344] ############################################################################
2022-03-28 11:06:54 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:348] Delete all resources for testPauseReconciliationInKafkaAndKafkaConnectWithConnector
2022-03-28 11:06:54 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:241] Delete of NetworkPolicy my-cluster-66f23d43-allow in namespace namespace-8
2022-03-28 11:06:54 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of Deployment my-cluster-66f23d43-kafka-clients in namespace namespace-8
2022-03-28 11:06:54 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of Kafka my-cluster-66f23d43 in namespace namespace-8
2022-03-28 11:06:54 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:241] Delete of KafkaConnector my-cluster-66f23d43 in namespace namespace-8
2022-03-28 11:06:54 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-66f23d43-kafka-clients
2022-03-28 11:06:54 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-66f23d43
2022-03-28 11:06:54 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource NetworkPolicy:my-cluster-66f23d43-allow
2022-03-28 11:06:54 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaConnector:my-cluster-66f23d43
2022-03-28 11:06:55 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-66f23d43 not ready, will try again in 10000 ms (839891ms till timeout)
2022-03-28 11:06:55 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:241] Delete of KafkaConnect my-cluster-66f23d43 in namespace namespace-8
2022-03-28 11:06:55 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-66f23d43-kafka-clients not ready, will try again in 10000 ms (479846ms till timeout)
2022-03-28 11:06:55 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaConnector:my-cluster-66f23d43 not ready, will try again in 10000 ms (239846ms till timeout)
2022-03-28 11:06:55 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaConnect:my-cluster-66f23d43
2022-03-28 11:06:55 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaConnect:my-cluster-66f23d43 not ready, will try again in 10000 ms (599743ms till timeout)
2022-03-28 11:06:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (213669ms till timeout)
2022-03-28 11:06:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (212497ms till timeout)
2022-03-28 11:06:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (211270ms till timeout)
2022-03-28 11:06:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (210112ms till timeout)
2022-03-28 11:07:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (208949ms till timeout)
2022-03-28 11:07:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (207880ms till timeout)
2022-03-28 11:07:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (206806ms till timeout)
2022-03-28 11:07:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (205640ms till timeout)
2022-03-28 11:07:03 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of Deployment my-cluster-66f23d43-scraper in namespace namespace-8
2022-03-28 11:07:03 [ForkJoinPool-1-worker-17] INFO  [ResourceManager:369] ############################################################################
2022-03-28 11:07:03 [ForkJoinPool-1-worker-17] DEBUG [SuiteThreadController:267] testThrottlingQuotasCreateAlterPartitions - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTls, testSendSimpleMessageTlsScramSha, testConfigurationFileIsCreated, testReceiveSimpleMessageTls, testDeployAndUnDeployCruiseControl, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testConfigurationPerformanceOptions] to and randomly select one to start execution
2022-03-28 11:07:03 [ForkJoinPool-1-worker-17] DEBUG [SuiteThreadController:93] [operators.topic.ThrottlingQuotaST] - Removing parallel test: testThrottlingQuotasCreateAlterPartitions
2022-03-28 11:07:03 [ForkJoinPool-1-worker-17] DEBUG [SuiteThreadController:97] [operators.topic.ThrottlingQuotaST] - Parallel test count: 2
2022-03-28 11:07:03 [ForkJoinPool-1-worker-17] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testThrottlingQuotasCreateAlterPartitions-FINISHED
2022-03-28 11:07:03 [ForkJoinPool-1-worker-17] INFO  [TestSeparator:30] ############################################################################
2022-03-28 11:07:03 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-66f23d43-scraper
2022-03-28 11:07:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-66f23d43-scraper not ready, will try again in 10000 ms (479880ms till timeout)
2022-03-28 11:07:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (204589ms till timeout)
2022-03-28 11:07:05 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-66f23d43-kafka-clients not ready, will try again in 10000 ms (469644ms till timeout)
2022-03-28 11:07:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (203485ms till timeout)
2022-03-28 11:07:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (202360ms till timeout)
2022-03-28 11:07:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (201220ms till timeout)
2022-03-28 11:07:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (200120ms till timeout)
2022-03-28 11:07:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (199070ms till timeout)
2022-03-28 11:07:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (198013ms till timeout)
2022-03-28 11:07:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (196941ms till timeout)
2022-03-28 11:07:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (195889ms till timeout)
2022-03-28 11:07:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-66f23d43-scraper not ready, will try again in 10000 ms (469580ms till timeout)
2022-03-28 11:07:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (194716ms till timeout)
2022-03-28 11:07:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (193664ms till timeout)
2022-03-28 11:07:15 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-66f23d43-kafka-clients not ready, will try again in 10000 ms (459498ms till timeout)
2022-03-28 11:07:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (192570ms till timeout)
2022-03-28 11:07:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (191475ms till timeout)
2022-03-28 11:07:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (190388ms till timeout)
2022-03-28 11:07:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (189339ms till timeout)
2022-03-28 11:07:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (188295ms till timeout)
2022-03-28 11:07:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (187269ms till timeout)
2022-03-28 11:07:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (186200ms till timeout)
2022-03-28 11:07:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (185110ms till timeout)
2022-03-28 11:07:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-66f23d43-scraper not ready, will try again in 10000 ms (459521ms till timeout)
2022-03-28 11:07:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (184050ms till timeout)
2022-03-28 11:07:25 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-66f23d43-kafka-clients not ready, will try again in 10000 ms (449342ms till timeout)
2022-03-28 11:07:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (183005ms till timeout)
2022-03-28 11:07:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (181884ms till timeout)
2022-03-28 11:07:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (180772ms till timeout)
2022-03-28 11:07:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (179672ms till timeout)
2022-03-28 11:07:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (178592ms till timeout)
2022-03-28 11:07:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (177495ms till timeout)
2022-03-28 11:07:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (176363ms till timeout)
2022-03-28 11:07:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (175217ms till timeout)
2022-03-28 11:07:34 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-66f23d43-scraper not ready, will try again in 10000 ms (449421ms till timeout)
2022-03-28 11:07:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (174157ms till timeout)
2022-03-28 11:07:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (173095ms till timeout)
2022-03-28 11:07:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (171969ms till timeout)
2022-03-28 11:07:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (170849ms till timeout)
2022-03-28 11:07:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (169771ms till timeout)
2022-03-28 11:07:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (168746ms till timeout)
2022-03-28 11:07:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (167651ms till timeout)
2022-03-28 11:07:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (166620ms till timeout)
2022-03-28 11:07:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (165564ms till timeout)
2022-03-28 11:07:44 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:369] ############################################################################
2022-03-28 11:07:44 [ForkJoinPool-1-worker-15] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-8 for test case:testPauseReconciliationInKafkaAndKafkaConnectWithConnector
2022-03-28 11:07:44 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Namespace namespace-8 removal
2022-03-28 11:07:44 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 11:07:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (164510ms till timeout)
2022-03-28 11:07:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (163403ms till timeout)
2022-03-28 11:07:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (162330ms till timeout)
2022-03-28 11:07:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (161242ms till timeout)
2022-03-28 11:07:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (160145ms till timeout)
2022-03-28 11:07:49 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 11:07:49 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 11:07:49 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (474662ms till timeout)
2022-03-28 11:07:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (159099ms till timeout)
2022-03-28 11:07:50 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 11:07:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (157998ms till timeout)
2022-03-28 11:07:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (156824ms till timeout)
2022-03-28 11:07:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (155718ms till timeout)
2022-03-28 11:07:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (154516ms till timeout)
2022-03-28 11:07:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (153433ms till timeout)
2022-03-28 11:07:55 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 11:07:55 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 11:07:55 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (468392ms till timeout)
2022-03-28 11:07:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (152308ms till timeout)
2022-03-28 11:07:56 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 11:07:57 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 11:07:57 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 11:07:57 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (467020ms till timeout)
2022-03-28 11:07:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (151260ms till timeout)
2022-03-28 11:07:58 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 11:07:58 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 11:07:58 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 11:07:58 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (465731ms till timeout)
2022-03-28 11:07:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (150150ms till timeout)
2022-03-28 11:07:59 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 11:07:59 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 11:07:59 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 11:07:59 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (464358ms till timeout)
2022-03-28 11:08:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (149039ms till timeout)
2022-03-28 11:08:00 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 11:08:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (147966ms till timeout)
2022-03-28 11:08:01 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 11:08:01 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 11:08:01 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (462978ms till timeout)
2022-03-28 11:08:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (146842ms till timeout)
2022-03-28 11:08:02 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 11:08:02 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 11:08:02 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 11:08:02 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (461658ms till timeout)
2022-03-28 11:08:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (145720ms till timeout)
2022-03-28 11:08:03 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 11:08:04 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 11:08:04 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 11:08:04 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (460344ms till timeout)
2022-03-28 11:08:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (144617ms till timeout)
2022-03-28 11:08:05 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 11:08:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (143475ms till timeout)
2022-03-28 11:08:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (142436ms till timeout)
2022-03-28 11:08:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (141359ms till timeout)
2022-03-28 11:08:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (140235ms till timeout)
2022-03-28 11:08:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (139147ms till timeout)
2022-03-28 11:08:10 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 11:08:10 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 1
2022-03-28 11:08:10 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 11:08:10 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-8" not found
2022-03-28 11:08:10 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] ======STDERR END======
2022-03-28 11:08:10 [ForkJoinPool-1-worker-15] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[infra-namespace, reconciliation-st], io.strimzi.test.logs.CollectorElement@f4405b3b=[], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 11:08:10 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:267] testPauseReconciliationInKafkaAndKafkaConnectWithConnector - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTls, testSendSimpleMessageTlsScramSha, testConfigurationFileIsCreated, testReceiveSimpleMessageTls, testDeployAndUnDeployCruiseControl, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testConfigurationPerformanceOptions] to and randomly select one to start execution
2022-03-28 11:08:10 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:93] [operators.ReconciliationST] - Removing parallel test: testPauseReconciliationInKafkaAndKafkaConnectWithConnector
2022-03-28 11:08:10 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:97] [operators.ReconciliationST] - Parallel test count: 1
2022-03-28 11:08:10 [ForkJoinPool-1-worker-15] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.ReconciliationST.testPauseReconciliationInKafkaAndKafkaConnectWithConnector-FINISHED
2022-03-28 11:08:10 [ForkJoinPool-1-worker-15] INFO  [TestSeparator:30] ############################################################################
2022-03-28 11:08:10 [ForkJoinPool-1-worker-15] DEBUG [AbstractST:689] ============================================================================
2022-03-28 11:08:10 [ForkJoinPool-1-worker-15] DEBUG [AbstractST:690] [operators.ReconciliationST - After All] - Clean up after test suite
2022-03-28 11:08:10 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:344] ############################################################################
2022-03-28 11:08:10 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:346] In context ReconciliationST is everything deleted.
2022-03-28 11:08:10 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:369] ############################################################################
2022-03-28 11:08:10 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Namespace reconciliation-st removal
2022-03-28 11:08:10 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 11:08:10 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 11:08:10 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 11:08:10 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace reconciliation-st removal not ready, will try again in 1000 ms (479769ms till timeout)
2022-03-28 11:08:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (138056ms till timeout)
2022-03-28 11:08:11 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 11:08:11 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 11:08:11 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 11:08:11 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace reconciliation-st removal not ready, will try again in 1000 ms (478503ms till timeout)
2022-03-28 11:08:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (137013ms till timeout)
2022-03-28 11:08:12 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 11:08:13 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 11:08:13 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 11:08:13 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace reconciliation-st removal not ready, will try again in 1000 ms (477235ms till timeout)
2022-03-28 11:08:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (135939ms till timeout)
2022-03-28 11:08:14 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 11:08:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (134798ms till timeout)
2022-03-28 11:08:14 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 11:08:14 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 11:08:14 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace reconciliation-st removal not ready, will try again in 1000 ms (475992ms till timeout)
2022-03-28 11:08:15 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 11:08:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (133677ms till timeout)
2022-03-28 11:08:15 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 11:08:15 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 11:08:15 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace reconciliation-st removal not ready, will try again in 1000 ms (474677ms till timeout)
2022-03-28 11:08:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (132528ms till timeout)
2022-03-28 11:08:16 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 11:08:16 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 11:08:16 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 11:08:16 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace reconciliation-st removal not ready, will try again in 1000 ms (473363ms till timeout)
2022-03-28 11:08:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (131448ms till timeout)
2022-03-28 11:08:17 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 11:08:18 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 11:08:18 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 11:08:18 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace reconciliation-st removal not ready, will try again in 1000 ms (472103ms till timeout)
2022-03-28 11:08:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (130353ms till timeout)
2022-03-28 11:08:19 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 11:08:19 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 11:08:19 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 11:08:19 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace reconciliation-st removal not ready, will try again in 1000 ms (470872ms till timeout)
2022-03-28 11:08:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (129257ms till timeout)
2022-03-28 11:08:20 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 11:08:20 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 11:08:20 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 11:08:20 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace reconciliation-st removal not ready, will try again in 1000 ms (469493ms till timeout)
2022-03-28 11:08:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (128169ms till timeout)
2022-03-28 11:08:21 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 11:08:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (127075ms till timeout)
2022-03-28 11:08:22 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 11:08:22 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 11:08:22 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace reconciliation-st removal not ready, will try again in 1000 ms (468222ms till timeout)
2022-03-28 11:08:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (125954ms till timeout)
2022-03-28 11:08:23 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 11:08:23 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 11:08:23 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 1
2022-03-28 11:08:23 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 11:08:23 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Error from server (NotFound): namespaces "reconciliation-st" not found
2022-03-28 11:08:23 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] ======STDERR END======
2022-03-28 11:08:23 [ForkJoinPool-1-worker-15] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[], io.strimzi.test.logs.CollectorElement@b850ef2a=[infra-namespace, reconciliation-st], io.strimzi.test.logs.CollectorElement@f4405b3b=[], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 11:08:23 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:254] ReconciliationST - Notifies waiting test suites:[UserST, HttpBridgeTlsST, TopicST, CruiseControlConfigurationST, HttpBridgeScramShaST, ThrottlingQuotaST] to and randomly select one to start execution
2022-03-28 11:08:23 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:85] [operators.ReconciliationST] - Removing parallel suite: ReconciliationST
2022-03-28 11:08:23 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:89] [operators.ReconciliationST] - Parallel suites count: 2
[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1,270.92 s - in io.strimzi.systemtest.operators.ReconciliationST
2022-03-28 11:08:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (124846ms till timeout)
2022-03-28 11:08:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (123771ms till timeout)
2022-03-28 11:08:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (122555ms till timeout)
2022-03-28 11:08:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (121480ms till timeout)
2022-03-28 11:08:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (120422ms till timeout)
2022-03-28 11:08:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (119298ms till timeout)
2022-03-28 11:08:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (118158ms till timeout)
2022-03-28 11:08:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (117048ms till timeout)
2022-03-28 11:08:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (115991ms till timeout)
2022-03-28 11:08:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (114882ms till timeout)
2022-03-28 11:08:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (113817ms till timeout)
2022-03-28 11:08:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (112768ms till timeout)
2022-03-28 11:08:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (111544ms till timeout)
2022-03-28 11:08:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (110388ms till timeout)
2022-03-28 11:08:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (109285ms till timeout)
2022-03-28 11:08:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (108167ms till timeout)
2022-03-28 11:08:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (107078ms till timeout)
2022-03-28 11:08:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (106001ms till timeout)
2022-03-28 11:08:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (104916ms till timeout)
2022-03-28 11:08:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (103797ms till timeout)
2022-03-28 11:08:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (102666ms till timeout)
2022-03-28 11:08:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (101440ms till timeout)
2022-03-28 11:08:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (100302ms till timeout)
2022-03-28 11:08:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (99099ms till timeout)
2022-03-28 11:08:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (98034ms till timeout)
2022-03-28 11:08:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (96949ms till timeout)
2022-03-28 11:08:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (95855ms till timeout)
2022-03-28 11:08:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (94791ms till timeout)
2022-03-28 11:08:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (93722ms till timeout)
2022-03-28 11:08:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (92616ms till timeout)
2022-03-28 11:08:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (91487ms till timeout)
2022-03-28 11:08:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (90330ms till timeout)
2022-03-28 11:08:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (89230ms till timeout)
2022-03-28 11:09:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (88088ms till timeout)
2022-03-28 11:09:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (86965ms till timeout)
2022-03-28 11:09:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (85869ms till timeout)
2022-03-28 11:09:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (84738ms till timeout)
2022-03-28 11:09:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (83593ms till timeout)
2022-03-28 11:09:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (82514ms till timeout)
2022-03-28 11:09:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (81458ms till timeout)
2022-03-28 11:09:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (80419ms till timeout)
2022-03-28 11:09:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (79298ms till timeout)
2022-03-28 11:09:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (78251ms till timeout)
2022-03-28 11:09:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (77186ms till timeout)
2022-03-28 11:09:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (76114ms till timeout)
2022-03-28 11:09:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (75006ms till timeout)
2022-03-28 11:09:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (73904ms till timeout)
2022-03-28 11:09:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (72809ms till timeout)
2022-03-28 11:09:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (71635ms till timeout)
2022-03-28 11:09:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (70501ms till timeout)
2022-03-28 11:09:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (69432ms till timeout)
2022-03-28 11:09:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (68203ms till timeout)
2022-03-28 11:09:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (67056ms till timeout)
2022-03-28 11:09:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (65989ms till timeout)
2022-03-28 11:09:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (64854ms till timeout)
2022-03-28 11:09:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (63748ms till timeout)
2022-03-28 11:09:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (62680ms till timeout)
2022-03-28 11:09:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (61584ms till timeout)
2022-03-28 11:09:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (60495ms till timeout)
2022-03-28 11:09:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (59245ms till timeout)
2022-03-28 11:09:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (57984ms till timeout)
2022-03-28 11:09:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (56809ms till timeout)
2022-03-28 11:09:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (55696ms till timeout)
2022-03-28 11:09:34 [ForkJoinPool-1-worker-9] INFO  [PodUtils:189] Message org.apache.kafka.common.errors.ThrottlingQuotaExceededException: The throttling quota has been exceeded. found in delete-admin-my-cluster-8f262de5-kafka-clients-7qhcl log
2022-03-28 11:09:34 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment delete-admin-my-cluster-8f262de5-kafka-clients deletion
2022-03-28 11:09:34 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for ReplicaSet delete-admin-my-cluster-8f262de5-kafka-clients to be deleted
2022-03-28 11:09:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] ReplicaSet delete-admin-my-cluster-8f262de5-kafka-clients to be deleted not ready, will try again in 5000 ms (179856ms till timeout)
2022-03-28 11:09:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] ReplicaSet delete-admin-my-cluster-8f262de5-kafka-clients to be deleted not ready, will try again in 5000 ms (174779ms till timeout)
2022-03-28 11:09:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] ReplicaSet delete-admin-my-cluster-8f262de5-kafka-clients to be deleted not ready, will try again in 5000 ms (169686ms till timeout)
2022-03-28 11:09:50 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:40] Job delete-admin-my-cluster-8f262de5-kafka-clients was deleted
2022-03-28 11:09:50 [ForkJoinPool-1-worker-9] INFO  [ThrottlingQuotaST:144] Executing 1/5 iteration for delete-admin-my-cluster-8f262de5-kafka-clients.
2022-03-28 11:09:50 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st
2022-03-28 11:09:50 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:delete-admin-my-cluster-8f262de5-kafka-clients
2022-03-28 11:09:50 [ForkJoinPool-1-worker-9] INFO  [JobUtils:81] Waiting for job: delete-admin-my-cluster-8f262de5-kafka-clients will be in active state
2022-03-28 11:09:50 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 11:09:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job active not ready, will try again in 1000 ms (179929ms till timeout)
2022-03-28 11:09:51 [ForkJoinPool-1-worker-9] INFO  [ClientUtils:76] Waiting for producer/consumer:delete-admin-my-cluster-8f262de5-kafka-clients to finished
2022-03-28 11:09:51 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 11:09:51 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:09:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129949ms till timeout)
2022-03-28 11:09:52 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:09:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (128873ms till timeout)
2022-03-28 11:09:53 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:09:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (127834ms till timeout)
2022-03-28 11:09:54 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:09:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (126760ms till timeout)
2022-03-28 11:09:55 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:09:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (125542ms till timeout)
2022-03-28 11:09:56 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:09:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (124453ms till timeout)
2022-03-28 11:09:58 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:09:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (123358ms till timeout)
2022-03-28 11:09:59 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:09:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (122269ms till timeout)
2022-03-28 11:10:00 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (121236ms till timeout)
2022-03-28 11:10:01 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (120164ms till timeout)
2022-03-28 11:10:02 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (119050ms till timeout)
2022-03-28 11:10:03 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (117898ms till timeout)
2022-03-28 11:10:04 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (116827ms till timeout)
2022-03-28 11:10:05 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (115784ms till timeout)
2022-03-28 11:10:06 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (114635ms till timeout)
2022-03-28 11:10:07 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (113584ms till timeout)
2022-03-28 11:10:08 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (112542ms till timeout)
2022-03-28 11:10:09 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (111468ms till timeout)
2022-03-28 11:10:10 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (110411ms till timeout)
2022-03-28 11:10:12 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (109379ms till timeout)
2022-03-28 11:10:13 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (108292ms till timeout)
2022-03-28 11:10:14 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (107179ms till timeout)
2022-03-28 11:10:15 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (106056ms till timeout)
2022-03-28 11:10:16 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (105000ms till timeout)
2022-03-28 11:10:17 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (103869ms till timeout)
2022-03-28 11:10:18 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (102791ms till timeout)
2022-03-28 11:10:19 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (101670ms till timeout)
2022-03-28 11:10:20 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (100644ms till timeout)
2022-03-28 11:10:21 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (99593ms till timeout)
2022-03-28 11:10:22 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (98472ms till timeout)
2022-03-28 11:10:23 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (97394ms till timeout)
2022-03-28 11:10:25 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (96324ms till timeout)
2022-03-28 11:10:26 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (95242ms till timeout)
2022-03-28 11:10:27 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (94188ms till timeout)
2022-03-28 11:10:28 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (93081ms till timeout)
2022-03-28 11:10:29 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (92035ms till timeout)
2022-03-28 11:10:30 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (90976ms till timeout)
2022-03-28 11:10:31 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (89914ms till timeout)
2022-03-28 11:10:32 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (88788ms till timeout)
2022-03-28 11:10:33 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (87728ms till timeout)
2022-03-28 11:10:34 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (86676ms till timeout)
2022-03-28 11:10:35 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (85620ms till timeout)
2022-03-28 11:10:36 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (84558ms till timeout)
2022-03-28 11:10:37 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (83502ms till timeout)
2022-03-28 11:10:38 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (82425ms till timeout)
2022-03-28 11:10:40 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (81358ms till timeout)
2022-03-28 11:10:41 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (80290ms till timeout)
2022-03-28 11:10:42 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (79173ms till timeout)
2022-03-28 11:10:43 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (78091ms till timeout)
2022-03-28 11:10:44 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (76927ms till timeout)
2022-03-28 11:10:45 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (75818ms till timeout)
2022-03-28 11:10:46 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (74660ms till timeout)
2022-03-28 11:10:47 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (73615ms till timeout)
2022-03-28 11:10:48 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (72542ms till timeout)
2022-03-28 11:10:49 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (71449ms till timeout)
2022-03-28 11:10:51 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (70360ms till timeout)
2022-03-28 11:10:52 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (69234ms till timeout)
2022-03-28 11:10:53 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (68167ms till timeout)
2022-03-28 11:10:54 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (67054ms till timeout)
2022-03-28 11:10:55 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (65800ms till timeout)
2022-03-28 11:10:56 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T11:10:54Z, conditions=[JobCondition(lastProbeTime=2022-03-28T11:10:54Z, lastTransitionTime=2022-03-28T11:10:54Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T11:09:47Z, succeeded=1, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:10:56 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment delete-admin-my-cluster-8f262de5-kafka-clients deletion
2022-03-28 11:10:56 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for ReplicaSet delete-admin-my-cluster-8f262de5-kafka-clients to be deleted
2022-03-28 11:10:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] ReplicaSet delete-admin-my-cluster-8f262de5-kafka-clients to be deleted not ready, will try again in 5000 ms (179931ms till timeout)
2022-03-28 11:11:01 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:40] Job delete-admin-my-cluster-8f262de5-kafka-clients was deleted
2022-03-28 11:11:01 [ForkJoinPool-1-worker-9] INFO  [ThrottlingQuotaST:144] Executing 2/5 iteration for delete-admin-my-cluster-8f262de5-kafka-clients.
2022-03-28 11:11:01 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st
2022-03-28 11:11:02 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:delete-admin-my-cluster-8f262de5-kafka-clients
2022-03-28 11:11:02 [ForkJoinPool-1-worker-9] INFO  [JobUtils:81] Waiting for job: delete-admin-my-cluster-8f262de5-kafka-clients will be in active state
2022-03-28 11:11:02 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 11:11:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job active not ready, will try again in 1000 ms (179948ms till timeout)
2022-03-28 11:11:03 [ForkJoinPool-1-worker-9] INFO  [ClientUtils:76] Waiting for producer/consumer:delete-admin-my-cluster-8f262de5-kafka-clients to finished
2022-03-28 11:11:03 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 11:11:03 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129935ms till timeout)
2022-03-28 11:11:04 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (128846ms till timeout)
2022-03-28 11:11:05 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (127788ms till timeout)
2022-03-28 11:11:06 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (126749ms till timeout)
2022-03-28 11:11:07 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (125648ms till timeout)
2022-03-28 11:11:08 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (124502ms till timeout)
2022-03-28 11:11:09 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (123391ms till timeout)
2022-03-28 11:11:11 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (122303ms till timeout)
2022-03-28 11:11:12 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (121201ms till timeout)
2022-03-28 11:11:13 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (120158ms till timeout)
2022-03-28 11:11:14 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (119081ms till timeout)
2022-03-28 11:11:15 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (117987ms till timeout)
2022-03-28 11:11:16 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (116862ms till timeout)
2022-03-28 11:11:17 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (115767ms till timeout)
2022-03-28 11:11:18 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (114706ms till timeout)
2022-03-28 11:11:19 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (113612ms till timeout)
2022-03-28 11:11:20 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (112527ms till timeout)
2022-03-28 11:11:21 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (111425ms till timeout)
2022-03-28 11:11:23 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (110300ms till timeout)
2022-03-28 11:11:24 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (109216ms till timeout)
2022-03-28 11:11:25 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (108122ms till timeout)
2022-03-28 11:11:26 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (106983ms till timeout)
2022-03-28 11:11:27 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (105856ms till timeout)
2022-03-28 11:11:28 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (104798ms till timeout)
2022-03-28 11:11:29 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (103714ms till timeout)
2022-03-28 11:11:30 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (102510ms till timeout)
2022-03-28 11:11:31 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (101420ms till timeout)
2022-03-28 11:11:33 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (100168ms till timeout)
2022-03-28 11:11:34 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (99018ms till timeout)
2022-03-28 11:11:35 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (97939ms till timeout)
2022-03-28 11:11:36 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (96875ms till timeout)
2022-03-28 11:11:37 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (95813ms till timeout)
2022-03-28 11:11:38 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (94724ms till timeout)
2022-03-28 11:11:39 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (93646ms till timeout)
2022-03-28 11:11:40 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (92565ms till timeout)
2022-03-28 11:11:41 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (91492ms till timeout)
2022-03-28 11:11:42 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (90423ms till timeout)
2022-03-28 11:11:44 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (89290ms till timeout)
2022-03-28 11:11:45 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (88221ms till timeout)
2022-03-28 11:11:46 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (87131ms till timeout)
2022-03-28 11:11:47 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (85996ms till timeout)
2022-03-28 11:11:48 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (84953ms till timeout)
2022-03-28 11:11:49 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (83834ms till timeout)
2022-03-28 11:11:50 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (82778ms till timeout)
2022-03-28 11:11:51 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (81684ms till timeout)
2022-03-28 11:11:52 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (80632ms till timeout)
2022-03-28 11:11:53 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (79558ms till timeout)
2022-03-28 11:11:54 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (78425ms till timeout)
2022-03-28 11:11:55 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (77350ms till timeout)
2022-03-28 11:11:57 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (76259ms till timeout)
2022-03-28 11:11:58 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (75165ms till timeout)
2022-03-28 11:11:59 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:11:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (74050ms till timeout)
2022-03-28 11:12:00 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:12:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (72997ms till timeout)
2022-03-28 11:12:01 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:12:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (71968ms till timeout)
2022-03-28 11:12:02 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:12:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (70929ms till timeout)
2022-03-28 11:12:03 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:12:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (69871ms till timeout)
2022-03-28 11:12:04 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:12:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (68707ms till timeout)
2022-03-28 11:12:05 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:12:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (67617ms till timeout)
2022-03-28 11:12:06 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:12:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (66577ms till timeout)
2022-03-28 11:12:07 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:12:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (65524ms till timeout)
2022-03-28 11:12:08 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T11:12:05Z, conditions=[JobCondition(lastProbeTime=2022-03-28T11:12:05Z, lastTransitionTime=2022-03-28T11:12:05Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T11:10:59Z, succeeded=1, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:12:08 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment delete-admin-my-cluster-8f262de5-kafka-clients deletion
2022-03-28 11:12:08 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for ReplicaSet delete-admin-my-cluster-8f262de5-kafka-clients to be deleted
2022-03-28 11:12:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] ReplicaSet delete-admin-my-cluster-8f262de5-kafka-clients to be deleted not ready, will try again in 5000 ms (179937ms till timeout)
2022-03-28 11:12:14 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:40] Job delete-admin-my-cluster-8f262de5-kafka-clients was deleted
2022-03-28 11:12:14 [ForkJoinPool-1-worker-9] INFO  [ThrottlingQuotaST:144] Executing 3/5 iteration for delete-admin-my-cluster-8f262de5-kafka-clients.
2022-03-28 11:12:14 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st
2022-03-28 11:12:14 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:delete-admin-my-cluster-8f262de5-kafka-clients
2022-03-28 11:12:14 [ForkJoinPool-1-worker-9] INFO  [JobUtils:81] Waiting for job: delete-admin-my-cluster-8f262de5-kafka-clients will be in active state
2022-03-28 11:12:14 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 11:12:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job active not ready, will try again in 1000 ms (179945ms till timeout)
2022-03-28 11:12:15 [ForkJoinPool-1-worker-9] INFO  [ClientUtils:76] Waiting for producer/consumer:delete-admin-my-cluster-8f262de5-kafka-clients to finished
2022-03-28 11:12:15 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 11:12:15 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:12:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129885ms till timeout)
2022-03-28 11:12:16 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:12:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (128690ms till timeout)
2022-03-28 11:12:17 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:12:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (127595ms till timeout)
2022-03-28 11:12:18 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:12:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (126508ms till timeout)
2022-03-28 11:12:19 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:12:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (125404ms till timeout)
2022-03-28 11:12:20 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:12:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (124302ms till timeout)
2022-03-28 11:12:22 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:12:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (123220ms till timeout)
2022-03-28 11:12:23 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:12:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (122120ms till timeout)
2022-03-28 11:12:24 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:12:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (121037ms till timeout)
2022-03-28 11:12:25 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:12:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (119884ms till timeout)
2022-03-28 11:12:26 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:12:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (118740ms till timeout)
2022-03-28 11:12:27 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:12:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (117560ms till timeout)
2022-03-28 11:12:28 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:12:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (116453ms till timeout)
2022-03-28 11:12:29 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:12:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (115333ms till timeout)
2022-03-28 11:12:31 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:12:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (114239ms till timeout)
2022-03-28 11:12:32 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:12:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (113078ms till timeout)
2022-03-28 11:12:33 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:12:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (111994ms till timeout)
2022-03-28 11:12:34 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:12:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (110882ms till timeout)
2022-03-28 11:12:35 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:12:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (109816ms till timeout)
2022-03-28 11:12:36 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:12:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (108544ms till timeout)
2022-03-28 11:12:38 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:12:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (107301ms till timeout)
2022-03-28 11:12:39 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:12:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (106216ms till timeout)
2022-03-28 11:12:40 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:12:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (105143ms till timeout)
2022-03-28 11:12:41 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:12:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (104106ms till timeout)
2022-03-28 11:12:42 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:12:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (102990ms till timeout)
2022-03-28 11:12:43 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:12:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (101935ms till timeout)
2022-03-28 11:12:44 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:12:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (100864ms till timeout)
2022-03-28 11:12:45 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:12:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (99789ms till timeout)
2022-03-28 11:12:46 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:12:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (98631ms till timeout)
2022-03-28 11:12:47 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:12:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (97527ms till timeout)
2022-03-28 11:12:48 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:12:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (96378ms till timeout)
2022-03-28 11:12:50 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:12:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (95222ms till timeout)
2022-03-28 11:12:51 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:12:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (94160ms till timeout)
2022-03-28 11:12:52 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:12:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (93036ms till timeout)
2022-03-28 11:12:53 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:12:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (91878ms till timeout)
2022-03-28 11:12:54 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:12:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (90817ms till timeout)
2022-03-28 11:12:55 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:12:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (89720ms till timeout)
2022-03-28 11:12:56 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:12:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (88539ms till timeout)
2022-03-28 11:12:57 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:12:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (87376ms till timeout)
2022-03-28 11:12:59 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:12:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (86263ms till timeout)
2022-03-28 11:13:00 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:13:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (85172ms till timeout)
2022-03-28 11:13:01 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:13:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (84130ms till timeout)
2022-03-28 11:13:02 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:13:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (83028ms till timeout)
2022-03-28 11:13:03 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:13:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (81830ms till timeout)
2022-03-28 11:13:04 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:13:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (80692ms till timeout)
2022-03-28 11:13:05 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:13:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (79626ms till timeout)
2022-03-28 11:13:06 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:13:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (78584ms till timeout)
2022-03-28 11:13:07 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:13:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (77408ms till timeout)
2022-03-28 11:13:08 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:13:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (76363ms till timeout)
2022-03-28 11:13:10 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:13:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (75144ms till timeout)
2022-03-28 11:13:11 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:13:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (74011ms till timeout)
2022-03-28 11:13:12 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:13:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (72931ms till timeout)
2022-03-28 11:13:13 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:13:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (71702ms till timeout)
2022-03-28 11:13:14 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:13:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (70621ms till timeout)
2022-03-28 11:13:15 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:13:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (69480ms till timeout)
2022-03-28 11:13:17 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:13:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (68210ms till timeout)
2022-03-28 11:13:18 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:13:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (67081ms till timeout)
2022-03-28 11:13:19 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:13:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (65986ms till timeout)
2022-03-28 11:13:20 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:13:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (64813ms till timeout)
2022-03-28 11:13:21 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T11:13:18Z, conditions=[JobCondition(lastProbeTime=2022-03-28T11:13:18Z, lastTransitionTime=2022-03-28T11:13:18Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T11:12:11Z, succeeded=1, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:13:21 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment delete-admin-my-cluster-8f262de5-kafka-clients deletion
2022-03-28 11:13:21 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for ReplicaSet delete-admin-my-cluster-8f262de5-kafka-clients to be deleted
2022-03-28 11:13:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] ReplicaSet delete-admin-my-cluster-8f262de5-kafka-clients to be deleted not ready, will try again in 5000 ms (179885ms till timeout)
2022-03-28 11:13:27 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:40] Job delete-admin-my-cluster-8f262de5-kafka-clients was deleted
2022-03-28 11:13:27 [ForkJoinPool-1-worker-9] INFO  [ThrottlingQuotaST:144] Executing 4/5 iteration for delete-admin-my-cluster-8f262de5-kafka-clients.
2022-03-28 11:13:27 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st
2022-03-28 11:13:27 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:delete-admin-my-cluster-8f262de5-kafka-clients
2022-03-28 11:13:27 [ForkJoinPool-1-worker-9] INFO  [JobUtils:81] Waiting for job: delete-admin-my-cluster-8f262de5-kafka-clients will be in active state
2022-03-28 11:13:27 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 11:13:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job active not ready, will try again in 1000 ms (179935ms till timeout)
2022-03-28 11:13:28 [ForkJoinPool-1-worker-9] INFO  [ClientUtils:76] Waiting for producer/consumer:delete-admin-my-cluster-8f262de5-kafka-clients to finished
2022-03-28 11:13:28 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 11:13:28 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:13:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129943ms till timeout)
2022-03-28 11:13:29 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:13:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (128860ms till timeout)
2022-03-28 11:13:30 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:13:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (127623ms till timeout)
2022-03-28 11:13:31 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:13:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (126424ms till timeout)
2022-03-28 11:13:33 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:13:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (125270ms till timeout)
2022-03-28 11:13:34 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:13:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (124201ms till timeout)
2022-03-28 11:13:35 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:13:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (123159ms till timeout)
2022-03-28 11:13:36 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:13:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (122004ms till timeout)
2022-03-28 11:13:37 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:13:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (120889ms till timeout)
2022-03-28 11:13:38 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:13:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (119814ms till timeout)
2022-03-28 11:13:39 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:13:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (118777ms till timeout)
2022-03-28 11:13:40 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:13:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (117644ms till timeout)
2022-03-28 11:13:41 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:13:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (116604ms till timeout)
2022-03-28 11:13:42 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:13:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (115568ms till timeout)
2022-03-28 11:13:43 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:13:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (114413ms till timeout)
2022-03-28 11:13:45 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:13:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (113316ms till timeout)
2022-03-28 11:13:46 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:13:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (112209ms till timeout)
2022-03-28 11:13:47 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:13:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (111016ms till timeout)
2022-03-28 11:13:48 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:13:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (109979ms till timeout)
2022-03-28 11:13:49 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:13:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (108946ms till timeout)
2022-03-28 11:13:50 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:13:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (107848ms till timeout)
2022-03-28 11:13:51 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:13:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (106532ms till timeout)
2022-03-28 11:13:52 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:13:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (105467ms till timeout)
2022-03-28 11:13:53 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:13:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (104378ms till timeout)
2022-03-28 11:13:55 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:13:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (103327ms till timeout)
2022-03-28 11:13:56 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:13:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (102217ms till timeout)
2022-03-28 11:13:57 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:13:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (101115ms till timeout)
2022-03-28 11:13:58 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:13:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (100051ms till timeout)
2022-03-28 11:13:59 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:13:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (98973ms till timeout)
2022-03-28 11:14:00 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:14:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (97888ms till timeout)
2022-03-28 11:14:01 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:14:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (96851ms till timeout)
2022-03-28 11:14:02 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:14:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (95752ms till timeout)
2022-03-28 11:14:03 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:14:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (94603ms till timeout)
2022-03-28 11:14:04 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:14:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (93545ms till timeout)
2022-03-28 11:14:05 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:14:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (92461ms till timeout)
2022-03-28 11:14:06 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:14:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (91401ms till timeout)
2022-03-28 11:14:08 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:14:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (90316ms till timeout)
2022-03-28 11:14:09 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:14:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (89271ms till timeout)
2022-03-28 11:14:10 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:14:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (88183ms till timeout)
2022-03-28 11:14:11 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:14:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (87092ms till timeout)
2022-03-28 11:14:12 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:14:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (86052ms till timeout)
2022-03-28 11:14:13 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:14:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (84979ms till timeout)
2022-03-28 11:14:14 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:14:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (83827ms till timeout)
2022-03-28 11:14:15 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:14:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (82665ms till timeout)
2022-03-28 11:14:16 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:14:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (81588ms till timeout)
2022-03-28 11:14:17 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:14:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (80543ms till timeout)
2022-03-28 11:14:18 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:14:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (79465ms till timeout)
2022-03-28 11:14:19 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:14:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (78357ms till timeout)
2022-03-28 11:14:21 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:14:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (77274ms till timeout)
2022-03-28 11:14:22 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:14:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (76174ms till timeout)
2022-03-28 11:14:23 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:14:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (75093ms till timeout)
2022-03-28 11:14:24 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:14:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (73964ms till timeout)
2022-03-28 11:14:25 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:14:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (72911ms till timeout)
2022-03-28 11:14:26 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:14:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (71875ms till timeout)
2022-03-28 11:14:27 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:14:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (70827ms till timeout)
2022-03-28 11:14:28 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:14:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (69761ms till timeout)
2022-03-28 11:14:29 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:14:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (68713ms till timeout)
2022-03-28 11:14:30 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:14:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (67526ms till timeout)
2022-03-28 11:14:31 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:14:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (66487ms till timeout)
2022-03-28 11:14:32 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:14:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (65335ms till timeout)
2022-03-28 11:14:34 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:14:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (64253ms till timeout)
2022-03-28 11:14:35 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T11:14:32Z, conditions=[JobCondition(lastProbeTime=2022-03-28T11:14:32Z, lastTransitionTime=2022-03-28T11:14:32Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T11:13:25Z, succeeded=1, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:14:35 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment delete-admin-my-cluster-8f262de5-kafka-clients deletion
2022-03-28 11:14:35 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for ReplicaSet delete-admin-my-cluster-8f262de5-kafka-clients to be deleted
2022-03-28 11:14:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] ReplicaSet delete-admin-my-cluster-8f262de5-kafka-clients to be deleted not ready, will try again in 5000 ms (179990ms till timeout)
2022-03-28 11:14:40 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:40] Job delete-admin-my-cluster-8f262de5-kafka-clients was deleted
2022-03-28 11:14:40 [ForkJoinPool-1-worker-9] INFO  [ThrottlingQuotaST:144] Executing 5/5 iteration for delete-admin-my-cluster-8f262de5-kafka-clients.
2022-03-28 11:14:40 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st
2022-03-28 11:14:40 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:delete-admin-my-cluster-8f262de5-kafka-clients
2022-03-28 11:14:40 [ForkJoinPool-1-worker-9] INFO  [JobUtils:81] Waiting for job: delete-admin-my-cluster-8f262de5-kafka-clients will be in active state
2022-03-28 11:14:40 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 11:14:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job active not ready, will try again in 1000 ms (179946ms till timeout)
2022-03-28 11:14:41 [ForkJoinPool-1-worker-9] INFO  [ClientUtils:76] Waiting for producer/consumer:delete-admin-my-cluster-8f262de5-kafka-clients to finished
2022-03-28 11:14:41 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 11:14:41 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:14:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129925ms till timeout)
2022-03-28 11:14:42 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:14:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (128841ms till timeout)
2022-03-28 11:14:43 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:14:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (127808ms till timeout)
2022-03-28 11:14:44 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:14:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (126744ms till timeout)
2022-03-28 11:14:45 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:14:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (125614ms till timeout)
2022-03-28 11:14:46 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:14:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (124525ms till timeout)
2022-03-28 11:14:48 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:14:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (123479ms till timeout)
2022-03-28 11:14:49 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:14:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (122389ms till timeout)
2022-03-28 11:14:50 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:14:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (121301ms till timeout)
2022-03-28 11:14:51 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:14:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (120234ms till timeout)
2022-03-28 11:14:52 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:14:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (119160ms till timeout)
2022-03-28 11:14:53 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:14:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (118069ms till timeout)
2022-03-28 11:14:54 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:14:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (116960ms till timeout)
2022-03-28 11:14:55 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:14:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (115829ms till timeout)
2022-03-28 11:14:56 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:14:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (114732ms till timeout)
2022-03-28 11:14:57 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:14:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (113484ms till timeout)
2022-03-28 11:14:59 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:14:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (112352ms till timeout)
2022-03-28 11:15:00 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:15:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (111225ms till timeout)
2022-03-28 11:15:01 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:15:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (110160ms till timeout)
2022-03-28 11:15:02 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:15:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (109084ms till timeout)
2022-03-28 11:15:03 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:15:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (107992ms till timeout)
2022-03-28 11:15:04 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:15:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (106905ms till timeout)
2022-03-28 11:15:05 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:15:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (105808ms till timeout)
2022-03-28 11:15:06 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:15:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (104681ms till timeout)
2022-03-28 11:15:07 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:15:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (103446ms till timeout)
2022-03-28 11:15:09 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:15:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (102368ms till timeout)
2022-03-28 11:15:10 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:15:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (101221ms till timeout)
2022-03-28 11:15:11 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:15:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (100041ms till timeout)
2022-03-28 11:15:12 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:15:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (98956ms till timeout)
2022-03-28 11:15:13 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:15:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (97793ms till timeout)
2022-03-28 11:15:14 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:15:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (96682ms till timeout)
2022-03-28 11:15:15 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:15:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (95605ms till timeout)
2022-03-28 11:15:17 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:15:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (94466ms till timeout)
2022-03-28 11:15:18 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:15:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (93376ms till timeout)
2022-03-28 11:15:19 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:15:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (92312ms till timeout)
2022-03-28 11:15:20 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:15:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (91266ms till timeout)
2022-03-28 11:15:21 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:15:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (90078ms till timeout)
2022-03-28 11:15:22 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:15:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (89004ms till timeout)
2022-03-28 11:15:23 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:15:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (87903ms till timeout)
2022-03-28 11:15:24 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:15:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (86779ms till timeout)
2022-03-28 11:15:25 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:15:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (85683ms till timeout)
2022-03-28 11:15:26 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:15:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (84477ms till timeout)
2022-03-28 11:15:28 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:15:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (83134ms till timeout)
2022-03-28 11:15:29 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:15:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (82041ms till timeout)
2022-03-28 11:15:30 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:15:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (80953ms till timeout)
2022-03-28 11:15:31 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:15:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (79854ms till timeout)
2022-03-28 11:15:32 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:15:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (78789ms till timeout)
2022-03-28 11:15:33 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:15:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (77608ms till timeout)
2022-03-28 11:15:34 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:15:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (76526ms till timeout)
2022-03-28 11:15:36 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:15:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (75492ms till timeout)
2022-03-28 11:15:37 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:15:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (74389ms till timeout)
2022-03-28 11:15:38 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:15:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (73079ms till timeout)
2022-03-28 11:15:39 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:15:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (72021ms till timeout)
2022-03-28 11:15:40 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:15:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (70988ms till timeout)
2022-03-28 11:15:41 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:15:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (69921ms till timeout)
2022-03-28 11:15:42 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:15:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (68811ms till timeout)
2022-03-28 11:15:43 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:15:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (67730ms till timeout)
2022-03-28 11:15:44 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=null, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:15:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (66641ms till timeout)
2022-03-28 11:15:45 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T11:15:42Z, conditions=[JobCondition(lastProbeTime=2022-03-28T11:15:42Z, lastTransitionTime=2022-03-28T11:15:42Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T11:14:37Z, succeeded=1, uncountedTerminatedPods=null, additionalProperties={})
2022-03-28 11:15:46 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment delete-admin-my-cluster-8f262de5-kafka-clients deletion
2022-03-28 11:15:46 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for ReplicaSet delete-admin-my-cluster-8f262de5-kafka-clients to be deleted
2022-03-28 11:15:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] ReplicaSet delete-admin-my-cluster-8f262de5-kafka-clients to be deleted not ready, will try again in 5000 ms (179922ms till timeout)
2022-03-28 11:15:51 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:40] Job delete-admin-my-cluster-8f262de5-kafka-clients was deleted
2022-03-28 11:15:51 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:674] ============================================================================
2022-03-28 11:15:51 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:675] [operators.topic.ThrottlingQuotaST - After Each] - Clean up after test
2022-03-28 11:15:51 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:344] ############################################################################
2022-03-28 11:15:51 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:348] Delete all resources for testThrottlingQuotasDeleteTopic
2022-03-28 11:15:51 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:241] Delete of Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st
2022-03-28 11:15:51 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st
2022-03-28 11:15:51 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of KafkaUser my-user-1558977140-777236615 in namespace throttling-quota-st
2022-03-28 11:15:51 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:241] Delete of Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st
2022-03-28 11:15:51 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st
2022-03-28 11:15:51 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:241] Delete of Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st
2022-03-28 11:15:51 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:create-admin-my-cluster-8f262de5-kafka-clients
2022-03-28 11:15:51 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:create-admin-my-cluster-8f262de5-kafka-clients
2022-03-28 11:15:51 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:create-admin-my-cluster-8f262de5-kafka-clients
2022-03-28 11:15:51 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:delete-admin-my-cluster-8f262de5-kafka-clients
2022-03-28 11:15:51 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of Job create-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st
2022-03-28 11:15:51 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1558977140-777236615
2022-03-28 11:15:51 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:create-admin-my-cluster-8f262de5-kafka-clients
2022-03-28 11:15:51 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st
2022-03-28 11:15:51 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:241] Delete of Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st
2022-03-28 11:15:51 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:241] Delete of Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st
2022-03-28 11:15:51 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:create-admin-my-cluster-8f262de5-kafka-clients
2022-03-28 11:15:51 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:241] Delete of Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st
2022-03-28 11:15:51 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:delete-admin-my-cluster-8f262de5-kafka-clients
2022-03-28 11:15:51 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:delete-admin-my-cluster-8f262de5-kafka-clients
2022-03-28 11:15:51 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:delete-admin-my-cluster-8f262de5-kafka-clients
2022-03-28 11:15:51 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:delete-admin-my-cluster-8f262de5-kafka-clients
2022-03-28 11:15:51 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of Job delete-admin-my-cluster-8f262de5-kafka-clients in namespace throttling-quota-st
2022-03-28 11:15:51 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1558977140-777236615 not ready, will try again in 10000 ms (179846ms till timeout)
2022-03-28 11:15:51 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:delete-admin-my-cluster-8f262de5-kafka-clients
2022-03-28 11:16:01 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:369] ############################################################################
2022-03-28 11:16:01 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:267] testThrottlingQuotasDeleteTopic - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTls, testSendSimpleMessageTlsScramSha, testConfigurationFileIsCreated, testReceiveSimpleMessageTls, testDeployAndUnDeployCruiseControl, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testConfigurationPerformanceOptions] to and randomly select one to start execution
2022-03-28 11:16:01 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:93] [operators.topic.ThrottlingQuotaST] - Removing parallel test: testThrottlingQuotasDeleteTopic
2022-03-28 11:16:01 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:97] [operators.topic.ThrottlingQuotaST] - Parallel test count: 0
2022-03-28 11:16:01 [ForkJoinPool-1-worker-9] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testThrottlingQuotasDeleteTopic-FINISHED
2022-03-28 11:16:01 [ForkJoinPool-1-worker-9] INFO  [TestSeparator:30] ############################################################################
2022-03-28 11:16:01 [ForkJoinPool-1-worker-9] INFO  [ThrottlingQuotaST:353] Tearing down resources after all test
2022-03-28 11:16:01 [ForkJoinPool-1-worker-11] INFO  [TestSeparator:23] ############################################################################
2022-03-28 11:16:01 [ForkJoinPool-1-worker-11] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.topic.TopicST.testDeleteTopicEnableFalse-STARTED
2022-03-28 11:16:01 [ForkJoinPool-1-worker-11] DEBUG [AbstractST:658] ============================================================================
2022-03-28 11:16:01 [ForkJoinPool-1-worker-11] DEBUG [AbstractST:659] [operators.topic.TopicST - Before Each] - Setup test case environment
2022-03-28 11:16:01 [ForkJoinPool-1-worker-11] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 11:16:01 [ForkJoinPool-1-worker-11] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383, testThrottlingQuotasCreateAlterPartitions=my-cluster-28d9dd2e, testSendSimpleMessageTls=my-cluster-27ddd6ba, testConfigurationReflection=my-cluster-d1e2168e, testUserWithNameMoreThan64Chars=my-cluster-35e66bba, testDeployAndUnDeployCruiseControl=my-cluster-c0830cbe, testCapacityFile=my-cluster-fd0fb61a, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-66f23d43, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-f50d425d, testTlsExternalUser=my-cluster-3ba9cc5b, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5, testCreateTopicAfterUnsupportedOperation=my-cluster-b31d4894, testScramUserWithQuotas=my-cluster-4941482a, testTopicModificationOfReplicationFactor=my-cluster-38e659b2, testKafkaAdminTopicOperations=my-cluster-5a0bfe14, testConfigurationFileIsCreated=my-cluster-e0fac774, testCreateTopicViaKafka=my-cluster-d73fc0b6, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c, testThrottlingQuotasDeleteTopic=my-cluster-8f262de5, testUpdateUser=my-cluster-2f4b361c, testDeleteTopicEnableFalse=my-cluster-a0692855, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-da91577a, testTlsExternalUserWithQuotas=my-cluster-bb24d987, testTlsUserWithQuotas=my-cluster-d49e42e8, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8, testSendingMessagesToNonExistingTopic=my-cluster-241cc7c0, testReceiveSimpleMessageTls=my-cluster-c6a15963, testConfigurationPerformanceOptions=my-cluster-b6310693, testUserTemplate=my-cluster-3189676d, testMoreReplicasThanAvailableBrokers=my-cluster-59a64014}
2022-03-28 11:16:01 [ForkJoinPool-1-worker-11] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-1915916223-1265293973, testThrottlingQuotasCreateAlterPartitions=my-user-1933547616-1024431140, testSendSimpleMessageTls=my-user-738229993-1095121647, testConfigurationReflection=my-user-1445653023-93200027, testUserWithNameMoreThan64Chars=my-user-1703808150-314159031, testDeployAndUnDeployCruiseControl=my-user-1988711145-439861161, testCapacityFile=my-user-937015144-1959439396, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-1150765500-2076224625, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-745974575-1852384454, testTlsExternalUser=my-user-1382292264-1641298587, testSendSimpleMessageTlsScramSha=my-user-1499386234-1249154328, testCreateTopicAfterUnsupportedOperation=my-user-893702009-1709424309, testScramUserWithQuotas=my-user-797280497-2138800976, testTopicModificationOfReplicationFactor=my-user-2003504850-304348854, testKafkaAdminTopicOperations=my-user-1708656731-1087276375, testConfigurationFileIsCreated=my-user-180611180-50530491, testCreateTopicViaKafka=my-user-372778533-473457568, testCreatingUsersWithSecretPrefix=my-user-1450786684-1808961062, testThrottlingQuotasDeleteTopic=my-user-1558977140-777236615, testUpdateUser=my-user-2098198927-950610275, testDeleteTopicEnableFalse=my-user-1059904876-226844741, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-1260379615-1050949907, testTlsExternalUserWithQuotas=my-user-656620760-372265072, testTlsUserWithQuotas=my-user-811912745-1441809840, testThrottlingQuotasCreateTopic=my-user-1131806865-947217230, testSendingMessagesToNonExistingTopic=my-user-927451395-640848463, testReceiveSimpleMessageTls=my-user-6088058-1142028780, testConfigurationPerformanceOptions=my-user-930975101-640523779, testUserTemplate=my-user-1919254789-1384584070, testMoreReplicasThanAvailableBrokers=my-user-1511857485-1227147676}
2022-03-28 11:16:01 [ForkJoinPool-1-worker-11] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-1834944466-2094449929, testThrottlingQuotasCreateAlterPartitions=my-topic-609849288-1810034325, testSendSimpleMessageTls=my-topic-563023482-1697359353, testConfigurationReflection=my-topic-640815392-1092406112, testUserWithNameMoreThan64Chars=my-topic-2021464661-1313035472, testDeployAndUnDeployCruiseControl=my-topic-1511079187-95055629, testCapacityFile=my-topic-1155641705-902584520, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-842198221-1227962115, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-254848724-189700675, testTlsExternalUser=my-topic-5642373-135861890, testSendSimpleMessageTlsScramSha=my-topic-1028951580-1656411053, testCreateTopicAfterUnsupportedOperation=my-topic-1151452928-1452134654, testScramUserWithQuotas=my-topic-2004840350-265526537, testTopicModificationOfReplicationFactor=my-topic-1789870897-1466152225, testKafkaAdminTopicOperations=my-topic-1564170108-1493323202, testConfigurationFileIsCreated=my-topic-1296407588-313880708, testCreateTopicViaKafka=my-topic-1960957209-999941425, testCreatingUsersWithSecretPrefix=my-topic-1479520843-618891199, testThrottlingQuotasDeleteTopic=my-topic-2128658681-1688111273, testUpdateUser=my-topic-1741945237-916990305, testDeleteTopicEnableFalse=my-topic-49240108-312582418, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-1674392683-1637010870, testTlsExternalUserWithQuotas=my-topic-1953803944-1840309972, testTlsUserWithQuotas=my-topic-418284173-1689090259, testThrottlingQuotasCreateTopic=my-topic-728672490-1194703402, testSendingMessagesToNonExistingTopic=my-topic-645675602-646682485, testReceiveSimpleMessageTls=my-topic-478514536-2132782608, testConfigurationPerformanceOptions=my-topic-213912446-1165431808, testUserTemplate=my-topic-2114982563-185666735, testMoreReplicasThanAvailableBrokers=my-topic-1591366435-591944736}
2022-03-28 11:16:01 [ForkJoinPool-1-worker-11] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-28d9dd2e-kafka-clients, testSendSimpleMessageTls=my-cluster-27ddd6ba-kafka-clients, testConfigurationReflection=my-cluster-d1e2168e-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-35e66bba-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-c0830cbe-kafka-clients, testCapacityFile=my-cluster-fd0fb61a-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-66f23d43-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-f50d425d-kafka-clients, testTlsExternalUser=my-cluster-3ba9cc5b-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5-kafka-clients, testCreateTopicAfterUnsupportedOperation=my-cluster-b31d4894-kafka-clients, testScramUserWithQuotas=my-cluster-4941482a-kafka-clients, testTopicModificationOfReplicationFactor=my-cluster-38e659b2-kafka-clients, testKafkaAdminTopicOperations=my-cluster-5a0bfe14-kafka-clients, testConfigurationFileIsCreated=my-cluster-e0fac774-kafka-clients, testCreateTopicViaKafka=my-cluster-d73fc0b6-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-8f262de5-kafka-clients, testUpdateUser=my-cluster-2f4b361c-kafka-clients, testDeleteTopicEnableFalse=my-cluster-a0692855-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-da91577a-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-bb24d987-kafka-clients, testTlsUserWithQuotas=my-cluster-d49e42e8-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8-kafka-clients, testSendingMessagesToNonExistingTopic=my-cluster-241cc7c0-kafka-clients, testReceiveSimpleMessageTls=my-cluster-c6a15963-kafka-clients, testConfigurationPerformanceOptions=my-cluster-b6310693-kafka-clients, testUserTemplate=my-cluster-3189676d-kafka-clients, testMoreReplicasThanAvailableBrokers=my-cluster-59a64014-kafka-clients}
2022-03-28 11:16:01 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-a0692855-isolated in namespace topic-st
2022-03-28 11:16:01 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-a0692855-isolated
2022-03-28 11:16:01 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-a0692855-isolated will have desired state: Ready
2022-03-28 11:16:01 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-a0692855-isolated will have desired state: Ready
2022-03-28 11:16:01 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (839984ms till timeout)
2022-03-28 11:16:01 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-10
2022-03-28 11:16:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-10
2022-03-28 11:16:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:02 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-115
2022-03-28 11:16:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-115
2022-03-28 11:16:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:02 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-127
2022-03-28 11:16:02 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (838974ms till timeout)
2022-03-28 11:16:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-127
2022-03-28 11:16:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:03 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-14
2022-03-28 11:16:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-14
2022-03-28 11:16:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:03 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-141
2022-03-28 11:16:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-141
2022-03-28 11:16:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:03 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-143
2022-03-28 11:16:03 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (837943ms till timeout)
2022-03-28 11:16:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-143
2022-03-28 11:16:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:04 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-181
2022-03-28 11:16:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-181
2022-03-28 11:16:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:04 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-229
2022-03-28 11:16:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-229
2022-03-28 11:16:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:04 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-248
2022-03-28 11:16:04 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (836900ms till timeout)
2022-03-28 11:16:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-248
2022-03-28 11:16:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:04 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-266
2022-03-28 11:16:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-266
2022-03-28 11:16:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:05 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-278
2022-03-28 11:16:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-278
2022-03-28 11:16:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:05 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-303
2022-03-28 11:16:05 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (835855ms till timeout)
2022-03-28 11:16:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-303
2022-03-28 11:16:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:05 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-359
2022-03-28 11:16:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-359
2022-03-28 11:16:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:06 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-377
2022-03-28 11:16:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-377
2022-03-28 11:16:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:06 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-390
2022-03-28 11:16:06 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (834790ms till timeout)
2022-03-28 11:16:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-390
2022-03-28 11:16:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:07 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-391
2022-03-28 11:16:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-391
2022-03-28 11:16:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:07 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-420
2022-03-28 11:16:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-420
2022-03-28 11:16:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:07 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-425
2022-03-28 11:16:08 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (833720ms till timeout)
2022-03-28 11:16:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-425
2022-03-28 11:16:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:08 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-426
2022-03-28 11:16:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-426
2022-03-28 11:16:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:08 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-453
2022-03-28 11:16:09 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (832678ms till timeout)
2022-03-28 11:16:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-453
2022-03-28 11:16:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:09 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-467
2022-03-28 11:16:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-467
2022-03-28 11:16:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:09 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-47
2022-03-28 11:16:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-47
2022-03-28 11:16:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:09 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-478
2022-03-28 11:16:10 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (831657ms till timeout)
2022-03-28 11:16:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-478
2022-03-28 11:16:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:10 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-489
2022-03-28 11:16:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-489
2022-03-28 11:16:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:10 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-58
2022-03-28 11:16:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-58
2022-03-28 11:16:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:10 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-64
2022-03-28 11:16:11 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (830602ms till timeout)
2022-03-28 11:16:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-64
2022-03-28 11:16:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:11 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-76
2022-03-28 11:16:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-76
2022-03-28 11:16:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:11 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-93
2022-03-28 11:16:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-93
2022-03-28 11:16:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:11 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-103
2022-03-28 11:16:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-103
2022-03-28 11:16:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:12 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-104
2022-03-28 11:16:12 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (829464ms till timeout)
2022-03-28 11:16:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-104
2022-03-28 11:16:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:12 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-130
2022-03-28 11:16:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-130
2022-03-28 11:16:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:12 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-131
2022-03-28 11:16:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-131
2022-03-28 11:16:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:12 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-132
2022-03-28 11:16:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-132
2022-03-28 11:16:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:13 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-133
2022-03-28 11:16:13 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (828446ms till timeout)
2022-03-28 11:16:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-133
2022-03-28 11:16:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:13 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-134
2022-03-28 11:16:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-134
2022-03-28 11:16:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:13 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-135
2022-03-28 11:16:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-135
2022-03-28 11:16:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:14 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-136
2022-03-28 11:16:14 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (827400ms till timeout)
2022-03-28 11:16:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-136
2022-03-28 11:16:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:14 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-137
2022-03-28 11:16:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-137
2022-03-28 11:16:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:15 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-138
2022-03-28 11:16:15 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (826339ms till timeout)
2022-03-28 11:16:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-138
2022-03-28 11:16:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:15 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-139
2022-03-28 11:16:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-139
2022-03-28 11:16:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:16 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-140
2022-03-28 11:16:16 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (825283ms till timeout)
2022-03-28 11:16:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-140
2022-03-28 11:16:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:16 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-141
2022-03-28 11:16:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-141
2022-03-28 11:16:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:17 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-142
2022-03-28 11:16:17 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (824227ms till timeout)
2022-03-28 11:16:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-142
2022-03-28 11:16:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:17 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-143
2022-03-28 11:16:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-143
2022-03-28 11:16:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:18 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-144
2022-03-28 11:16:18 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (823148ms till timeout)
2022-03-28 11:16:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-144
2022-03-28 11:16:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:18 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-145
2022-03-28 11:16:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-145
2022-03-28 11:16:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:19 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-146
2022-03-28 11:16:19 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (822110ms till timeout)
2022-03-28 11:16:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-146
2022-03-28 11:16:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:19 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-147
2022-03-28 11:16:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-147
2022-03-28 11:16:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:20 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-148
2022-03-28 11:16:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-148
2022-03-28 11:16:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:20 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-149
2022-03-28 11:16:20 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (821082ms till timeout)
2022-03-28 11:16:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-149
2022-03-28 11:16:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:20 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-150
2022-03-28 11:16:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-150
2022-03-28 11:16:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:21 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-151
2022-03-28 11:16:21 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (820024ms till timeout)
2022-03-28 11:16:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-151
2022-03-28 11:16:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:21 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-152
2022-03-28 11:16:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-152
2022-03-28 11:16:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:22 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-153
2022-03-28 11:16:22 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (818880ms till timeout)
2022-03-28 11:16:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-153
2022-03-28 11:16:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:22 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-154
2022-03-28 11:16:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-154
2022-03-28 11:16:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:23 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-155
2022-03-28 11:16:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-155
2022-03-28 11:16:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:23 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-156
2022-03-28 11:16:23 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (817848ms till timeout)
2022-03-28 11:16:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-156
2022-03-28 11:16:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:24 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-157
2022-03-28 11:16:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-157
2022-03-28 11:16:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:24 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-158
2022-03-28 11:16:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-158
2022-03-28 11:16:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:24 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-159
2022-03-28 11:16:24 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (816753ms till timeout)
2022-03-28 11:16:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-159
2022-03-28 11:16:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:25 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-160
2022-03-28 11:16:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-160
2022-03-28 11:16:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:25 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-161
2022-03-28 11:16:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-161
2022-03-28 11:16:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:25 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-162
2022-03-28 11:16:26 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (815686ms till timeout)
2022-03-28 11:16:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-162
2022-03-28 11:16:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:26 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-163
2022-03-28 11:16:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-163
2022-03-28 11:16:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:26 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-164
2022-03-28 11:16:27 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (814599ms till timeout)
2022-03-28 11:16:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-164
2022-03-28 11:16:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:27 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-165
2022-03-28 11:16:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-165
2022-03-28 11:16:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:27 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-166
2022-03-28 11:16:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-166
2022-03-28 11:16:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:28 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-167
2022-03-28 11:16:28 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (813510ms till timeout)
2022-03-28 11:16:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-167
2022-03-28 11:16:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:28 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-168
2022-03-28 11:16:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-168
2022-03-28 11:16:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:28 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-169
2022-03-28 11:16:29 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (812419ms till timeout)
2022-03-28 11:16:30 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (811322ms till timeout)
2022-03-28 11:16:31 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (810268ms till timeout)
2022-03-28 11:16:32 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (809210ms till timeout)
2022-03-28 11:16:33 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (807974ms till timeout)
2022-03-28 11:16:34 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (806845ms till timeout)
2022-03-28 11:16:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-169
2022-03-28 11:16:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:34 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-170
2022-03-28 11:16:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-170
2022-03-28 11:16:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:35 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-171
2022-03-28 11:16:35 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (805781ms till timeout)
2022-03-28 11:16:36 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (804728ms till timeout)
2022-03-28 11:16:38 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (803662ms till timeout)
2022-03-28 11:16:39 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (802605ms till timeout)
2022-03-28 11:16:40 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (801497ms till timeout)
2022-03-28 11:16:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-171
2022-03-28 11:16:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:40 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-172
2022-03-28 11:16:41 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (800466ms till timeout)
2022-03-28 11:16:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-172
2022-03-28 11:16:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:41 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-173
2022-03-28 11:16:42 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (799391ms till timeout)
2022-03-28 11:16:43 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (798364ms till timeout)
2022-03-28 11:16:44 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (797126ms till timeout)
2022-03-28 11:16:45 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (796020ms till timeout)
2022-03-28 11:16:46 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (794939ms till timeout)
2022-03-28 11:16:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-173
2022-03-28 11:16:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:46 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-174
2022-03-28 11:16:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-174
2022-03-28 11:16:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:47 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-175
2022-03-28 11:16:47 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (793866ms till timeout)
2022-03-28 11:16:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-175
2022-03-28 11:16:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:48 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-176
2022-03-28 11:16:48 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (792847ms till timeout)
2022-03-28 11:16:49 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (791808ms till timeout)
2022-03-28 11:16:50 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (790777ms till timeout)
2022-03-28 11:16:51 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (789743ms till timeout)
2022-03-28 11:16:53 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (788687ms till timeout)
2022-03-28 11:16:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-176
2022-03-28 11:16:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:54 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-177
2022-03-28 11:16:54 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (787668ms till timeout)
2022-03-28 11:16:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-177
2022-03-28 11:16:54 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:54 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-178
2022-03-28 11:16:55 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (786539ms till timeout)
2022-03-28 11:16:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-178
2022-03-28 11:16:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:55 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-179
2022-03-28 11:16:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-179
2022-03-28 11:16:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:16:55 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-180
2022-03-28 11:16:56 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (785514ms till timeout)
2022-03-28 11:16:57 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (784462ms till timeout)
2022-03-28 11:16:58 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (783424ms till timeout)
2022-03-28 11:16:59 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (782364ms till timeout)
2022-03-28 11:17:00 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (781333ms till timeout)
2022-03-28 11:17:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-180
2022-03-28 11:17:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:01 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-181
2022-03-28 11:17:01 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (780279ms till timeout)
2022-03-28 11:17:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-181
2022-03-28 11:17:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:01 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-182
2022-03-28 11:17:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-182
2022-03-28 11:17:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:01 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-183
2022-03-28 11:17:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-183
2022-03-28 11:17:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:02 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-184
2022-03-28 11:17:02 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (779198ms till timeout)
2022-03-28 11:17:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-184
2022-03-28 11:17:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:02 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-185
2022-03-28 11:17:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-185
2022-03-28 11:17:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:03 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-186
2022-03-28 11:17:03 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (778127ms till timeout)
2022-03-28 11:17:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-186
2022-03-28 11:17:03 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:03 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-187
2022-03-28 11:17:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-187
2022-03-28 11:17:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:04 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-188
2022-03-28 11:17:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-188
2022-03-28 11:17:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:04 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-189
2022-03-28 11:17:04 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (777063ms till timeout)
2022-03-28 11:17:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-189
2022-03-28 11:17:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:04 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-190
2022-03-28 11:17:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-190
2022-03-28 11:17:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:05 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-191
2022-03-28 11:17:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-191
2022-03-28 11:17:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:05 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-192
2022-03-28 11:17:05 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (775902ms till timeout)
2022-03-28 11:17:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-192
2022-03-28 11:17:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:06 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-193
2022-03-28 11:17:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-193
2022-03-28 11:17:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:06 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-194
2022-03-28 11:17:06 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (774853ms till timeout)
2022-03-28 11:17:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-194
2022-03-28 11:17:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:06 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-195
2022-03-28 11:17:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-195
2022-03-28 11:17:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:07 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-196
2022-03-28 11:17:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-196
2022-03-28 11:17:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:07 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-197
2022-03-28 11:17:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-197
2022-03-28 11:17:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:07 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-198
2022-03-28 11:17:07 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (773793ms till timeout)
2022-03-28 11:17:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-198
2022-03-28 11:17:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:08 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-199
2022-03-28 11:17:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-199
2022-03-28 11:17:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:08 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-200
2022-03-28 11:17:08 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (772759ms till timeout)
2022-03-28 11:17:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-200
2022-03-28 11:17:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:09 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-201
2022-03-28 11:17:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-201
2022-03-28 11:17:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:09 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-202
2022-03-28 11:17:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-202
2022-03-28 11:17:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:09 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-203
2022-03-28 11:17:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-203
2022-03-28 11:17:09 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:09 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-204
2022-03-28 11:17:10 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (771701ms till timeout)
2022-03-28 11:17:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-204
2022-03-28 11:17:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:10 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-205
2022-03-28 11:17:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-205
2022-03-28 11:17:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:10 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-206
2022-03-28 11:17:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-206
2022-03-28 11:17:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:10 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-207
2022-03-28 11:17:11 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (770675ms till timeout)
2022-03-28 11:17:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-207
2022-03-28 11:17:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:11 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-219
2022-03-28 11:17:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-219
2022-03-28 11:17:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:11 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-225
2022-03-28 11:17:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-225
2022-03-28 11:17:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:11 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-250
2022-03-28 11:17:12 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (769655ms till timeout)
2022-03-28 11:17:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-250
2022-03-28 11:17:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:12 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-251
2022-03-28 11:17:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-251
2022-03-28 11:17:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:12 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-252
2022-03-28 11:17:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-252
2022-03-28 11:17:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:12 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-253
2022-03-28 11:17:13 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (768615ms till timeout)
2022-03-28 11:17:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-253
2022-03-28 11:17:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:13 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-254
2022-03-28 11:17:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-254
2022-03-28 11:17:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:13 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-255
2022-03-28 11:17:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-255
2022-03-28 11:17:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:13 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-256
2022-03-28 11:17:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-256
2022-03-28 11:17:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:14 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-257
2022-03-28 11:17:14 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (767593ms till timeout)
2022-03-28 11:17:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-257
2022-03-28 11:17:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:14 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-258
2022-03-28 11:17:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-258
2022-03-28 11:17:14 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:14 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-259
2022-03-28 11:17:15 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (766516ms till timeout)
2022-03-28 11:17:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-259
2022-03-28 11:17:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:15 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-260
2022-03-28 11:17:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-260
2022-03-28 11:17:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:15 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-261
2022-03-28 11:17:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-261
2022-03-28 11:17:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:16 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-262
2022-03-28 11:17:16 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (765422ms till timeout)
2022-03-28 11:17:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-262
2022-03-28 11:17:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:16 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-263
2022-03-28 11:17:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-263
2022-03-28 11:17:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:16 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-264
2022-03-28 11:17:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-264
2022-03-28 11:17:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:17 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-265
2022-03-28 11:17:17 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (764358ms till timeout)
2022-03-28 11:17:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-265
2022-03-28 11:17:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:17 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-266
2022-03-28 11:17:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-266
2022-03-28 11:17:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:17 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-267
2022-03-28 11:17:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-267
2022-03-28 11:17:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:18 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-268
2022-03-28 11:17:18 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (763325ms till timeout)
2022-03-28 11:17:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-268
2022-03-28 11:17:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:18 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-269
2022-03-28 11:17:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-269
2022-03-28 11:17:18 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:18 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-270
2022-03-28 11:17:19 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (762249ms till timeout)
2022-03-28 11:17:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-270
2022-03-28 11:17:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:19 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-271
2022-03-28 11:17:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-271
2022-03-28 11:17:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:19 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-272
2022-03-28 11:17:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-272
2022-03-28 11:17:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:20 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-273
2022-03-28 11:17:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-273
2022-03-28 11:17:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:20 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-274
2022-03-28 11:17:20 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (761192ms till timeout)
2022-03-28 11:17:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-274
2022-03-28 11:17:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:20 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-275
2022-03-28 11:17:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-275
2022-03-28 11:17:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:21 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-276
2022-03-28 11:17:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-276
2022-03-28 11:17:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:21 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-277
2022-03-28 11:17:21 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (760142ms till timeout)
2022-03-28 11:17:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-277
2022-03-28 11:17:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:21 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-278
2022-03-28 11:17:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-278
2022-03-28 11:17:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:22 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-279
2022-03-28 11:17:22 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (759055ms till timeout)
2022-03-28 11:17:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-279
2022-03-28 11:17:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:22 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-280
2022-03-28 11:17:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-280
2022-03-28 11:17:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:23 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-281
2022-03-28 11:17:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-281
2022-03-28 11:17:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:23 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-282
2022-03-28 11:17:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-282
2022-03-28 11:17:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:23 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-283
2022-03-28 11:17:23 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (758014ms till timeout)
2022-03-28 11:17:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-283
2022-03-28 11:17:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:23 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-284
2022-03-28 11:17:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-284
2022-03-28 11:17:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:24 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-285
2022-03-28 11:17:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-285
2022-03-28 11:17:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:24 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-286
2022-03-28 11:17:24 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (756997ms till timeout)
2022-03-28 11:17:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-286
2022-03-28 11:17:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:24 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-287
2022-03-28 11:17:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-287
2022-03-28 11:17:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:25 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-288
2022-03-28 11:17:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-288
2022-03-28 11:17:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:25 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-289
2022-03-28 11:17:25 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (755954ms till timeout)
2022-03-28 11:17:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-289
2022-03-28 11:17:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:25 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-290
2022-03-28 11:17:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-290
2022-03-28 11:17:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:26 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-291
2022-03-28 11:17:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-291
2022-03-28 11:17:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:26 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-292
2022-03-28 11:17:26 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (754829ms till timeout)
2022-03-28 11:17:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-292
2022-03-28 11:17:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:26 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-293
2022-03-28 11:17:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-293
2022-03-28 11:17:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:27 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-294
2022-03-28 11:17:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-294
2022-03-28 11:17:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:27 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-295
2022-03-28 11:17:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-295
2022-03-28 11:17:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:27 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-296
2022-03-28 11:17:27 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (753769ms till timeout)
2022-03-28 11:17:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-296
2022-03-28 11:17:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:28 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-297
2022-03-28 11:17:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-297
2022-03-28 11:17:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:28 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-298
2022-03-28 11:17:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-298
2022-03-28 11:17:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:28 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-299
2022-03-28 11:17:28 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (752747ms till timeout)
2022-03-28 11:17:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-299
2022-03-28 11:17:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:29 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-300
2022-03-28 11:17:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-300
2022-03-28 11:17:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:29 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-301
2022-03-28 11:17:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-301
2022-03-28 11:17:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:29 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-302
2022-03-28 11:17:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-302
2022-03-28 11:17:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:29 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-303
2022-03-28 11:17:30 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (751715ms till timeout)
2022-03-28 11:17:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-303
2022-03-28 11:17:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:30 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-304
2022-03-28 11:17:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-304
2022-03-28 11:17:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:30 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-305
2022-03-28 11:17:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-305
2022-03-28 11:17:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:30 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-306
2022-03-28 11:17:31 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (750688ms till timeout)
2022-03-28 11:17:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-306
2022-03-28 11:17:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:31 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-307
2022-03-28 11:17:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-307
2022-03-28 11:17:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:31 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-308
2022-03-28 11:17:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-308
2022-03-28 11:17:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:31 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-309
2022-03-28 11:17:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-309
2022-03-28 11:17:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:31 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-310
2022-03-28 11:17:32 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (749611ms till timeout)
2022-03-28 11:17:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-310
2022-03-28 11:17:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:32 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-311
2022-03-28 11:17:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-311
2022-03-28 11:17:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:32 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-312
2022-03-28 11:17:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-312
2022-03-28 11:17:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:32 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-313
2022-03-28 11:17:33 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (748559ms till timeout)
2022-03-28 11:17:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-313
2022-03-28 11:17:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:33 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-314
2022-03-28 11:17:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-314
2022-03-28 11:17:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:33 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-315
2022-03-28 11:17:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-315
2022-03-28 11:17:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:33 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-316
2022-03-28 11:17:34 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (747502ms till timeout)
2022-03-28 11:17:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-316
2022-03-28 11:17:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:34 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-317
2022-03-28 11:17:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-317
2022-03-28 11:17:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:34 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-318
2022-03-28 11:17:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-318
2022-03-28 11:17:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:34 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-319
2022-03-28 11:17:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-319
2022-03-28 11:17:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:35 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-320
2022-03-28 11:17:35 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (746466ms till timeout)
2022-03-28 11:17:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-320
2022-03-28 11:17:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:35 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-321
2022-03-28 11:17:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-321
2022-03-28 11:17:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:35 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-322
2022-03-28 11:17:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-322
2022-03-28 11:17:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:36 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-323
2022-03-28 11:17:36 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (745371ms till timeout)
2022-03-28 11:17:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-323
2022-03-28 11:17:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:36 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-324
2022-03-28 11:17:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-324
2022-03-28 11:17:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:36 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-325
2022-03-28 11:17:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-325
2022-03-28 11:17:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:37 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-326
2022-03-28 11:17:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-326
2022-03-28 11:17:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:37 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-327
2022-03-28 11:17:37 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: my-cluster-a0692855-isolated will have desired state: Ready not ready, will try again in 1000 ms (744206ms till timeout)
2022-03-28 11:17:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-327
2022-03-28 11:17:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:37 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-328
2022-03-28 11:17:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-328
2022-03-28 11:17:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:38 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-345
2022-03-28 11:17:38 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:444] Kafka: my-cluster-a0692855-isolated is in desired state: Ready
2022-03-28 11:17:38 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update Deployment my-cluster-a0692855-isolated-kafka-clients in namespace topic-st
2022-03-28 11:17:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-345
2022-03-28 11:17:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:38 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-346
2022-03-28 11:17:38 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-a0692855-isolated-kafka-clients
2022-03-28 11:17:38 [ForkJoinPool-1-worker-11] INFO  [DeploymentUtils:161] Wait for Deployment: my-cluster-a0692855-isolated-kafka-clients will be ready
2022-03-28 11:17:38 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Wait for Deployment: my-cluster-a0692855-isolated-kafka-clients will be ready
2022-03-28 11:17:38 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: my-cluster-a0692855-isolated-kafka-clients will be ready not ready, will try again in 1000 ms (479967ms till timeout)
2022-03-28 11:17:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-346
2022-03-28 11:17:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:38 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-370
2022-03-28 11:17:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-370
2022-03-28 11:17:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:39 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-371
2022-03-28 11:17:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-371
2022-03-28 11:17:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:39 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-372
2022-03-28 11:17:39 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: my-cluster-a0692855-isolated-kafka-clients will be ready not ready, will try again in 1000 ms (478936ms till timeout)
2022-03-28 11:17:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-372
2022-03-28 11:17:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:39 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-373
2022-03-28 11:17:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-373
2022-03-28 11:17:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:40 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-374
2022-03-28 11:17:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-374
2022-03-28 11:17:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:40 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-375
2022-03-28 11:17:40 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: my-cluster-a0692855-isolated-kafka-clients will be ready not ready, will try again in 1000 ms (477866ms till timeout)
2022-03-28 11:17:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-375
2022-03-28 11:17:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:40 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-376
2022-03-28 11:17:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-376
2022-03-28 11:17:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:41 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-377
2022-03-28 11:17:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-377
2022-03-28 11:17:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:41 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-378
2022-03-28 11:17:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-378
2022-03-28 11:17:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:41 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-379
2022-03-28 11:17:41 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: my-cluster-a0692855-isolated-kafka-clients will be ready not ready, will try again in 1000 ms (476825ms till timeout)
2022-03-28 11:17:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-379
2022-03-28 11:17:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:42 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-380
2022-03-28 11:17:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-380
2022-03-28 11:17:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:42 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-381
2022-03-28 11:17:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-381
2022-03-28 11:17:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:42 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-382
2022-03-28 11:17:42 [ForkJoinPool-1-worker-11] INFO  [DeploymentUtils:168] Deployment: my-cluster-a0692855-isolated-kafka-clients is ready
2022-03-28 11:17:43 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update KafkaTopic my-topic-49240108-312582418 in namespace topic-st
2022-03-28 11:17:43 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-49240108-312582418
2022-03-28 11:17:43 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-49240108-312582418 will have desired state: Ready
2022-03-28 11:17:43 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-49240108-312582418 will have desired state: Ready
2022-03-28 11:17:43 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] KafkaTopic: my-topic-49240108-312582418 will have desired state: Ready not ready, will try again in 1000 ms (179975ms till timeout)
2022-03-28 11:17:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-382
2022-03-28 11:17:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:43 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-383
2022-03-28 11:17:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-383
2022-03-28 11:17:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:43 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-384
2022-03-28 11:17:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-384
2022-03-28 11:17:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:43 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-385
2022-03-28 11:17:44 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:444] KafkaTopic: my-topic-49240108-312582418 is in desired state: Ready
2022-03-28 11:17:44 [ForkJoinPool-1-worker-11] INFO  [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-03-28 11:17:44 [ForkJoinPool-1-worker-11] DEBUG [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@39b91193, which are set.
2022-03-28 11:17:44 [ForkJoinPool-1-worker-11] INFO  [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@594bdf06, messages=[], arguments=[--bootstrap-server, my-cluster-a0692855-isolated-kafka-bootstrap.topic-st.svc:9092, --max-messages, 100, --topic, my-topic-49240108-312582418], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-a0692855-isolated-kafka-clients-68b5f55779-pfp8s', podNamespace='topic-st', bootstrapServer='my-cluster-a0692855-isolated-kafka-bootstrap.topic-st.svc:9092', topicName='my-topic-49240108-312582418', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@39b91193}
2022-03-28 11:17:44 [ForkJoinPool-1-worker-11] INFO  [InternalKafkaClient:94] Producing 100 messages to my-cluster-a0692855-isolated-kafka-bootstrap.topic-st.svc:9092:my-topic-49240108-312582418 from pod my-cluster-a0692855-isolated-kafka-clients-68b5f55779-pfp8s
2022-03-28 11:17:44 [ForkJoinPool-1-worker-11] INFO  [VerifiableClient:192] Client command: oc exec my-cluster-a0692855-isolated-kafka-clients-68b5f55779-pfp8s -n topic-st -- /opt/kafka/producer.sh --bootstrap-server my-cluster-a0692855-isolated-kafka-bootstrap.topic-st.svc:9092 --max-messages 100 --topic my-topic-49240108-312582418
2022-03-28 11:17:44 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc exec my-cluster-a0692855-isolated-kafka-clients-68b5f55779-pfp8s -n topic-st -- /opt/kafka/producer.sh --bootstrap-server my-cluster-a0692855-isolated-kafka-bootstrap.topic-st.svc:9092 --max-messages 100 --topic my-topic-49240108-312582418
2022-03-28 11:17:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-385
2022-03-28 11:17:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:44 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-386
2022-03-28 11:17:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-386
2022-03-28 11:17:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:44 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-387
2022-03-28 11:17:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-387
2022-03-28 11:17:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:45 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-388
2022-03-28 11:17:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-388
2022-03-28 11:17:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:45 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-389
2022-03-28 11:17:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-389
2022-03-28 11:17:45 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:45 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-390
2022-03-28 11:17:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-390
2022-03-28 11:17:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:46 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-391
2022-03-28 11:17:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-391
2022-03-28 11:17:46 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:46 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-392
2022-03-28 11:17:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-392
2022-03-28 11:17:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:47 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-393
2022-03-28 11:17:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-393
2022-03-28 11:17:47 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:47 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-394
2022-03-28 11:17:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-394
2022-03-28 11:17:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:48 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-395
2022-03-28 11:17:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-395
2022-03-28 11:17:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:48 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-396
2022-03-28 11:17:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-396
2022-03-28 11:17:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:49 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-397
2022-03-28 11:17:49 [ForkJoinPool-1-worker-11] INFO  [InternalKafkaClient:97] Producer finished correctly: true
2022-03-28 11:17:49 [ForkJoinPool-1-worker-11] INFO  [InternalKafkaClient:101] Producer produced 100 messages
2022-03-28 11:17:49 [ForkJoinPool-1-worker-11] INFO  [TopicST:395] Deleting KafkaTopic: my-topic-49240108-312582418
2022-03-28 11:17:49 [ForkJoinPool-1-worker-11] INFO  [TopicST:397] KafkaTopic my-topic-49240108-312582418 deleted
2022-03-28 11:17:49 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Topic my-topic-49240108-312582418 has rolled
2022-03-28 11:17:49 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (299956ms till timeout)
2022-03-28 11:17:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-397
2022-03-28 11:17:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:49 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-398
2022-03-28 11:17:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-398
2022-03-28 11:17:49 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:49 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-399
2022-03-28 11:17:50 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (298922ms till timeout)
2022-03-28 11:17:51 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (297890ms till timeout)
2022-03-28 11:17:52 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (296818ms till timeout)
2022-03-28 11:17:53 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (295763ms till timeout)
2022-03-28 11:17:54 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (294665ms till timeout)
2022-03-28 11:17:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-399
2022-03-28 11:17:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:55 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-406
2022-03-28 11:17:55 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (293615ms till timeout)
2022-03-28 11:17:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-406
2022-03-28 11:17:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:56 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-407
2022-03-28 11:17:56 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (292519ms till timeout)
2022-03-28 11:17:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-407
2022-03-28 11:17:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:56 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-408
2022-03-28 11:17:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-408
2022-03-28 11:17:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:17:57 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-409
2022-03-28 11:17:57 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (291421ms till timeout)
2022-03-28 11:17:58 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (290390ms till timeout)
2022-03-28 11:17:59 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (289325ms till timeout)
2022-03-28 11:18:00 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (288303ms till timeout)
2022-03-28 11:18:01 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (287282ms till timeout)
2022-03-28 11:18:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-409
2022-03-28 11:18:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:02 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-410
2022-03-28 11:18:02 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (286251ms till timeout)
2022-03-28 11:18:03 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (285198ms till timeout)
2022-03-28 11:18:04 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (284104ms till timeout)
2022-03-28 11:18:06 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (283038ms till timeout)
2022-03-28 11:18:07 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (282004ms till timeout)
2022-03-28 11:18:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-410
2022-03-28 11:18:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:08 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-411
2022-03-28 11:18:08 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (280831ms till timeout)
2022-03-28 11:18:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-411
2022-03-28 11:18:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:08 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-412
2022-03-28 11:18:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-412
2022-03-28 11:18:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:08 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-413
2022-03-28 11:18:09 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (279799ms till timeout)
2022-03-28 11:18:10 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (278753ms till timeout)
2022-03-28 11:18:11 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (277704ms till timeout)
2022-03-28 11:18:12 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (276639ms till timeout)
2022-03-28 11:18:13 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (275612ms till timeout)
2022-03-28 11:18:14 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (274537ms till timeout)
2022-03-28 11:18:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-413
2022-03-28 11:18:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:15 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-414
2022-03-28 11:18:15 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (273497ms till timeout)
2022-03-28 11:18:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-414
2022-03-28 11:18:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:15 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-415
2022-03-28 11:18:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-415
2022-03-28 11:18:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:16 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-416
2022-03-28 11:18:16 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (272453ms till timeout)
2022-03-28 11:18:17 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (271411ms till timeout)
2022-03-28 11:18:18 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (270325ms till timeout)
2022-03-28 11:18:19 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (269252ms till timeout)
2022-03-28 11:18:20 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (268202ms till timeout)
2022-03-28 11:18:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-416
2022-03-28 11:18:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:21 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-417
2022-03-28 11:18:21 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (267173ms till timeout)
2022-03-28 11:18:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-417
2022-03-28 11:18:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:22 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-418
2022-03-28 11:18:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-418
2022-03-28 11:18:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:22 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-419
2022-03-28 11:18:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-419
2022-03-28 11:18:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:22 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-420
2022-03-28 11:18:22 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (266153ms till timeout)
2022-03-28 11:18:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-420
2022-03-28 11:18:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:23 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-421
2022-03-28 11:18:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-421
2022-03-28 11:18:23 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:23 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-422
2022-03-28 11:18:23 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (265127ms till timeout)
2022-03-28 11:18:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-422
2022-03-28 11:18:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:24 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-423
2022-03-28 11:18:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-423
2022-03-28 11:18:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:24 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-424
2022-03-28 11:18:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-424
2022-03-28 11:18:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:24 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-425
2022-03-28 11:18:25 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (264064ms till timeout)
2022-03-28 11:18:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-425
2022-03-28 11:18:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:25 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-426
2022-03-28 11:18:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-426
2022-03-28 11:18:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:25 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-427
2022-03-28 11:18:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-427
2022-03-28 11:18:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:25 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-428
2022-03-28 11:18:26 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (263029ms till timeout)
2022-03-28 11:18:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-428
2022-03-28 11:18:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:26 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-429
2022-03-28 11:18:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-429
2022-03-28 11:18:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:26 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-430
2022-03-28 11:18:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-430
2022-03-28 11:18:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:26 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-431
2022-03-28 11:18:27 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (262002ms till timeout)
2022-03-28 11:18:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-431
2022-03-28 11:18:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:27 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-432
2022-03-28 11:18:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-432
2022-03-28 11:18:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:27 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-433
2022-03-28 11:18:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-433
2022-03-28 11:18:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:27 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-434
2022-03-28 11:18:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-434
2022-03-28 11:18:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:27 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-435
2022-03-28 11:18:28 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (260946ms till timeout)
2022-03-28 11:18:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-435
2022-03-28 11:18:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:28 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-436
2022-03-28 11:18:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-436
2022-03-28 11:18:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:28 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-437
2022-03-28 11:18:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-437
2022-03-28 11:18:28 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:28 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-438
2022-03-28 11:18:29 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (259865ms till timeout)
2022-03-28 11:18:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-438
2022-03-28 11:18:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:29 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-439
2022-03-28 11:18:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-439
2022-03-28 11:18:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:29 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-440
2022-03-28 11:18:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-440
2022-03-28 11:18:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:30 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-441
2022-03-28 11:18:30 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (258784ms till timeout)
2022-03-28 11:18:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-441
2022-03-28 11:18:30 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:30 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-442
2022-03-28 11:18:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-442
2022-03-28 11:18:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:31 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-443
2022-03-28 11:18:31 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (257734ms till timeout)
2022-03-28 11:18:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-443
2022-03-28 11:18:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:31 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-444
2022-03-28 11:18:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-444
2022-03-28 11:18:31 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:31 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-445
2022-03-28 11:18:32 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (256640ms till timeout)
2022-03-28 11:18:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-445
2022-03-28 11:18:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:32 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-446
2022-03-28 11:18:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-446
2022-03-28 11:18:32 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:32 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-447
2022-03-28 11:18:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-447
2022-03-28 11:18:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:33 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-448
2022-03-28 11:18:33 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (255587ms till timeout)
2022-03-28 11:18:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-448
2022-03-28 11:18:33 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:33 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-449
2022-03-28 11:18:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-449
2022-03-28 11:18:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:34 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-466
2022-03-28 11:18:34 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (254487ms till timeout)
2022-03-28 11:18:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-466
2022-03-28 11:18:34 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:34 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-467
2022-03-28 11:18:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-467
2022-03-28 11:18:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:35 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-490
2022-03-28 11:18:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-490
2022-03-28 11:18:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:35 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-491
2022-03-28 11:18:35 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (253452ms till timeout)
2022-03-28 11:18:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-491
2022-03-28 11:18:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:35 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-492
2022-03-28 11:18:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-492
2022-03-28 11:18:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:36 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-493
2022-03-28 11:18:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-493
2022-03-28 11:18:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:36 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-494
2022-03-28 11:18:36 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (252413ms till timeout)
2022-03-28 11:18:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-494
2022-03-28 11:18:36 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:36 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-495
2022-03-28 11:18:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-495
2022-03-28 11:18:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:37 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-496
2022-03-28 11:18:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-496
2022-03-28 11:18:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:37 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-497
2022-03-28 11:18:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-497
2022-03-28 11:18:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:37 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-498
2022-03-28 11:18:37 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (251360ms till timeout)
2022-03-28 11:18:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-498
2022-03-28 11:18:37 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:37 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-499
2022-03-28 11:18:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-499
2022-03-28 11:18:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:38 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-0
2022-03-28 11:18:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-0
2022-03-28 11:18:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:38 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-1
2022-03-28 11:18:38 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (250333ms till timeout)
2022-03-28 11:18:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-1
2022-03-28 11:18:38 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:38 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-10
2022-03-28 11:18:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-10
2022-03-28 11:18:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:39 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-11
2022-03-28 11:18:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-11
2022-03-28 11:18:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:39 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-12
2022-03-28 11:18:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-12
2022-03-28 11:18:39 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:39 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-13
2022-03-28 11:18:39 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (249317ms till timeout)
2022-03-28 11:18:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-13
2022-03-28 11:18:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:40 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-14
2022-03-28 11:18:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-14
2022-03-28 11:18:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:40 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-15
2022-03-28 11:18:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-15
2022-03-28 11:18:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:40 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-16
2022-03-28 11:18:40 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (248275ms till timeout)
2022-03-28 11:18:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-16
2022-03-28 11:18:40 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:40 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-17
2022-03-28 11:18:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-17
2022-03-28 11:18:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:41 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-18
2022-03-28 11:18:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-18
2022-03-28 11:18:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:41 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-19
2022-03-28 11:18:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-19
2022-03-28 11:18:41 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:41 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-2
2022-03-28 11:18:41 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (247249ms till timeout)
2022-03-28 11:18:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-2
2022-03-28 11:18:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:42 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-3
2022-03-28 11:18:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-3
2022-03-28 11:18:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:42 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-4
2022-03-28 11:18:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-4
2022-03-28 11:18:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:42 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-5
2022-03-28 11:18:42 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (246212ms till timeout)
2022-03-28 11:18:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-5
2022-03-28 11:18:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:42 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-6
2022-03-28 11:18:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-6
2022-03-28 11:18:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:43 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-7
2022-03-28 11:18:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-7
2022-03-28 11:18:43 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:43 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-8
2022-03-28 11:18:43 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (245164ms till timeout)
2022-03-28 11:18:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-8
2022-03-28 11:18:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:44 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-9
2022-03-28 11:18:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-9
2022-03-28 11:18:44 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:44 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:689] ============================================================================
2022-03-28 11:18:44 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:690] [operators.topic.ThrottlingQuotaST - After All] - Clean up after test suite
2022-03-28 11:18:44 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:344] ############################################################################
2022-03-28 11:18:44 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:348] Delete all resources for ThrottlingQuotaST
2022-03-28 11:18:44 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:241] Delete of Kafka quota-cluster in namespace throttling-quota-st
2022-03-28 11:18:44 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:quota-cluster
2022-03-28 11:18:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:quota-cluster not ready, will try again in 10000 ms (839966ms till timeout)
2022-03-28 11:18:45 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (244012ms till timeout)
2022-03-28 11:18:46 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (242863ms till timeout)
2022-03-28 11:18:47 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (241759ms till timeout)
2022-03-28 11:18:48 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (240685ms till timeout)
2022-03-28 11:18:49 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (239568ms till timeout)
2022-03-28 11:18:50 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (238539ms till timeout)
2022-03-28 11:18:51 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (237480ms till timeout)
2022-03-28 11:18:52 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (236405ms till timeout)
2022-03-28 11:18:53 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (235355ms till timeout)
2022-03-28 11:18:54 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:369] ############################################################################
2022-03-28 11:18:54 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Namespace throttling-quota-st removal
2022-03-28 11:18:54 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:18:54 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (234275ms till timeout)
2022-03-28 11:18:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:18:55 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Namespace throttling-quota-st removal not ready, will try again in 1000 ms (479685ms till timeout)
2022-03-28 11:18:55 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (233240ms till timeout)
2022-03-28 11:18:56 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:18:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:18:56 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Namespace throttling-quota-st removal not ready, will try again in 1000 ms (478343ms till timeout)
2022-03-28 11:18:56 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (232213ms till timeout)
2022-03-28 11:18:57 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:18:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:18:57 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Namespace throttling-quota-st removal not ready, will try again in 1000 ms (477032ms till timeout)
2022-03-28 11:18:57 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (231128ms till timeout)
2022-03-28 11:18:58 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:18:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:18:58 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:18:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Namespace throttling-quota-st removal not ready, will try again in 1000 ms (475829ms till timeout)
2022-03-28 11:18:58 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (230098ms till timeout)
2022-03-28 11:18:59 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:19:00 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (229024ms till timeout)
2022-03-28 11:19:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:19:00 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:19:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Namespace throttling-quota-st removal not ready, will try again in 1000 ms (474433ms till timeout)
2022-03-28 11:19:01 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (227947ms till timeout)
2022-03-28 11:19:01 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:19:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:19:01 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:19:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Namespace throttling-quota-st removal not ready, will try again in 1000 ms (473202ms till timeout)
2022-03-28 11:19:02 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (226900ms till timeout)
2022-03-28 11:19:02 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:19:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:19:02 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:19:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Namespace throttling-quota-st removal not ready, will try again in 1000 ms (471934ms till timeout)
2022-03-28 11:19:03 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (225812ms till timeout)
2022-03-28 11:19:03 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:19:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:19:04 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:19:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Namespace throttling-quota-st removal not ready, will try again in 1000 ms (470745ms till timeout)
2022-03-28 11:19:04 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (224760ms till timeout)
2022-03-28 11:19:05 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:19:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:19:05 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:19:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Namespace throttling-quota-st removal not ready, will try again in 1000 ms (469530ms till timeout)
2022-03-28 11:19:05 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (223728ms till timeout)
2022-03-28 11:19:06 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:19:06 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (222687ms till timeout)
2022-03-28 11:19:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:19:06 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:19:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Namespace throttling-quota-st removal not ready, will try again in 1000 ms (468324ms till timeout)
2022-03-28 11:19:07 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (221667ms till timeout)
2022-03-28 11:19:07 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:19:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:19:07 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:19:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Namespace throttling-quota-st removal not ready, will try again in 1000 ms (467053ms till timeout)
2022-03-28 11:19:08 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (220635ms till timeout)
2022-03-28 11:19:08 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:19:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:19:08 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:19:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Namespace throttling-quota-st removal not ready, will try again in 1000 ms (465843ms till timeout)
2022-03-28 11:19:09 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (219601ms till timeout)
2022-03-28 11:19:09 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:19:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:19:10 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:19:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Namespace throttling-quota-st removal not ready, will try again in 1000 ms (464509ms till timeout)
2022-03-28 11:19:10 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (218512ms till timeout)
2022-03-28 11:19:11 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:19:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:19:11 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:19:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Namespace throttling-quota-st removal not ready, will try again in 1000 ms (463246ms till timeout)
2022-03-28 11:19:11 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (217445ms till timeout)
2022-03-28 11:19:12 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:19:12 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (216383ms till timeout)
2022-03-28 11:19:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:19:12 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:19:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Namespace throttling-quota-st removal not ready, will try again in 1000 ms (462040ms till timeout)
2022-03-28 11:19:13 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (215340ms till timeout)
2022-03-28 11:19:13 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:19:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:19:13 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:19:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Namespace throttling-quota-st removal not ready, will try again in 1000 ms (460832ms till timeout)
2022-03-28 11:19:14 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (214316ms till timeout)
2022-03-28 11:19:14 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:19:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:19:15 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:19:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Namespace throttling-quota-st removal not ready, will try again in 1000 ms (459579ms till timeout)
2022-03-28 11:19:15 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (213280ms till timeout)
2022-03-28 11:19:16 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:19:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:19:16 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:19:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Namespace throttling-quota-st removal not ready, will try again in 1000 ms (458359ms till timeout)
2022-03-28 11:19:16 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (212237ms till timeout)
2022-03-28 11:19:17 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:19:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:19:17 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:19:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Namespace throttling-quota-st removal not ready, will try again in 1000 ms (457077ms till timeout)
2022-03-28 11:19:17 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (211213ms till timeout)
2022-03-28 11:19:18 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:19:18 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (210187ms till timeout)
2022-03-28 11:19:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:19:19 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:19:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Namespace throttling-quota-st removal not ready, will try again in 1000 ms (455790ms till timeout)
2022-03-28 11:19:19 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (209141ms till timeout)
2022-03-28 11:19:20 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:19:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:19:20 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:19:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Namespace throttling-quota-st removal not ready, will try again in 1000 ms (454477ms till timeout)
2022-03-28 11:19:21 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (208065ms till timeout)
2022-03-28 11:19:21 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:19:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:19:21 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:19:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Namespace throttling-quota-st removal not ready, will try again in 1000 ms (453261ms till timeout)
2022-03-28 11:19:22 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (207028ms till timeout)
2022-03-28 11:19:22 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:19:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:19:22 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:19:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Namespace throttling-quota-st removal not ready, will try again in 1000 ms (451979ms till timeout)
2022-03-28 11:19:23 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (205953ms till timeout)
2022-03-28 11:19:23 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:19:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:19:24 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:19:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Namespace throttling-quota-st removal not ready, will try again in 1000 ms (450712ms till timeout)
2022-03-28 11:19:24 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (204913ms till timeout)
2022-03-28 11:19:25 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:19:25 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (203885ms till timeout)
2022-03-28 11:19:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:19:25 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:19:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Namespace throttling-quota-st removal not ready, will try again in 1000 ms (449451ms till timeout)
2022-03-28 11:19:26 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (202841ms till timeout)
2022-03-28 11:19:26 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:19:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:19:26 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 11:19:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Namespace throttling-quota-st removal not ready, will try again in 1000 ms (448185ms till timeout)
2022-03-28 11:19:27 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (201793ms till timeout)
2022-03-28 11:19:27 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:19:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 11:19:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 1
2022-03-28 11:19:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 11:19:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Error from server (NotFound): namespaces "throttling-quota-st" not found
2022-03-28 11:19:27 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] ======STDERR END======
2022-03-28 11:19:27 [ForkJoinPool-1-worker-9] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@34093228=[], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[], io.strimzi.test.logs.CollectorElement@b850ef2a=[infra-namespace, reconciliation-st], io.strimzi.test.logs.CollectorElement@f4405b3b=[], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 11:19:27 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:254] ThrottlingQuotaST - Notifies waiting test suites:[UserST, HttpBridgeTlsST, TopicST, CruiseControlConfigurationST, HttpBridgeScramShaST, ThrottlingQuotaST] to and randomly select one to start execution
2022-03-28 11:19:27 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:85] [operators.topic.ThrottlingQuotaST] - Removing parallel suite: ThrottlingQuotaST
2022-03-28 11:19:27 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:89] [operators.topic.ThrottlingQuotaST] - Parallel suites count: 1
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1,942.408 s - in io.strimzi.systemtest.operators.topic.ThrottlingQuotaST
2022-03-28 11:19:28 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Topic my-topic-49240108-312582418 has rolled not ready, will try again in 1000 ms (200764ms till timeout)
2022-03-28 11:19:29 [ForkJoinPool-1-worker-11] INFO  [TopicST:401] Wait KafkaTopic my-topic-49240108-312582418 recreation
2022-03-28 11:19:29 [ForkJoinPool-1-worker-11] INFO  [KafkaTopicUtils:78] Waiting for KafkaTopic my-topic-49240108-312582418 creation 
2022-03-28 11:19:29 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for KafkaTopic creation my-topic-49240108-312582418
2022-03-28 11:19:29 [ForkJoinPool-1-worker-11] INFO  [TopicST:403] KafkaTopic my-topic-49240108-312582418 recreated
2022-03-28 11:19:29 [ForkJoinPool-1-worker-11] DEBUG [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@2e3baca8, which are set.
2022-03-28 11:19:29 [ForkJoinPool-1-worker-11] INFO  [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@298ff586, messages=[], arguments=[--bootstrap-server, my-cluster-a0692855-isolated-kafka-bootstrap.topic-st.svc:9092, --group-id, my-consumer-group-1428835569, --max-messages, 100, --group-instance-id, instance492326300, --topic, my-topic-49240108-312582418], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-a0692855-isolated-kafka-clients-68b5f55779-pfp8s', podNamespace='topic-st', bootstrapServer='my-cluster-a0692855-isolated-kafka-bootstrap.topic-st.svc:9092', topicName='my-topic-49240108-312582418', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-1428835569', consumerInstanceId='instance492326300', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@2e3baca8}
2022-03-28 11:19:29 [ForkJoinPool-1-worker-11] INFO  [InternalKafkaClient:157] Consuming 100 messages from my-cluster-a0692855-isolated-kafka-bootstrap.topic-st.svc:9092#my-topic-49240108-312582418 from pod my-cluster-a0692855-isolated-kafka-clients-68b5f55779-pfp8s
2022-03-28 11:19:29 [ForkJoinPool-1-worker-11] INFO  [VerifiableClient:192] Client command: oc exec my-cluster-a0692855-isolated-kafka-clients-68b5f55779-pfp8s -n topic-st -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-a0692855-isolated-kafka-bootstrap.topic-st.svc:9092 --group-id my-consumer-group-1428835569 --max-messages 100 --group-instance-id instance492326300 --topic my-topic-49240108-312582418
2022-03-28 11:19:29 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc exec my-cluster-a0692855-isolated-kafka-clients-68b5f55779-pfp8s -n topic-st -- /opt/kafka/consumer.sh --bootstrap-server my-cluster-a0692855-isolated-kafka-bootstrap.topic-st.svc:9092 --group-id my-consumer-group-1428835569 --max-messages 100 --group-instance-id instance492326300 --topic my-topic-49240108-312582418
2022-03-28 11:19:36 [ForkJoinPool-1-worker-11] INFO  [InternalKafkaClient:160] Consumer finished correctly: true
2022-03-28 11:19:36 [ForkJoinPool-1-worker-11] INFO  [InternalKafkaClient:163] Consumer consumed 100 messages
2022-03-28 11:19:36 [ForkJoinPool-1-worker-11] DEBUG [AbstractST:674] ============================================================================
2022-03-28 11:19:36 [ForkJoinPool-1-worker-11] DEBUG [AbstractST:675] [operators.topic.TopicST - After Each] - Clean up after test
2022-03-28 11:19:36 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:344] ############################################################################
2022-03-28 11:19:36 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:348] Delete all resources for testDeleteTopicEnableFalse
2022-03-28 11:19:36 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:241] Delete of Deployment my-cluster-a0692855-isolated-kafka-clients in namespace topic-st
2022-03-28 11:19:36 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of Kafka my-cluster-a0692855-isolated in namespace topic-st
2022-03-28 11:19:36 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of KafkaTopic my-topic-49240108-312582418 in namespace topic-st
2022-03-28 11:19:36 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-a0692855-isolated
2022-03-28 11:19:36 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-a0692855-isolated-kafka-clients
2022-03-28 11:19:36 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-49240108-312582418
2022-03-28 11:19:36 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-a0692855-isolated not ready, will try again in 10000 ms (839945ms till timeout)
2022-03-28 11:19:36 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-49240108-312582418 not ready, will try again in 10000 ms (179891ms till timeout)
2022-03-28 11:19:36 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-a0692855-isolated-kafka-clients not ready, will try again in 10000 ms (479868ms till timeout)
2022-03-28 11:19:47 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-a0692855-isolated-kafka-clients not ready, will try again in 10000 ms (469684ms till timeout)
2022-03-28 11:19:57 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-a0692855-isolated-kafka-clients not ready, will try again in 10000 ms (459499ms till timeout)
2022-03-28 11:20:07 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-a0692855-isolated-kafka-clients not ready, will try again in 10000 ms (449374ms till timeout)
2022-03-28 11:20:17 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:369] ############################################################################
2022-03-28 11:20:17 [ForkJoinPool-1-worker-11] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.topic.TopicST.testDeleteTopicEnableFalse-FINISHED
2022-03-28 11:20:17 [ForkJoinPool-1-worker-11] INFO  [TestSeparator:30] ############################################################################
2022-03-28 11:20:17 [ForkJoinPool-1-worker-11] DEBUG [AbstractST:689] ============================================================================
2022-03-28 11:20:17 [ForkJoinPool-1-worker-11] DEBUG [AbstractST:690] [operators.topic.TopicST - After All] - Clean up after test suite
2022-03-28 11:20:17 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:344] ############################################################################
2022-03-28 11:20:17 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:348] Delete all resources for TopicST
2022-03-28 11:20:17 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:241] Delete of Kafka topic-cluster-name in namespace topic-st
2022-03-28 11:20:17 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:topic-cluster-name
2022-03-28 11:20:17 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:topic-cluster-name not ready, will try again in 10000 ms (839910ms till timeout)
2022-03-28 11:20:27 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:369] ############################################################################
2022-03-28 11:20:27 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Namespace topic-st removal
2022-03-28 11:20:27 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:20:28 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:20:28 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 11:20:28 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (479707ms till timeout)
2022-03-28 11:20:29 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:20:29 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:20:29 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 11:20:29 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (478406ms till timeout)
2022-03-28 11:20:30 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:20:30 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:20:30 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 11:20:30 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (477145ms till timeout)
2022-03-28 11:20:31 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:20:32 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:20:32 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 11:20:32 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (475747ms till timeout)
2022-03-28 11:20:33 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:20:33 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:20:33 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 11:20:33 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (474434ms till timeout)
2022-03-28 11:20:34 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:20:34 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:20:34 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 11:20:34 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (473132ms till timeout)
2022-03-28 11:20:35 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:20:36 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:20:36 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 11:20:36 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (471897ms till timeout)
2022-03-28 11:20:37 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:20:37 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:20:37 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 11:20:37 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (470686ms till timeout)
2022-03-28 11:20:38 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:20:38 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:20:38 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 11:20:38 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (469483ms till timeout)
2022-03-28 11:20:39 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:20:39 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:20:39 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 11:20:39 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (468214ms till timeout)
2022-03-28 11:20:40 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:20:40 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:20:40 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 11:20:40 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (467006ms till timeout)
2022-03-28 11:20:41 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:20:42 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:20:42 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 11:20:42 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (465753ms till timeout)
2022-03-28 11:20:43 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:20:43 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:20:43 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 11:20:43 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (464450ms till timeout)
2022-03-28 11:20:44 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:20:44 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:20:44 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 11:20:44 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (462990ms till timeout)
2022-03-28 11:20:45 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:20:46 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:20:46 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 11:20:46 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (461668ms till timeout)
2022-03-28 11:20:47 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:20:47 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:20:47 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 11:20:47 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (460401ms till timeout)
2022-03-28 11:20:48 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:20:48 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:20:48 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 11:20:48 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (459106ms till timeout)
2022-03-28 11:20:49 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:20:50 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:20:50 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 11:20:50 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (457797ms till timeout)
2022-03-28 11:20:51 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:20:51 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:20:51 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 11:20:51 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (456443ms till timeout)
2022-03-28 11:20:52 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:20:52 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:20:52 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 11:20:52 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (455140ms till timeout)
2022-03-28 11:20:53 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:20:54 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:20:54 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 11:20:54 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (453777ms till timeout)
2022-03-28 11:20:55 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:20:55 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:20:55 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 11:20:55 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (452546ms till timeout)
2022-03-28 11:20:56 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:20:56 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:20:56 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 11:20:56 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (451241ms till timeout)
2022-03-28 11:20:57 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:20:58 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:20:58 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 11:20:58 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (449900ms till timeout)
2022-03-28 11:20:59 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:20:59 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:20:59 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 11:20:59 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (448644ms till timeout)
2022-03-28 11:21:00 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:21:00 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:21:00 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 11:21:00 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (447185ms till timeout)
2022-03-28 11:21:01 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:21:01 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:21:01 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 11:21:01 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (445969ms till timeout)
2022-03-28 11:21:02 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:21:03 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 11:21:03 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 1
2022-03-28 11:21:03 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 11:21:03 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Error from server (NotFound): namespaces "topic-st" not found
2022-03-28 11:21:03 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] ======STDERR END======
2022-03-28 11:21:03 [ForkJoinPool-1-worker-11] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@34093228=[], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[], io.strimzi.test.logs.CollectorElement@b850ef2a=[infra-namespace, reconciliation-st], io.strimzi.test.logs.CollectorElement@f4405b3b=[], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 11:21:03 [ForkJoinPool-1-worker-11] DEBUG [SuiteThreadController:254] TopicST - Notifies waiting test suites:[UserST, HttpBridgeTlsST, TopicST, CruiseControlConfigurationST, HttpBridgeScramShaST, ThrottlingQuotaST] to and randomly select one to start execution
2022-03-28 11:21:03 [ForkJoinPool-1-worker-11] DEBUG [SuiteThreadController:85] [operators.topic.TopicST] - Removing parallel suite: TopicST
2022-03-28 11:21:03 [ForkJoinPool-1-worker-11] DEBUG [SuiteThreadController:89] [operators.topic.TopicST] - Parallel suites count: 0
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2,037.696 s - in io.strimzi.systemtest.operators.topic.TopicST
2022-03-28 11:21:03 [ForkJoinPool-1-worker-3] INFO  [SetupClusterOperator:618] ============================================================================
2022-03-28 11:21:03 [ForkJoinPool-1-worker-3] INFO  [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-03-28 11:21:03 [ForkJoinPool-1-worker-3] INFO  [SetupClusterOperator:620] ============================================================================
2022-03-28 11:21:03 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:344] ############################################################################
2022-03-28 11:21:03 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-03-28 11:21:03 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-28 11:21:03 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-28 11:21:03 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-28 11:21:03 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-28 11:21:03 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-28 11:21:03 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-28 11:21:03 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-28 11:21:03 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-28 11:21:03 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-28 11:21:03 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-28 11:21:03 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-28 11:21:03 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-28 11:21:03 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io not ready, will try again in 10000 ms (179536ms till timeout)
2022-03-28 11:21:03 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-28 11:21:03 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-28 11:21:03 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-28 11:21:03 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io not ready, will try again in 10000 ms (179502ms till timeout)
2022-03-28 11:21:04 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-28 11:21:04 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-28 11:21:04 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io not ready, will try again in 10000 ms (179413ms till timeout)
2022-03-28 11:21:04 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-28 11:21:04 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-28 11:21:04 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-28 11:21:04 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-28 11:21:04 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-28 11:21:04 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-28 11:21:04 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-28 11:21:04 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-28 11:21:04 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-28 11:21:04 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-28 11:21:04 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-28 11:21:04 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-28 11:21:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io not ready, will try again in 10000 ms (179723ms till timeout)
2022-03-28 11:21:05 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-28 11:21:05 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io not ready, will try again in 10000 ms (179642ms till timeout)
2022-03-28 11:21:05 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-28 11:21:05 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-28 11:21:05 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-28 11:21:05 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-28 11:21:05 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-28 11:21:05 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-28 11:21:06 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-28 11:21:06 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-28 11:21:06 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-28 11:21:06 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-28 11:21:06 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-28 11:21:06 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-28 11:21:07 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io not ready, will try again in 10000 ms (179526ms till timeout)
2022-03-28 11:21:14 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-28 11:21:14 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-28 11:21:14 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-28 11:21:14 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-28 11:21:14 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-28 11:21:14 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-28 11:21:14 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io not ready, will try again in 10000 ms (179425ms till timeout)
2022-03-28 11:21:14 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io not ready, will try again in 10000 ms (179646ms till timeout)
2022-03-28 11:21:15 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-28 11:21:15 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-28 11:21:15 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io not ready, will try again in 10000 ms (179226ms till timeout)
2022-03-28 11:21:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io not ready, will try again in 10000 ms (179453ms till timeout)
2022-03-28 11:21:25 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:369] ############################################################################
2022-03-28 11:21:25 [ForkJoinPool-1-worker-3] DEBUG [KubeClusterResource:216] Deleting Namespace: infra-namespace
2022-03-28 11:21:25 [ForkJoinPool-1-worker-3] DEBUG [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-28 11:21:25 [ForkJoinPool-1-worker-3] DEBUG [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-28 11:21:26 [ForkJoinPool-1-worker-3] DEBUG [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v1889127
2022-03-28 11:21:26 [ForkJoinPool-1-worker-3] DEBUG [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v1889127
2022-03-28 11:21:26 [ForkJoinPool-1-worker-3] DEBUG [AbstractWatchManager:222] Watching https://api.morsak-46.strimzi.app-services-dev.net:6443/api/v1/namespaces?fieldSelector=metadata.name%3Dinfra-namespace&resourceVersion=1889127&allowWatchBookmarks=true&watch=true...
2022-03-28 11:21:26 [OkHttp https://api.morsak-46.strimzi.app-services-dev.net:6443/...] DEBUG [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-28 11:21:26 [OkHttp https://api.morsak-46.strimzi.app-services-dev.net:6443/...] DEBUG [Reflector:139] Event received MODIFIED Namespace resourceVersion 1889128
2022-03-28 11:21:40 [OkHttp https://api.morsak-46.strimzi.app-services-dev.net:6443/...] DEBUG [Reflector:139] Event received MODIFIED Namespace resourceVersion 1889214
2022-03-28 11:21:50 [OkHttp https://api.morsak-46.strimzi.app-services-dev.net:6443/...] DEBUG [Reflector:139] Event received MODIFIED Namespace resourceVersion 1889260
2022-03-28 11:21:50 [OkHttp https://api.morsak-46.strimzi.app-services-dev.net:6443/...] DEBUG [Reflector:139] Event received DELETED Namespace resourceVersion 1889261
2022-03-28 11:21:50 [OkHttp https://api.morsak-46.strimzi.app-services-dev.net:6443/...] DEBUG [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v1889260 in namespace default
2022-03-28 11:21:50 [OkHttp https://api.morsak-46.strimzi.app-services-dev.net:6443/...] DEBUG [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@7ed22b95
2022-03-28 11:21:50 [OkHttp https://api.morsak-46.strimzi.app-services-dev.net:6443/...] DEBUG [Reflector:181] Watch gracefully closed
2022-03-28 11:21:50 [OkHttp https://api.morsak-46.strimzi.app-services-dev.net:6443/...] DEBUG [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@1152edf1
2022-03-28 11:21:50 [ForkJoinPool-1-worker-3] DEBUG [KubeClusterResource:216] Deleting Namespace: reconciliation-st
2022-03-28 11:21:50 [ForkJoinPool-1-worker-9] DEBUG [KubeClusterResource:216] Deleting Namespace: infra-namespace
2022-03-28 11:21:50 [OkHttp https://api.morsak-46.strimzi.app-services-dev.net:6443/...] DEBUG [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@1152edf1
2022-03-28 11:21:50 [OkHttp https://api.morsak-46.strimzi.app-services-dev.net:6443/...] DEBUG [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@1152edf1
2022-03-28 11:21:50 [OkHttp https://api.morsak-46.strimzi.app-services-dev.net:6443/...] DEBUG [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-28 11:21:50 [OkHttp https://api.morsak-46.strimzi.app-services-dev.net:6443/...] DEBUG [AbstractWatchManager:140] Ignoring error for already closed/closing connection
2022-03-28 11:21:50 [ForkJoinPool-1-worker-9] DEBUG [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-28 11:21:50 [ForkJoinPool-1-worker-9] DEBUG [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-28 11:21:50 [ForkJoinPool-1-worker-3] DEBUG [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-28 11:21:50 [ForkJoinPool-1-worker-3] DEBUG [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-28 11:21:50 [ForkJoinPool-1-worker-9] DEBUG [Reflector:95] Listing items (0) for resource class io.fabric8.kubernetes.api.model.Namespace v1889262
2022-03-28 11:21:50 [ForkJoinPool-1-worker-3] DEBUG [Reflector:95] Listing items (0) for resource class io.fabric8.kubernetes.api.model.Namespace v1889262
2022-03-28 11:21:50 [main] INFO  [TestExecutionListener:40] =======================================================================
2022-03-28 11:21:50 [main] INFO  [TestExecutionListener:41] =======================================================================
2022-03-28 11:21:50 [main] INFO  [TestExecutionListener:42]                         Test run finished
2022-03-28 11:21:50 [main] INFO  [TestExecutionListener:43] =======================================================================
2022-03-28 11:21:50 [main] INFO  [TestExecutionListener:44] =======================================================================
2022-03-28 11:21:50 [main] INFO  [TestExecutionListener:29] =======================================================================
2022-03-28 11:21:50 [main] INFO  [TestExecutionListener:30] =======================================================================
2022-03-28 11:21:50 [main] INFO  [TestExecutionListener:31]                         Test run started
2022-03-28 11:21:50 [main] INFO  [TestExecutionListener:32] =======================================================================
2022-03-28 11:21:50 [main] INFO  [TestExecutionListener:33] =======================================================================
2022-03-28 11:21:50 [main] INFO  [TestExecutionListener:48] Following testclasses are selected for run:
2022-03-28 11:21:50 [main] INFO  [TestExecutionListener:51] -> io.strimzi.systemtest.operators.ReconciliationST
2022-03-28 11:21:50 [main] INFO  [TestExecutionListener:52] =======================================================================
2022-03-28 11:21:50 [main] INFO  [TestExecutionListener:53] =======================================================================
[INFO] Running io.strimzi.systemtest.operators.ReconciliationST
2022-03-28 11:21:50 [ForkJoinPool-2-worker-3] DEBUG [BeforeAllOnce:51] ============================================================================
2022-03-28 11:21:50 [ForkJoinPool-2-worker-3] DEBUG [BeforeAllOnce:52] [io.strimzi.systemtest.operators.ReconciliationST - Before Suite] - Setup Suite environment
2022-03-28 11:21:50 [ForkJoinPool-2-worker-3] INFO  [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@33cec2fa
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-03-28 11:21:50 [ForkJoinPool-2-worker-3] DEBUG [SetupClusterOperator:198] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@57380991, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@33cec2fa, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='*', bindingsNamespaces=[infra-namespace], operationTimeout=300000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-03-28 11:21:50 [ForkJoinPool-2-worker-3] INFO  [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-03-28 11:21:50 [ForkJoinPool-2-worker-3] INFO  [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-03-28 11:21:50 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Namespace infra-namespace
2022-03-28 11:21:50 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace infra-namespace -o json
2022-03-28 11:21:51 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace infra-namespace -o json
2022-03-28 11:21:51 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:21:51 [ForkJoinPool-2-worker-3] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c28,c17",
            "openshift.io/sa.scc.supplemental-groups": "1000790000/10000",
            "openshift.io/sa.scc.uid-range": "1000790000/10000"
        },
        "creationTimestamp": "2022-03-28T11:21:48Z",
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T11:21:18Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:status": {
                        "f:phase": {}
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T11:21:48Z"
            }
        ],
        "name": "infra-namespace",
        "resourceVersion": "1889270",
        "selfLink": "/api/v1/namespaces/infra-namespace",
        "uid": "b851e569-9d24-4091-b307-3270dbd9dbfd"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 11:21:51 [ForkJoinPool-2-worker-3] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace]}
2022-03-28 11:21:51 [ForkJoinPool-2-worker-3] INFO  [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-28 11:21:51 [ForkJoinPool-2-worker-3] DEBUG [SetupClusterOperator:478] Installation resource type: ServiceAccount
2022-03-28 11:21:51 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-28 11:21:51 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-28 11:21:51 [ForkJoinPool-2-worker-3] DEBUG [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-28 11:21:51 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-28 11:21:51 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-28 11:21:51 [ForkJoinPool-2-worker-3] DEBUG [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-28 11:21:51 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-28 11:21:51 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-28 11:21:51 [ForkJoinPool-2-worker-3] DEBUG [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-28 11:21:51 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-28 11:21:51 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-28 11:21:51 [ForkJoinPool-2-worker-3] DEBUG [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-28 11:21:51 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-28 11:21:51 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-28 11:21:51 [ForkJoinPool-2-worker-3] DEBUG [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-28 11:21:51 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-28 11:21:51 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-28 11:21:52 [ForkJoinPool-2-worker-3] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 11:21:52 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-28 11:21:52 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-28 11:21:52 [ForkJoinPool-2-worker-3] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 11:21:52 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-28 11:21:53 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-28 11:21:53 [ForkJoinPool-2-worker-3] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 11:21:53 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-28 11:21:53 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-28 11:21:53 [ForkJoinPool-2-worker-3] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 11:21:53 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-28 11:21:53 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-28 11:21:53 [ForkJoinPool-2-worker-3] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 11:21:53 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-28 11:21:53 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-28 11:21:53 [ForkJoinPool-2-worker-3] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 11:21:53 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-28 11:21:53 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-28 11:21:53 [ForkJoinPool-2-worker-3] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 11:21:53 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-28 11:21:54 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-28 11:21:54 [ForkJoinPool-2-worker-3] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 11:21:54 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-28 11:21:54 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-28 11:21:54 [ForkJoinPool-2-worker-3] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 11:21:54 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-28 11:21:54 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-28 11:21:54 [ForkJoinPool-2-worker-3] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 11:21:54 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-28 11:21:54 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-28 11:21:54 [ForkJoinPool-2-worker-3] DEBUG [SetupClusterOperator:478] Installation resource type: ConfigMap
2022-03-28 11:21:54 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-28 11:21:55 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-28 11:21:55 [ForkJoinPool-2-worker-3] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 11:21:55 [ForkJoinPool-2-worker-3] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-03-28 11:21:55 [ForkJoinPool-2-worker-3] INFO  [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/cloud-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-28 11:21:55 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-28 11:21:55 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-28 11:21:55 [ForkJoinPool-2-worker-3] INFO  [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/cloud-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-28 11:21:55 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-28 11:21:55 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-28 11:21:55 [ForkJoinPool-2-worker-3] INFO  [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/cloud-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-28 11:21:55 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-28 11:21:55 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-28 11:21:55 [ForkJoinPool-2-worker-3] INFO  [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/cloud-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-28 11:21:55 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-28 11:21:55 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-28 11:21:55 [ForkJoinPool-2-worker-3] INFO  [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/cloud-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-28 11:21:55 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-28 11:21:55 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-28 11:21:55 [ForkJoinPool-2-worker-3] INFO  [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-28 11:21:55 [ForkJoinPool-2-worker-3] INFO  [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-28 11:21:55 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-28 11:21:55 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-28 11:21:55 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-28 11:21:55 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-28 11:21:55 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-28 11:21:56 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-28 11:21:56 [ForkJoinPool-2-worker-3] INFO  [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-28 11:21:56 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-28 11:21:56 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (479959ms till timeout)
2022-03-28 11:21:57 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (478918ms till timeout)
2022-03-28 11:21:58 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (477856ms till timeout)
2022-03-28 11:21:59 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (476695ms till timeout)
2022-03-28 11:22:00 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (475581ms till timeout)
2022-03-28 11:22:01 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (474506ms till timeout)
2022-03-28 11:22:02 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (473449ms till timeout)
2022-03-28 11:22:03 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (472411ms till timeout)
2022-03-28 11:22:04 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (471334ms till timeout)
2022-03-28 11:22:05 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (470283ms till timeout)
2022-03-28 11:22:06 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (469252ms till timeout)
2022-03-28 11:22:07 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (468211ms till timeout)
2022-03-28 11:22:08 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (467179ms till timeout)
2022-03-28 11:22:10 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (466083ms till timeout)
2022-03-28 11:22:11 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (464970ms till timeout)
2022-03-28 11:22:12 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (463930ms till timeout)
2022-03-28 11:22:13 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (462897ms till timeout)
2022-03-28 11:22:14 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (461851ms till timeout)
2022-03-28 11:22:15 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (460792ms till timeout)
2022-03-28 11:22:16 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (459721ms till timeout)
2022-03-28 11:22:17 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (458671ms till timeout)
2022-03-28 11:22:18 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (457613ms till timeout)
2022-03-28 11:22:19 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (456577ms till timeout)
2022-03-28 11:22:20 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (455523ms till timeout)
2022-03-28 11:22:21 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (454423ms till timeout)
2022-03-28 11:22:22 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (453355ms till timeout)
2022-03-28 11:22:23 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (452287ms till timeout)
2022-03-28 11:22:24 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (451224ms till timeout)
2022-03-28 11:22:26 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (450075ms till timeout)
2022-03-28 11:22:27 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (449046ms till timeout)
2022-03-28 11:22:28 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (447991ms till timeout)
2022-03-28 11:22:29 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (446934ms till timeout)
2022-03-28 11:22:30 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (445900ms till timeout)
2022-03-28 11:22:31 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (444861ms till timeout)
2022-03-28 11:22:32 [ForkJoinPool-2-worker-3] INFO  [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-28 11:22:32 [ForkJoinPool-2-worker-3] INFO  [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-03-28 11:22:32 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-28 11:22:32 [ForkJoinPool-2-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-869dcfb9f7-bdfdt not ready: strimzi-cluster-operator)
2022-03-28 11:22:32 [ForkJoinPool-2-worker-3] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-869dcfb9f7-bdfdt are ready
2022-03-28 11:22:32 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599943ms till timeout)
2022-03-28 11:22:33 [ForkJoinPool-2-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-869dcfb9f7-bdfdt not ready: strimzi-cluster-operator)
2022-03-28 11:22:33 [ForkJoinPool-2-worker-3] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-869dcfb9f7-bdfdt are ready
2022-03-28 11:22:33 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598922ms till timeout)
2022-03-28 11:22:34 [ForkJoinPool-2-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-869dcfb9f7-bdfdt not ready: strimzi-cluster-operator)
2022-03-28 11:22:34 [ForkJoinPool-2-worker-3] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-869dcfb9f7-bdfdt are ready
2022-03-28 11:22:34 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597852ms till timeout)
2022-03-28 11:22:35 [ForkJoinPool-2-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-869dcfb9f7-bdfdt not ready: strimzi-cluster-operator)
2022-03-28 11:22:35 [ForkJoinPool-2-worker-3] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-869dcfb9f7-bdfdt are ready
2022-03-28 11:22:35 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596753ms till timeout)
2022-03-28 11:22:36 [ForkJoinPool-2-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-869dcfb9f7-bdfdt not ready: strimzi-cluster-operator)
2022-03-28 11:22:36 [ForkJoinPool-2-worker-3] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-869dcfb9f7-bdfdt are ready
2022-03-28 11:22:36 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595714ms till timeout)
2022-03-28 11:22:37 [ForkJoinPool-2-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-869dcfb9f7-bdfdt not ready: strimzi-cluster-operator)
2022-03-28 11:22:37 [ForkJoinPool-2-worker-3] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-869dcfb9f7-bdfdt are ready
2022-03-28 11:22:37 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594672ms till timeout)
2022-03-28 11:22:38 [ForkJoinPool-2-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-869dcfb9f7-bdfdt not ready: strimzi-cluster-operator)
2022-03-28 11:22:38 [ForkJoinPool-2-worker-3] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-869dcfb9f7-bdfdt are ready
2022-03-28 11:22:38 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593629ms till timeout)
2022-03-28 11:22:39 [ForkJoinPool-2-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-869dcfb9f7-bdfdt not ready: strimzi-cluster-operator)
2022-03-28 11:22:39 [ForkJoinPool-2-worker-3] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-869dcfb9f7-bdfdt are ready
2022-03-28 11:22:39 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592587ms till timeout)
2022-03-28 11:22:40 [ForkJoinPool-2-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-869dcfb9f7-bdfdt not ready: strimzi-cluster-operator)
2022-03-28 11:22:40 [ForkJoinPool-2-worker-3] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-869dcfb9f7-bdfdt are ready
2022-03-28 11:22:40 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591513ms till timeout)
2022-03-28 11:22:41 [ForkJoinPool-2-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-869dcfb9f7-bdfdt not ready: strimzi-cluster-operator)
2022-03-28 11:22:41 [ForkJoinPool-2-worker-3] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-869dcfb9f7-bdfdt are ready
2022-03-28 11:22:41 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590489ms till timeout)
2022-03-28 11:22:43 [ForkJoinPool-2-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-869dcfb9f7-bdfdt not ready: strimzi-cluster-operator)
2022-03-28 11:22:43 [ForkJoinPool-2-worker-3] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-869dcfb9f7-bdfdt are ready
2022-03-28 11:22:43 [ForkJoinPool-2-worker-3] INFO  [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-03-28 11:22:43 [ForkJoinPool-2-worker-3] DEBUG [AbstractST:666] ============================================================================
2022-03-28 11:22:43 [ForkJoinPool-2-worker-3] DEBUG [AbstractST:667] [operators.ReconciliationST - Before All] - Setup test suite environment
2022-03-28 11:22:43 [ForkJoinPool-2-worker-3] DEBUG [SuiteThreadController:69] [operators.ReconciliationST] - Adding parallel suite: ReconciliationST
2022-03-28 11:22:43 [ForkJoinPool-2-worker-3] DEBUG [SuiteThreadController:73] [operators.ReconciliationST] - Parallel suites count: 1
2022-03-28 11:22:43 [ForkJoinPool-2-worker-3] DEBUG [SuiteThreadController:184] ReconciliationST suite now can proceed its execution
2022-03-28 11:22:43 [ForkJoinPool-2-worker-3] DEBUG [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-28 11:22:43 [ForkJoinPool-2-worker-3] DEBUG [TestSuiteNamespaceManager:129] Test suite `ReconciliationST` creates these additional namespaces:[reconciliation-st]
2022-03-28 11:22:43 [ForkJoinPool-2-worker-3] INFO  [KubeClusterResource:156] Creating Namespace: reconciliation-st
2022-03-28 11:22:43 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Namespace reconciliation-st
2022-03-28 11:22:43 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace infra-namespace get Namespace reconciliation-st -o json
2022-03-28 11:22:43 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace infra-namespace get Namespace reconciliation-st -o json
2022-03-28 11:22:43 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:22:43 [ForkJoinPool-2-worker-3] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c28,c22",
            "openshift.io/sa.scc.supplemental-groups": "1000800000/10000",
            "openshift.io/sa.scc.uid-range": "1000800000/10000"
        },
        "creationTimestamp": "2022-03-28T11:22:40Z",
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T11:22:10Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:status": {
                        "f:phase": {}
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T11:22:40Z"
            }
        ],
        "name": "reconciliation-st",
        "resourceVersion": "1889617",
        "selfLink": "/api/v1/namespaces/reconciliation-st",
        "uid": "7c957bd6-e67c-4e0d-84fd-47453c205d98"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 11:22:43 [ForkJoinPool-2-worker-3] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st]}
2022-03-28 11:22:43 [ForkJoinPool-2-worker-3] INFO  [KubeClusterResource:82] Client use Namespace: reconciliation-st
2022-03-28 11:22:43 [ForkJoinPool-2-worker-3] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=reconciliation-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 11:22:43 [ForkJoinPool-2-worker-3] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: reconciliation-st
2022-03-28 11:22:43 [ForkJoinPool-2-worker-3] INFO  [TestSeparator:23] ############################################################################
2022-03-28 11:22:43 [ForkJoinPool-2-worker-3] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.ReconciliationST.testPauseReconciliationInKafkaRebalanceAndTopic-STARTED
2022-03-28 11:22:43 [ForkJoinPool-2-worker-3] DEBUG [AbstractST:658] ============================================================================
2022-03-28 11:22:43 [ForkJoinPool-2-worker-3] DEBUG [AbstractST:659] [operators.ReconciliationST - Before Each] - Setup test case environment
2022-03-28 11:22:43 [ForkJoinPool-2-worker-3] DEBUG [SuiteThreadController:77] [operators.ReconciliationST] - Adding parallel test: testPauseReconciliationInKafkaRebalanceAndTopic
2022-03-28 11:22:43 [ForkJoinPool-2-worker-3] DEBUG [SuiteThreadController:81] [operators.ReconciliationST] - Parallel test count: 1
2022-03-28 11:22:43 [ForkJoinPool-2-worker-3] DEBUG [SuiteThreadController:230] testPauseReconciliationInKafkaRebalanceAndTopic test now can proceed its execution
2022-03-28 11:22:43 [ForkJoinPool-2-worker-3] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 11:22:43 [ForkJoinPool-2-worker-3] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383, testThrottlingQuotasCreateAlterPartitions=my-cluster-28d9dd2e, testSendSimpleMessageTls=my-cluster-27ddd6ba, testConfigurationReflection=my-cluster-d1e2168e, testUserWithNameMoreThan64Chars=my-cluster-35e66bba, testDeployAndUnDeployCruiseControl=my-cluster-c0830cbe, testCapacityFile=my-cluster-fd0fb61a, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-66f23d43, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-f50d425d, testTlsExternalUser=my-cluster-3ba9cc5b, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5, testCreateTopicAfterUnsupportedOperation=my-cluster-b31d4894, testScramUserWithQuotas=my-cluster-4941482a, testTopicModificationOfReplicationFactor=my-cluster-38e659b2, testKafkaAdminTopicOperations=my-cluster-5a0bfe14, testConfigurationFileIsCreated=my-cluster-e0fac774, testCreateTopicViaKafka=my-cluster-d73fc0b6, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c, testThrottlingQuotasDeleteTopic=my-cluster-8f262de5, testUpdateUser=my-cluster-2f4b361c, testDeleteTopicEnableFalse=my-cluster-a0692855, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-d8b313ba, testTlsExternalUserWithQuotas=my-cluster-bb24d987, testTlsUserWithQuotas=my-cluster-d49e42e8, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8, testSendingMessagesToNonExistingTopic=my-cluster-241cc7c0, testReceiveSimpleMessageTls=my-cluster-c6a15963, testConfigurationPerformanceOptions=my-cluster-b6310693, testUserTemplate=my-cluster-3189676d, testMoreReplicasThanAvailableBrokers=my-cluster-59a64014}
2022-03-28 11:22:43 [ForkJoinPool-2-worker-3] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-1915916223-1265293973, testThrottlingQuotasCreateAlterPartitions=my-user-1933547616-1024431140, testSendSimpleMessageTls=my-user-738229993-1095121647, testConfigurationReflection=my-user-1445653023-93200027, testUserWithNameMoreThan64Chars=my-user-1703808150-314159031, testDeployAndUnDeployCruiseControl=my-user-1988711145-439861161, testCapacityFile=my-user-937015144-1959439396, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-1150765500-2076224625, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-745974575-1852384454, testTlsExternalUser=my-user-1382292264-1641298587, testSendSimpleMessageTlsScramSha=my-user-1499386234-1249154328, testCreateTopicAfterUnsupportedOperation=my-user-893702009-1709424309, testScramUserWithQuotas=my-user-797280497-2138800976, testTopicModificationOfReplicationFactor=my-user-2003504850-304348854, testKafkaAdminTopicOperations=my-user-1708656731-1087276375, testConfigurationFileIsCreated=my-user-180611180-50530491, testCreateTopicViaKafka=my-user-372778533-473457568, testCreatingUsersWithSecretPrefix=my-user-1450786684-1808961062, testThrottlingQuotasDeleteTopic=my-user-1558977140-777236615, testUpdateUser=my-user-2098198927-950610275, testDeleteTopicEnableFalse=my-user-1059904876-226844741, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-313830399-1018203124, testTlsExternalUserWithQuotas=my-user-656620760-372265072, testTlsUserWithQuotas=my-user-811912745-1441809840, testThrottlingQuotasCreateTopic=my-user-1131806865-947217230, testSendingMessagesToNonExistingTopic=my-user-927451395-640848463, testReceiveSimpleMessageTls=my-user-6088058-1142028780, testConfigurationPerformanceOptions=my-user-930975101-640523779, testUserTemplate=my-user-1919254789-1384584070, testMoreReplicasThanAvailableBrokers=my-user-1511857485-1227147676}
2022-03-28 11:22:43 [ForkJoinPool-2-worker-3] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-1834944466-2094449929, testThrottlingQuotasCreateAlterPartitions=my-topic-609849288-1810034325, testSendSimpleMessageTls=my-topic-563023482-1697359353, testConfigurationReflection=my-topic-640815392-1092406112, testUserWithNameMoreThan64Chars=my-topic-2021464661-1313035472, testDeployAndUnDeployCruiseControl=my-topic-1511079187-95055629, testCapacityFile=my-topic-1155641705-902584520, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-842198221-1227962115, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-254848724-189700675, testTlsExternalUser=my-topic-5642373-135861890, testSendSimpleMessageTlsScramSha=my-topic-1028951580-1656411053, testCreateTopicAfterUnsupportedOperation=my-topic-1151452928-1452134654, testScramUserWithQuotas=my-topic-2004840350-265526537, testTopicModificationOfReplicationFactor=my-topic-1789870897-1466152225, testKafkaAdminTopicOperations=my-topic-1564170108-1493323202, testConfigurationFileIsCreated=my-topic-1296407588-313880708, testCreateTopicViaKafka=my-topic-1960957209-999941425, testCreatingUsersWithSecretPrefix=my-topic-1479520843-618891199, testThrottlingQuotasDeleteTopic=my-topic-2128658681-1688111273, testUpdateUser=my-topic-1741945237-916990305, testDeleteTopicEnableFalse=my-topic-49240108-312582418, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-1306521865-1110190015, testTlsExternalUserWithQuotas=my-topic-1953803944-1840309972, testTlsUserWithQuotas=my-topic-418284173-1689090259, testThrottlingQuotasCreateTopic=my-topic-728672490-1194703402, testSendingMessagesToNonExistingTopic=my-topic-645675602-646682485, testReceiveSimpleMessageTls=my-topic-478514536-2132782608, testConfigurationPerformanceOptions=my-topic-213912446-1165431808, testUserTemplate=my-topic-2114982563-185666735, testMoreReplicasThanAvailableBrokers=my-topic-1591366435-591944736}
2022-03-28 11:22:43 [ForkJoinPool-2-worker-3] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-60b45383-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-28d9dd2e-kafka-clients, testSendSimpleMessageTls=my-cluster-27ddd6ba-kafka-clients, testConfigurationReflection=my-cluster-d1e2168e-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-35e66bba-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-c0830cbe-kafka-clients, testCapacityFile=my-cluster-fd0fb61a-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-66f23d43-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-f50d425d-kafka-clients, testTlsExternalUser=my-cluster-3ba9cc5b-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-c836ddc5-kafka-clients, testCreateTopicAfterUnsupportedOperation=my-cluster-b31d4894-kafka-clients, testScramUserWithQuotas=my-cluster-4941482a-kafka-clients, testTopicModificationOfReplicationFactor=my-cluster-38e659b2-kafka-clients, testKafkaAdminTopicOperations=my-cluster-5a0bfe14-kafka-clients, testConfigurationFileIsCreated=my-cluster-e0fac774-kafka-clients, testCreateTopicViaKafka=my-cluster-d73fc0b6-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-ede8537c-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-8f262de5-kafka-clients, testUpdateUser=my-cluster-2f4b361c-kafka-clients, testDeleteTopicEnableFalse=my-cluster-a0692855-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-d8b313ba-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-bb24d987-kafka-clients, testTlsUserWithQuotas=my-cluster-d49e42e8-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-83f3f4b8-kafka-clients, testSendingMessagesToNonExistingTopic=my-cluster-241cc7c0-kafka-clients, testReceiveSimpleMessageTls=my-cluster-c6a15963-kafka-clients, testConfigurationPerformanceOptions=my-cluster-b6310693-kafka-clients, testUserTemplate=my-cluster-3189676d-kafka-clients, testMoreReplicasThanAvailableBrokers=my-cluster-59a64014-kafka-clients}
2022-03-28 11:22:43 [ForkJoinPool-2-worker-3] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-10 for test case:testPauseReconciliationInKafkaRebalanceAndTopic
2022-03-28 11:22:43 [ForkJoinPool-2-worker-3] INFO  [KubeClusterResource:156] Creating Namespace: namespace-10
2022-03-28 11:22:43 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Namespace namespace-10
2022-03-28 11:22:43 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace reconciliation-st get Namespace namespace-10 -o json
2022-03-28 11:22:44 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace reconciliation-st get Namespace namespace-10 -o json
2022-03-28 11:22:44 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:22:44 [ForkJoinPool-2-worker-3] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c28,c27",
            "openshift.io/sa.scc.supplemental-groups": "1000810000/10000",
            "openshift.io/sa.scc.uid-range": "1000810000/10000"
        },
        "creationTimestamp": "2022-03-28T11:22:41Z",
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T11:22:11Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:status": {
                        "f:phase": {}
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T11:22:41Z"
            }
        ],
        "name": "namespace-10",
        "resourceVersion": "1889646",
        "selfLink": "/api/v1/namespaces/namespace-10",
        "uid": "a34f72a6-0ca3-48cc-8e0f-b941d2a3c416"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 11:22:44 [ForkJoinPool-2-worker-3] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-10]}
2022-03-28 11:22:44 [ForkJoinPool-2-worker-3] INFO  [KubeClusterResource:82] Client use Namespace: namespace-10
2022-03-28 11:22:44 [ForkJoinPool-2-worker-3] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-10, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 11:22:44 [ForkJoinPool-2-worker-3] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-10
2022-03-28 11:22:44 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-d8b313ba in namespace namespace-10
2022-03-28 11:22:44 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:164] Using Namespace: namespace-10
2022-03-28 11:22:44 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-d8b313ba
2022-03-28 11:22:44 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-d8b313ba will have desired state: Ready
2022-03-28 11:22:44 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-d8b313ba will have desired state: Ready
2022-03-28 11:22:44 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1319966ms till timeout)
2022-03-28 11:22:45 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1318904ms till timeout)
2022-03-28 11:22:46 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1317816ms till timeout)
2022-03-28 11:22:47 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1316736ms till timeout)
2022-03-28 11:22:49 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1315676ms till timeout)
2022-03-28 11:22:50 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1314580ms till timeout)
2022-03-28 11:22:51 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1313539ms till timeout)
2022-03-28 11:22:52 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1312474ms till timeout)
2022-03-28 11:22:53 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1311218ms till timeout)
2022-03-28 11:22:54 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1310123ms till timeout)
2022-03-28 11:22:55 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1308958ms till timeout)
2022-03-28 11:22:56 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1307923ms till timeout)
2022-03-28 11:22:57 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1306858ms till timeout)
2022-03-28 11:22:58 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1305811ms till timeout)
2022-03-28 11:22:59 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1304790ms till timeout)
2022-03-28 11:23:00 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1303744ms till timeout)
2022-03-28 11:23:02 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1302641ms till timeout)
2022-03-28 11:23:03 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1301569ms till timeout)
2022-03-28 11:23:04 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1300091ms till timeout)
2022-03-28 11:23:05 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1298946ms till timeout)
2022-03-28 11:23:06 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1297904ms till timeout)
2022-03-28 11:23:07 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1296858ms till timeout)
2022-03-28 11:23:08 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1295839ms till timeout)
2022-03-28 11:23:09 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1294761ms till timeout)
2022-03-28 11:23:11 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1293720ms till timeout)
2022-03-28 11:23:12 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1292681ms till timeout)
2022-03-28 11:23:13 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1291663ms till timeout)
2022-03-28 11:23:14 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1290619ms till timeout)
2022-03-28 11:23:15 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1289525ms till timeout)
2022-03-28 11:23:16 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1288455ms till timeout)
2022-03-28 11:23:17 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1287371ms till timeout)
2022-03-28 11:23:18 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1286317ms till timeout)
2022-03-28 11:23:19 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1285273ms till timeout)
2022-03-28 11:23:20 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1284125ms till timeout)
2022-03-28 11:23:21 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1283080ms till timeout)
2022-03-28 11:23:22 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1282005ms till timeout)
2022-03-28 11:23:23 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1280964ms till timeout)
2022-03-28 11:23:24 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1279886ms till timeout)
2022-03-28 11:23:26 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1278464ms till timeout)
2022-03-28 11:23:27 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1277382ms till timeout)
2022-03-28 11:23:28 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1276355ms till timeout)
2022-03-28 11:23:29 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1275325ms till timeout)
2022-03-28 11:23:30 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1274302ms till timeout)
2022-03-28 11:23:31 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1273162ms till timeout)
2022-03-28 11:23:32 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1271997ms till timeout)
2022-03-28 11:23:33 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1270933ms till timeout)
2022-03-28 11:23:34 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1269847ms till timeout)
2022-03-28 11:23:35 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1268788ms till timeout)
2022-03-28 11:23:37 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1267674ms till timeout)
2022-03-28 11:23:38 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1266606ms till timeout)
2022-03-28 11:23:39 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1265514ms till timeout)
2022-03-28 11:23:40 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1264463ms till timeout)
2022-03-28 11:23:41 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1263405ms till timeout)
2022-03-28 11:23:42 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1262379ms till timeout)
2022-03-28 11:23:43 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1261288ms till timeout)
2022-03-28 11:23:44 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1260217ms till timeout)
2022-03-28 11:23:45 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1259170ms till timeout)
2022-03-28 11:23:46 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1258114ms till timeout)
2022-03-28 11:23:47 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1257023ms till timeout)
2022-03-28 11:23:48 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1256001ms till timeout)
2022-03-28 11:23:49 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1254950ms till timeout)
2022-03-28 11:23:50 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1253923ms till timeout)
2022-03-28 11:23:51 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1252885ms till timeout)
2022-03-28 11:23:52 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1251787ms till timeout)
2022-03-28 11:23:54 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1250719ms till timeout)
2022-03-28 11:23:55 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1249679ms till timeout)
2022-03-28 11:23:56 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1248641ms till timeout)
2022-03-28 11:23:57 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1247524ms till timeout)
2022-03-28 11:23:58 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1246282ms till timeout)
2022-03-28 11:23:59 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1245252ms till timeout)
2022-03-28 11:24:00 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1244211ms till timeout)
2022-03-28 11:24:01 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1243188ms till timeout)
2022-03-28 11:24:02 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1242145ms till timeout)
2022-03-28 11:24:03 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1241079ms till timeout)
2022-03-28 11:24:04 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1239960ms till timeout)
2022-03-28 11:24:05 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1238902ms till timeout)
2022-03-28 11:24:06 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1237857ms till timeout)
2022-03-28 11:24:07 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1236826ms till timeout)
2022-03-28 11:24:09 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1235710ms till timeout)
2022-03-28 11:24:10 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1234618ms till timeout)
2022-03-28 11:24:11 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1233561ms till timeout)
2022-03-28 11:24:12 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1232480ms till timeout)
2022-03-28 11:24:13 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1231409ms till timeout)
2022-03-28 11:24:14 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1230358ms till timeout)
2022-03-28 11:24:15 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1229314ms till timeout)
2022-03-28 11:24:16 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1228229ms till timeout)
2022-03-28 11:24:17 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1227195ms till timeout)
2022-03-28 11:24:18 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1226159ms till timeout)
2022-03-28 11:24:19 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1225049ms till timeout)
2022-03-28 11:24:20 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1224030ms till timeout)
2022-03-28 11:24:21 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1222928ms till timeout)
2022-03-28 11:24:22 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1221888ms till timeout)
2022-03-28 11:24:23 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1220846ms till timeout)
2022-03-28 11:24:24 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1219803ms till timeout)
2022-03-28 11:24:25 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1218758ms till timeout)
2022-03-28 11:24:27 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1217701ms till timeout)
2022-03-28 11:24:28 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1216658ms till timeout)
2022-03-28 11:24:29 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1215624ms till timeout)
2022-03-28 11:24:30 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1214513ms till timeout)
2022-03-28 11:24:31 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1213453ms till timeout)
2022-03-28 11:24:32 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1212327ms till timeout)
2022-03-28 11:24:33 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1211257ms till timeout)
2022-03-28 11:24:34 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1210179ms till timeout)
2022-03-28 11:24:35 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1209157ms till timeout)
2022-03-28 11:24:36 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1208064ms till timeout)
2022-03-28 11:24:37 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1206973ms till timeout)
2022-03-28 11:24:38 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1205935ms till timeout)
2022-03-28 11:24:39 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1204860ms till timeout)
2022-03-28 11:24:40 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1203778ms till timeout)
2022-03-28 11:24:41 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1202754ms till timeout)
2022-03-28 11:24:43 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1201717ms till timeout)
2022-03-28 11:24:44 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1200658ms till timeout)
2022-03-28 11:24:45 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1199598ms till timeout)
2022-03-28 11:24:46 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1198560ms till timeout)
2022-03-28 11:24:47 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1197461ms till timeout)
2022-03-28 11:24:48 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1196425ms till timeout)
2022-03-28 11:24:49 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1195369ms till timeout)
2022-03-28 11:24:50 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1194345ms till timeout)
2022-03-28 11:24:51 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1193266ms till timeout)
2022-03-28 11:24:52 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1192248ms till timeout)
2022-03-28 11:24:53 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1191211ms till timeout)
2022-03-28 11:24:54 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1190185ms till timeout)
2022-03-28 11:24:55 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1189168ms till timeout)
2022-03-28 11:24:56 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1188150ms till timeout)
2022-03-28 11:24:57 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1187115ms till timeout)
2022-03-28 11:24:58 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1186092ms till timeout)
2022-03-28 11:24:59 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1185052ms till timeout)
2022-03-28 11:25:00 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (1184022ms till timeout)
2022-03-28 11:25:01 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:444] Kafka: my-cluster-d8b313ba is in desired state: Ready
2022-03-28 11:25:01 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update KafkaTopic my-topic-1306521865-1110190015 in namespace namespace-10
2022-03-28 11:25:01 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:164] Using Namespace: namespace-10
2022-03-28 11:25:01 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1306521865-1110190015
2022-03-28 11:25:01 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-1306521865-1110190015 will have desired state: Ready
2022-03-28 11:25:01 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-1306521865-1110190015 will have desired state: Ready
2022-03-28 11:25:02 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaTopic: my-topic-1306521865-1110190015 will have desired state: Ready not ready, will try again in 1000 ms (179966ms till timeout)
2022-03-28 11:25:03 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaTopic: my-topic-1306521865-1110190015 will have desired state: Ready not ready, will try again in 1000 ms (178912ms till timeout)
2022-03-28 11:25:04 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:444] KafkaTopic: my-topic-1306521865-1110190015 is in desired state: Ready
2022-03-28 11:25:04 [ForkJoinPool-2-worker-3] INFO  [ReconciliationST:147] Adding pause annotation into KafkaTopic resource and changing replication factor
2022-03-28 11:25:04 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-1306521865-1110190015 will have desired state: ReconciliationPaused
2022-03-28 11:25:04 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-1306521865-1110190015 will have desired state: ReconciliationPaused
2022-03-28 11:25:04 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaTopic: my-topic-1306521865-1110190015 will have desired state: ReconciliationPaused not ready, will try again in 1000 ms (179979ms till timeout)
2022-03-28 11:25:05 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:444] KafkaTopic: my-topic-1306521865-1110190015 is in desired state: ReconciliationPaused
2022-03-28 11:25:05 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-d8b313ba-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-1306521865-1110190015 --describe --bootstrap-server my-cluster-d8b313ba-kafka-bootstrap:9092
2022-03-28 11:25:09 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-d8b313ba-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-1306521865-1110190015 --describe --bootstrap-server my-cluster-d8b313ba-kafka-bootstrap:9092
2022-03-28 11:25:09 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 11:25:09 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for KafkaTopic's spec will be stable
2022-03-28 11:25:09 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-d8b313ba-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-1306521865-1110190015 --describe --bootstrap-server my-cluster-d8b313ba-kafka-bootstrap:9092
2022-03-28 11:25:13 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-d8b313ba-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-1306521865-1110190015 --describe --bootstrap-server my-cluster-d8b313ba-kafka-bootstrap:9092
2022-03-28 11:25:13 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 11:25:13 [ForkJoinPool-2-worker-3] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 19 polls
2022-03-28 11:25:13 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (175932ms till timeout)
2022-03-28 11:25:14 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-d8b313ba-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-1306521865-1110190015 --describe --bootstrap-server my-cluster-d8b313ba-kafka-bootstrap:9092
2022-03-28 11:25:18 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-d8b313ba-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-1306521865-1110190015 --describe --bootstrap-server my-cluster-d8b313ba-kafka-bootstrap:9092
2022-03-28 11:25:18 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 11:25:18 [ForkJoinPool-2-worker-3] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 18 polls
2022-03-28 11:25:18 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (170679ms till timeout)
2022-03-28 11:25:19 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-d8b313ba-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-1306521865-1110190015 --describe --bootstrap-server my-cluster-d8b313ba-kafka-bootstrap:9092
2022-03-28 11:25:24 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-d8b313ba-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-1306521865-1110190015 --describe --bootstrap-server my-cluster-d8b313ba-kafka-bootstrap:9092
2022-03-28 11:25:24 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 11:25:24 [ForkJoinPool-2-worker-3] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 17 polls
2022-03-28 11:25:24 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (165375ms till timeout)
2022-03-28 11:25:25 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-d8b313ba-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-1306521865-1110190015 --describe --bootstrap-server my-cluster-d8b313ba-kafka-bootstrap:9092
2022-03-28 11:25:29 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-d8b313ba-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-1306521865-1110190015 --describe --bootstrap-server my-cluster-d8b313ba-kafka-bootstrap:9092
2022-03-28 11:25:29 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 11:25:29 [ForkJoinPool-2-worker-3] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 16 polls
2022-03-28 11:25:29 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (160272ms till timeout)
2022-03-28 11:25:30 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-d8b313ba-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-1306521865-1110190015 --describe --bootstrap-server my-cluster-d8b313ba-kafka-bootstrap:9092
2022-03-28 11:25:34 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-d8b313ba-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-1306521865-1110190015 --describe --bootstrap-server my-cluster-d8b313ba-kafka-bootstrap:9092
2022-03-28 11:25:34 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 11:25:34 [ForkJoinPool-2-worker-3] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 15 polls
2022-03-28 11:25:34 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (155002ms till timeout)
2022-03-28 11:25:35 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-d8b313ba-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-1306521865-1110190015 --describe --bootstrap-server my-cluster-d8b313ba-kafka-bootstrap:9092
2022-03-28 11:25:39 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-d8b313ba-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-1306521865-1110190015 --describe --bootstrap-server my-cluster-d8b313ba-kafka-bootstrap:9092
2022-03-28 11:25:39 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 11:25:39 [ForkJoinPool-2-worker-3] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 14 polls
2022-03-28 11:25:39 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (150095ms till timeout)
2022-03-28 11:25:40 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-d8b313ba-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-1306521865-1110190015 --describe --bootstrap-server my-cluster-d8b313ba-kafka-bootstrap:9092
2022-03-28 11:25:44 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-d8b313ba-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-1306521865-1110190015 --describe --bootstrap-server my-cluster-d8b313ba-kafka-bootstrap:9092
2022-03-28 11:25:44 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 11:25:44 [ForkJoinPool-2-worker-3] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 13 polls
2022-03-28 11:25:44 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (144976ms till timeout)
2022-03-28 11:25:45 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-d8b313ba-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-1306521865-1110190015 --describe --bootstrap-server my-cluster-d8b313ba-kafka-bootstrap:9092
2022-03-28 11:25:49 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-d8b313ba-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-1306521865-1110190015 --describe --bootstrap-server my-cluster-d8b313ba-kafka-bootstrap:9092
2022-03-28 11:25:49 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 11:25:49 [ForkJoinPool-2-worker-3] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 12 polls
2022-03-28 11:25:49 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (139769ms till timeout)
2022-03-28 11:25:50 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-d8b313ba-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-1306521865-1110190015 --describe --bootstrap-server my-cluster-d8b313ba-kafka-bootstrap:9092
2022-03-28 11:25:54 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-d8b313ba-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-1306521865-1110190015 --describe --bootstrap-server my-cluster-d8b313ba-kafka-bootstrap:9092
2022-03-28 11:25:54 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 11:25:54 [ForkJoinPool-2-worker-3] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 11 polls
2022-03-28 11:25:54 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (134552ms till timeout)
2022-03-28 11:25:55 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-d8b313ba-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-1306521865-1110190015 --describe --bootstrap-server my-cluster-d8b313ba-kafka-bootstrap:9092
2022-03-28 11:26:00 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-d8b313ba-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-1306521865-1110190015 --describe --bootstrap-server my-cluster-d8b313ba-kafka-bootstrap:9092
2022-03-28 11:26:00 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 11:26:00 [ForkJoinPool-2-worker-3] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 10 polls
2022-03-28 11:26:00 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (129498ms till timeout)
2022-03-28 11:26:01 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-d8b313ba-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-1306521865-1110190015 --describe --bootstrap-server my-cluster-d8b313ba-kafka-bootstrap:9092
2022-03-28 11:26:05 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-d8b313ba-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-1306521865-1110190015 --describe --bootstrap-server my-cluster-d8b313ba-kafka-bootstrap:9092
2022-03-28 11:26:05 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 11:26:05 [ForkJoinPool-2-worker-3] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 9 polls
2022-03-28 11:26:05 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (124491ms till timeout)
2022-03-28 11:26:06 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-d8b313ba-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-1306521865-1110190015 --describe --bootstrap-server my-cluster-d8b313ba-kafka-bootstrap:9092
2022-03-28 11:26:10 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-d8b313ba-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-1306521865-1110190015 --describe --bootstrap-server my-cluster-d8b313ba-kafka-bootstrap:9092
2022-03-28 11:26:10 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 11:26:10 [ForkJoinPool-2-worker-3] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 8 polls
2022-03-28 11:26:10 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (119172ms till timeout)
2022-03-28 11:26:11 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-d8b313ba-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-1306521865-1110190015 --describe --bootstrap-server my-cluster-d8b313ba-kafka-bootstrap:9092
2022-03-28 11:26:16 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-d8b313ba-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-1306521865-1110190015 --describe --bootstrap-server my-cluster-d8b313ba-kafka-bootstrap:9092
2022-03-28 11:26:16 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 11:26:16 [ForkJoinPool-2-worker-3] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 7 polls
2022-03-28 11:26:16 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (113226ms till timeout)
2022-03-28 11:26:17 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-d8b313ba-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-1306521865-1110190015 --describe --bootstrap-server my-cluster-d8b313ba-kafka-bootstrap:9092
2022-03-28 11:26:21 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-d8b313ba-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-1306521865-1110190015 --describe --bootstrap-server my-cluster-d8b313ba-kafka-bootstrap:9092
2022-03-28 11:26:21 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 11:26:21 [ForkJoinPool-2-worker-3] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 6 polls
2022-03-28 11:26:21 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (107749ms till timeout)
2022-03-28 11:26:22 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-d8b313ba-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-1306521865-1110190015 --describe --bootstrap-server my-cluster-d8b313ba-kafka-bootstrap:9092
2022-03-28 11:26:26 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-d8b313ba-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-1306521865-1110190015 --describe --bootstrap-server my-cluster-d8b313ba-kafka-bootstrap:9092
2022-03-28 11:26:26 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 11:26:26 [ForkJoinPool-2-worker-3] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 5 polls
2022-03-28 11:26:26 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (102622ms till timeout)
2022-03-28 11:26:27 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-d8b313ba-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-1306521865-1110190015 --describe --bootstrap-server my-cluster-d8b313ba-kafka-bootstrap:9092
2022-03-28 11:26:32 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-d8b313ba-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-1306521865-1110190015 --describe --bootstrap-server my-cluster-d8b313ba-kafka-bootstrap:9092
2022-03-28 11:26:32 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 11:26:32 [ForkJoinPool-2-worker-3] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 4 polls
2022-03-28 11:26:32 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (97348ms till timeout)
2022-03-28 11:26:33 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-d8b313ba-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-1306521865-1110190015 --describe --bootstrap-server my-cluster-d8b313ba-kafka-bootstrap:9092
2022-03-28 11:26:42 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-d8b313ba-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-1306521865-1110190015 --describe --bootstrap-server my-cluster-d8b313ba-kafka-bootstrap:9092
2022-03-28 11:26:42 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 11:26:42 [ForkJoinPool-2-worker-3] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 3 polls
2022-03-28 11:26:42 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (87467ms till timeout)
2022-03-28 11:26:43 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-d8b313ba-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-1306521865-1110190015 --describe --bootstrap-server my-cluster-d8b313ba-kafka-bootstrap:9092
2022-03-28 11:26:47 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-d8b313ba-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-1306521865-1110190015 --describe --bootstrap-server my-cluster-d8b313ba-kafka-bootstrap:9092
2022-03-28 11:26:47 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 11:26:47 [ForkJoinPool-2-worker-3] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 2 polls
2022-03-28 11:26:47 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (82070ms till timeout)
2022-03-28 11:26:48 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-d8b313ba-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-1306521865-1110190015 --describe --bootstrap-server my-cluster-d8b313ba-kafka-bootstrap:9092
2022-03-28 11:26:57 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-d8b313ba-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-1306521865-1110190015 --describe --bootstrap-server my-cluster-d8b313ba-kafka-bootstrap:9092
2022-03-28 11:26:57 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 11:26:57 [ForkJoinPool-2-worker-3] INFO  [KafkaTopicUtils:205] KafkaTopic's spec gonna be stable in 1 polls
2022-03-28 11:26:57 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (71888ms till timeout)
2022-03-28 11:26:58 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-d8b313ba-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-1306521865-1110190015 --describe --bootstrap-server my-cluster-d8b313ba-kafka-bootstrap:9092
2022-03-28 11:27:02 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-d8b313ba-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-1306521865-1110190015 --describe --bootstrap-server my-cluster-d8b313ba-kafka-bootstrap:9092
2022-03-28 11:27:02 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 11:27:02 [ForkJoinPool-2-worker-3] INFO  [KafkaTopicUtils:197] KafkaTopic's spec is stable for 20 polls intervals
2022-03-28 11:27:02 [ForkJoinPool-2-worker-3] INFO  [ReconciliationST:156] Setting annotation to "false", partitions should be scaled to 4
2022-03-28 11:27:03 [ForkJoinPool-2-worker-3] INFO  [KafkaTopicUtils:124] Waiting for KafkaTopic change my-topic-1306521865-1110190015
2022-03-28 11:27:03 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for KafkaTopic change my-topic-1306521865-1110190015
2022-03-28 11:27:03 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update KafkaRebalance my-cluster-d8b313ba in namespace namespace-10
2022-03-28 11:27:03 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:164] Using Namespace: namespace-10
2022-03-28 11:27:03 [ForkJoinPool-2-worker-3] WARN  [VersionUsageUtils:60] The client is using resource type 'kafkarebalances' with unstable version 'v1beta2'
2022-03-28 11:27:03 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaRebalance:my-cluster-d8b313ba
2022-03-28 11:27:03 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:433] Wait for KafkaRebalance: my-cluster-d8b313ba will have desired state: PendingProposal
2022-03-28 11:27:03 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for KafkaRebalance: my-cluster-d8b313ba will have desired state: PendingProposal
2022-03-28 11:27:03 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: PendingProposal not ready, will try again in 1000 ms (359922ms till timeout)
2022-03-28 11:27:04 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:444] KafkaRebalance: my-cluster-d8b313ba is in desired state: PendingProposal
2022-03-28 11:27:04 [ForkJoinPool-2-worker-3] INFO  [ReconciliationST:163] Waiting for ProposalReady, then add pause and rebalance annotation, rebalancing should not be triggered
2022-03-28 11:27:04 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:433] Wait for KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady
2022-03-28 11:27:04 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady
2022-03-28 11:27:04 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (599960ms till timeout)
2022-03-28 11:27:05 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (598929ms till timeout)
2022-03-28 11:27:06 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (597917ms till timeout)
2022-03-28 11:27:07 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (596905ms till timeout)
2022-03-28 11:27:08 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (595792ms till timeout)
2022-03-28 11:27:09 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (594710ms till timeout)
2022-03-28 11:27:10 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (593675ms till timeout)
2022-03-28 11:27:11 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (592659ms till timeout)
2022-03-28 11:27:12 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (591610ms till timeout)
2022-03-28 11:27:13 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (590582ms till timeout)
2022-03-28 11:27:14 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (589517ms till timeout)
2022-03-28 11:27:15 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (588505ms till timeout)
2022-03-28 11:27:17 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (587448ms till timeout)
2022-03-28 11:27:18 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (586421ms till timeout)
2022-03-28 11:27:19 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (585373ms till timeout)
2022-03-28 11:27:20 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (584332ms till timeout)
2022-03-28 11:27:21 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (583291ms till timeout)
2022-03-28 11:27:22 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (582240ms till timeout)
2022-03-28 11:27:23 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (581209ms till timeout)
2022-03-28 11:27:24 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (580174ms till timeout)
2022-03-28 11:27:25 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (579054ms till timeout)
2022-03-28 11:27:26 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (578042ms till timeout)
2022-03-28 11:27:27 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (577011ms till timeout)
2022-03-28 11:27:28 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (575979ms till timeout)
2022-03-28 11:27:29 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (574960ms till timeout)
2022-03-28 11:27:30 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (573920ms till timeout)
2022-03-28 11:27:31 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (572894ms till timeout)
2022-03-28 11:27:32 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (571864ms till timeout)
2022-03-28 11:27:33 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (570824ms till timeout)
2022-03-28 11:27:34 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (569808ms till timeout)
2022-03-28 11:27:35 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (568630ms till timeout)
2022-03-28 11:27:36 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (567605ms till timeout)
2022-03-28 11:27:37 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (566585ms till timeout)
2022-03-28 11:27:38 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (565569ms till timeout)
2022-03-28 11:27:39 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (564531ms till timeout)
2022-03-28 11:27:40 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (563491ms till timeout)
2022-03-28 11:27:42 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (562460ms till timeout)
2022-03-28 11:27:43 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (561417ms till timeout)
2022-03-28 11:27:44 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (560385ms till timeout)
2022-03-28 11:27:45 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (559320ms till timeout)
2022-03-28 11:27:46 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (558263ms till timeout)
2022-03-28 11:27:47 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (557205ms till timeout)
2022-03-28 11:27:48 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (556186ms till timeout)
2022-03-28 11:27:49 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (555154ms till timeout)
2022-03-28 11:27:50 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (554125ms till timeout)
2022-03-28 11:27:51 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (553095ms till timeout)
2022-03-28 11:27:52 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (552052ms till timeout)
2022-03-28 11:27:53 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (550998ms till timeout)
2022-03-28 11:27:54 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (549981ms till timeout)
2022-03-28 11:27:55 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (548880ms till timeout)
2022-03-28 11:27:56 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (547799ms till timeout)
2022-03-28 11:27:57 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (546769ms till timeout)
2022-03-28 11:27:58 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (545749ms till timeout)
2022-03-28 11:27:59 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (544721ms till timeout)
2022-03-28 11:28:00 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (543675ms till timeout)
2022-03-28 11:28:01 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (542664ms till timeout)
2022-03-28 11:28:02 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (541643ms till timeout)
2022-03-28 11:28:03 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (540585ms till timeout)
2022-03-28 11:28:04 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (539543ms till timeout)
2022-03-28 11:28:05 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (538500ms till timeout)
2022-03-28 11:28:07 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (537458ms till timeout)
2022-03-28 11:28:08 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (536446ms till timeout)
2022-03-28 11:28:09 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (535399ms till timeout)
2022-03-28 11:28:10 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (534380ms till timeout)
2022-03-28 11:28:11 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (533363ms till timeout)
2022-03-28 11:28:12 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (532348ms till timeout)
2022-03-28 11:28:13 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (531326ms till timeout)
2022-03-28 11:28:14 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (530306ms till timeout)
2022-03-28 11:28:15 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (529276ms till timeout)
2022-03-28 11:28:16 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (528240ms till timeout)
2022-03-28 11:28:17 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (527177ms till timeout)
2022-03-28 11:28:18 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (526130ms till timeout)
2022-03-28 11:28:19 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (525086ms till timeout)
2022-03-28 11:28:20 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (524047ms till timeout)
2022-03-28 11:28:21 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (523026ms till timeout)
2022-03-28 11:28:22 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (521990ms till timeout)
2022-03-28 11:28:23 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (520955ms till timeout)
2022-03-28 11:28:24 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (519937ms till timeout)
2022-03-28 11:28:25 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (518906ms till timeout)
2022-03-28 11:28:26 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (517880ms till timeout)
2022-03-28 11:28:27 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (516786ms till timeout)
2022-03-28 11:28:28 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (515741ms till timeout)
2022-03-28 11:28:29 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (514726ms till timeout)
2022-03-28 11:28:30 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (513643ms till timeout)
2022-03-28 11:28:31 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (512586ms till timeout)
2022-03-28 11:28:32 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (511547ms till timeout)
2022-03-28 11:28:33 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (510476ms till timeout)
2022-03-28 11:28:35 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (509421ms till timeout)
2022-03-28 11:28:36 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (508345ms till timeout)
2022-03-28 11:28:37 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (507326ms till timeout)
2022-03-28 11:28:38 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (506205ms till timeout)
2022-03-28 11:28:39 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (505144ms till timeout)
2022-03-28 11:28:40 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (504132ms till timeout)
2022-03-28 11:28:41 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (503092ms till timeout)
2022-03-28 11:28:42 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (502076ms till timeout)
2022-03-28 11:28:43 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (501048ms till timeout)
2022-03-28 11:28:44 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (500011ms till timeout)
2022-03-28 11:28:45 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (498896ms till timeout)
2022-03-28 11:28:46 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (497872ms till timeout)
2022-03-28 11:28:47 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (496836ms till timeout)
2022-03-28 11:28:48 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (495765ms till timeout)
2022-03-28 11:28:49 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (494713ms till timeout)
2022-03-28 11:28:50 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (493653ms till timeout)
2022-03-28 11:28:51 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (492629ms till timeout)
2022-03-28 11:28:52 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (491570ms till timeout)
2022-03-28 11:28:53 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (490519ms till timeout)
2022-03-28 11:28:54 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (489499ms till timeout)
2022-03-28 11:28:56 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (488454ms till timeout)
2022-03-28 11:28:57 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (487424ms till timeout)
2022-03-28 11:28:58 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (486368ms till timeout)
2022-03-28 11:28:59 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (485266ms till timeout)
2022-03-28 11:29:00 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (484199ms till timeout)
2022-03-28 11:29:01 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (483124ms till timeout)
2022-03-28 11:29:02 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (482104ms till timeout)
2022-03-28 11:29:03 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (481040ms till timeout)
2022-03-28 11:29:04 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (479912ms till timeout)
2022-03-28 11:29:05 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (478862ms till timeout)
2022-03-28 11:29:06 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (477829ms till timeout)
2022-03-28 11:29:07 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (476763ms till timeout)
2022-03-28 11:29:08 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (475737ms till timeout)
2022-03-28 11:29:09 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (474620ms till timeout)
2022-03-28 11:29:10 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (473595ms till timeout)
2022-03-28 11:29:11 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (472535ms till timeout)
2022-03-28 11:29:12 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (471503ms till timeout)
2022-03-28 11:29:14 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (470456ms till timeout)
2022-03-28 11:29:15 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (469428ms till timeout)
2022-03-28 11:29:16 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (468359ms till timeout)
2022-03-28 11:29:17 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (467306ms till timeout)
2022-03-28 11:29:18 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (466274ms till timeout)
2022-03-28 11:29:19 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (465197ms till timeout)
2022-03-28 11:29:20 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (464143ms till timeout)
2022-03-28 11:29:21 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (463085ms till timeout)
2022-03-28 11:29:22 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (462042ms till timeout)
2022-03-28 11:29:23 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (461008ms till timeout)
2022-03-28 11:29:24 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (459996ms till timeout)
2022-03-28 11:29:25 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (458924ms till timeout)
2022-03-28 11:29:26 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (457890ms till timeout)
2022-03-28 11:29:27 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (456832ms till timeout)
2022-03-28 11:29:28 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (455816ms till timeout)
2022-03-28 11:29:29 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (454779ms till timeout)
2022-03-28 11:29:30 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (453710ms till timeout)
2022-03-28 11:29:31 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (452689ms till timeout)
2022-03-28 11:29:32 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (451656ms till timeout)
2022-03-28 11:29:33 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (450623ms till timeout)
2022-03-28 11:29:34 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (449583ms till timeout)
2022-03-28 11:29:35 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (448528ms till timeout)
2022-03-28 11:29:37 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (447438ms till timeout)
2022-03-28 11:29:38 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (446405ms till timeout)
2022-03-28 11:29:39 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (445380ms till timeout)
2022-03-28 11:29:40 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (444317ms till timeout)
2022-03-28 11:29:41 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (443257ms till timeout)
2022-03-28 11:29:42 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (442200ms till timeout)
2022-03-28 11:29:43 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (441181ms till timeout)
2022-03-28 11:29:44 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (440143ms till timeout)
2022-03-28 11:29:45 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (439119ms till timeout)
2022-03-28 11:29:46 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (438082ms till timeout)
2022-03-28 11:29:47 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (437053ms till timeout)
2022-03-28 11:29:48 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (436037ms till timeout)
2022-03-28 11:29:49 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (435004ms till timeout)
2022-03-28 11:29:50 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (433970ms till timeout)
2022-03-28 11:29:51 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (432873ms till timeout)
2022-03-28 11:29:52 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (431837ms till timeout)
2022-03-28 11:29:53 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (430818ms till timeout)
2022-03-28 11:29:54 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (429793ms till timeout)
2022-03-28 11:29:55 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (428770ms till timeout)
2022-03-28 11:29:56 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (427722ms till timeout)
2022-03-28 11:29:57 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (426701ms till timeout)
2022-03-28 11:29:58 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (425670ms till timeout)
2022-03-28 11:29:59 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (424657ms till timeout)
2022-03-28 11:30:00 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (423634ms till timeout)
2022-03-28 11:30:01 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (422584ms till timeout)
2022-03-28 11:30:02 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (421525ms till timeout)
2022-03-28 11:30:03 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (420500ms till timeout)
2022-03-28 11:30:05 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (419429ms till timeout)
2022-03-28 11:30:06 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (418376ms till timeout)
2022-03-28 11:30:07 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (417330ms till timeout)
2022-03-28 11:30:08 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (416272ms till timeout)
2022-03-28 11:30:09 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (415225ms till timeout)
2022-03-28 11:30:10 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (414179ms till timeout)
2022-03-28 11:30:11 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (413126ms till timeout)
2022-03-28 11:30:12 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (411965ms till timeout)
2022-03-28 11:30:13 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (410922ms till timeout)
2022-03-28 11:30:14 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (409873ms till timeout)
2022-03-28 11:30:15 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (408840ms till timeout)
2022-03-28 11:30:16 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (407800ms till timeout)
2022-03-28 11:30:17 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (406779ms till timeout)
2022-03-28 11:30:18 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (405730ms till timeout)
2022-03-28 11:30:19 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (404681ms till timeout)
2022-03-28 11:30:20 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (403644ms till timeout)
2022-03-28 11:30:21 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (402600ms till timeout)
2022-03-28 11:30:22 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (401492ms till timeout)
2022-03-28 11:30:23 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (400479ms till timeout)
2022-03-28 11:30:24 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (399464ms till timeout)
2022-03-28 11:30:26 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (398417ms till timeout)
2022-03-28 11:30:27 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (397332ms till timeout)
2022-03-28 11:30:28 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (396308ms till timeout)
2022-03-28 11:30:29 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (395253ms till timeout)
2022-03-28 11:30:30 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (394168ms till timeout)
2022-03-28 11:30:31 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (393124ms till timeout)
2022-03-28 11:30:32 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (392097ms till timeout)
2022-03-28 11:30:33 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (391030ms till timeout)
2022-03-28 11:30:34 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (390008ms till timeout)
2022-03-28 11:30:35 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (388982ms till timeout)
2022-03-28 11:30:36 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (387930ms till timeout)
2022-03-28 11:30:37 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (386908ms till timeout)
2022-03-28 11:30:38 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (385885ms till timeout)
2022-03-28 11:30:39 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (384848ms till timeout)
2022-03-28 11:30:40 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (383764ms till timeout)
2022-03-28 11:30:41 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (382736ms till timeout)
2022-03-28 11:30:42 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (381693ms till timeout)
2022-03-28 11:30:43 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (380613ms till timeout)
2022-03-28 11:30:44 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (379546ms till timeout)
2022-03-28 11:30:45 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (378514ms till timeout)
2022-03-28 11:30:46 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (377474ms till timeout)
2022-03-28 11:30:48 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (376442ms till timeout)
2022-03-28 11:30:49 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (375409ms till timeout)
2022-03-28 11:30:50 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (374353ms till timeout)
2022-03-28 11:30:51 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (373319ms till timeout)
2022-03-28 11:30:52 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (372281ms till timeout)
2022-03-28 11:30:53 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (371252ms till timeout)
2022-03-28 11:30:54 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (370137ms till timeout)
2022-03-28 11:30:55 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (369100ms till timeout)
2022-03-28 11:30:56 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (368043ms till timeout)
2022-03-28 11:30:57 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (367027ms till timeout)
2022-03-28 11:30:58 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (365980ms till timeout)
2022-03-28 11:30:59 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (364938ms till timeout)
2022-03-28 11:31:00 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (363897ms till timeout)
2022-03-28 11:31:01 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (362857ms till timeout)
2022-03-28 11:31:02 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (361811ms till timeout)
2022-03-28 11:31:03 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (360786ms till timeout)
2022-03-28 11:31:04 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (359698ms till timeout)
2022-03-28 11:31:05 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (358608ms till timeout)
2022-03-28 11:31:06 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (357592ms till timeout)
2022-03-28 11:31:07 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (356575ms till timeout)
2022-03-28 11:31:08 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (355541ms till timeout)
2022-03-28 11:31:09 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (354501ms till timeout)
2022-03-28 11:31:10 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (353487ms till timeout)
2022-03-28 11:31:11 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (352473ms till timeout)
2022-03-28 11:31:13 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (351449ms till timeout)
2022-03-28 11:31:14 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (350398ms till timeout)
2022-03-28 11:31:15 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (349285ms till timeout)
2022-03-28 11:31:16 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (348251ms till timeout)
2022-03-28 11:31:17 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (347220ms till timeout)
2022-03-28 11:31:18 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (346152ms till timeout)
2022-03-28 11:31:19 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (345124ms till timeout)
2022-03-28 11:31:20 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (344091ms till timeout)
2022-03-28 11:31:21 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (343042ms till timeout)
2022-03-28 11:31:22 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (342026ms till timeout)
2022-03-28 11:31:23 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (340993ms till timeout)
2022-03-28 11:31:24 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (339949ms till timeout)
2022-03-28 11:31:25 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (338776ms till timeout)
2022-03-28 11:31:26 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (337747ms till timeout)
2022-03-28 11:31:27 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (336689ms till timeout)
2022-03-28 11:31:28 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (335654ms till timeout)
2022-03-28 11:31:29 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (334619ms till timeout)
2022-03-28 11:31:30 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (333605ms till timeout)
2022-03-28 11:31:31 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (332589ms till timeout)
2022-03-28 11:31:32 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (331537ms till timeout)
2022-03-28 11:31:33 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (330497ms till timeout)
2022-03-28 11:31:34 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (329481ms till timeout)
2022-03-28 11:31:36 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (328436ms till timeout)
2022-03-28 11:31:37 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (327400ms till timeout)
2022-03-28 11:31:38 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (326337ms till timeout)
2022-03-28 11:31:39 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (325302ms till timeout)
2022-03-28 11:31:40 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (324281ms till timeout)
2022-03-28 11:31:41 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (323254ms till timeout)
2022-03-28 11:31:42 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (322178ms till timeout)
2022-03-28 11:31:43 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (321143ms till timeout)
2022-03-28 11:31:44 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (320072ms till timeout)
2022-03-28 11:31:45 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (319048ms till timeout)
2022-03-28 11:31:46 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (317915ms till timeout)
2022-03-28 11:31:47 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (316880ms till timeout)
2022-03-28 11:31:48 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (315839ms till timeout)
2022-03-28 11:31:49 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (314794ms till timeout)
2022-03-28 11:31:50 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (313728ms till timeout)
2022-03-28 11:31:51 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (312683ms till timeout)
2022-03-28 11:31:52 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (311609ms till timeout)
2022-03-28 11:31:53 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (310568ms till timeout)
2022-03-28 11:31:54 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (309530ms till timeout)
2022-03-28 11:31:55 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (308484ms till timeout)
2022-03-28 11:31:57 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (307387ms till timeout)
2022-03-28 11:31:58 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (306338ms till timeout)
2022-03-28 11:31:59 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (305295ms till timeout)
2022-03-28 11:32:00 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (304270ms till timeout)
2022-03-28 11:32:01 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (303247ms till timeout)
2022-03-28 11:32:02 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (302185ms till timeout)
2022-03-28 11:32:03 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (301159ms till timeout)
2022-03-28 11:32:04 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (300141ms till timeout)
2022-03-28 11:32:05 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (299110ms till timeout)
2022-03-28 11:32:06 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (298064ms till timeout)
2022-03-28 11:32:07 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (296970ms till timeout)
2022-03-28 11:32:08 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (295926ms till timeout)
2022-03-28 11:32:09 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (294887ms till timeout)
2022-03-28 11:32:10 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (293848ms till timeout)
2022-03-28 11:32:11 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (292814ms till timeout)
2022-03-28 11:32:12 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (291794ms till timeout)
2022-03-28 11:32:13 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (290767ms till timeout)
2022-03-28 11:32:14 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (289739ms till timeout)
2022-03-28 11:32:15 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (288698ms till timeout)
2022-03-28 11:32:16 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (287571ms till timeout)
2022-03-28 11:32:17 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (286495ms till timeout)
2022-03-28 11:32:18 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (285481ms till timeout)
2022-03-28 11:32:20 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (284431ms till timeout)
2022-03-28 11:32:21 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (283374ms till timeout)
2022-03-28 11:32:22 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (282323ms till timeout)
2022-03-28 11:32:23 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (281311ms till timeout)
2022-03-28 11:32:24 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (280277ms till timeout)
2022-03-28 11:32:25 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (279223ms till timeout)
2022-03-28 11:32:26 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (278189ms till timeout)
2022-03-28 11:32:27 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (277145ms till timeout)
2022-03-28 11:32:28 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (275968ms till timeout)
2022-03-28 11:32:29 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (274914ms till timeout)
2022-03-28 11:32:30 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (273898ms till timeout)
2022-03-28 11:32:31 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (272879ms till timeout)
2022-03-28 11:32:32 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (271851ms till timeout)
2022-03-28 11:32:33 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (270832ms till timeout)
2022-03-28 11:32:34 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (269782ms till timeout)
2022-03-28 11:32:35 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (268736ms till timeout)
2022-03-28 11:32:36 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (267667ms till timeout)
2022-03-28 11:32:37 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (266627ms till timeout)
2022-03-28 11:32:38 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (265524ms till timeout)
2022-03-28 11:32:39 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (264505ms till timeout)
2022-03-28 11:32:40 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (263464ms till timeout)
2022-03-28 11:32:42 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (262429ms till timeout)
2022-03-28 11:32:43 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (261400ms till timeout)
2022-03-28 11:32:44 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (260370ms till timeout)
2022-03-28 11:32:45 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (259340ms till timeout)
2022-03-28 11:32:46 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (258316ms till timeout)
2022-03-28 11:32:47 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (257252ms till timeout)
2022-03-28 11:32:48 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (256234ms till timeout)
2022-03-28 11:32:49 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (255113ms till timeout)
2022-03-28 11:32:50 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (254076ms till timeout)
2022-03-28 11:32:51 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (253045ms till timeout)
2022-03-28 11:32:52 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (252017ms till timeout)
2022-03-28 11:32:53 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (250988ms till timeout)
2022-03-28 11:32:54 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (249960ms till timeout)
2022-03-28 11:32:55 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (248889ms till timeout)
2022-03-28 11:32:56 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (247818ms till timeout)
2022-03-28 11:32:57 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (246775ms till timeout)
2022-03-28 11:32:58 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (245743ms till timeout)
2022-03-28 11:32:59 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (244698ms till timeout)
2022-03-28 11:33:00 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (243673ms till timeout)
2022-03-28 11:33:01 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (242659ms till timeout)
2022-03-28 11:33:02 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (241641ms till timeout)
2022-03-28 11:33:03 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (240629ms till timeout)
2022-03-28 11:33:04 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (239614ms till timeout)
2022-03-28 11:33:05 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (238598ms till timeout)
2022-03-28 11:33:06 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (237530ms till timeout)
2022-03-28 11:33:07 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (236480ms till timeout)
2022-03-28 11:33:09 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (235446ms till timeout)
2022-03-28 11:33:10 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (234308ms till timeout)
2022-03-28 11:33:11 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (233286ms till timeout)
2022-03-28 11:33:12 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (232245ms till timeout)
2022-03-28 11:33:13 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (231196ms till timeout)
2022-03-28 11:33:14 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (230135ms till timeout)
2022-03-28 11:33:15 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (229062ms till timeout)
2022-03-28 11:33:16 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (228023ms till timeout)
2022-03-28 11:33:17 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (226969ms till timeout)
2022-03-28 11:33:18 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (225896ms till timeout)
2022-03-28 11:33:19 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (224848ms till timeout)
2022-03-28 11:33:20 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (223783ms till timeout)
2022-03-28 11:33:21 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (222768ms till timeout)
2022-03-28 11:33:22 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (221741ms till timeout)
2022-03-28 11:33:23 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (220728ms till timeout)
2022-03-28 11:33:24 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (219699ms till timeout)
2022-03-28 11:33:25 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (218663ms till timeout)
2022-03-28 11:33:26 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (217641ms till timeout)
2022-03-28 11:33:27 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (216615ms till timeout)
2022-03-28 11:33:28 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (215545ms till timeout)
2022-03-28 11:33:29 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (214509ms till timeout)
2022-03-28 11:33:31 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (213365ms till timeout)
2022-03-28 11:33:32 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (212314ms till timeout)
2022-03-28 11:33:33 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (211296ms till timeout)
2022-03-28 11:33:34 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (210283ms till timeout)
2022-03-28 11:33:35 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (209241ms till timeout)
2022-03-28 11:33:36 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (208206ms till timeout)
2022-03-28 11:33:37 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (207148ms till timeout)
2022-03-28 11:33:38 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (206089ms till timeout)
2022-03-28 11:33:39 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (205052ms till timeout)
2022-03-28 11:33:40 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (204042ms till timeout)
2022-03-28 11:33:41 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (202963ms till timeout)
2022-03-28 11:33:42 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (201926ms till timeout)
2022-03-28 11:33:43 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (200883ms till timeout)
2022-03-28 11:33:44 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (199805ms till timeout)
2022-03-28 11:33:45 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (198794ms till timeout)
2022-03-28 11:33:46 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (197720ms till timeout)
2022-03-28 11:33:47 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (196685ms till timeout)
2022-03-28 11:33:48 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (195667ms till timeout)
2022-03-28 11:33:49 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (194656ms till timeout)
2022-03-28 11:33:50 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (193629ms till timeout)
2022-03-28 11:33:51 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (192572ms till timeout)
2022-03-28 11:33:52 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (191550ms till timeout)
2022-03-28 11:33:53 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (190522ms till timeout)
2022-03-28 11:33:54 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (189504ms till timeout)
2022-03-28 11:33:55 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (188492ms till timeout)
2022-03-28 11:33:56 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (187476ms till timeout)
2022-03-28 11:33:58 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (186458ms till timeout)
2022-03-28 11:33:59 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (185294ms till timeout)
2022-03-28 11:34:00 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (184250ms till timeout)
2022-03-28 11:34:01 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (183239ms till timeout)
2022-03-28 11:34:02 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (182154ms till timeout)
2022-03-28 11:34:03 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (181118ms till timeout)
2022-03-28 11:34:04 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (180086ms till timeout)
2022-03-28 11:34:05 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (179044ms till timeout)
2022-03-28 11:34:06 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (177984ms till timeout)
2022-03-28 11:34:07 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (176961ms till timeout)
2022-03-28 11:34:08 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (175926ms till timeout)
2022-03-28 11:34:09 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (174916ms till timeout)
2022-03-28 11:34:10 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (173886ms till timeout)
2022-03-28 11:34:11 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (172852ms till timeout)
2022-03-28 11:34:12 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (171789ms till timeout)
2022-03-28 11:34:13 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (170703ms till timeout)
2022-03-28 11:34:14 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (169688ms till timeout)
2022-03-28 11:34:15 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (168673ms till timeout)
2022-03-28 11:34:16 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (167617ms till timeout)
2022-03-28 11:34:17 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (166584ms till timeout)
2022-03-28 11:34:18 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (165538ms till timeout)
2022-03-28 11:34:19 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (164470ms till timeout)
2022-03-28 11:34:21 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (163446ms till timeout)
2022-03-28 11:34:22 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (162429ms till timeout)
2022-03-28 11:34:23 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (161310ms till timeout)
2022-03-28 11:34:24 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (160293ms till timeout)
2022-03-28 11:34:25 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (159265ms till timeout)
2022-03-28 11:34:26 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (158212ms till timeout)
2022-03-28 11:34:27 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (157174ms till timeout)
2022-03-28 11:34:28 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (156144ms till timeout)
2022-03-28 11:34:29 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (154991ms till timeout)
2022-03-28 11:34:30 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (153964ms till timeout)
2022-03-28 11:34:31 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (152927ms till timeout)
2022-03-28 11:34:32 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (151863ms till timeout)
2022-03-28 11:34:33 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (150789ms till timeout)
2022-03-28 11:34:34 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (149725ms till timeout)
2022-03-28 11:34:35 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (148709ms till timeout)
2022-03-28 11:34:36 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (147672ms till timeout)
2022-03-28 11:34:37 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (146645ms till timeout)
2022-03-28 11:34:38 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (145622ms till timeout)
2022-03-28 11:34:39 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (144569ms till timeout)
2022-03-28 11:34:40 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (143524ms till timeout)
2022-03-28 11:34:41 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (142495ms till timeout)
2022-03-28 11:34:43 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (141401ms till timeout)
2022-03-28 11:34:44 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (140292ms till timeout)
2022-03-28 11:34:45 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (139215ms till timeout)
2022-03-28 11:34:46 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (138158ms till timeout)
2022-03-28 11:34:47 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (137083ms till timeout)
2022-03-28 11:34:48 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (136041ms till timeout)
2022-03-28 11:34:49 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (134981ms till timeout)
2022-03-28 11:34:50 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (133917ms till timeout)
2022-03-28 11:34:51 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (132861ms till timeout)
2022-03-28 11:34:52 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (131842ms till timeout)
2022-03-28 11:34:53 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (130793ms till timeout)
2022-03-28 11:34:54 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (129726ms till timeout)
2022-03-28 11:34:55 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (128700ms till timeout)
2022-03-28 11:34:56 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (127656ms till timeout)
2022-03-28 11:34:57 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (126590ms till timeout)
2022-03-28 11:34:58 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (125542ms till timeout)
2022-03-28 11:34:59 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (124514ms till timeout)
2022-03-28 11:35:01 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (123446ms till timeout)
2022-03-28 11:35:02 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (122420ms till timeout)
2022-03-28 11:35:03 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (121400ms till timeout)
2022-03-28 11:35:04 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (120343ms till timeout)
2022-03-28 11:35:05 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (119203ms till timeout)
2022-03-28 11:35:06 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (118154ms till timeout)
2022-03-28 11:35:07 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (117130ms till timeout)
2022-03-28 11:35:08 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (116108ms till timeout)
2022-03-28 11:35:09 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (115074ms till timeout)
2022-03-28 11:35:10 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (114026ms till timeout)
2022-03-28 11:35:11 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (112966ms till timeout)
2022-03-28 11:35:12 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (111942ms till timeout)
2022-03-28 11:35:13 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (110902ms till timeout)
2022-03-28 11:35:14 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (109863ms till timeout)
2022-03-28 11:35:15 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (108791ms till timeout)
2022-03-28 11:35:16 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (107775ms till timeout)
2022-03-28 11:35:17 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (106761ms till timeout)
2022-03-28 11:35:18 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (105719ms till timeout)
2022-03-28 11:35:19 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (104690ms till timeout)
2022-03-28 11:35:20 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (103654ms till timeout)
2022-03-28 11:35:21 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (102588ms till timeout)
2022-03-28 11:35:22 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (101558ms till timeout)
2022-03-28 11:35:23 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (100537ms till timeout)
2022-03-28 11:35:24 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (99512ms till timeout)
2022-03-28 11:35:26 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (98429ms till timeout)
2022-03-28 11:35:27 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (97375ms till timeout)
2022-03-28 11:35:28 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (96337ms till timeout)
2022-03-28 11:35:29 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (95274ms till timeout)
2022-03-28 11:35:30 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (94254ms till timeout)
2022-03-28 11:35:31 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (93224ms till timeout)
2022-03-28 11:35:32 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (92201ms till timeout)
2022-03-28 11:35:33 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (91170ms till timeout)
2022-03-28 11:35:34 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (90154ms till timeout)
2022-03-28 11:35:35 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (89118ms till timeout)
2022-03-28 11:35:36 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (88021ms till timeout)
2022-03-28 11:35:37 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (86959ms till timeout)
2022-03-28 11:35:38 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (85938ms till timeout)
2022-03-28 11:35:39 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (84920ms till timeout)
2022-03-28 11:35:40 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (83869ms till timeout)
2022-03-28 11:35:41 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (82840ms till timeout)
2022-03-28 11:35:42 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (81805ms till timeout)
2022-03-28 11:35:43 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (80783ms till timeout)
2022-03-28 11:35:44 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (79773ms till timeout)
2022-03-28 11:35:45 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (78741ms till timeout)
2022-03-28 11:35:46 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (77658ms till timeout)
2022-03-28 11:35:47 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (76627ms till timeout)
2022-03-28 11:35:48 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (75608ms till timeout)
2022-03-28 11:35:49 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (74585ms till timeout)
2022-03-28 11:35:50 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (73562ms till timeout)
2022-03-28 11:35:51 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (72504ms till timeout)
2022-03-28 11:35:52 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (71473ms till timeout)
2022-03-28 11:35:54 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (70443ms till timeout)
2022-03-28 11:35:55 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:444] KafkaRebalance: my-cluster-d8b313ba is in desired state: ProposalReady
2022-03-28 11:35:55 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:433] Wait for KafkaRebalance: my-cluster-d8b313ba will have desired state: ReconciliationPaused
2022-03-28 11:35:55 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for KafkaRebalance: my-cluster-d8b313ba will have desired state: ReconciliationPaused
2022-03-28 11:35:55 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ReconciliationPaused not ready, will try again in 1000 ms (359944ms till timeout)
2022-03-28 11:35:56 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:444] KafkaRebalance: my-cluster-d8b313ba is in desired state: ReconciliationPaused
2022-03-28 11:35:56 [ForkJoinPool-2-worker-3] INFO  [KafkaRebalanceUtils:63] Reconciliation #1(test) KafkaRebalance(namespace-10/my-cluster-d8b313ba): Annotating KafkaRebalance:my-cluster-d8b313ba with annotation approve
2022-03-28 11:35:56 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 annotate kafkarebalance my-cluster-d8b313ba strimzi.io/rebalance=approve
2022-03-28 11:35:56 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 annotate kafkarebalance my-cluster-d8b313ba strimzi.io/rebalance=approve
2022-03-28 11:35:56 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:35:56 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for KafkaRebalance status will be stable
2022-03-28 11:35:57 [ForkJoinPool-2-worker-3] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-d8b313ba): KafkaRebalance status gonna be stable in 19 polls
2022-03-28 11:35:57 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (179937ms till timeout)
2022-03-28 11:35:58 [ForkJoinPool-2-worker-3] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-d8b313ba): KafkaRebalance status gonna be stable in 18 polls
2022-03-28 11:35:58 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (178902ms till timeout)
2022-03-28 11:35:59 [ForkJoinPool-2-worker-3] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-d8b313ba): KafkaRebalance status gonna be stable in 17 polls
2022-03-28 11:35:59 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (177854ms till timeout)
2022-03-28 11:36:00 [ForkJoinPool-2-worker-3] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-d8b313ba): KafkaRebalance status gonna be stable in 16 polls
2022-03-28 11:36:00 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (176810ms till timeout)
2022-03-28 11:36:01 [ForkJoinPool-2-worker-3] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-d8b313ba): KafkaRebalance status gonna be stable in 15 polls
2022-03-28 11:36:01 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (175787ms till timeout)
2022-03-28 11:36:02 [ForkJoinPool-2-worker-3] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-d8b313ba): KafkaRebalance status gonna be stable in 14 polls
2022-03-28 11:36:02 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (174751ms till timeout)
2022-03-28 11:36:03 [ForkJoinPool-2-worker-3] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-d8b313ba): KafkaRebalance status gonna be stable in 13 polls
2022-03-28 11:36:03 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (173720ms till timeout)
2022-03-28 11:36:04 [ForkJoinPool-2-worker-3] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-d8b313ba): KafkaRebalance status gonna be stable in 12 polls
2022-03-28 11:36:04 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (172685ms till timeout)
2022-03-28 11:36:05 [ForkJoinPool-2-worker-3] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-d8b313ba): KafkaRebalance status gonna be stable in 11 polls
2022-03-28 11:36:05 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (171666ms till timeout)
2022-03-28 11:36:06 [ForkJoinPool-2-worker-3] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-d8b313ba): KafkaRebalance status gonna be stable in 10 polls
2022-03-28 11:36:06 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (170630ms till timeout)
2022-03-28 11:36:07 [ForkJoinPool-2-worker-3] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-d8b313ba): KafkaRebalance status gonna be stable in 9 polls
2022-03-28 11:36:07 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (169602ms till timeout)
2022-03-28 11:36:08 [ForkJoinPool-2-worker-3] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-d8b313ba): KafkaRebalance status gonna be stable in 8 polls
2022-03-28 11:36:08 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (168571ms till timeout)
2022-03-28 11:36:09 [ForkJoinPool-2-worker-3] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-d8b313ba): KafkaRebalance status gonna be stable in 7 polls
2022-03-28 11:36:09 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (167524ms till timeout)
2022-03-28 11:36:10 [ForkJoinPool-2-worker-3] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-d8b313ba): KafkaRebalance status gonna be stable in 6 polls
2022-03-28 11:36:10 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (166465ms till timeout)
2022-03-28 11:36:11 [ForkJoinPool-2-worker-3] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-d8b313ba): KafkaRebalance status gonna be stable in 5 polls
2022-03-28 11:36:11 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (165443ms till timeout)
2022-03-28 11:36:12 [ForkJoinPool-2-worker-3] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-d8b313ba): KafkaRebalance status gonna be stable in 4 polls
2022-03-28 11:36:12 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (164411ms till timeout)
2022-03-28 11:36:13 [ForkJoinPool-2-worker-3] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-d8b313ba): KafkaRebalance status gonna be stable in 3 polls
2022-03-28 11:36:13 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (163390ms till timeout)
2022-03-28 11:36:14 [ForkJoinPool-2-worker-3] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-d8b313ba): KafkaRebalance status gonna be stable in 2 polls
2022-03-28 11:36:14 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (162360ms till timeout)
2022-03-28 11:36:15 [ForkJoinPool-2-worker-3] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-d8b313ba): KafkaRebalance status gonna be stable in 1 polls
2022-03-28 11:36:15 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (161344ms till timeout)
2022-03-28 11:36:16 [ForkJoinPool-2-worker-3] INFO  [KafkaRebalanceUtils:118] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-d8b313ba): KafkaRebalance status is stable for 20 polls intervals
2022-03-28 11:36:16 [ForkJoinPool-2-worker-3] INFO  [ReconciliationST:177] Setting annotation to "false" and waiting for KafkaRebalance to be in Ready state
2022-03-28 11:36:16 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:433] Wait for KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady
2022-03-28 11:36:16 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady
2022-03-28 11:36:16 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: ProposalReady not ready, will try again in 1000 ms (599946ms till timeout)
2022-03-28 11:36:17 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:444] KafkaRebalance: my-cluster-d8b313ba is in desired state: ProposalReady
2022-03-28 11:36:17 [ForkJoinPool-2-worker-3] INFO  [KafkaRebalanceUtils:63] Reconciliation #3(test) KafkaRebalance(namespace-10/my-cluster-d8b313ba): Annotating KafkaRebalance:my-cluster-d8b313ba with annotation approve
2022-03-28 11:36:17 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 annotate kafkarebalance my-cluster-d8b313ba strimzi.io/rebalance=approve
2022-03-28 11:36:18 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 annotate kafkarebalance my-cluster-d8b313ba strimzi.io/rebalance=approve
2022-03-28 11:36:18 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:36:18 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:433] Wait for KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready
2022-03-28 11:36:18 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready
2022-03-28 11:36:18 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (599961ms till timeout)
2022-03-28 11:36:19 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (598926ms till timeout)
2022-03-28 11:36:20 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (597874ms till timeout)
2022-03-28 11:36:21 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (596838ms till timeout)
2022-03-28 11:36:22 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (595817ms till timeout)
2022-03-28 11:36:23 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (594799ms till timeout)
2022-03-28 11:36:24 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (593761ms till timeout)
2022-03-28 11:36:25 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (592651ms till timeout)
2022-03-28 11:36:26 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (591603ms till timeout)
2022-03-28 11:36:28 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (590422ms till timeout)
2022-03-28 11:36:29 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (589342ms till timeout)
2022-03-28 11:36:30 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (588282ms till timeout)
2022-03-28 11:36:31 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (587221ms till timeout)
2022-03-28 11:36:32 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (586192ms till timeout)
2022-03-28 11:36:33 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (585154ms till timeout)
2022-03-28 11:36:34 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (584053ms till timeout)
2022-03-28 11:36:35 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (583017ms till timeout)
2022-03-28 11:36:36 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (581976ms till timeout)
2022-03-28 11:36:37 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (580960ms till timeout)
2022-03-28 11:36:38 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (579917ms till timeout)
2022-03-28 11:36:39 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (578887ms till timeout)
2022-03-28 11:36:40 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (577866ms till timeout)
2022-03-28 11:36:41 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (576840ms till timeout)
2022-03-28 11:36:42 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (575794ms till timeout)
2022-03-28 11:36:43 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (574771ms till timeout)
2022-03-28 11:36:44 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (573747ms till timeout)
2022-03-28 11:36:45 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (572722ms till timeout)
2022-03-28 11:36:46 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (571658ms till timeout)
2022-03-28 11:36:47 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (570590ms till timeout)
2022-03-28 11:36:49 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (569540ms till timeout)
2022-03-28 11:36:50 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (568499ms till timeout)
2022-03-28 11:36:51 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (567418ms till timeout)
2022-03-28 11:36:52 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (566392ms till timeout)
2022-03-28 11:36:53 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (565361ms till timeout)
2022-03-28 11:36:54 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (564347ms till timeout)
2022-03-28 11:36:55 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (563315ms till timeout)
2022-03-28 11:36:56 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (562271ms till timeout)
2022-03-28 11:36:57 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (561225ms till timeout)
2022-03-28 11:36:58 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (560191ms till timeout)
2022-03-28 11:36:59 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (559035ms till timeout)
2022-03-28 11:37:00 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (558010ms till timeout)
2022-03-28 11:37:01 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (556968ms till timeout)
2022-03-28 11:37:02 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (555931ms till timeout)
2022-03-28 11:37:03 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (554913ms till timeout)
2022-03-28 11:37:04 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (553875ms till timeout)
2022-03-28 11:37:05 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (552838ms till timeout)
2022-03-28 11:37:06 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (551800ms till timeout)
2022-03-28 11:37:07 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (550775ms till timeout)
2022-03-28 11:37:08 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (549759ms till timeout)
2022-03-28 11:37:10 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (548557ms till timeout)
2022-03-28 11:37:11 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (547534ms till timeout)
2022-03-28 11:37:12 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (546501ms till timeout)
2022-03-28 11:37:13 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (545435ms till timeout)
2022-03-28 11:37:14 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (544402ms till timeout)
2022-03-28 11:37:15 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (543355ms till timeout)
2022-03-28 11:37:16 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (542293ms till timeout)
2022-03-28 11:37:17 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (541241ms till timeout)
2022-03-28 11:37:18 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (540187ms till timeout)
2022-03-28 11:37:19 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (539139ms till timeout)
2022-03-28 11:37:20 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (538086ms till timeout)
2022-03-28 11:37:21 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (537050ms till timeout)
2022-03-28 11:37:22 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (536002ms till timeout)
2022-03-28 11:37:23 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (534982ms till timeout)
2022-03-28 11:37:24 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (533958ms till timeout)
2022-03-28 11:37:25 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (532888ms till timeout)
2022-03-28 11:37:26 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (531841ms till timeout)
2022-03-28 11:37:27 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (530804ms till timeout)
2022-03-28 11:37:28 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (529750ms till timeout)
2022-03-28 11:37:29 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (528678ms till timeout)
2022-03-28 11:37:31 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (527584ms till timeout)
2022-03-28 11:37:32 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (526486ms till timeout)
2022-03-28 11:37:33 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (525448ms till timeout)
2022-03-28 11:37:34 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (524381ms till timeout)
2022-03-28 11:37:35 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (523343ms till timeout)
2022-03-28 11:37:36 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (522287ms till timeout)
2022-03-28 11:37:37 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (521231ms till timeout)
2022-03-28 11:37:38 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (520201ms till timeout)
2022-03-28 11:37:39 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (519169ms till timeout)
2022-03-28 11:37:40 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (518151ms till timeout)
2022-03-28 11:37:41 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (517002ms till timeout)
2022-03-28 11:37:42 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (515979ms till timeout)
2022-03-28 11:37:43 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (514916ms till timeout)
2022-03-28 11:37:44 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (513861ms till timeout)
2022-03-28 11:37:45 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (512809ms till timeout)
2022-03-28 11:37:46 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (511755ms till timeout)
2022-03-28 11:37:47 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (510734ms till timeout)
2022-03-28 11:37:48 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (509665ms till timeout)
2022-03-28 11:37:49 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (508607ms till timeout)
2022-03-28 11:37:51 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (507539ms till timeout)
2022-03-28 11:37:52 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (506472ms till timeout)
2022-03-28 11:37:53 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (505441ms till timeout)
2022-03-28 11:37:54 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (504422ms till timeout)
2022-03-28 11:37:55 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (503377ms till timeout)
2022-03-28 11:37:56 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (502360ms till timeout)
2022-03-28 11:37:57 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (501339ms till timeout)
2022-03-28 11:37:58 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (500322ms till timeout)
2022-03-28 11:37:59 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (499297ms till timeout)
2022-03-28 11:38:00 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (498273ms till timeout)
2022-03-28 11:38:01 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (497254ms till timeout)
2022-03-28 11:38:02 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (496161ms till timeout)
2022-03-28 11:38:03 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (495142ms till timeout)
2022-03-28 11:38:04 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (494091ms till timeout)
2022-03-28 11:38:05 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (493065ms till timeout)
2022-03-28 11:38:06 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (492011ms till timeout)
2022-03-28 11:38:07 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (490988ms till timeout)
2022-03-28 11:38:08 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (489940ms till timeout)
2022-03-28 11:38:09 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (488837ms till timeout)
2022-03-28 11:38:10 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (487805ms till timeout)
2022-03-28 11:38:11 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (486777ms till timeout)
2022-03-28 11:38:12 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (485716ms till timeout)
2022-03-28 11:38:13 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-d8b313ba will have desired state: Ready not ready, will try again in 1000 ms (484699ms till timeout)
2022-03-28 11:38:14 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:444] KafkaRebalance: my-cluster-d8b313ba is in desired state: Ready
2022-03-28 11:38:14 [ForkJoinPool-2-worker-3] DEBUG [AbstractST:674] ============================================================================
2022-03-28 11:38:14 [ForkJoinPool-2-worker-3] DEBUG [AbstractST:675] [operators.ReconciliationST - After Each] - Clean up after test
2022-03-28 11:38:14 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:344] ############################################################################
2022-03-28 11:38:14 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:348] Delete all resources for testPauseReconciliationInKafkaRebalanceAndTopic
2022-03-28 11:38:14 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:241] Delete of KafkaTopic my-topic-1306521865-1110190015 in namespace namespace-10
2022-03-28 11:38:14 [ForkJoinPool-2-worker-7] INFO  [ResourceManager:241] Delete of KafkaRebalance my-cluster-d8b313ba in namespace namespace-10
2022-03-28 11:38:14 [ForkJoinPool-2-worker-5] INFO  [ResourceManager:241] Delete of Kafka my-cluster-d8b313ba in namespace namespace-10
2022-03-28 11:38:14 [ForkJoinPool-2-worker-5] INFO  [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-10, for cruise control Kafka cluster my-cluster-d8b313ba
2022-03-28 11:38:15 [ForkJoinPool-2-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaRebalance:my-cluster-d8b313ba
2022-03-28 11:38:15 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1306521865-1110190015
2022-03-28 11:38:15 [ForkJoinPool-2-worker-7] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaRebalance:my-cluster-d8b313ba not ready, will try again in 10000 ms (179857ms till timeout)
2022-03-28 11:38:15 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1306521865-1110190015 not ready, will try again in 10000 ms (179772ms till timeout)
2022-03-28 11:38:15 [ForkJoinPool-2-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-d8b313ba
2022-03-28 11:38:15 [ForkJoinPool-2-worker-5] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-d8b313ba not ready, will try again in 10000 ms (839911ms till timeout)
2022-03-28 11:38:25 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:369] ############################################################################
2022-03-28 11:38:25 [ForkJoinPool-2-worker-3] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-10 for test case:testPauseReconciliationInKafkaRebalanceAndTopic
2022-03-28 11:38:25 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Namespace namespace-10 removal
2022-03-28 11:38:25 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:25 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:25 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:38:25 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (479697ms till timeout)
2022-03-28 11:38:26 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:27 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:27 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:38:27 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (478372ms till timeout)
2022-03-28 11:38:28 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:28 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:28 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:38:28 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (477133ms till timeout)
2022-03-28 11:38:29 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:29 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:29 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:38:29 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (475894ms till timeout)
2022-03-28 11:38:30 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:30 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:30 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:38:30 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (474552ms till timeout)
2022-03-28 11:38:31 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:32 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:32 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:38:32 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (473106ms till timeout)
2022-03-28 11:38:33 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:33 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:33 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:38:33 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (471758ms till timeout)
2022-03-28 11:38:34 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:35 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:35 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:38:35 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (470467ms till timeout)
2022-03-28 11:38:36 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:36 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:36 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:38:36 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (469244ms till timeout)
2022-03-28 11:38:37 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:37 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:37 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:38:37 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (467997ms till timeout)
2022-03-28 11:38:38 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:38 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:38 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:38:38 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (466613ms till timeout)
2022-03-28 11:38:39 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:40 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:40 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:38:40 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (465370ms till timeout)
2022-03-28 11:38:41 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:41 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:41 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:38:41 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (464080ms till timeout)
2022-03-28 11:38:42 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:42 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:42 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:38:42 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (462790ms till timeout)
2022-03-28 11:38:43 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:43 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:43 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:38:43 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (461569ms till timeout)
2022-03-28 11:38:44 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:45 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:45 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:38:45 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (460280ms till timeout)
2022-03-28 11:38:46 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:46 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:46 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:38:46 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (458927ms till timeout)
2022-03-28 11:38:47 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:47 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:47 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:38:47 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (457674ms till timeout)
2022-03-28 11:38:48 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:49 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:49 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:38:49 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (456418ms till timeout)
2022-03-28 11:38:50 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:50 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:50 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:38:50 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (455144ms till timeout)
2022-03-28 11:38:51 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:51 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:51 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:38:51 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (453697ms till timeout)
2022-03-28 11:38:52 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:53 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:53 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:38:53 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (452482ms till timeout)
2022-03-28 11:38:54 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:54 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:54 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:38:54 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (451269ms till timeout)
2022-03-28 11:38:55 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:55 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:55 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:38:55 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (450048ms till timeout)
2022-03-28 11:38:56 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:56 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:56 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:38:56 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (448832ms till timeout)
2022-03-28 11:38:57 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:57 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:57 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:38:57 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (447595ms till timeout)
2022-03-28 11:38:58 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:59 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:38:59 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:38:59 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (446311ms till timeout)
2022-03-28 11:39:00 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:39:00 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:39:00 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:39:00 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (445044ms till timeout)
2022-03-28 11:39:01 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:39:01 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:39:01 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:39:01 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (443768ms till timeout)
2022-03-28 11:39:02 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:39:03 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:39:03 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:39:03 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (442506ms till timeout)
2022-03-28 11:39:04 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:39:04 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:39:04 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:39:04 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (441186ms till timeout)
2022-03-28 11:39:05 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:39:05 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:39:05 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:39:05 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (439966ms till timeout)
2022-03-28 11:39:06 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:39:06 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:39:06 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:39:06 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (438743ms till timeout)
2022-03-28 11:39:07 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:39:08 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:39:08 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:39:08 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (437406ms till timeout)
2022-03-28 11:39:09 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:39:09 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:39:09 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:39:09 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (436130ms till timeout)
2022-03-28 11:39:10 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:39:10 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:39:10 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:39:10 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (434914ms till timeout)
2022-03-28 11:39:11 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:39:11 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:39:11 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:39:11 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (433699ms till timeout)
2022-03-28 11:39:12 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:39:13 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:39:13 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:39:13 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (432328ms till timeout)
2022-03-28 11:39:14 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:39:14 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:39:14 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:39:14 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (431130ms till timeout)
2022-03-28 11:39:15 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:39:15 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 11:39:15 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 1
2022-03-28 11:39:15 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 11:39:15 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-10" not found
2022-03-28 11:39:15 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] ======STDERR END======
2022-03-28 11:39:15 [ForkJoinPool-2-worker-3] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[]}
2022-03-28 11:39:15 [ForkJoinPool-2-worker-3] DEBUG [SuiteThreadController:267] testPauseReconciliationInKafkaRebalanceAndTopic - Notifies waiting test cases:[testCapacityFile, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testReceiveSimpleMessageTlsScramSha, testSendSimpleMessageTls, testSendSimpleMessageTlsScramSha, testConfigurationFileIsCreated, testReceiveSimpleMessageTls, testDeployAndUnDeployCruiseControl, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testConfigurationPerformanceOptions, testPauseReconciliationInKafkaRebalanceAndTopic] to and randomly select one to start execution
2022-03-28 11:39:15 [ForkJoinPool-2-worker-3] DEBUG [SuiteThreadController:93] [operators.ReconciliationST] - Removing parallel test: testPauseReconciliationInKafkaRebalanceAndTopic
2022-03-28 11:39:15 [ForkJoinPool-2-worker-3] DEBUG [SuiteThreadController:97] [operators.ReconciliationST] - Parallel test count: 0
2022-03-28 11:39:15 [ForkJoinPool-2-worker-3] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.ReconciliationST.testPauseReconciliationInKafkaRebalanceAndTopic-FINISHED
2022-03-28 11:39:15 [ForkJoinPool-2-worker-3] INFO  [TestSeparator:30] ############################################################################
2022-03-28 11:39:15 [ForkJoinPool-2-worker-3] DEBUG [AbstractST:689] ============================================================================
2022-03-28 11:39:15 [ForkJoinPool-2-worker-3] DEBUG [AbstractST:690] [operators.ReconciliationST - After All] - Clean up after test suite
2022-03-28 11:39:15 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:344] ############################################################################
2022-03-28 11:39:15 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:346] In context ReconciliationST is everything deleted.
2022-03-28 11:39:15 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:369] ############################################################################
2022-03-28 11:39:15 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Namespace reconciliation-st removal
2022-03-28 11:39:15 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace reconciliation-st -o yaml
2022-03-28 11:39:16 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace reconciliation-st -o yaml
2022-03-28 11:39:16 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:39:16 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace reconciliation-st removal not ready, will try again in 1000 ms (479588ms till timeout)
2022-03-28 11:39:17 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace reconciliation-st -o yaml
2022-03-28 11:39:17 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace reconciliation-st -o yaml
2022-03-28 11:39:17 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:39:17 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace reconciliation-st removal not ready, will try again in 1000 ms (478266ms till timeout)
2022-03-28 11:39:18 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace reconciliation-st -o yaml
2022-03-28 11:39:18 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace reconciliation-st -o yaml
2022-03-28 11:39:18 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:39:18 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace reconciliation-st removal not ready, will try again in 1000 ms (476983ms till timeout)
2022-03-28 11:39:19 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace reconciliation-st -o yaml
2022-03-28 11:39:20 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace reconciliation-st -o yaml
2022-03-28 11:39:20 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:39:20 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace reconciliation-st removal not ready, will try again in 1000 ms (475717ms till timeout)
2022-03-28 11:39:21 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace reconciliation-st -o yaml
2022-03-28 11:39:21 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace reconciliation-st -o yaml
2022-03-28 11:39:21 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:39:21 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace reconciliation-st removal not ready, will try again in 1000 ms (474489ms till timeout)
2022-03-28 11:39:22 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace reconciliation-st -o yaml
2022-03-28 11:39:22 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace reconciliation-st -o yaml
2022-03-28 11:39:22 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:39:22 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace reconciliation-st removal not ready, will try again in 1000 ms (473263ms till timeout)
2022-03-28 11:39:23 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace reconciliation-st -o yaml
2022-03-28 11:39:23 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace reconciliation-st -o yaml
2022-03-28 11:39:23 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:39:23 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace reconciliation-st removal not ready, will try again in 1000 ms (472043ms till timeout)
2022-03-28 11:39:24 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace reconciliation-st -o yaml
2022-03-28 11:39:24 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace reconciliation-st -o yaml
2022-03-28 11:39:24 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 11:39:24 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace reconciliation-st removal not ready, will try again in 1000 ms (470797ms till timeout)
2022-03-28 11:39:25 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace reconciliation-st -o yaml
2022-03-28 11:39:26 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-10 get Namespace reconciliation-st -o yaml
2022-03-28 11:39:26 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 1
2022-03-28 11:39:26 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 11:39:26 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Error from server (NotFound): namespaces "reconciliation-st" not found
2022-03-28 11:39:26 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] ======STDERR END======
2022-03-28 11:39:26 [ForkJoinPool-2-worker-3] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@62b9e483=[], io.strimzi.test.logs.CollectorElement@b850ef2a=[]}
2022-03-28 11:39:26 [ForkJoinPool-2-worker-3] DEBUG [SuiteThreadController:254] ReconciliationST - Notifies waiting test suites:[UserST, HttpBridgeTlsST, TopicST, CruiseControlConfigurationST, HttpBridgeScramShaST, ThrottlingQuotaST, ReconciliationST] to and randomly select one to start execution
2022-03-28 11:39:26 [ForkJoinPool-2-worker-3] DEBUG [SuiteThreadController:85] [operators.ReconciliationST] - Removing parallel suite: ReconciliationST
2022-03-28 11:39:26 [ForkJoinPool-2-worker-3] DEBUG [SuiteThreadController:89] [operators.ReconciliationST] - Parallel suites count: 0
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1,055.452 s - in io.strimzi.systemtest.operators.ReconciliationST
2022-03-28 11:39:26 [ForkJoinPool-2-worker-3] INFO  [SetupClusterOperator:618] ============================================================================
2022-03-28 11:39:26 [ForkJoinPool-2-worker-3] INFO  [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-03-28 11:39:26 [ForkJoinPool-2-worker-3] INFO  [SetupClusterOperator:620] ============================================================================
2022-03-28 11:39:26 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:344] ############################################################################
2022-03-28 11:39:26 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-03-28 11:39:26 [ForkJoinPool-2-worker-5] INFO  [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-28 11:39:26 [ForkJoinPool-2-worker-13] INFO  [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-28 11:39:26 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-28 11:39:26 [ForkJoinPool-2-worker-7] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-28 11:39:26 [ForkJoinPool-2-worker-9] INFO  [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-28 11:39:26 [ForkJoinPool-2-worker-11] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-28 11:39:26 [ForkJoinPool-2-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-28 11:39:26 [ForkJoinPool-2-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-28 11:39:26 [ForkJoinPool-2-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-28 11:39:26 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-28 11:39:26 [ForkJoinPool-2-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-28 11:39:26 [ForkJoinPool-2-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-28 11:39:26 [ForkJoinPool-2-worker-5] INFO  [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-28 11:39:26 [ForkJoinPool-2-worker-11] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io not ready, will try again in 10000 ms (179707ms till timeout)
2022-03-28 11:39:26 [ForkJoinPool-2-worker-9] INFO  [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-28 11:39:26 [ForkJoinPool-2-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-28 11:39:26 [ForkJoinPool-2-worker-13] INFO  [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-28 11:39:26 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io not ready, will try again in 10000 ms (179750ms till timeout)
2022-03-28 11:39:26 [ForkJoinPool-2-worker-7] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io not ready, will try again in 10000 ms (179620ms till timeout)
2022-03-28 11:39:26 [ForkJoinPool-2-worker-5] INFO  [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-28 11:39:26 [ForkJoinPool-2-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-28 11:39:26 [ForkJoinPool-2-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-28 11:39:26 [ForkJoinPool-2-worker-13] INFO  [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-28 11:39:26 [ForkJoinPool-2-worker-9] INFO  [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-28 11:39:26 [ForkJoinPool-2-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-28 11:39:26 [ForkJoinPool-2-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-28 11:39:26 [ForkJoinPool-2-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-28 11:39:27 [ForkJoinPool-2-worker-5] INFO  [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-28 11:39:27 [ForkJoinPool-2-worker-9] INFO  [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-28 11:39:27 [ForkJoinPool-2-worker-13] INFO  [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-28 11:39:27 [ForkJoinPool-2-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-28 11:39:27 [ForkJoinPool-2-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-28 11:39:27 [ForkJoinPool-2-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-28 11:39:27 [ForkJoinPool-2-worker-9] INFO  [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-28 11:39:27 [ForkJoinPool-2-worker-13] INFO  [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-28 11:39:27 [ForkJoinPool-2-worker-5] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-28 11:39:27 [ForkJoinPool-2-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-28 11:39:27 [ForkJoinPool-2-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-28 11:39:27 [ForkJoinPool-2-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-28 11:39:27 [ForkJoinPool-2-worker-13] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-28 11:39:27 [ForkJoinPool-2-worker-9] INFO  [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-28 11:39:27 [ForkJoinPool-2-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-28 11:39:27 [ForkJoinPool-2-worker-5] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io not ready, will try again in 10000 ms (179780ms till timeout)
2022-03-28 11:39:27 [ForkJoinPool-2-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-28 11:39:27 [ForkJoinPool-2-worker-9] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-28 11:39:27 [ForkJoinPool-2-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io not ready, will try again in 10000 ms (179799ms till timeout)
2022-03-28 11:39:27 [ForkJoinPool-2-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-28 11:39:28 [ForkJoinPool-2-worker-9] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io not ready, will try again in 10000 ms (179661ms till timeout)
2022-03-28 11:39:36 [ForkJoinPool-2-worker-11] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-28 11:39:36 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-28 11:39:36 [ForkJoinPool-2-worker-7] INFO  [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-28 11:39:36 [ForkJoinPool-2-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-28 11:39:36 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-28 11:39:36 [ForkJoinPool-2-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-28 11:39:37 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io not ready, will try again in 10000 ms (179724ms till timeout)
2022-03-28 11:39:37 [ForkJoinPool-2-worker-11] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io not ready, will try again in 10000 ms (179698ms till timeout)
2022-03-28 11:39:37 [ForkJoinPool-2-worker-7] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io not ready, will try again in 10000 ms (179758ms till timeout)
2022-03-28 11:39:37 [ForkJoinPool-2-worker-5] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-28 11:39:37 [ForkJoinPool-2-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-28 11:39:38 [ForkJoinPool-2-worker-5] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io not ready, will try again in 10000 ms (179544ms till timeout)
2022-03-28 11:39:48 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:369] ############################################################################
2022-03-28 11:39:48 [ForkJoinPool-2-worker-3] DEBUG [KubeClusterResource:216] Deleting Namespace: infra-namespace
2022-03-28 11:39:48 [ForkJoinPool-2-worker-3] DEBUG [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-28 11:39:48 [ForkJoinPool-2-worker-3] DEBUG [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-28 11:39:48 [ForkJoinPool-2-worker-3] DEBUG [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v1895483
2022-03-28 11:39:48 [ForkJoinPool-2-worker-3] DEBUG [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v1895483
2022-03-28 11:39:48 [ForkJoinPool-2-worker-3] DEBUG [AbstractWatchManager:222] Watching https://api.morsak-46.strimzi.app-services-dev.net:6443/api/v1/namespaces?fieldSelector=metadata.name%3Dinfra-namespace&resourceVersion=1895483&allowWatchBookmarks=true&watch=true...
2022-03-28 11:39:48 [OkHttp https://api.morsak-46.strimzi.app-services-dev.net:6443/...] DEBUG [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-28 11:39:48 [OkHttp https://api.morsak-46.strimzi.app-services-dev.net:6443/...] DEBUG [Reflector:139] Event received MODIFIED Namespace resourceVersion 1895487
2022-03-28 11:40:00 [OkHttp https://api.morsak-46.strimzi.app-services-dev.net:6443/...] DEBUG [Reflector:139] Event received MODIFIED Namespace resourceVersion 1895579
2022-03-28 11:40:00 [OkHttp https://api.morsak-46.strimzi.app-services-dev.net:6443/...] DEBUG [Reflector:139] Event received DELETED Namespace resourceVersion 1895582
2022-03-28 11:40:00 [OkHttp https://api.morsak-46.strimzi.app-services-dev.net:6443/...] DEBUG [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v1895579 in namespace default
2022-03-28 11:40:00 [OkHttp https://api.morsak-46.strimzi.app-services-dev.net:6443/...] DEBUG [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@5695eb90
2022-03-28 11:40:00 [OkHttp https://api.morsak-46.strimzi.app-services-dev.net:6443/...] DEBUG [Reflector:181] Watch gracefully closed
2022-03-28 11:40:00 [OkHttp https://api.morsak-46.strimzi.app-services-dev.net:6443/...] DEBUG [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@52aa659a
2022-03-28 11:40:00 [OkHttp https://api.morsak-46.strimzi.app-services-dev.net:6443/...] DEBUG [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@52aa659a
2022-03-28 11:40:00 [OkHttp https://api.morsak-46.strimzi.app-services-dev.net:6443/...] DEBUG [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@52aa659a
2022-03-28 11:40:00 [main] INFO  [TestExecutionListener:40] =======================================================================
2022-03-28 11:40:00 [main] INFO  [TestExecutionListener:41] =======================================================================
2022-03-28 11:40:00 [main] INFO  [TestExecutionListener:42]                         Test run finished
2022-03-28 11:40:00 [main] INFO  [TestExecutionListener:43] =======================================================================
2022-03-28 11:40:00 [main] INFO  [TestExecutionListener:44] =======================================================================
[INFO] 
[INFO] Results:
[INFO] 
[WARNING] Flakes: 
[WARNING] io.strimzi.systemtest.operators.ReconciliationST.testPauseReconciliationInKafkaRebalanceAndTopic(ExtensionContext)
[ERROR]   Run 1: ReconciliationST.testPauseReconciliationInKafkaRebalanceAndTopic:154 ? NotFound
[INFO]   Run 2: PASS
[INFO] 
[INFO] 
[WARNING] Tests run: 30, Failures: 0, Errors: 0, Skipped: 0, Flakes: 1
[INFO] 
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:verify (default) @ systemtest ---
2022-03-28 11:40:00 [OkHttp https://api.morsak-46.strimzi.app-services-dev.net:6443/...] DEBUG [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-28 11:40:00 [OkHttp https://api.morsak-46.strimzi.app-services-dev.net:6443/...] DEBUG [AbstractWatchManager:140] Ignoring error for already closed/closing connection
[INFO] 
[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (analyze) @ systemtest ---
[INFO] No dependency problems found
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary for Strimzi - Apache Kafka on Kubernetes and OpenShift 0.29.0-SNAPSHOT:
[INFO] 
[INFO] Strimzi - Apache Kafka on Kubernetes and OpenShift . SUCCESS [  3.555 s]
[INFO] test ............................................... SUCCESS [  5.585 s]
[INFO] crd-annotations .................................... SUCCESS [  4.059 s]
[INFO] crd-generator ...................................... SUCCESS [  6.933 s]
[INFO] api ................................................ SUCCESS [ 27.875 s]
[INFO] mockkube ........................................... SUCCESS [  4.523 s]
[INFO] config-model ....................................... SUCCESS [  3.510 s]
[INFO] certificate-manager ................................ SUCCESS [  3.823 s]
[INFO] operator-common .................................... SUCCESS [  7.518 s]
[INFO] systemtest ......................................... SUCCESS [53:16 min]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  54:24 min
[INFO] Finished at: 2022-03-28T07:40:00-04:00
[INFO] ------------------------------------------------------------------------
