[INFO] Scanning for projects...
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Build Order:
[INFO] 
[INFO] Strimzi - Apache Kafka on Kubernetes and OpenShift                 [pom]
[INFO] test                                                               [jar]
[INFO] crd-annotations                                                    [jar]
[INFO] crd-generator                                                      [jar]
[INFO] api                                                                [jar]
[INFO] mockkube                                                           [jar]
[INFO] config-model                                                       [jar]
[INFO] certificate-manager                                                [jar]
[INFO] operator-common                                                    [jar]
[INFO] systemtest                                                         [jar]
[INFO] 
[INFO] -------------------------< io.strimzi:strimzi >-------------------------
[INFO] Building Strimzi - Apache Kafka on Kubernetes and OpenShift 0.29.0-SNAPSHOT [1/10]
[INFO] --------------------------------[ pom ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ strimzi ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ strimzi ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ strimzi ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/target/jacoco.exec
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:report (report) @ strimzi ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] >>> maven-source-plugin:3.0.1:jar (attach-sources) > generate-sources @ strimzi >>>
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ strimzi ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ strimzi ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ strimzi ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/target/jacoco.exec
[INFO] 
[INFO] <<< maven-source-plugin:3.0.1:jar (attach-sources) < generate-sources @ strimzi <<<
[INFO] 
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar (attach-sources) @ strimzi ---
[INFO] 
[INFO] --- maven-javadoc-plugin:3.1.0:jar (attach-javadocs) @ strimzi ---
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:integration-test (default) @ strimzi ---
[INFO] No tests to run.
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:verify (default) @ strimzi ---
[INFO] 
[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (analyze) @ strimzi ---
[INFO] Skipping pom project
[INFO] 
[INFO] --------------------------< io.strimzi:test >---------------------------
[INFO] Building test 0.29.0-SNAPSHOT                                     [2/10]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ test ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ test ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ test ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/test/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ test ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/test/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ test ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ test ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/test/src/test/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ test ---
[INFO] No sources to compile
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M5:test (default-test) @ test ---
[INFO] No tests to run.
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ test ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:report (report) @ test ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] >>> maven-source-plugin:3.0.1:jar (attach-sources) > generate-sources @ test >>>
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ test ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ test ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ test ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/test/target/jacoco.exec
[INFO] 
[INFO] <<< maven-source-plugin:3.0.1:jar (attach-sources) < generate-sources @ test <<<
[INFO] 
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar (attach-sources) @ test ---
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/test/target/test-0.29.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] --- maven-javadoc-plugin:3.1.0:jar (attach-javadocs) @ test ---
[INFO] 
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/EmbeddedZooKeeper.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/WaitException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/executor/ExecResult.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/executor/Exec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/cluster/OpenShift.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/cluster/KubeCluster.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/cluster/Kubernetes.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/cluster/Minikube.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/cmdClient/Kubectl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/cmdClient/Oc.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/cmdClient/BaseCmdKubeClient.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/cmdClient/KubeCmdClient.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/exceptions/KubeClusterException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/exceptions/NoClusterException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/HelmClient.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/KubeClient.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/k8s/KubeClusterResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/annotations/IsolatedTest.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/annotations/ParallelSuite.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/annotations/ParallelTest.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/annotations/IsolatedSuite.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/interfaces/ExtensionContextParameterResolver.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/interfaces/TestSeparator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/TestUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/test/src/main/java/io/strimzi/test/logs/CollectorElement.java...
Constructing Javadoc information...
Standard Doclet version 11.0.11
Building tree for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/IsolatedSuite.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/IsolatedTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/ParallelSuite.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/ParallelTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/KubeCluster.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/Kubernetes.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/Minikube.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/OpenShift.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/BaseCmdKubeClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/KubeCmdClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/Kubectl.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/Oc.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/KubeClusterException.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/KubeClusterException.AlreadyExists.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/KubeClusterException.InvalidResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/KubeClusterException.NotFound.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/NoClusterException.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/executor/Exec.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/executor/ExecResult.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/interfaces/ExtensionContextParameterResolver.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/interfaces/TestSeparator.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/HelmClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/KubeClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/KubeClusterResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/logs/CollectorElement.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/EmbeddedZooKeeper.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/TestUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/WaitException.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/executor/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/executor/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/interfaces/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/interfaces/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/logs/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/logs/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/constant-values.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/serialized-form.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/class-use/EmbeddedZooKeeper.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/class-use/WaitException.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/executor/class-use/ExecResult.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/executor/class-use/Exec.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/class-use/OpenShift.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/class-use/KubeCluster.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/class-use/Kubernetes.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/class-use/Minikube.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/class-use/Kubectl.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/class-use/Oc.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/class-use/BaseCmdKubeClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/class-use/KubeCmdClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/class-use/KubeClusterException.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/class-use/KubeClusterException.InvalidResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/class-use/KubeClusterException.AlreadyExists.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/class-use/KubeClusterException.NotFound.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/class-use/NoClusterException.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/class-use/HelmClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/class-use/KubeClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/class-use/KubeClusterResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/class-use/IsolatedTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/class-use/ParallelSuite.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/class-use/ParallelTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/class-use/IsolatedSuite.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/interfaces/class-use/ExtensionContextParameterResolver.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/interfaces/class-use/TestSeparator.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/class-use/TestUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/logs/class-use/CollectorElement.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/annotations/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/executor/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/interfaces/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cluster/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/cmdClient/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/k8s/exceptions/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/io/strimzi/test/logs/package-use.html...
Building index for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/overview-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/index-all.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/allclasses-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/allpackages-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/deprecated-list.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/index.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/overview-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/test/target/apidocs/help-doc.html...
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/test/target/test-0.29.0-SNAPSHOT-javadoc.jar
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:integration-test (default) @ test ---
[INFO] No tests to run.
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:verify (default) @ test ---
[INFO] 
[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (analyze) @ test ---
[INFO] No dependency problems found
[INFO] 
[INFO] ---------------------< io.strimzi:crd-annotations >---------------------
[INFO] Building crd-annotations 0.29.0-SNAPSHOT                          [3/10]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ crd-annotations ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ crd-annotations ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ crd-annotations ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/crd-annotations/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ crd-annotations ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/crd-annotations/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ crd-annotations ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ crd-annotations ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/crd-annotations/src/test/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ crd-annotations ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M5:test (default-test) @ crd-annotations ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ crd-annotations ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:report (report) @ crd-annotations ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] >>> maven-source-plugin:3.0.1:jar (attach-sources) > generate-sources @ crd-annotations >>>
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ crd-annotations ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ crd-annotations ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ crd-annotations ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/crd-annotations/target/jacoco.exec
[INFO] 
[INFO] <<< maven-source-plugin:3.0.1:jar (attach-sources) < generate-sources @ crd-annotations <<<
[INFO] 
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar (attach-sources) @ crd-annotations ---
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/crd-annotations-0.29.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] --- maven-javadoc-plugin:3.1.0:jar (attach-javadocs) @ crd-annotations ---
[INFO] 
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-annotations/src/main/java/io/strimzi/api/annotations/DeprecatedProperty.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-annotations/src/main/java/io/strimzi/api/annotations/ApiVersion.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-annotations/src/main/java/io/strimzi/api/annotations/DeprecatedType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-annotations/src/main/java/io/strimzi/api/annotations/KubeVersion.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-annotations/src/main/java/io/strimzi/api/annotations/VersionRange.java...
Constructing Javadoc information...
Standard Doclet version 11.0.11
Building tree for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/ApiVersion.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/DeprecatedProperty.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/DeprecatedType.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/KubeVersion.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/VersionRange.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/constant-values.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/class-use/DeprecatedProperty.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/class-use/ApiVersion.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/class-use/DeprecatedType.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/class-use/KubeVersion.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/class-use/VersionRange.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/io/strimzi/api/annotations/package-use.html...
Building index for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/overview-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/index-all.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/allclasses-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/allpackages-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/deprecated-list.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/index.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/apidocs/help-doc.html...
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/crd-annotations/target/crd-annotations-0.29.0-SNAPSHOT-javadoc.jar
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:integration-test (default) @ crd-annotations ---
[WARNING] useSystemClassLoader setting has no effect when not forking
[WARNING] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:verify (default) @ crd-annotations ---
[INFO] 
[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (analyze) @ crd-annotations ---
[INFO] No dependency problems found
[INFO] 
[INFO] ----------------------< io.strimzi:crd-generator >----------------------
[INFO] Building crd-generator 0.29.0-SNAPSHOT                            [4/10]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ crd-generator ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ crd-generator ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ crd-generator ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/crd-generator/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ crd-generator ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ crd-generator ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ crd-generator ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 7 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ crd-generator ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M5:test (default-test) @ crd-generator ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ crd-generator ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:report (report) @ crd-generator ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] >>> maven-source-plugin:3.0.1:jar (attach-sources) > generate-sources @ crd-generator >>>
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ crd-generator ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ crd-generator ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ crd-generator ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/crd-generator/target/jacoco.exec
[INFO] 
[INFO] <<< maven-source-plugin:3.0.1:jar (attach-sources) < generate-sources @ crd-generator <<<
[INFO] 
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar (attach-sources) @ crd-generator ---
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/crd-generator/target/crd-generator-0.29.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] --- maven-javadoc-plugin:3.1.0:jar (attach-javadocs) @ crd-generator ---
[INFO] 
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/KubeLinker.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/Linker.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/OpenShiftOriginLinker.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/PropertyType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/Schema.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/Alternation.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/DescriptionFile.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/Example.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/KubeLink.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/OneOf.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/Type.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/Alternative.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/Description.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/Maximum.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/Minimum.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/MinimumItems.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/PresentInVersions.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/Crd.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/annotations/Pattern.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/Property.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/CrdGenerator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/crd-generator/src/main/java/io/strimzi/crdgenerator/DocGenerator.java...
Constructing Javadoc information...
Standard Doclet version 11.0.11
Building tree for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Alternation.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Alternative.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Crd.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Crd.Spec.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Crd.Spec.AdditionalPrinterColumn.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Crd.Spec.Names.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Crd.Spec.Subresources.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Crd.Spec.Subresources.Scale.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Crd.Spec.Subresources.Status.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Crd.Spec.Version.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Description.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Description.List.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/DescriptionFile.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Example.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/KubeLink.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Maximum.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Maximum.List.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Minimum.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Minimum.List.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/MinimumItems.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/MinimumItems.List.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/OneOf.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/OneOf.Alternative.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/OneOf.Alternative.Property.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Pattern.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Pattern.List.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/PresentInVersions.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/Type.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/CrdGenerator.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/CrdGenerator.ConversionStrategy.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/CrdGenerator.DefaultReporter.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/CrdGenerator.NoneConversionStrategy.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/CrdGenerator.Reporter.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/CrdGenerator.WebhookConversionStrategy.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/DocGenerator.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/KubeLinker.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/Linker.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/OpenShiftOriginLinker.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/PropertyType.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/constant-values.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/KubeLinker.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/Linker.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/OpenShiftOriginLinker.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/PropertyType.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Alternation.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/DescriptionFile.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Example.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/KubeLink.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/OneOf.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/OneOf.Alternative.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/OneOf.Alternative.Property.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Type.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Alternative.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Description.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Description.List.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Maximum.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Maximum.List.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Minimum.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Minimum.List.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/MinimumItems.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/MinimumItems.List.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/PresentInVersions.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Crd.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Crd.Spec.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Crd.Spec.AdditionalPrinterColumn.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Crd.Spec.Subresources.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Crd.Spec.Subresources.Scale.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Crd.Spec.Subresources.Status.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Crd.Spec.Version.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Crd.Spec.Names.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Pattern.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/class-use/Pattern.List.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/CrdGenerator.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/CrdGenerator.WebhookConversionStrategy.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/CrdGenerator.NoneConversionStrategy.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/CrdGenerator.ConversionStrategy.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/CrdGenerator.DefaultReporter.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/CrdGenerator.Reporter.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/class-use/DocGenerator.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/io/strimzi/crdgenerator/annotations/package-use.html...
Building index for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/overview-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/index-all.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/allclasses-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/allpackages-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/deprecated-list.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/index.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/overview-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/crd-generator/target/apidocs/help-doc.html...
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/crd-generator/target/crd-generator-0.29.0-SNAPSHOT-javadoc.jar
[INFO] 
[INFO] --- maven-shade-plugin:3.1.0:shade (default) @ crd-generator ---
[INFO] Including io.strimzi:crd-annotations:jar:0.29.0-SNAPSHOT in the shaded jar.
[INFO] Including com.fasterxml.jackson.core:jackson-core:jar:2.12.6 in the shaded jar.
[INFO] Including com.fasterxml.jackson.core:jackson-databind:jar:2.12.6 in the shaded jar.
[INFO] Including com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:jar:2.12.6 in the shaded jar.
[INFO] Including org.yaml:snakeyaml:jar:1.27 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-client:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-rbac:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-admissionregistration:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-apps:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-autoscaling:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-apiextensions:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-batch:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-certificates:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-coordination:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-discovery:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-events:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-extensions:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-flowcontrol:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-networking:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-metrics:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-policy:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-scheduling:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-storageclass:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-node:jar:5.12.0 in the shaded jar.
[INFO] Including com.squareup.okhttp3:okhttp:jar:3.12.12 in the shaded jar.
[INFO] Including com.squareup.okio:okio:jar:1.15.0 in the shaded jar.
[INFO] Including com.squareup.okhttp3:logging-interceptor:jar:3.12.12 in the shaded jar.
[INFO] Including org.slf4j:slf4j-api:jar:1.7.36 in the shaded jar.
[INFO] Including com.fasterxml.jackson.datatype:jackson-datatype-jsr310:jar:2.13.1 in the shaded jar.
[INFO] Including io.fabric8:zjsonpatch:jar:0.3.0 in the shaded jar.
[INFO] Including com.github.mifmif:generex:jar:1.0.2 in the shaded jar.
[INFO] Including dk.brics.automaton:automaton:jar:1.11-8 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-core:jar:5.12.0 in the shaded jar.
[INFO] Including io.fabric8:kubernetes-model-common:jar:5.12.0 in the shaded jar.
[INFO] Including com.fasterxml.jackson.core:jackson-annotations:jar:2.12.6 in the shaded jar.
[WARNING] Discovered module-info.class. Shading will break its strong encapsulation.
[WARNING] Discovered module-info.class. Shading will break its strong encapsulation.
[WARNING] Discovered module-info.class. Shading will break its strong encapsulation.
[WARNING] Discovered module-info.class. Shading will break its strong encapsulation.
[WARNING] kubernetes-model-coordination-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 18 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseListFluentImpl$ItemsNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseFluentImpl$SpecNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseSpec
[WARNING]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseFluent$SpecNested
[WARNING]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseListFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseSpecBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseSpecFluent
[WARNING]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseFluentImpl$MetadataNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseFluent
[WARNING]   - io.fabric8.kubernetes.api.model.coordination.v1.LeaseListFluent
[WARNING]   - 8 more...
[WARNING] logging-interceptor-3.12.12.jar, crd-generator-0.29.0-SNAPSHOT.jar define 8 overlapping classes: 
[WARNING]   - okhttp3.logging.HttpLoggingInterceptor$Logger$1
[WARNING]   - okhttp3.logging.LoggingEventListener$Factory
[WARNING]   - okhttp3.logging.HttpLoggingInterceptor$Level
[WARNING]   - okhttp3.logging.HttpLoggingInterceptor
[WARNING]   - okhttp3.logging.package-info
[WARNING]   - okhttp3.logging.LoggingEventListener
[WARNING]   - okhttp3.logging.LoggingEventListener$1
[WARNING]   - okhttp3.logging.HttpLoggingInterceptor$Logger
[WARNING] kubernetes-client-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 536 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.client.internal.CertUtils
[WARNING]   - io.fabric8.kubernetes.client.CustomResource
[WARNING]   - io.fabric8.kubernetes.client.osgi.ManagedKubernetesClient
[WARNING]   - io.fabric8.kubernetes.client.V1beta1ApiextensionAPIGroupDSL
[WARNING]   - io.fabric8.kubernetes.client.internal.PatchUtils$SingletonHolder
[WARNING]   - io.fabric8.kubernetes.client.VersionInfo$1
[WARNING]   - io.fabric8.kubernetes.client.utils.ReplaceValueStream
[WARNING]   - io.fabric8.kubernetes.client.dsl.CreateFromServerGettable
[WARNING]   - io.fabric8.kubernetes.client.dsl.ApiextensionsAPIGroupDSL
[WARNING]   - io.fabric8.kubernetes.client.dsl.Containerable
[WARNING]   - 526 more...
[WARNING] kubernetes-model-events-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 44 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventFluent$MetadataNested
[WARNING]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventFluent$SeriesNested
[WARNING]   - io.fabric8.kubernetes.api.model.events.v1.EventList
[WARNING]   - io.fabric8.kubernetes.api.model.events.v1.EventListBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventFluent$RegardingNested
[WARNING]   - io.fabric8.kubernetes.api.model.events.v1.EventSeriesFluent
[WARNING]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventFluent
[WARNING]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventListFluent$ItemsNested
[WARNING]   - io.fabric8.kubernetes.api.model.events.v1beta1.EventListFluentImpl$ItemsNestedImpl
[WARNING]   - 34 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, automaton-1.11-8.jar define 25 overlapping classes: 
[WARNING]   - dk.brics.automaton.AutomatonMatcher
[WARNING]   - dk.brics.automaton.ShuffleOperations$ShuffleConfiguration
[WARNING]   - dk.brics.automaton.RegExp$Kind
[WARNING]   - dk.brics.automaton.RunAutomaton
[WARNING]   - dk.brics.automaton.Automaton
[WARNING]   - dk.brics.automaton.RegExp
[WARNING]   - dk.brics.automaton.AutomatonProvider
[WARNING]   - dk.brics.automaton.RegExp$1
[WARNING]   - dk.brics.automaton.MinimizationOperations$StateListNode
[WARNING]   - dk.brics.automaton.State
[WARNING]   - 15 more...
[WARNING] kubernetes-model-metrics-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 30 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.PodMetricsFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetricsListFluentImpl$ItemsNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.ContainerMetricsFluent
[WARNING]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetrics
[WARNING]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetricsListFluent
[WARNING]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetricsListFluent$ItemsNested
[WARNING]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.NodeMetricsFluent$MetadataNested
[WARNING]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.PodMetricsFluentImpl$ContainersNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.PodMetricsListFluentImpl$ItemsNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.metrics.v1beta1.PodMetricsListBuilder
[WARNING]   - 20 more...
[WARNING] kubernetes-model-networking-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 234 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.networking.v1beta1.IngressClassListFluent$ItemsNested
[WARNING]   - io.fabric8.kubernetes.api.model.networking.v1beta1.IngressClassListFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.networking.v1.IngressServiceBackend
[WARNING]   - io.fabric8.kubernetes.api.model.networking.v1.NetworkPolicyPort
[WARNING]   - io.fabric8.kubernetes.api.model.networking.v1beta1.IngressFluentImpl$StatusNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.networking.v1beta1.IngressClassSpec
[WARNING]   - io.fabric8.kubernetes.api.model.networking.v1.IngressStatus
[WARNING]   - io.fabric8.kubernetes.api.model.networking.v1.IngressClassFluent$SpecNested
[WARNING]   - io.fabric8.kubernetes.api.model.networking.v1.NetworkPolicyBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.networking.v1.IngressRuleFluentImpl$HttpNestedImpl
[WARNING]   - 224 more...
[WARNING] kubernetes-model-apiextensions-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 350 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.CustomResourceDefinitionVersionBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.JSONSchemaPropsOrBoolBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.CustomResourceDefinitionSpecFluent$ValidationNested
[WARNING]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.CustomResourceDefinitionVersionFluentImpl$SchemaNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.JSONSchemaPropsOrStringArraySerDe$Deserializer$1
[WARNING]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.CustomResourceValidationFluentImpl$OpenAPIV3SchemaNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.JSONSchemaPropsFluentImpl$NotNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.WebhookClientConfigFluentImpl$ServiceNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.apiextensions.v1beta1.JSONSchemaPropsOrArrayFluent$SchemaNested
[WARNING]   - io.fabric8.kubernetes.api.model.apiextensions.v1.JSONSchemaPropsOrBoolSerDe
[WARNING]   - 340 more...
[WARNING] generex-1.0.2.jar, crd-generator-0.29.0-SNAPSHOT.jar define 7 overlapping classes: 
[WARNING]   - com.mifmif.common.regex.GenerexIterator
[WARNING]   - com.mifmif.common.regex.Generex
[WARNING]   - com.mifmif.common.regex.GenerexIterator$Step
[WARNING]   - com.mifmif.common.regex.Node
[WARNING]   - com.mifmif.common.regex.Main
[WARNING]   - com.mifmif.common.regex.util.Iterable
[WARNING]   - com.mifmif.common.regex.util.Iterator
[WARNING] kubernetes-model-autoscaling-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 350 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.MetricSpecFluentImpl$ObjectNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta2.HorizontalPodAutoscalerSpec
[WARNING]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.CrossVersionObjectReferenceBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.HorizontalPodAutoscalerStatusFluent
[WARNING]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta2.HorizontalPodAutoscalerFluent$SpecNested
[WARNING]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta2.ContainerResourceMetricStatusFluent
[WARNING]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.HorizontalPodAutoscalerStatus
[WARNING]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.MetricStatusFluent$ObjectNested
[WARNING]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta2.HorizontalPodAutoscalerSpecFluent$ScaleTargetRefNested
[WARNING]   - io.fabric8.kubernetes.api.model.autoscaling.v2beta1.HorizontalPodAutoscalerFluent$SpecNested
[WARNING]   - 340 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, zjsonpatch-0.3.0.jar define 24 overlapping classes: 
[WARNING]   - io.fabric8.zjsonpatch.internal.collections4.sequence.InsertCommand
[WARNING]   - io.fabric8.zjsonpatch.Operation
[WARNING]   - io.fabric8.zjsonpatch.internal.collections4.sequence.CommandVisitor
[WARNING]   - io.fabric8.zjsonpatch.internal.guava.Strings
[WARNING]   - io.fabric8.zjsonpatch.internal.collections4.sequence.EditCommand
[WARNING]   - io.fabric8.zjsonpatch.JsonDiff$EncodePathFunction
[WARNING]   - io.fabric8.zjsonpatch.internal.collections4.sequence.SequencesComparator
[WARNING]   - io.fabric8.zjsonpatch.Diff
[WARNING]   - io.fabric8.zjsonpatch.internal.collections4.ListUtils
[WARNING]   - io.fabric8.zjsonpatch.JsonPatch
[WARNING]   - 14 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-admissionregistration-5.12.0.jar define 362 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.admissionregistration.v1beta1.MutatingWebhookFluent$ObjectSelectorNested
[WARNING]   - io.fabric8.kubernetes.api.model.authorization.v1.SubjectAccessReviewSpecFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.authorization.v1beta1.SubjectRulesReviewStatusBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.admissionregistration.v1.ValidatingWebhookConfigurationBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.authentication.TokenReviewFluentImpl$MetadataNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.authorization.v1beta1.SelfSubjectRulesReviewSpec
[WARNING]   - io.fabric8.kubernetes.api.model.admissionregistration.v1beta1.MutatingWebhookFluentImpl$NamespaceSelectorNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.admissionregistration.v1beta1.MutatingWebhookConfigurationFluentImpl$WebhooksNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.authorization.v1beta1.SelfSubjectAccessReviewFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.admissionregistration.v1.MutatingWebhookFluent$ClientConfigNested
[WARNING]   - 352 more...
[WARNING] kubernetes-model-rbac-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 80 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.rbac.SubjectBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.rbac.ClusterRoleBindingFluent
[WARNING]   - io.fabric8.kubernetes.api.model.rbac.RoleListFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.rbac.ClusterRoleBindingFluent$MetadataNested
[WARNING]   - io.fabric8.kubernetes.api.model.rbac.RoleBindingBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.rbac.ClusterRoleBindingBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.rbac.AggregationRuleFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.rbac.SubjectFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.rbac.ClusterRoleListFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.rbac.PolicyRuleFluent
[WARNING]   - 70 more...
[WARNING] kubernetes-model-certificates-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 60 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestStatusFluent$ConditionsNested
[WARNING]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestFluent$SpecNested
[WARNING]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestFluentImpl$StatusNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestListFluent$ItemsNested
[WARNING]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestStatusFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestFluent$MetadataNested
[WARNING]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestConditionFluent
[WARNING]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestStatusBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.certificates.v1beta1.CertificateSigningRequestStatusFluent$ConditionsNested
[WARNING]   - io.fabric8.kubernetes.api.model.certificates.v1.CertificateSigningRequestFluentImpl
[WARNING]   - 50 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-policy-5.12.0.jar define 162 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.policy.v1.PodDisruptionBudgetList
[WARNING]   - io.fabric8.kubernetes.api.model.policy.v1beta1.HostPortRangeBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.policy.v1beta1.EvictionFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.policy.v1beta1.PodSecurityPolicyFluent
[WARNING]   - io.fabric8.kubernetes.api.model.policy.v1beta1.PodSecurityPolicySpecFluentImpl$AllowedCSIDriversNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.policy.v1beta1.PodSecurityPolicyListFluentImpl$ItemsNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.policy.v1beta1.AllowedFlexVolumeBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.policy.v1beta1.IDRangeFluent
[WARNING]   - io.fabric8.kubernetes.api.model.policy.v1beta1.SELinuxStrategyOptionsFluent
[WARNING]   - io.fabric8.kubernetes.api.model.policy.v1beta1.PodSecurityPolicySpecFluentImpl$FsGroupNestedImpl
[WARNING]   - 152 more...
[WARNING] jackson-datatype-jsr310-2.13.1.jar, crd-generator-0.29.0-SNAPSHOT.jar define 59 overlapping classes: 
[WARNING]   - com.fasterxml.jackson.datatype.jsr310.deser.LocalDateDeserializer
[WARNING]   - com.fasterxml.jackson.datatype.jsr310.deser.key.Jsr310KeyDeserializer
[WARNING]   - com.fasterxml.jackson.datatype.jsr310.PackageVersion
[WARNING]   - com.fasterxml.jackson.datatype.jsr310.deser.YearDeserializer
[WARNING]   - com.fasterxml.jackson.datatype.jsr310.ser.key.Jsr310NullKeySerializer
[WARNING]   - com.fasterxml.jackson.datatype.jsr310.deser.key.LocalDateTimeKeyDeserializer
[WARNING]   - com.fasterxml.jackson.datatype.jsr310.util.DurationUnitConverter
[WARNING]   - com.fasterxml.jackson.datatype.jsr310.ser.InstantSerializerBase
[WARNING]   - com.fasterxml.jackson.datatype.jsr310.ser.LocalDateTimeSerializer
[WARNING]   - com.fasterxml.jackson.datatype.jsr310.ser.OffsetDateTimeSerializer
[WARNING]   - 49 more...
[WARNING] kubernetes-model-flowcontrol-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 132 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfigurationFluentImpl$SpecNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.FlowSchemaConditionFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.FlowDistinguisherMethodBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfigurationReferenceBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.QueuingConfiguration
[WARNING]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.QueuingConfigurationFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfiguration
[WARNING]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfigurationReference
[WARNING]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PolicyRulesWithSubjects
[WARNING]   - io.fabric8.kubernetes.api.model.flowcontrol.v1beta1.PriorityLevelConfigurationListFluent$ItemsNested
[WARNING]   - 122 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, crd-annotations-0.29.0-SNAPSHOT.jar define 8 overlapping classes: 
[WARNING]   - io.strimzi.api.annotations.VersionRange
[WARNING]   - io.strimzi.api.annotations.ApiVersion
[WARNING]   - io.strimzi.api.annotations.ApiVersion$Stability
[WARNING]   - io.strimzi.api.annotations.ApiVersion$1
[WARNING]   - io.strimzi.api.annotations.DeprecatedType
[WARNING]   - io.strimzi.api.annotations.DeprecatedProperty
[WARNING]   - io.strimzi.api.annotations.VersionRange$VersionParser
[WARNING]   - io.strimzi.api.annotations.KubeVersion
[WARNING] jackson-core-2.12.6.jar, crd-generator-0.29.0-SNAPSHOT.jar define 124 overlapping classes: 
[WARNING]   - com.fasterxml.jackson.core.JsonGenerator$Feature
[WARNING]   - com.fasterxml.jackson.core.json.JsonReadFeature
[WARNING]   - com.fasterxml.jackson.core.util.ThreadLocalBufferManager$ThreadLocalBufferManagerHolder
[WARNING]   - com.fasterxml.jackson.core.util.Separators
[WARNING]   - com.fasterxml.jackson.core.io.SegmentedStringWriter
[WARNING]   - com.fasterxml.jackson.core.TreeNode
[WARNING]   - com.fasterxml.jackson.core.sym.Name
[WARNING]   - com.fasterxml.jackson.core.util.RequestPayload
[WARNING]   - com.fasterxml.jackson.core.util.JsonGeneratorDelegate
[WARNING]   - com.fasterxml.jackson.core.async.NonBlockingInputFeeder
[WARNING]   - 114 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, okio-1.15.0.jar define 44 overlapping classes: 
[WARNING]   - okio.ByteString
[WARNING]   - okio.Source
[WARNING]   - okio.ForwardingSink
[WARNING]   - okio.BufferedSource
[WARNING]   - okio.Util
[WARNING]   - okio.AsyncTimeout$1
[WARNING]   - okio.HashingSource
[WARNING]   - okio.GzipSink
[WARNING]   - okio.Okio$1
[WARNING]   - okio.Pipe$PipeSink
[WARNING]   - 34 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, jackson-databind-2.12.6.jar define 700 overlapping classes: 
[WARNING]   - com.fasterxml.jackson.databind.introspect.AnnotationCollector$NoAnnotations
[WARNING]   - com.fasterxml.jackson.databind.jsontype.BasicPolymorphicTypeValidator$Builder
[WARNING]   - com.fasterxml.jackson.databind.BeanDescription
[WARNING]   - com.fasterxml.jackson.databind.deser.impl.BeanAsArrayBuilderDeserializer
[WARNING]   - com.fasterxml.jackson.databind.introspect.AnnotatedMethodMap
[WARNING]   - com.fasterxml.jackson.databind.SerializerProvider
[WARNING]   - com.fasterxml.jackson.databind.introspect.AnnotationCollector$OneAnnotation
[WARNING]   - com.fasterxml.jackson.databind.ser.std.StaticListSerializerBase
[WARNING]   - com.fasterxml.jackson.databind.ser.std.NumberSerializers$ShortSerializer
[WARNING]   - com.fasterxml.jackson.databind.ser.BeanSerializerFactory
[WARNING]   - 690 more...
[WARNING] kubernetes-model-discovery-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 88 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointSliceFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointFluent
[WARNING]   - io.fabric8.kubernetes.api.model.discovery.v1.EndpointSliceBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointPort
[WARNING]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.ForZoneBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.discovery.v1.EndpointFluent$TargetRefNested
[WARNING]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointFluentImpl$ConditionsNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.discovery.v1.EndpointSliceListFluentImpl$ItemsNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointConditionsFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.discovery.v1beta1.EndpointSliceListFluentImpl$ItemsNestedImpl
[WARNING]   - 78 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-extensions-5.12.0.jar define 264 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.extensions.DaemonSetConditionBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.extensions.NetworkPolicyListFluent$ItemsNested
[WARNING]   - io.fabric8.kubernetes.api.model.extensions.DeploymentStrategyFluent
[WARNING]   - io.fabric8.kubernetes.api.model.extensions.IngressListFluentImpl$ItemsNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.extensions.NetworkPolicySpecFluent$IngressNested
[WARNING]   - io.fabric8.kubernetes.api.model.extensions.IngressStatus
[WARNING]   - io.fabric8.kubernetes.api.model.extensions.DaemonSetFluentImpl$SpecNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.extensions.IngressSpecFluent$RulesNested
[WARNING]   - io.fabric8.kubernetes.api.model.extensions.DaemonSetSpecFluent$UpdateStrategyNested
[WARNING]   - io.fabric8.kubernetes.api.model.extensions.NetworkPolicyPeerBuilder
[WARNING]   - 254 more...
[WARNING] snakeyaml-1.27.jar, crd-generator-0.29.0-SNAPSHOT.jar define 216 overlapping classes: 
[WARNING]   - org.yaml.snakeyaml.emitter.Emitter$ExpectBlockMappingValue
[WARNING]   - org.yaml.snakeyaml.parser.ParserImpl$ParseBlockNode
[WARNING]   - org.yaml.snakeyaml.emitter.Emitter$ExpectBlockMappingSimpleValue
[WARNING]   - org.yaml.snakeyaml.emitter.Emitter$ExpectDocumentEnd
[WARNING]   - org.yaml.snakeyaml.Yaml$3
[WARNING]   - org.yaml.snakeyaml.emitter.Emitter$ExpectBlockSequenceItem
[WARNING]   - org.yaml.snakeyaml.parser.ParserImpl$ParseBlockSequenceEntry
[WARNING]   - org.yaml.snakeyaml.util.ArrayUtils
[WARNING]   - org.yaml.snakeyaml.tokens.Token$ID
[WARNING]   - org.yaml.snakeyaml.reader.StreamReader
[WARNING]   - 206 more...
[WARNING] kubernetes-model-node-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 78 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.node.v1beta1.OverheadBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.node.v1.RuntimeClassListFluent$ItemsNested
[WARNING]   - io.fabric8.kubernetes.api.model.node.v1alpha1.Scheduling
[WARNING]   - io.fabric8.kubernetes.api.model.node.v1beta1.RuntimeClassFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.node.v1beta1.RuntimeClassListBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.node.v1.RuntimeClassFluent
[WARNING]   - io.fabric8.kubernetes.api.model.node.v1alpha1.SchedulingFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.node.v1alpha1.RuntimeClassSpecFluent$OverheadNested
[WARNING]   - io.fabric8.kubernetes.api.model.node.v1.RuntimeClassBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.node.v1.RuntimeClassFluentImpl
[WARNING]   - 68 more...
[WARNING] kubernetes-model-core-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 2394 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.BaseKubernetesListFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.StatusBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.KubeSchemaFluentImpl$APIResourceNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.NodeListBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.ResourceQuotaListFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.WatchEventFluentImpl$APIServiceStatusObjectNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.WatchEventFluent$VsphereVirtualDiskVolumeSourceObjectNested
[WARNING]   - io.fabric8.kubernetes.api.model.ProbeFluentImpl$HttpGetNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.PatchOptionsFluent
[WARNING]   - io.fabric8.kubernetes.api.model.ServerAddressByClientCIDRFluentImpl
[WARNING]   - 2384 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, jackson-annotations-2.12.6.jar define 71 overlapping classes: 
[WARNING]   - com.fasterxml.jackson.annotation.JsonAutoDetect
[WARNING]   - com.fasterxml.jackson.annotation.JsonInclude
[WARNING]   - com.fasterxml.jackson.annotation.ObjectIdGenerators
[WARNING]   - com.fasterxml.jackson.annotation.JsonFormat$Features
[WARNING]   - com.fasterxml.jackson.annotation.JsonFormat$Feature
[WARNING]   - com.fasterxml.jackson.annotation.JsonIgnore
[WARNING]   - com.fasterxml.jackson.annotation.JsonSetter
[WARNING]   - com.fasterxml.jackson.annotation.JsonTypeInfo$None
[WARNING]   - com.fasterxml.jackson.annotation.JsonFormat$Shape
[WARNING]   - com.fasterxml.jackson.annotation.JsonSubTypes
[WARNING]   - 61 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, slf4j-api-1.7.36.jar define 34 overlapping classes: 
[WARNING]   - org.slf4j.helpers.SubstituteLogger
[WARNING]   - org.slf4j.helpers.NamedLoggerBase
[WARNING]   - org.slf4j.helpers.NOPMDCAdapter
[WARNING]   - org.slf4j.MarkerFactory
[WARNING]   - org.slf4j.helpers.BasicMarker
[WARNING]   - org.slf4j.spi.LoggerFactoryBinder
[WARNING]   - org.slf4j.MDC$MDCCloseable
[WARNING]   - org.slf4j.spi.LocationAwareLogger
[WARNING]   - org.slf4j.helpers.MessageFormatter
[WARNING]   - org.slf4j.helpers.Util$ClassContextSecurityManager
[WARNING]   - 24 more...
[WARNING] jackson-dataformat-yaml-2.12.6.jar, crd-generator-0.29.0-SNAPSHOT.jar define 17 overlapping classes: 
[WARNING]   - com.fasterxml.jackson.dataformat.yaml.YAMLMapper$Builder
[WARNING]   - com.fasterxml.jackson.dataformat.yaml.snakeyaml.error.Mark
[WARNING]   - com.fasterxml.jackson.dataformat.yaml.YAMLGenerator
[WARNING]   - com.fasterxml.jackson.dataformat.yaml.UTF8Reader
[WARNING]   - com.fasterxml.jackson.dataformat.yaml.YAMLParser
[WARNING]   - com.fasterxml.jackson.dataformat.yaml.util.StringQuotingChecker
[WARNING]   - com.fasterxml.jackson.dataformat.yaml.YAMLGenerator$Feature
[WARNING]   - com.fasterxml.jackson.dataformat.yaml.JacksonYAMLParseException
[WARNING]   - com.fasterxml.jackson.dataformat.yaml.YAMLParser$Feature
[WARNING]   - com.fasterxml.jackson.dataformat.yaml.util.StringQuotingChecker$Default
[WARNING]   - 7 more...
[WARNING] kubernetes-model-apps-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 212 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.apps.ReplicaSetSpec
[WARNING]   - io.fabric8.kubernetes.api.model.apps.StatefulSetConditionFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.apps.DeploymentStrategyFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.apps.ControllerRevisionFluent$DeploymentDataNested
[WARNING]   - io.fabric8.kubernetes.api.model.apps.ReplicaSetSpecFluent
[WARNING]   - io.fabric8.kubernetes.api.model.apps.DeploymentFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.apps.ReplicaSetStatusFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.apps.ControllerRevisionFluentImpl$PersistentVolumeClaimDataNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.apps.ReplicaSetCondition
[WARNING]   - io.fabric8.kubernetes.api.model.apps.StatefulSetSpecFluent$UpdateStrategyNested
[WARNING]   - 202 more...
[WARNING] kubernetes-model-scheduling-5.12.0.jar, crd-generator-0.29.0-SNAPSHOT.jar define 24 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassFluent
[WARNING]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassFluentImpl$MetadataNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassListFluent$ItemsNested
[WARNING]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassList
[WARNING]   - io.fabric8.kubernetes.api.model.scheduling.v1beta1.PriorityClassListFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassListFluent
[WARNING]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClass
[WARNING]   - io.fabric8.kubernetes.api.model.scheduling.v1.PriorityClassListFluentImpl$ItemsNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.scheduling.v1beta1.PriorityClassBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.scheduling.v1beta1.PriorityClassFluent$MetadataNested
[WARNING]   - 14 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-common-5.12.0.jar define 16 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.model.annotation.Plural
[WARNING]   - io.fabric8.kubernetes.model.annotation.Group
[WARNING]   - io.fabric8.kubernetes.model.jackson.JsonUnwrappedDeserializer$CancelUnwrapped
[WARNING]   - io.fabric8.kubernetes.model.jackson.JsonUnwrappedDeserializer
[WARNING]   - io.fabric8.kubernetes.model.annotation.PrinterColumn
[WARNING]   - io.fabric8.kubernetes.model.jackson.UnwrappedTypeResolverBuilder
[WARNING]   - io.fabric8.kubernetes.model.annotation.Singular
[WARNING]   - io.fabric8.kubernetes.model.annotation.StatusReplicas
[WARNING]   - io.fabric8.kubernetes.model.annotation.SpecReplicas
[WARNING]   - io.fabric8.kubernetes.model.annotation.Version
[WARNING]   - 6 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-batch-5.12.0.jar define 112 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.batch.v1.JobFluentImpl$StatusNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.batch.v1.CronJobStatusFluentImpl$ActiveNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.batch.v1.JobSpecFluent$TemplateNested
[WARNING]   - io.fabric8.kubernetes.api.model.batch.v1beta1.CronJobListFluentImpl$ItemsNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.batch.v1beta1.CronJobSpecFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.batch.v1.JobSpecFluentImpl$TemplateNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.batch.v1.Job
[WARNING]   - io.fabric8.kubernetes.api.model.batch.v1beta1.CronJobFluent
[WARNING]   - io.fabric8.kubernetes.api.model.batch.v1beta1.CronJobListFluent
[WARNING]   - io.fabric8.kubernetes.api.model.batch.v1.CronJobListFluent
[WARNING]   - 102 more...
[WARNING] crd-generator-0.29.0-SNAPSHOT.jar, kubernetes-model-storageclass-5.12.0.jar define 172 overlapping classes: 
[WARNING]   - io.fabric8.kubernetes.api.model.storage.v1beta1.CSIStorageCapacityListFluentImpl$ItemsNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.storage.CSINodeFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.storage.CSINodeDriverFluentImpl$AllocatableNestedImpl
[WARNING]   - io.fabric8.kubernetes.api.model.storage.StorageClass
[WARNING]   - io.fabric8.kubernetes.api.model.storage.VolumeAttachmentSourceBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.storage.VolumeAttachmentSourceFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.storage.TokenRequestFluentImpl
[WARNING]   - io.fabric8.kubernetes.api.model.storage.v1beta1.CSINodeDriverFluent$AllocatableNested
[WARNING]   - io.fabric8.kubernetes.api.model.storage.v1beta1.CSIDriverSpecBuilder
[WARNING]   - io.fabric8.kubernetes.api.model.storage.VolumeAttachmentSpecFluent
[WARNING]   - 162 more...
[WARNING] okhttp-3.12.12.jar, crd-generator-0.29.0-SNAPSHOT.jar define 208 overlapping classes: 
[WARNING]   - okhttp3.WebSocket
[WARNING]   - okhttp3.Cookie$Builder
[WARNING]   - okhttp3.internal.http.HttpHeaders
[WARNING]   - okhttp3.internal.http2.Http2Connection$ReaderRunnable
[WARNING]   - okhttp3.internal.http2.Http2Reader$ContinuationSource
[WARNING]   - okhttp3.internal.tls.OkHostnameVerifier
[WARNING]   - okhttp3.Cache$Entry
[WARNING]   - okhttp3.internal.http2.Http2Connection$3
[WARNING]   - okhttp3.internal.ws.RealWebSocket$Streams
[WARNING]   - okhttp3.CacheControl$Builder
[WARNING]   - 198 more...
[WARNING] maven-shade-plugin has detected that some class files are
[WARNING] present in two or more JARs. When this happens, only one
[WARNING] single version of the class is copied to the uber jar.
[WARNING] Usually this is not harmful and you can skip these warnings,
[WARNING] otherwise try to manually exclude artifacts based on
[WARNING] mvn dependency:tree -Ddetail=true and the above output.
[WARNING] See http://maven.apache.org/plugins/maven-shade-plugin/
[INFO] Replacing original artifact with shaded artifact.
[INFO] Replacing /home/cloud-user/strimzi-kafka-operator/crd-generator/target/crd-generator-0.29.0-SNAPSHOT.jar with /home/cloud-user/strimzi-kafka-operator/crd-generator/target/crd-generator-0.29.0-SNAPSHOT-shaded.jar
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:integration-test (default) @ crd-generator ---
[WARNING] useSystemClassLoader setting has no effect when not forking
[WARNING] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:verify (default) @ crd-generator ---
[INFO] 
[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (analyze) @ crd-generator ---
[INFO] No dependency problems found
[INFO] 
[INFO] ---------------------------< io.strimzi:api >---------------------------
[INFO] Building api 0.29.0-SNAPSHOT                                      [5/10]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ api ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ api ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ api ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/api/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ api ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/api/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ api ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- exec-maven-plugin:1.6.0:exec (generate-crd-co-install-v1) @ api ---
[INFO] 
[INFO] --- exec-maven-plugin:1.6.0:exec (generate-crd-co-install-v1-eo) @ api ---
[INFO] 
[INFO] --- exec-maven-plugin:1.6.0:exec (generate-doc) @ api ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ api ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 99 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ api ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-test-compile) @ api ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M5:test (default-test) @ api ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ api ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:report (report) @ api ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] >>> maven-source-plugin:3.0.1:jar (attach-sources) > generate-sources @ api >>>
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ api ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ api ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ api ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/api/target/jacoco.exec
[INFO] 
[INFO] <<< maven-source-plugin:3.0.1:jar (attach-sources) < generate-sources @ api <<<
[INFO] 
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar (attach-sources) @ api ---
[INFO] 
[INFO] --- maven-javadoc-plugin:3.1.0:jar (attach-javadocs) @ api ---
[INFO] 
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/KafkaBridgeList.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/KafkaConnectList.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/KafkaConnectorList.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/KafkaList.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/KafkaMirrorMaker2List.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/KafkaMirrorMakerList.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/KafkaRebalanceList.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/KafkaTopicList.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/KafkaUserList.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AclRuleClusterResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/CruiseControlResources.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AclOperation.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AclResourcePatternType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AclRuleResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AclRuleGroupResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AclRuleTopicResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AclRuleTransactionalIdResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/ExternalLogging.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AclRuleType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/CertAndKeySecretSource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/CertSecretSource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/CruiseControlSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/CertificateExpirationPolicy.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaConnect.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/ContainerEnvVar.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/EntityOperatorSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/EntityTopicOperatorSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/EntityUserOperatorSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/HasConfigurableMetrics.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/InlineLogging.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/JvmOptions.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/GenericSecretSource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/ClientTls.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/JmxTransResources.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/JmxTransSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaAuthorization.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaAuthorizationKeycloak.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaAuthorizationOpa.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaAuthorizationSimple.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaBridge.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/Probe.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaBridgeClientSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaBridgeConsumerSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaBridgeHttpConfig.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaBridgeHttpCors.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaBridgeProducerSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaBridgeResources.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaClusterSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaConnector.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/Constants.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaConnectResources.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/Spec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AbstractKafkaConnectSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AclRule.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/CertificateAuthority.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaConnectSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaExporterSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaJmxAuthentication.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaConnectorSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaExporterResources.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaJmxAuthenticationPassword.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMaker.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaJmxOptions.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMaker2.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMaker2Spec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMakerSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMaker2ConnectorSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMaker2Resources.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaRebalance.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/HasSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMakerConsumerSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMakerProducerSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaMirrorMakerResources.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaResources.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaBridgeSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaRebalanceSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaTopic.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaUser.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaTopicSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaUserAuthentication.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaUserAuthorization.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaUserAuthorizationSimple.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaUserQuotas.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaUserScramSha512ClientAuthentication.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaUserSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaUserTlsClientAuthentication.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/MetricsConfig.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/Password.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/Logging.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/PasswordSecretSource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/Rack.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/Sidecar.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/SystemProperty.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/TlsClientAuthentication.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/TlsSidecar.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/TlsSidecarLogLevel.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/Kafka.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/UnknownPropertyPreserving.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/ZookeeperClusterSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/authentication/KafkaClientAuthentication.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuth.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationPlain.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScram.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha256.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha512.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationTls.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/balancing/KafkaRebalanceAnnotation.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/balancing/KafkaRebalanceState.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/balancing/BrokerCapacity.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/ConnectorPlugin.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnv.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/ExternalConfigurationVolumeSource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvVarSource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/Build.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/Plugin.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/Artifact.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/DockerOutput.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/DownloadableArtifact.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/ImageStreamOutput.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/JarArtifact.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/MavenArtifact.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/OtherArtifact.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/Output.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/TgzArtifact.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/build/ZipArtifact.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/connect/ExternalConfiguration.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/NodeAddressType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/arraylistener/KafkaListenerType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListener.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBootstrap.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBroker.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfiguration.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/KafkaListenerAuthentication.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationCustom.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuth.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationScramSha512.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationTls.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/Condition.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/KafkaBridgeStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/KafkaConnectStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/KafkaMirrorMaker2Status.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/KafkaMirrorMakerStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/KafkaRebalanceStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/KafkaUserStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/ListenerAddress.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/HasStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/KafkaConnectorStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/KafkaStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/KafkaTopicStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/Status.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/ListenerStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/status/StrimziPodSetStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/storage/PersistentClaimStorageOverride.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/storage/PersistentClaimStorage.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/storage/EphemeralStorage.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/storage/JbodStorage.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/storage/SingleVolumeStorage.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/storage/Storage.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/ContainerTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/ExternalTrafficPolicy.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/JmxTransOutputDefinitionTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/KafkaUserTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/PodManagementPolicy.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/StatefulSetTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/DeploymentStrategy.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/DeploymentTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/InternalServiceTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/IpFamily.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/IpFamilyPolicy.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/MetadataTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/PodDisruptionBudgetTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/PodTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/ResourceTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/BuildConfigTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/CruiseControlTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/EntityOperatorTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/JmxTransQueryTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/JmxTransTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/KafkaBridgeTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/KafkaClusterTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/KafkaConnectTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/KafkaExporterTemplate.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2Template.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/tracing/JaegerTracing.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/tracing/Tracing.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/AbstractConnectorSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/ExternalConfigurationReference.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/JmxPrometheusExporterMetrics.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaAuthorizationCustom.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaBridgeAdminClientSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/KafkaUserTlsExternalClientAuthentication.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/PasswordSource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/StrimziPodSet.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/model/StrimziPodSetSpec.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/Crds.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/src/main/java/io/strimzi/api/kafka/StrimziPodSetList.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaStatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaStatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaStatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/ListenerStatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/ListenerStatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/ListenerStatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/ConditionFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/ConditionFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/ConditionBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaBridgeStatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaBridgeStatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaBridgeStatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaMirrorMaker2StatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaMirrorMaker2StatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaMirrorMaker2StatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaRebalanceStatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaRebalanceStatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaRebalanceStatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaUserStatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaUserStatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaUserStatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaConnectorStatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaConnectorStatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaConnectorStatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/StatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/StatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaTopicStatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaTopicStatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaTopicStatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaMirrorMakerStatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaMirrorMakerStatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaMirrorMakerStatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/ListenerAddressFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/ListenerAddressFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/ListenerAddressBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaConnectStatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaConnectStatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/KafkaConnectStatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/StrimziPodSetStatusFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/StrimziPodSetStatusFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/status/StrimziPodSetStatusBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ProbeFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ProbeFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ProbeBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2SpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2SpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2SpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/TgzArtifactFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/TgzArtifactFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/TgzArtifactBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/BuildFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/BuildFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/BuildBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/DockerOutputFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/DockerOutputFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/DockerOutputBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/MavenArtifactFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/MavenArtifactFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/MavenArtifactBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/DownloadableArtifactFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/DownloadableArtifactFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/JarArtifactFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/JarArtifactFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/JarArtifactBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/OtherArtifactFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/OtherArtifactFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/OtherArtifactBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/ZipArtifactFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/ZipArtifactFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/ZipArtifactBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/ImageStreamOutputFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/ImageStreamOutputFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/ImageStreamOutputBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/PluginFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/PluginFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/build/PluginBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationVolumeSourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationVolumeSourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationVolumeSourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ConnectorPluginFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ConnectorPluginFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ConnectorPluginBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvVarSourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvVarSourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvVarSourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationScramSha512Fluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationScramSha512FluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationScramSha512Builder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuthFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuthFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuthBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBootstrapFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBootstrapFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBootstrapBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBrokerFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBrokerFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBrokerBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationCustomFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationCustomFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationCustomBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationTlsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationTlsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationTlsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserAuthorizationSimpleFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserAuthorizationSimpleFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserAuthorizationSimpleBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/StrimziPodSetSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/StrimziPodSetSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/StrimziPodSetSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/EntityOperatorTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/PodTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/PodTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/PodTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/PodDisruptionBudgetTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/PodDisruptionBudgetTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/PodDisruptionBudgetTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/JmxTransOutputDefinitionTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/JmxTransOutputDefinitionTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/JmxTransOutputDefinitionTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/StatefulSetTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/StatefulSetTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/StatefulSetTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/CruiseControlTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/MetadataTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/MetadataTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/MetadataTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaExporterTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaClusterTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/ContainerTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/ContainerTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/ContainerTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaConnectTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaUserTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaUserTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaUserTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/JmxTransQueryTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/JmxTransQueryTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/JmxTransQueryTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/InternalServiceTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/InternalServiceTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/InternalServiceTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/ResourceTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/ResourceTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/ResourceTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/JmxTransTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/JmxTransTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/JmxTransTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/DeploymentTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/DeploymentTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/DeploymentTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/BuildConfigTemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/BuildConfigTemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/BuildConfigTemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaJmxOptionsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaJmxOptionsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaJmxOptionsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/JbodStorageFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/JbodStorageFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/JbodStorageBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/PersistentClaimStorageOverrideFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/PersistentClaimStorageOverrideFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/PersistentClaimStorageOverrideBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/EphemeralStorageFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/EphemeralStorageFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/EphemeralStorageBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/PersistentClaimStorageFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/PersistentClaimStorageFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/storage/PersistentClaimStorageBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/RackFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/RackFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/RackBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/TlsClientAuthenticationFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/TlsClientAuthenticationFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/TlsClientAuthenticationBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2Fluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2FluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2Builder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ClientTlsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ClientTlsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ClientTlsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectorSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectorSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectorSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/EntityUserOperatorSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaTopicFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaTopicFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaTopicBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserTlsClientAuthenticationFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserTlsClientAuthenticationFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserTlsClientAuthenticationBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeHttpConfigFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeHttpConfigFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeHttpConfigBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/JmxPrometheusExporterMetricsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/JmxPrometheusExporterMetricsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/JmxPrometheusExporterMetricsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/SystemPropertyFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/SystemPropertyFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/SystemPropertyBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaTopicSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaTopicSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaTopicSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CruiseControlSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CruiseControlSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserTlsExternalClientAuthenticationFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserTlsExternalClientAuthenticationFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserTlsExternalClientAuthenticationBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ZookeeperClusterSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/StrimziPodSetFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/StrimziPodSetFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/StrimziPodSetBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2ConnectorSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2ConnectorSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2ConnectorSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/balancing/BrokerCapacityFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/balancing/BrokerCapacityFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/balancing/BrokerCapacityBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/PasswordSourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/tracing/JaegerTracingFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/tracing/JaegerTracingFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/tracing/JaegerTracingBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/PasswordSourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/PasswordSourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaRebalanceSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaRebalanceSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaRebalanceSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaJmxAuthenticationPasswordFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaJmxAuthenticationPasswordFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaJmxAuthenticationPasswordBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha256Fluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha256FluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha256Builder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha512Fluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha512FluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha512Builder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationPlainFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationPlainFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationPlainBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationTlsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationTlsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationTlsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/TlsSidecarFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/TlsSidecarFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/TlsSidecarBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CertificateAuthorityFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CertificateAuthorityFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CertificateAuthorityBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeConsumerSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeConsumerSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeConsumerSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleTopicResourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleTopicResourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleTopicResourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleGroupResourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleGroupResourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleGroupResourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AbstractConnectorSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AbstractConnectorSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/EntityOperatorSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/EntityOperatorSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/EntityOperatorSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeHttpCorsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeHttpCorsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeHttpCorsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/EntityTopicOperatorSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/SidecarFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/SidecarFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/SidecarBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/JvmOptionsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/JvmOptionsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/JvmOptionsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaRebalanceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaRebalanceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaRebalanceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationOpaFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationOpaFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationOpaBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeProducerSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeProducerSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeProducerSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ExternalLoggingFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ExternalLoggingFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ExternalLoggingBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/PasswordSecretSourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/PasswordSecretSourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/PasswordSecretSourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationCustomFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationCustomFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationCustomBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/PasswordFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/PasswordFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/PasswordBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserScramSha512ClientAuthenticationFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserScramSha512ClientAuthenticationFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserScramSha512ClientAuthenticationBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CertAndKeySecretSourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CertAndKeySecretSourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CertAndKeySecretSourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerConsumerSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerConsumerSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerConsumerSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CertSecretSourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CertSecretSourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/CertSecretSourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/InlineLoggingFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/InlineLoggingFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/InlineLoggingBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserQuotasFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserQuotasFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaUserQuotasBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ExternalConfigurationReferenceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ExternalConfigurationReferenceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ExternalConfigurationReferenceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationSimpleFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationSimpleFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationSimpleBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleTransactionalIdResourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleTransactionalIdResourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleTransactionalIdResourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerProducerSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerProducerSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerProducerSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/GenericSecretSourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/GenericSecretSourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/GenericSecretSourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/JmxTransSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/JmxTransSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/JmxTransSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaClusterSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectorFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectorFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectorBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/SpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/SpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeClientSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeClientSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ContainerEnvVarFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ContainerEnvVarFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/ContainerEnvVarBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationKeycloakFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationKeycloakFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaAuthorizationKeycloakBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeAdminClientSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeAdminClientSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaBridgeAdminClientSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaExporterSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaExporterSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaExporterSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleClusterResourceFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleClusterResourceFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/AclRuleClusterResourceBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/api/target/generated-sources/annotations/io/strimzi/api/kafka/model/KafkaConnectBuilder.java...
Constructing Javadoc information...
Standard Doclet version 11.0.11
Building tree for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListener.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfiguration.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBootstrap.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBootstrapBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBootstrapFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBootstrapFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBroker.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBrokerBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBrokerFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBrokerFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationFluent.BootstrapNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationFluent.BrokerCertChainAndKeyNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationFluent.BrokersNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationFluentImpl.BootstrapNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationFluentImpl.BrokerCertChainAndKeyNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerConfigurationFluentImpl.BrokersNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluent.ConfigurationNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluent.KafkaListenerAuthenticationCustomAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluent.KafkaListenerAuthenticationOAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluent.KafkaListenerAuthenticationScramSha512AuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluent.KafkaListenerAuthenticationTlsAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluentImpl.ConfigurationNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluentImpl.KafkaListenerAuthenticationCustomAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluentImpl.KafkaListenerAuthenticationOAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluentImpl.KafkaListenerAuthenticationScramSha512AuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/GenericKafkaListenerFluentImpl.KafkaListenerAuthenticationTlsAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/KafkaListenerType.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuth.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluent.AccessTokenNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluent.ClientSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluent.RefreshTokenNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluent.TlsTrustedCertificatesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluentImpl.AccessTokenNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluentImpl.ClientSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluentImpl.RefreshTokenNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationOAuthFluentImpl.TlsTrustedCertificatesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationPlain.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationPlainBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationPlainFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationPlainFluent.PasswordSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationPlainFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationPlainFluentImpl.PasswordSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScram.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha256.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha256Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha256Fluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha256Fluent.PasswordSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha256FluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha256FluentImpl.PasswordSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha512.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha512Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha512Fluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha512Fluent.PasswordSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha512FluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationScramSha512FluentImpl.PasswordSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationTls.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationTlsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationTlsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationTlsFluent.CertificateAndKeyNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationTlsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/KafkaClientAuthenticationTlsFluentImpl.CertificateAndKeyNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/BrokerCapacity.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/BrokerCapacityBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/BrokerCapacityFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/BrokerCapacityFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/KafkaRebalanceAnnotation.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/KafkaRebalanceState.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/Artifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/Build.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/BuildBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/BuildFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/BuildFluent.DockerOutputNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/BuildFluent.ImageStreamOutputNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/BuildFluent.PluginsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/BuildFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/BuildFluentImpl.DockerOutputNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/BuildFluentImpl.ImageStreamOutputNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/BuildFluentImpl.PluginsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/DockerOutput.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/DockerOutputBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/DockerOutputFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/DockerOutputFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/DownloadableArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/DownloadableArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/DownloadableArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/ImageStreamOutput.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/ImageStreamOutputBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/ImageStreamOutputFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/ImageStreamOutputFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/JarArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/JarArtifactBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/JarArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/JarArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/MavenArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/MavenArtifactBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/MavenArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/MavenArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/OtherArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/OtherArtifactBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/OtherArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/OtherArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/Output.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/Plugin.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluent.JarArtifactArtifactsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluent.MavenArtifactArtifactsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluent.OtherArtifactArtifactsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluent.TgzArtifactArtifactsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluent.ZipArtifactArtifactsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluentImpl.JarArtifactArtifactsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluentImpl.MavenArtifactArtifactsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluentImpl.OtherArtifactArtifactsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluentImpl.TgzArtifactArtifactsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/PluginFluentImpl.ZipArtifactArtifactsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/TgzArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/TgzArtifactBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/TgzArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/TgzArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/ZipArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/ZipArtifactBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/ZipArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/ZipArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ConnectorPlugin.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ConnectorPluginBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ConnectorPluginFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ConnectorPluginFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfiguration.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnv.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvFluent.ValueFromNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvFluentImpl.ValueFromNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvVarSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvVarSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvVarSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationEnvVarSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationFluent.EnvNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationFluent.VolumesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationFluentImpl.EnvNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationFluentImpl.VolumesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationVolumeSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationVolumeSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationVolumeSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/ExternalConfigurationVolumeSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/Crds.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/KafkaBridgeList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/KafkaConnectList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/KafkaConnectorList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/KafkaList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/KafkaMirrorMaker2List.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/KafkaMirrorMakerList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/KafkaRebalanceList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/KafkaTopicList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/KafkaUserList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/StrimziPodSetList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationCustom.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationCustomBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationCustomFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationCustomFluent.SecretsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationCustomFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationCustomFluentImpl.SecretsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuth.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuthBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuthFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuthFluent.ClientSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuthFluent.TlsTrustedCertificatesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuthFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuthFluentImpl.ClientSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationOAuthFluentImpl.TlsTrustedCertificatesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationScramSha512.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationScramSha512Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationScramSha512Fluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationScramSha512FluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationTls.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationTlsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationTlsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/KafkaListenerAuthenticationTlsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/NodeAddressType.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractConnectorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractConnectorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractConnectorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.ExternalConfigurationNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.JaegerTracingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.JmxOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.JmxPrometheusExporterMetricsConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.ExternalConfigurationNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.JaegerTracingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.JmxOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.JmxPrometheusExporterMetricsConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AbstractKafkaConnectSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclOperation.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclResourcePatternType.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRule.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleClusterResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleClusterResourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleClusterResourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleClusterResourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleFluent.AclRuleClusterResourceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleFluent.AclRuleGroupResourceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleFluent.AclRuleTopicResourceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleFluent.AclRuleTransactionalIdResourceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleFluentImpl.AclRuleClusterResourceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleFluentImpl.AclRuleGroupResourceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleFluentImpl.AclRuleTopicResourceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleFluentImpl.AclRuleTransactionalIdResourceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleGroupResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleGroupResourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleGroupResourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleGroupResourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleTopicResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleTopicResourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleTopicResourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleTopicResourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleTransactionalIdResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleTransactionalIdResourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleTransactionalIdResourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleTransactionalIdResourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/AclRuleType.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertAndKeySecretSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertAndKeySecretSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertAndKeySecretSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertAndKeySecretSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertificateAuthority.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertificateAuthorityBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertificateAuthorityFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertificateAuthorityFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertificateExpirationPolicy.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertSecretSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertSecretSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertSecretSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CertSecretSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ClientTls.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ClientTlsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ClientTlsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ClientTlsFluent.TrustedCertificatesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ClientTlsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ClientTlsFluentImpl.TrustedCertificatesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/Constants.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ContainerEnvVar.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ContainerEnvVarBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ContainerEnvVarFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ContainerEnvVarFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluent.BrokerCapacityNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluent.JmxPrometheusExporterMetricsConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluent.TlsSidecarNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.BrokerCapacityNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.JmxPrometheusExporterMetricsConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/CruiseControlSpecFluentImpl.TlsSidecarNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecFluent.TlsSidecarNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecFluent.TopicOperatorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecFluent.UserOperatorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecFluentImpl.TlsSidecarNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecFluentImpl.TopicOperatorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityOperatorSpecFluentImpl.UserOperatorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluent.StartupProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityTopicOperatorSpecFluentImpl.StartupProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/EntityUserOperatorSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ExternalConfigurationReference.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ExternalConfigurationReferenceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ExternalConfigurationReferenceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ExternalConfigurationReferenceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ExternalLogging.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ExternalLoggingBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ExternalLoggingFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ExternalLoggingFluent.ValueFromNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ExternalLoggingFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ExternalLoggingFluentImpl.ValueFromNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/GenericSecretSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/GenericSecretSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/GenericSecretSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/GenericSecretSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/HasConfigurableMetrics.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/HasSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/InlineLogging.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/InlineLoggingBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/InlineLoggingFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/InlineLoggingFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxPrometheusExporterMetrics.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxPrometheusExporterMetricsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxPrometheusExporterMetricsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxPrometheusExporterMetricsFluent.ValueFromNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxPrometheusExporterMetricsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxPrometheusExporterMetricsFluentImpl.ValueFromNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransSpecFluent.KafkaQueriesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransSpecFluent.OutputDefinitionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransSpecFluentImpl.KafkaQueriesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransSpecFluentImpl.OutputDefinitionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JmxTransSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JvmOptions.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JvmOptionsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JvmOptionsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JvmOptionsFluent.JavaSystemPropertiesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JvmOptionsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/JvmOptionsFluentImpl.JavaSystemPropertiesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/Kafka.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorization.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationCustom.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationCustomBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationCustomFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationCustomFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationKeycloak.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationKeycloakBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationKeycloakFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationKeycloakFluent.TlsTrustedCertificatesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationKeycloakFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationKeycloakFluentImpl.TlsTrustedCertificatesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationOpa.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationOpaBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationOpaFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationOpaFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationSimple.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationSimpleBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationSimpleFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaAuthorizationSimpleFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridge.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeAdminClientSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeAdminClientSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeAdminClientSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeAdminClientSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeClientSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeClientSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeClientSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeConsumerSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeConsumerSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeConsumerSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeConsumerSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeHttpConfig.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeHttpConfigBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeHttpConfigFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeHttpConfigFluent.CorsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeHttpConfigFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeHttpConfigFluentImpl.CorsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeHttpCors.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeHttpCorsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeHttpCorsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeHttpCorsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeProducerSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeProducerSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeProducerSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeProducerSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.AdminClientNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.ConsumerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.HttpNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.JaegerTracingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.KafkaClientAuthenticationOAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.KafkaClientAuthenticationPlainNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.KafkaClientAuthenticationScramSha256Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.KafkaClientAuthenticationScramSha512Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.KafkaClientAuthenticationTlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.ProducerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluent.TlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.AdminClientNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.ConsumerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.HttpNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.JaegerTracingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.KafkaClientAuthenticationOAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.KafkaClientAuthenticationPlainNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.KafkaClientAuthenticationScramSha256NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.KafkaClientAuthenticationScramSha512NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.KafkaClientAuthenticationTlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.ProducerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBridgeSpecFluentImpl.TlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.EphemeralStorageNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.JbodStorageNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.JmxOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.JmxPrometheusExporterMetricsConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.KafkaAuthorizationCustomNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.KafkaAuthorizationKeycloakNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.KafkaAuthorizationOpaNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.KafkaAuthorizationSimpleNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.ListenersNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.PersistentClaimStorageNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.RackNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.EphemeralStorageNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.JbodStorageNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.JmxOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.JmxPrometheusExporterMetricsConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.KafkaAuthorizationCustomNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.KafkaAuthorizationKeycloakNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.KafkaAuthorizationOpaNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.KafkaAuthorizationSimpleNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.ListenersNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.PersistentClaimStorageNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.RackNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaClusterSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnect.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnector.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluent.BuildNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluent.KafkaClientAuthenticationOAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluent.KafkaClientAuthenticationPlainNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluent.KafkaClientAuthenticationScramSha256Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluent.KafkaClientAuthenticationScramSha512Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluent.KafkaClientAuthenticationTlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluent.RackNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluent.TlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluentImpl.BuildNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluentImpl.KafkaClientAuthenticationOAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluentImpl.KafkaClientAuthenticationPlainNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluentImpl.KafkaClientAuthenticationScramSha256NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluentImpl.KafkaClientAuthenticationScramSha512NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluentImpl.KafkaClientAuthenticationTlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluentImpl.RackNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaConnectSpecFluentImpl.TlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaExporterSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxAuthenticationPassword.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxAuthenticationPasswordBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxAuthenticationPasswordFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxAuthenticationPasswordFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxOptions.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxOptionsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxOptionsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxOptionsFluent.KafkaJmxAuthenticationPasswordNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxOptionsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaJmxOptionsFluentImpl.KafkaJmxAuthenticationPasswordNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluent.KafkaClientAuthenticationOAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluent.KafkaClientAuthenticationPlainNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluent.KafkaClientAuthenticationScramSha256Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluent.KafkaClientAuthenticationScramSha512Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluent.KafkaClientAuthenticationTlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluent.TlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluentImpl.KafkaClientAuthenticationOAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluentImpl.KafkaClientAuthenticationPlainNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluentImpl.KafkaClientAuthenticationScramSha256NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluentImpl.KafkaClientAuthenticationScramSha512NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluentImpl.KafkaClientAuthenticationTlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ClusterSpecFluentImpl.TlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ConnectorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ConnectorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ConnectorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2ConnectorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2Fluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2Fluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2Fluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2Fluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2FluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2FluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2FluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2FluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecFluent.CheckpointConnectorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecFluent.HeartbeatConnectorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecFluent.SourceConnectorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecFluentImpl.CheckpointConnectorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecFluentImpl.HeartbeatConnectorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2MirrorSpecFluentImpl.SourceConnectorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2Resources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2Spec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2SpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2SpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2SpecFluent.ClustersNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2SpecFluent.MirrorsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2SpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2SpecFluentImpl.ClustersNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMaker2SpecFluentImpl.MirrorsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluent.KafkaClientAuthenticationOAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluent.KafkaClientAuthenticationPlainNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluent.KafkaClientAuthenticationScramSha256Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluent.KafkaClientAuthenticationScramSha512Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluent.KafkaClientAuthenticationTlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluent.TlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluentImpl.KafkaClientAuthenticationOAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluentImpl.KafkaClientAuthenticationPlainNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluentImpl.KafkaClientAuthenticationScramSha256NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluentImpl.KafkaClientAuthenticationScramSha512NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluentImpl.KafkaClientAuthenticationTlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerClientSpecFluentImpl.TlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerConsumerSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerConsumerSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerConsumerSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerConsumerSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerProducerSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerProducerSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerProducerSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerProducerSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.ConsumerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.JaegerTracingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.JmxPrometheusExporterMetricsConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.ProducerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.ConsumerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.JaegerTracingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.JmxPrometheusExporterMetricsConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.ProducerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaMirrorMakerSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalance.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaRebalanceSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluent.ClientsCaNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluent.ClusterCaNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluent.CruiseControlNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluent.EntityOperatorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluent.JmxTransNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluent.KafkaExporterNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluent.KafkaNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluent.ZookeeperNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluentImpl.ClientsCaNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluentImpl.ClusterCaNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluentImpl.CruiseControlNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluentImpl.EntityOperatorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluentImpl.JmxTransNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluentImpl.KafkaExporterNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluentImpl.KafkaNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaSpecFluentImpl.ZookeeperNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopic.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaTopicSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUser.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserAuthorization.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserAuthorizationSimple.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserAuthorizationSimpleBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserAuthorizationSimpleFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserAuthorizationSimpleFluent.AclsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserAuthorizationSimpleFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserAuthorizationSimpleFluentImpl.AclsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserQuotas.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserQuotasBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserQuotasFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserQuotasFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserScramSha512ClientAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserScramSha512ClientAuthenticationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserScramSha512ClientAuthenticationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserScramSha512ClientAuthenticationFluent.PasswordNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserScramSha512ClientAuthenticationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserScramSha512ClientAuthenticationFluentImpl.PasswordNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluent.KafkaUserAuthorizationSimpleNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluent.KafkaUserScramSha512ClientAuthenticationNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluent.KafkaUserTlsClientAuthenticationNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluent.KafkaUserTlsExternalClientAuthenticationNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluent.QuotasNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluentImpl.KafkaUserAuthorizationSimpleNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluentImpl.KafkaUserScramSha512ClientAuthenticationNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluentImpl.KafkaUserTlsClientAuthenticationNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluentImpl.KafkaUserTlsExternalClientAuthenticationNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluentImpl.QuotasNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserTlsClientAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserTlsClientAuthenticationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserTlsClientAuthenticationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserTlsClientAuthenticationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserTlsExternalClientAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserTlsExternalClientAuthenticationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserTlsExternalClientAuthenticationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/KafkaUserTlsExternalClientAuthenticationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/Logging.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/MetricsConfig.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/Password.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordFluent.ValueFromNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordFluentImpl.ValueFromNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordSecretSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordSecretSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordSecretSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordSecretSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/PasswordSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/Probe.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ProbeBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ProbeFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ProbeFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/Rack.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/RackBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/RackFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/RackFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/Sidecar.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/SidecarBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/SidecarFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/SidecarFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/Spec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/SpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/SpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSet.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/StrimziPodSetSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/SystemProperty.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/SystemPropertyBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/SystemPropertyFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/SystemPropertyFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsClientAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsClientAuthenticationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsClientAuthenticationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsClientAuthenticationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsSidecar.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsSidecarBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsSidecarFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsSidecarFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsSidecarFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsSidecarFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsSidecarFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsSidecarFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/TlsSidecarLogLevel.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/UnknownPropertyPreserving.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.EphemeralStorageNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.JmxOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.JmxPrometheusExporterMetricsConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.PersistentClaimStorageNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.EphemeralStorageNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.JmxOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.JmxPrometheusExporterMetricsConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.PersistentClaimStorageNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/ZookeeperClusterSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/Condition.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ConditionBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ConditionFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ConditionFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/HasStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaBridgeStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaBridgeStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaBridgeStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaBridgeStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaConnectorStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaConnectorStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaConnectorStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaConnectorStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaConnectStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaConnectStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaConnectStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaConnectStatusFluent.ConnectorPluginsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaConnectStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaConnectStatusFluentImpl.ConnectorPluginsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaMirrorMaker2Status.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaMirrorMaker2StatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaMirrorMaker2StatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaMirrorMaker2StatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaMirrorMakerStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaMirrorMakerStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaMirrorMakerStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaMirrorMakerStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaRebalanceStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaRebalanceStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaRebalanceStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaRebalanceStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaStatusFluent.ListenersNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaStatusFluentImpl.ListenersNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaTopicStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaTopicStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaTopicStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaTopicStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaUserStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaUserStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaUserStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/KafkaUserStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ListenerAddress.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ListenerAddressBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ListenerAddressFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ListenerAddressFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ListenerStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ListenerStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ListenerStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ListenerStatusFluent.AddressesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ListenerStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/ListenerStatusFluentImpl.AddressesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/Status.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/StatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/StatusFluent.ConditionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/StatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/StatusFluentImpl.ConditionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/StrimziPodSetStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/StrimziPodSetStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/StrimziPodSetStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/StrimziPodSetStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/EphemeralStorage.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/EphemeralStorageBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/EphemeralStorageFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/EphemeralStorageFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/JbodStorage.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/JbodStorageBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/JbodStorageFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/JbodStorageFluent.EphemeralStorageVolumesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/JbodStorageFluent.PersistentClaimStorageVolumesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/JbodStorageFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/JbodStorageFluentImpl.EphemeralStorageVolumesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/JbodStorageFluentImpl.PersistentClaimStorageVolumesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/PersistentClaimStorage.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/PersistentClaimStorageBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/PersistentClaimStorageFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/PersistentClaimStorageFluent.OverridesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/PersistentClaimStorageFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/PersistentClaimStorageFluentImpl.OverridesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/PersistentClaimStorageOverride.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/PersistentClaimStorageOverrideBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/PersistentClaimStorageOverrideFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/PersistentClaimStorageOverrideFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/SingleVolumeStorage.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/Storage.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/BuildConfigTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/BuildConfigTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/BuildConfigTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/BuildConfigTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/BuildConfigTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/BuildConfigTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ContainerTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ContainerTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ContainerTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ContainerTemplateFluent.EnvNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ContainerTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ContainerTemplateFluentImpl.EnvNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluent.ApiServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluent.CruiseControlContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluent.TlsSidecarContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluentImpl.ApiServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluentImpl.CruiseControlContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/CruiseControlTemplateFluentImpl.TlsSidecarContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/DeploymentStrategy.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/DeploymentTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/DeploymentTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/DeploymentTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/DeploymentTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/DeploymentTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/DeploymentTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluent.TlsSidecarContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluent.TopicOperatorContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluent.UserOperatorContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluentImpl.TlsSidecarContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluentImpl.TopicOperatorContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/EntityOperatorTemplateFluentImpl.UserOperatorContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ExternalTrafficPolicy.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/InternalServiceTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/InternalServiceTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/InternalServiceTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/InternalServiceTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/InternalServiceTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/InternalServiceTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/IpFamily.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/IpFamilyPolicy.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransOutputDefinitionTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransOutputDefinitionTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransOutputDefinitionTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransOutputDefinitionTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransQueryTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransQueryTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransQueryTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransQueryTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateFluent.ContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateFluentImpl.ContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/JmxTransTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluent.ApiServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluent.BridgeContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluentImpl.ApiServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluentImpl.BridgeContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaBridgeTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.BootstrapServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.BrokersServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.ClusterCaCertNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.ClusterRoleBindingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.ExternalBootstrapIngressNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.ExternalBootstrapRouteNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.ExternalBootstrapServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.InitContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.JmxSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.KafkaContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.PerPodIngressNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.PerPodRouteNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.PerPodServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.PersistentVolumeClaimNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.PodSetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluent.StatefulsetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.BootstrapServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.BrokersServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.ClusterCaCertNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.ClusterRoleBindingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.ExternalBootstrapIngressNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.ExternalBootstrapRouteNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.ExternalBootstrapServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.InitContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.JmxSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.KafkaContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.PerPodIngressNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.PerPodRouteNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.PerPodServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.PersistentVolumeClaimNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.PodSetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaClusterTemplateFluentImpl.StatefulsetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.ApiServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.BuildConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.BuildContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.BuildPodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.BuildServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.ClusterRoleBindingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.ConnectContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.InitContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.JmxSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.ApiServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.BuildConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.BuildContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.BuildPodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.BuildServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.ClusterRoleBindingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.ConnectContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.InitContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.JmxSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaConnectTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluent.ContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluent.ServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluentImpl.ContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaExporterTemplateFluentImpl.ServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2Template.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluent.ApiServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluent.JmxSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluent.MirrorMaker2ContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluentImpl.ApiServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluentImpl.JmxSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluentImpl.MirrorMaker2ContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMaker2TemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluent.MirrorMakerContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluentImpl.MirrorMakerContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaMirrorMakerTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaUserTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaUserTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaUserTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaUserTemplateFluent.SecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaUserTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/KafkaUserTemplateFluentImpl.SecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/MetadataTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/MetadataTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/MetadataTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/MetadataTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodDisruptionBudgetTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodDisruptionBudgetTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodDisruptionBudgetTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodDisruptionBudgetTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodDisruptionBudgetTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodDisruptionBudgetTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodManagementPolicy.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/PodTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ResourceTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ResourceTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ResourceTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ResourceTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ResourceTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ResourceTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/StatefulSetTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/StatefulSetTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/StatefulSetTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/StatefulSetTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/StatefulSetTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/StatefulSetTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.ClientServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.JmxSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.NodesServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.PersistentVolumeClaimNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.PodSetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.StatefulsetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluent.ZookeeperContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.ClientServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.JmxSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.NodesServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.PersistentVolumeClaimNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.PodSetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.StatefulsetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/ZookeeperClusterTemplateFluentImpl.ZookeeperContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/JaegerTracing.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/JaegerTracingBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/JaegerTracingFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/JaegerTracingFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/Tracing.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/constant-values.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/serialized-form.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/KafkaBridgeList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/KafkaConnectList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/KafkaConnectorList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/KafkaList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/KafkaMirrorMaker2List.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/KafkaMirrorMakerList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/KafkaRebalanceList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/KafkaTopicList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/KafkaUserList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleClusterResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclOperation.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclResourcePatternType.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleGroupResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleTopicResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleTransactionalIdResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ExternalLogging.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleType.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertAndKeySecretSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertSecretSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertificateExpirationPolicy.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnect.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ContainerEnvVar.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/HasConfigurableMetrics.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/InlineLogging.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JvmOptions.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/GenericSecretSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ClientTls.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorization.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationKeycloak.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationOpa.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationSimple.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridge.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/Probe.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeClientSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeConsumerSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeHttpConfig.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeHttpCors.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeProducerSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnector.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/Constants.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/Spec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRule.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertificateAuthority.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxAuthenticationPassword.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxOptions.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2Spec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ConnectorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2MirrorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2Resources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalance.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/HasSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerConsumerSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerProducerSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaResources.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopic.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUser.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserAuthorization.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserAuthorizationSimple.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserQuotas.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserScramSha512ClientAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserTlsClientAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/MetricsConfig.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/Password.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/Logging.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordSecretSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/Rack.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/Sidecar.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/SystemProperty.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsClientAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsSidecar.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsSidecarLogLevel.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/Kafka.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/UnknownPropertyPreserving.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuth.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationPlain.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScram.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha256.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha512.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationTls.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/class-use/KafkaRebalanceAnnotation.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/class-use/KafkaRebalanceState.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/class-use/BrokerCapacity.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ConnectorPlugin.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationEnv.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationVolumeSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationEnvVarSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/Build.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/Plugin.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/Artifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/DockerOutput.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/DownloadableArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/ImageStreamOutput.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/JarArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/MavenArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/OtherArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/Output.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/TgzArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/ZipArtifact.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfiguration.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/NodeAddressType.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/KafkaListenerType.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListener.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationBootstrap.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationBroker.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfiguration.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationCustom.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationOAuth.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationScramSha512.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationTls.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/Condition.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaBridgeStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaConnectStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaMirrorMaker2Status.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaMirrorMakerStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaRebalanceStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaUserStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ListenerAddress.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/HasStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaConnectorStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaTopicStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/Status.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ListenerStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/StrimziPodSetStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/PersistentClaimStorageOverride.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/PersistentClaimStorage.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/EphemeralStorage.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/JbodStorage.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/SingleVolumeStorage.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/Storage.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ContainerTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ExternalTrafficPolicy.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransOutputDefinitionTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaUserTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodManagementPolicy.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/StatefulSetTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/DeploymentStrategy.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/DeploymentTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/InternalServiceTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/IpFamily.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/IpFamilyPolicy.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/MetadataTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodDisruptionBudgetTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ResourceTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/BuildConfigTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransQueryTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplate.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2Template.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/class-use/JaegerTracing.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/class-use/Tracing.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractConnectorSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ExternalConfigurationReference.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxPrometheusExporterMetrics.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationCustom.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeAdminClientSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserTlsExternalClientAuthentication.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordSource.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSet.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetSpec.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/Crds.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/class-use/StrimziPodSetList.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaStatusFluent.ListenersNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaStatusFluentImpl.ListenersNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ListenerStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ListenerStatusFluent.AddressesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ListenerStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ListenerStatusFluentImpl.AddressesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ListenerStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ConditionFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ConditionFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ConditionBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaBridgeStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaBridgeStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaBridgeStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaMirrorMaker2StatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaMirrorMaker2StatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaMirrorMaker2StatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaRebalanceStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaRebalanceStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaRebalanceStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaUserStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaUserStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaUserStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaConnectorStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaConnectorStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaConnectorStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/StatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/StatusFluent.ConditionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/StatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/StatusFluentImpl.ConditionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaTopicStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaTopicStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaTopicStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaMirrorMakerStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaMirrorMakerStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaMirrorMakerStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ListenerAddressFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ListenerAddressFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/ListenerAddressBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaConnectStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaConnectStatusFluent.ConnectorPluginsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaConnectStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaConnectStatusFluentImpl.ConnectorPluginsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/KafkaConnectStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/StrimziPodSetStatusFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/StrimziPodSetStatusFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/class-use/StrimziPodSetStatusBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ProbeFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ProbeFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ProbeBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2SpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2SpecFluent.MirrorsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2SpecFluent.ClustersNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2SpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2SpecFluentImpl.MirrorsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2SpecFluentImpl.ClustersNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2SpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/TgzArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/TgzArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/TgzArtifactBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/BuildFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/BuildFluent.PluginsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/BuildFluent.DockerOutputNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/BuildFluent.ImageStreamOutputNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/BuildFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/BuildFluentImpl.PluginsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/BuildFluentImpl.DockerOutputNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/BuildFluentImpl.ImageStreamOutputNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/BuildBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/DockerOutputFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/DockerOutputFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/DockerOutputBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/MavenArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/MavenArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/MavenArtifactBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/DownloadableArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/DownloadableArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/JarArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/JarArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/JarArtifactBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/OtherArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/OtherArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/OtherArtifactBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/ZipArtifactFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/ZipArtifactFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/ZipArtifactBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/ImageStreamOutputFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/ImageStreamOutputFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/ImageStreamOutputBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluent.TgzArtifactArtifactsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluent.JarArtifactArtifactsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluent.OtherArtifactArtifactsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluent.ZipArtifactArtifactsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluent.MavenArtifactArtifactsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluentImpl.TgzArtifactArtifactsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluentImpl.JarArtifactArtifactsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluentImpl.OtherArtifactArtifactsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluentImpl.ZipArtifactArtifactsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginFluentImpl.MavenArtifactArtifactsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/class-use/PluginBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationVolumeSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationVolumeSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationVolumeSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ConnectorPluginFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ConnectorPluginFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ConnectorPluginBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationEnvVarSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationEnvVarSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationEnvVarSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationFluent.VolumesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationFluent.EnvNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationFluentImpl.VolumesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationFluentImpl.EnvNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationEnvFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationEnvFluent.ValueFromNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationEnvFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationEnvFluentImpl.ValueFromNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/class-use/ExternalConfigurationEnvBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationScramSha512Fluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationScramSha512FluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationScramSha512Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationOAuthFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationOAuthFluent.TlsTrustedCertificatesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationOAuthFluent.ClientSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationOAuthFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationOAuthFluentImpl.TlsTrustedCertificatesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationOAuthFluentImpl.ClientSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationOAuthBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationFluent.BrokersNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationFluent.BootstrapNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationFluent.BrokerCertChainAndKeyNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationFluentImpl.BrokersNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationFluentImpl.BootstrapNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationFluentImpl.BrokerCertChainAndKeyNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationBootstrapFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationBootstrapFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationBootstrapBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationBrokerFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationBrokerFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerConfigurationBrokerBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluent.ConfigurationNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluent.KafkaListenerAuthenticationCustomAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluent.KafkaListenerAuthenticationScramSha512AuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluent.KafkaListenerAuthenticationTlsAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluent.KafkaListenerAuthenticationOAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluentImpl.ConfigurationNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluentImpl.KafkaListenerAuthenticationCustomAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluentImpl.KafkaListenerAuthenticationScramSha512AuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluentImpl.KafkaListenerAuthenticationTlsAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerFluentImpl.KafkaListenerAuthenticationOAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/class-use/GenericKafkaListenerBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationCustomFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationCustomFluent.SecretsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationCustomFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationCustomFluentImpl.SecretsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationCustomBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationTlsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationTlsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationTlsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/class-use/KafkaListenerAuthenticationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.JaegerTracingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.AdminClientNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.ProducerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.ConsumerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.KafkaClientAuthenticationTlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.KafkaClientAuthenticationOAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.KafkaClientAuthenticationPlainNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.KafkaClientAuthenticationScramSha256Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.KafkaClientAuthenticationScramSha512Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.TlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluent.HttpNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.JaegerTracingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.AdminClientNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.ProducerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.ConsumerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.KafkaClientAuthenticationTlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.KafkaClientAuthenticationOAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.KafkaClientAuthenticationPlainNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.KafkaClientAuthenticationScramSha256NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.KafkaClientAuthenticationScramSha512NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.TlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecFluentImpl.HttpNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserAuthorizationSimpleFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserAuthorizationSimpleFluent.AclsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserAuthorizationSimpleFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserAuthorizationSimpleFluentImpl.AclsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserAuthorizationSimpleBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluent.TlsSidecarContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluent.UserOperatorContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluent.TopicOperatorContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluentImpl.TlsSidecarContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluentImpl.UserOperatorContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluentImpl.TopicOperatorContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/EntityOperatorTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodDisruptionBudgetTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodDisruptionBudgetTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodDisruptionBudgetTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodDisruptionBudgetTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/PodDisruptionBudgetTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransOutputDefinitionTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransOutputDefinitionTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransOutputDefinitionTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/StatefulSetTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/StatefulSetTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/StatefulSetTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/StatefulSetTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/StatefulSetTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluent.TlsSidecarContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluent.CruiseControlContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluent.ApiServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluentImpl.TlsSidecarContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluentImpl.CruiseControlContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluentImpl.ApiServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/CruiseControlTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/MetadataTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/MetadataTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/MetadataTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluent.ContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluent.ServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluentImpl.ContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluentImpl.ServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaExporterTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.JmxSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.ClusterRoleBindingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.InitContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.KafkaContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.ClusterCaCertNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.PersistentVolumeClaimNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.PerPodIngressNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.ExternalBootstrapIngressNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.PerPodRouteNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.ExternalBootstrapRouteNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.PerPodServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.ExternalBootstrapServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.BrokersServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.BootstrapServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.PodSetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluent.StatefulsetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.JmxSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.ClusterRoleBindingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.InitContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.KafkaContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.ClusterCaCertNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.PersistentVolumeClaimNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.PerPodIngressNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.ExternalBootstrapIngressNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.PerPodRouteNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.ExternalBootstrapRouteNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.PerPodServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.ExternalBootstrapServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.BrokersServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.BootstrapServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.PodSetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateFluentImpl.StatefulsetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaClusterTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluent.MirrorMakerContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluentImpl.MirrorMakerContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMakerTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ContainerTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ContainerTemplateFluent.EnvNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ContainerTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ContainerTemplateFluentImpl.EnvNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ContainerTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.JmxSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.BuildServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.ClusterRoleBindingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.BuildConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.BuildContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.InitContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.ConnectContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.ApiServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.BuildPodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.JmxSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.BuildServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.ClusterRoleBindingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.BuildConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.BuildContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.InitContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.ConnectContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.ApiServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.BuildPodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaConnectTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaUserTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaUserTemplateFluent.SecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaUserTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaUserTemplateFluentImpl.SecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaUserTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransQueryTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransQueryTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransQueryTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluent.BridgeContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluent.ApiServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluentImpl.BridgeContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluentImpl.ApiServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaBridgeTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/InternalServiceTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/InternalServiceTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/InternalServiceTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/InternalServiceTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/InternalServiceTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ResourceTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ResourceTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ResourceTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ResourceTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ResourceTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateFluent.ContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateFluentImpl.ContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/JmxTransTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.JmxSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.ZookeeperContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.PersistentVolumeClaimNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.NodesServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.ClientServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.PodSetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluent.StatefulsetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.JmxSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.ZookeeperContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.PersistentVolumeClaimNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.NodesServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.ClientServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.PodSetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateFluentImpl.StatefulsetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/ZookeeperClusterTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/DeploymentTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/DeploymentTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/DeploymentTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/DeploymentTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/DeploymentTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/BuildConfigTemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/BuildConfigTemplateFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/BuildConfigTemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/BuildConfigTemplateFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/BuildConfigTemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluent.JmxSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluent.ServiceAccountNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluent.MirrorMaker2ContainerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluent.PodDisruptionBudgetNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluent.ApiServiceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluent.PodNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluent.DeploymentNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluentImpl.JmxSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluentImpl.ServiceAccountNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluentImpl.MirrorMaker2ContainerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluentImpl.PodDisruptionBudgetNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluentImpl.ApiServiceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluentImpl.PodNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateFluentImpl.DeploymentNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/class-use/KafkaMirrorMaker2TemplateBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluent.QuotasNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluent.KafkaUserAuthorizationSimpleNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluent.KafkaUserScramSha512ClientAuthenticationNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluent.KafkaUserTlsExternalClientAuthenticationNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluent.KafkaUserTlsClientAuthenticationNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluentImpl.QuotasNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluentImpl.KafkaUserAuthorizationSimpleNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluentImpl.KafkaUserScramSha512ClientAuthenticationNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluentImpl.KafkaUserTlsExternalClientAuthenticationNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecFluentImpl.KafkaUserTlsClientAuthenticationNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxOptionsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxOptionsFluent.KafkaJmxAuthenticationPasswordNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxOptionsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxOptionsFluentImpl.KafkaJmxAuthenticationPasswordNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxOptionsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/JbodStorageFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/JbodStorageFluent.EphemeralStorageVolumesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/JbodStorageFluent.PersistentClaimStorageVolumesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/JbodStorageFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/JbodStorageFluentImpl.EphemeralStorageVolumesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/JbodStorageFluentImpl.PersistentClaimStorageVolumesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/JbodStorageBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/PersistentClaimStorageOverrideFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/PersistentClaimStorageOverrideFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/PersistentClaimStorageOverrideBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/EphemeralStorageFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/EphemeralStorageFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/EphemeralStorageBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/PersistentClaimStorageFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/PersistentClaimStorageFluent.OverridesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/PersistentClaimStorageFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/PersistentClaimStorageFluentImpl.OverridesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/class-use/PersistentClaimStorageBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/RackFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/RackFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/RackBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsClientAuthenticationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsClientAuthenticationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsClientAuthenticationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleFluent.AclRuleClusterResourceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleFluent.AclRuleTransactionalIdResourceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleFluent.AclRuleTopicResourceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleFluent.AclRuleGroupResourceNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleFluentImpl.AclRuleClusterResourceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleFluentImpl.AclRuleTransactionalIdResourceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleFluentImpl.AclRuleTopicResourceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleFluentImpl.AclRuleGroupResourceNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2Fluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2Fluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2Fluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2Fluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2FluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2FluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2FluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2FluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ClientTlsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ClientTlsFluent.TrustedCertificatesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ClientTlsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ClientTlsFluentImpl.TrustedCertificatesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ClientTlsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityUserOperatorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserTlsClientAuthenticationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserTlsClientAuthenticationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserTlsClientAuthenticationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeHttpConfigFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeHttpConfigFluent.CorsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeHttpConfigFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeHttpConfigFluentImpl.CorsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeHttpConfigBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxPrometheusExporterMetricsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxPrometheusExporterMetricsFluent.ValueFromNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxPrometheusExporterMetricsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxPrometheusExporterMetricsFluentImpl.ValueFromNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxPrometheusExporterMetricsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/SystemPropertyFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/SystemPropertyFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/SystemPropertyBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaTopicSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluent.JmxPrometheusExporterMetricsConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluent.BrokerCapacityNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluent.TlsSidecarNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluentImpl.JmxPrometheusExporterMetricsConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluentImpl.BrokerCapacityNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecFluentImpl.TlsSidecarNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CruiseControlSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserTlsExternalClientAuthenticationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserTlsExternalClientAuthenticationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserTlsExternalClientAuthenticationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.JmxPrometheusExporterMetricsConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.JmxOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.EphemeralStorageNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluent.PersistentClaimStorageNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.JmxPrometheusExporterMetricsConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.JmxOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.EphemeralStorageNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecFluentImpl.PersistentClaimStorageNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ZookeeperClusterSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/StrimziPodSetBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ConnectorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ConnectorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ConnectorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/class-use/BrokerCapacityFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/class-use/BrokerCapacityFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/class-use/BrokerCapacityBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/class-use/JaegerTracingFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/class-use/JaegerTracingFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/class-use/JaegerTracingBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.ExternalConfigurationNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.JaegerTracingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.JmxPrometheusExporterMetricsConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.JmxOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.ExternalConfigurationNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.JaegerTracingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.JmxPrometheusExporterMetricsConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.JmxOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractKafkaConnectSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxAuthenticationPasswordFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxAuthenticationPasswordFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaJmxAuthenticationPasswordBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2MirrorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2MirrorSpecFluent.HeartbeatConnectorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2MirrorSpecFluent.CheckpointConnectorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2MirrorSpecFluent.SourceConnectorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2MirrorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2MirrorSpecFluentImpl.HeartbeatConnectorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2MirrorSpecFluentImpl.CheckpointConnectorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2MirrorSpecFluentImpl.SourceConnectorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2MirrorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha256Fluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha256Fluent.PasswordSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha256FluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha256FluentImpl.PasswordSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha256Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthFluent.TlsTrustedCertificatesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthFluent.RefreshTokenNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthFluent.AccessTokenNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthFluent.ClientSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthFluentImpl.TlsTrustedCertificatesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthFluentImpl.RefreshTokenNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthFluentImpl.AccessTokenNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthFluentImpl.ClientSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationOAuthBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha512Fluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha512Fluent.PasswordSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha512FluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha512FluentImpl.PasswordSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationScramSha512Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationPlainFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationPlainFluent.PasswordSecretNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationPlainFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationPlainFluentImpl.PasswordSecretNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationPlainBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationTlsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationTlsFluent.CertificateAndKeyNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationTlsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationTlsFluentImpl.CertificateAndKeyNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/class-use/KafkaClientAuthenticationTlsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsSidecarFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsSidecarFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsSidecarFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsSidecarFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsSidecarFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsSidecarFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/TlsSidecarBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertificateAuthorityFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertificateAuthorityFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertificateAuthorityBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeConsumerSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeConsumerSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeConsumerSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluent.BuildNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluent.KafkaClientAuthenticationTlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluent.KafkaClientAuthenticationOAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluent.KafkaClientAuthenticationPlainNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluent.KafkaClientAuthenticationScramSha256Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluent.KafkaClientAuthenticationScramSha512Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluent.TlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluent.RackNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluentImpl.BuildNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluentImpl.KafkaClientAuthenticationTlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluentImpl.KafkaClientAuthenticationOAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluentImpl.KafkaClientAuthenticationPlainNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluentImpl.KafkaClientAuthenticationScramSha256NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluentImpl.KafkaClientAuthenticationScramSha512NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluentImpl.TlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecFluentImpl.RackNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleTopicResourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleTopicResourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleTopicResourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleGroupResourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleGroupResourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleGroupResourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractConnectorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AbstractConnectorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecFluent.TlsSidecarNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecFluent.UserOperatorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecFluent.TopicOperatorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecFluentImpl.TlsSidecarNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecFluentImpl.UserOperatorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecFluentImpl.TopicOperatorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityOperatorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeHttpCorsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeHttpCorsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeHttpCorsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluent.StartupProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecFluentImpl.StartupProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/EntityTopicOperatorSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/SidecarFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/SidecarFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/SidecarBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JvmOptionsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JvmOptionsFluent.JavaSystemPropertiesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JvmOptionsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JvmOptionsFluentImpl.JavaSystemPropertiesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JvmOptionsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaRebalanceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluent.KafkaClientAuthenticationTlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluent.KafkaClientAuthenticationOAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluent.KafkaClientAuthenticationPlainNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluent.KafkaClientAuthenticationScramSha256Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluent.KafkaClientAuthenticationScramSha512Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluent.TlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluentImpl.KafkaClientAuthenticationTlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluentImpl.KafkaClientAuthenticationOAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluentImpl.KafkaClientAuthenticationPlainNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluentImpl.KafkaClientAuthenticationScramSha256NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluentImpl.KafkaClientAuthenticationScramSha512NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecFluentImpl.TlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMaker2ClusterSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationOpaFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationOpaFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationOpaBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeProducerSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeProducerSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeProducerSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ExternalLoggingFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ExternalLoggingFluent.ValueFromNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ExternalLoggingFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ExternalLoggingFluentImpl.ValueFromNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ExternalLoggingBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordSecretSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordSecretSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordSecretSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationCustomFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationCustomFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationCustomBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordFluent.ValueFromNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordFluentImpl.ValueFromNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/PasswordBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluent.KafkaClientAuthenticationTlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluent.KafkaClientAuthenticationOAuthNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluent.KafkaClientAuthenticationPlainNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluent.KafkaClientAuthenticationScramSha256Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluent.KafkaClientAuthenticationScramSha512Nested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluent.TlsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluentImpl.KafkaClientAuthenticationTlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluentImpl.KafkaClientAuthenticationOAuthNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluentImpl.KafkaClientAuthenticationPlainNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluentImpl.KafkaClientAuthenticationScramSha256NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluentImpl.KafkaClientAuthenticationScramSha512NestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecFluentImpl.TlsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerClientSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserScramSha512ClientAuthenticationFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserScramSha512ClientAuthenticationFluent.PasswordNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserScramSha512ClientAuthenticationFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserScramSha512ClientAuthenticationFluentImpl.PasswordNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserScramSha512ClientAuthenticationBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertAndKeySecretSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertAndKeySecretSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertAndKeySecretSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerConsumerSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluent.ClientsCaNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluent.CruiseControlNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluent.KafkaExporterNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluent.JmxTransNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluent.ClusterCaNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluent.EntityOperatorNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluent.ZookeeperNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluent.KafkaNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerConsumerSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerConsumerSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertSecretSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertSecretSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/CertSecretSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluentImpl.ClientsCaNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluentImpl.CruiseControlNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluentImpl.KafkaExporterNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluentImpl.JmxTransNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluentImpl.ClusterCaNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluentImpl.EntityOperatorNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluentImpl.ZookeeperNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecFluentImpl.KafkaNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/InlineLoggingFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/InlineLoggingFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/InlineLoggingBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserQuotasFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserQuotasFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaUserQuotasBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ExternalConfigurationReferenceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ExternalConfigurationReferenceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ExternalConfigurationReferenceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationSimpleFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationSimpleFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationSimpleBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleTransactionalIdResourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleTransactionalIdResourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleTransactionalIdResourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerProducerSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerProducerSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerProducerSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/GenericSecretSourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/GenericSecretSourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/GenericSecretSourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransSpecFluent.KafkaQueriesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransSpecFluent.OutputDefinitionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransSpecFluentImpl.KafkaQueriesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransSpecFluentImpl.OutputDefinitionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/JmxTransSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.KafkaAuthorizationSimpleNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.KafkaAuthorizationKeycloakNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.KafkaAuthorizationCustomNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.KafkaAuthorizationOpaNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.ListenersNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.JmxPrometheusExporterMetricsConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.JmxOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.RackNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.JbodStorageNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.EphemeralStorageNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluent.PersistentClaimStorageNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.KafkaAuthorizationSimpleNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.KafkaAuthorizationKeycloakNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.KafkaAuthorizationCustomNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.KafkaAuthorizationOpaNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.ListenersNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.JmxPrometheusExporterMetricsConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.JmxOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.RackNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.JbodStorageNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.EphemeralStorageNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecFluentImpl.PersistentClaimStorageNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaClusterSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectorBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/SpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/SpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeClientSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeClientSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ContainerEnvVarFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ContainerEnvVarFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/ContainerEnvVarBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationKeycloakFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationKeycloakFluent.TlsTrustedCertificatesNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationKeycloakFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationKeycloakFluentImpl.TlsTrustedCertificatesNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaAuthorizationKeycloakBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeAdminClientSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeAdminClientSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaBridgeAdminClientSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaExporterSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleClusterResourceFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleClusterResourceFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/AclRuleClusterResourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.TemplateNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.JaegerTracingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.JmxPrometheusExporterMetricsConfigNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.InlineLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.ExternalLoggingNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.JvmOptionsNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.ReadinessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.LivenessProbeNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.ProducerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluent.ConsumerNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.TemplateNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.JaegerTracingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.JmxPrometheusExporterMetricsConfigNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.InlineLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.ExternalLoggingNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.JvmOptionsNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.ReadinessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.LivenessProbeNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.ProducerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecFluentImpl.ConsumerNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaMirrorMakerSpecBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectFluent.StatusNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectFluent.MetadataNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectFluent.SpecNested.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectFluentImpl.StatusNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectFluentImpl.MetadataNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectFluentImpl.SpecNestedImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/class-use/KafkaConnectBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/authentication/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/balancing/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/connect/build/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/listener/arraylistener/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/status/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/storage/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/template/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/io/strimzi/api/kafka/model/tracing/package-use.html...
Building index for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/overview-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/index-all.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/allclasses-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/allpackages-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/deprecated-list.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/index.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/overview-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/api/target/apidocs/help-doc.html...
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/api/target/api-0.29.0-SNAPSHOT-javadoc.jar
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:test-jar (default) @ api ---
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:integration-test (default) @ api ---
[WARNING] useSystemClassLoader setting has no effect when not forking
[WARNING] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:verify (default) @ api ---
[INFO] 
[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (analyze) @ api ---
[INFO] No dependency problems found
[INFO] 
[INFO] ------------------------< io.strimzi:mockkube >-------------------------
[INFO] Building mockkube 0.29.0-SNAPSHOT                                 [6/10]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ mockkube ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ mockkube ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ mockkube ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/mockkube/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ mockkube ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/mockkube/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ mockkube ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ mockkube ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ mockkube ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M5:test (default-test) @ mockkube ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ mockkube ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:report (report) @ mockkube ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] >>> maven-source-plugin:3.0.1:jar (attach-sources) > generate-sources @ mockkube >>>
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ mockkube ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ mockkube ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ mockkube ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/mockkube/target/jacoco.exec
[INFO] 
[INFO] <<< maven-source-plugin:3.0.1:jar (attach-sources) < generate-sources @ mockkube <<<
[INFO] 
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar (attach-sources) @ mockkube ---
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/mockkube/target/mockkube-0.29.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] --- maven-javadoc-plugin:3.1.0:jar (attach-javadocs) @ mockkube ---
[INFO] 
Loading source file /home/cloud-user/strimzi-kafka-operator/mockkube/src/main/java/io/strimzi/test/mockkube/Observer.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/mockkube/src/main/java/io/strimzi/test/mockkube/PredicatedWatcher.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/mockkube/src/main/java/io/strimzi/test/mockkube/CustomResourceMockBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/mockkube/src/main/java/io/strimzi/test/mockkube/ServiceMockBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/mockkube/src/main/java/io/strimzi/test/mockkube/DeploymentMockBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/mockkube/src/main/java/io/strimzi/test/mockkube/MockBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/mockkube/src/main/java/io/strimzi/test/mockkube/MockKube.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/mockkube/src/main/java/io/strimzi/test/mockkube/StatefulSetMockBuilder.java...
Constructing Javadoc information...
Standard Doclet version 11.0.11
Building tree for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/io/strimzi/test/mockkube/MockKube.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/io/strimzi/test/mockkube/MockKube.MockedCrd.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/io/strimzi/test/mockkube/Observer.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/io/strimzi/test/mockkube/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/io/strimzi/test/mockkube/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/constant-values.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/io/strimzi/test/mockkube/class-use/Observer.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/io/strimzi/test/mockkube/class-use/MockKube.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/io/strimzi/test/mockkube/class-use/MockKube.MockedCrd.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/io/strimzi/test/mockkube/package-use.html...
Building index for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/overview-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/index-all.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/allclasses-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/allpackages-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/deprecated-list.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/index.html...
Generating /home/cloud-user/strimzi-kafka-operator/mockkube/target/apidocs/help-doc.html...
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/mockkube/target/mockkube-0.29.0-SNAPSHOT-javadoc.jar
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:integration-test (default) @ mockkube ---
[WARNING] useSystemClassLoader setting has no effect when not forking
[WARNING] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:verify (default) @ mockkube ---
[INFO] 
[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (analyze) @ mockkube ---
[INFO] No dependency problems found
[INFO] 
[INFO] ----------------------< io.strimzi:config-model >-----------------------
[INFO] Building config-model 0.29.0-SNAPSHOT                             [7/10]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ config-model ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ config-model ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ config-model ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/config-model/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ config-model ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/config-model/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ config-model ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ config-model ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/config-model/src/test/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ config-model ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M5:test (default-test) @ config-model ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ config-model ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:report (report) @ config-model ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] >>> maven-source-plugin:3.0.1:jar (attach-sources) > generate-sources @ config-model >>>
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ config-model ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ config-model ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ config-model ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/config-model/target/jacoco.exec
[INFO] 
[INFO] <<< maven-source-plugin:3.0.1:jar (attach-sources) < generate-sources @ config-model <<<
[INFO] 
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar (attach-sources) @ config-model ---
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/config-model/target/config-model-0.29.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] --- maven-javadoc-plugin:3.1.0:jar (attach-javadocs) @ config-model ---
[INFO] 
Loading source file /home/cloud-user/strimzi-kafka-operator/config-model/src/main/java/io/strimzi/kafka/config/model/ConfigModel.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/config-model/src/main/java/io/strimzi/kafka/config/model/ConfigModels.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/config-model/src/main/java/io/strimzi/kafka/config/model/Scope.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/config-model/src/main/java/io/strimzi/kafka/config/model/Type.java...
Constructing Javadoc information...
Standard Doclet version 11.0.11
Building tree for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/ConfigModel.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/ConfigModels.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/Scope.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/Type.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/constant-values.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/class-use/ConfigModel.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/class-use/ConfigModels.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/class-use/Scope.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/class-use/Type.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/io/strimzi/kafka/config/model/package-use.html...
Building index for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/overview-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/index-all.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/allclasses-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/allpackages-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/deprecated-list.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/index.html...
Generating /home/cloud-user/strimzi-kafka-operator/config-model/target/apidocs/help-doc.html...
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/config-model/target/config-model-0.29.0-SNAPSHOT-javadoc.jar
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:integration-test (default) @ config-model ---
[WARNING] useSystemClassLoader setting has no effect when not forking
[WARNING] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:verify (default) @ config-model ---
[INFO] 
[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (analyze) @ config-model ---
[INFO] No dependency problems found
[INFO] 
[INFO] -------------------< io.strimzi:certificate-manager >-------------------
[INFO] Building certificate-manager 0.29.0-SNAPSHOT                      [8/10]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ certificate-manager ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ certificate-manager ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ certificate-manager ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/certificate-manager/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ certificate-manager ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ certificate-manager ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ certificate-manager ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 1 resource
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ certificate-manager ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M5:test (default-test) @ certificate-manager ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ certificate-manager ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:report (report) @ certificate-manager ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] >>> maven-source-plugin:3.0.1:jar (attach-sources) > generate-sources @ certificate-manager >>>
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ certificate-manager ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ certificate-manager ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ certificate-manager ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/certificate-manager/target/jacoco.exec
[INFO] 
[INFO] <<< maven-source-plugin:3.0.1:jar (attach-sources) < generate-sources @ certificate-manager <<<
[INFO] 
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar (attach-sources) @ certificate-manager ---
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/certificate-manager-0.29.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] --- maven-javadoc-plugin:3.1.0:jar (attach-javadocs) @ certificate-manager ---
[INFO] 
Loading source file /home/cloud-user/strimzi-kafka-operator/certificate-manager/src/main/java/io/strimzi/certs/CertAndKey.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/certificate-manager/src/main/java/io/strimzi/certs/CertManager.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/certificate-manager/src/main/java/io/strimzi/certs/OpenSslCertManager.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/certificate-manager/src/main/java/io/strimzi/certs/SecretCertProvider.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/certificate-manager/src/main/java/io/strimzi/certs/Subject.java...
Constructing Javadoc information...
Standard Doclet version 11.0.11
Building tree for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/CertAndKey.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/CertManager.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/OpenSslCertManager.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/SecretCertProvider.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/Subject.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/Subject.Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/constant-values.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/class-use/CertAndKey.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/class-use/CertManager.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/class-use/OpenSslCertManager.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/class-use/SecretCertProvider.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/class-use/Subject.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/class-use/Subject.Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/io/strimzi/certs/package-use.html...
Building index for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/overview-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/index-all.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/allclasses-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/allpackages-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/deprecated-list.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/index.html...
Generating /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/apidocs/help-doc.html...
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/certificate-manager/target/certificate-manager-0.29.0-SNAPSHOT-javadoc.jar
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:integration-test (default) @ certificate-manager ---
[WARNING] useSystemClassLoader setting has no effect when not forking
[WARNING] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:verify (default) @ certificate-manager ---
[INFO] 
[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (analyze) @ certificate-manager ---
[INFO] No dependency problems found
[INFO] 
[INFO] ---------------------< io.strimzi:operator-common >---------------------
[INFO] Building operator-common 0.29.0-SNAPSHOT                          [9/10]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ operator-common ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ operator-common ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ operator-common ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/operator-common/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ operator-common ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] skip non existing resourceDirectory /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ operator-common ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ operator-common ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 9 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ operator-common ---
[INFO] Nothing to compile - all classes are up to date
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M5:test (default-test) @ operator-common ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ operator-common ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:report (report) @ operator-common ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] >>> maven-source-plugin:3.0.1:jar (attach-sources) > generate-sources @ operator-common >>>
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ operator-common ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ operator-common ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ operator-common ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/operator-common/target/jacoco.exec
[INFO] 
[INFO] <<< maven-source-plugin:3.0.1:jar (attach-sources) < generate-sources @ operator-common <<<
[INFO] 
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar (attach-sources) @ operator-common ---
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/operator-common/target/operator-common-0.29.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] --- maven-javadoc-plugin:3.1.0:jar (attach-javadocs) @ operator-common ---
[INFO] 
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/model/InvalidResourceException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/model/NoSuchResourceException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/model/Ca.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/model/ClientsCa.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/model/NodeUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/model/StatusDiff.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlEndpoints.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlUserTaskStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlGoals.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlConfigurationParameters.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlLoadParameters.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlParameters.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlRebalanceKeys.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/AdminClientProvider.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/BackOff.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/InvalidConfigurationException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/MaxAttemptsExceededException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/MetricsProvider.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/PasswordGenerator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/model/NamespaceAndName.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/model/Labels.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/model/OrderedProperties.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/model/ResourceVisitor.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/model/ValidationVisitor.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/DeploymentOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/IngressV1Beta1Operator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/PodDisruptionBudgetOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/PvcOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/ResourceSupport.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/ServiceAccountOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/StatusUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/AbstractNonNamespacedResourceOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/AbstractScalableResourceOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/ClusterRoleOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/ReconcileResult.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/CrdOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/TimeoutException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/AbstractJsonDiff.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/AbstractResourceOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/AbstractWatchableStatusedResourceOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/BuildOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/EndpointOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/ImageStreamOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/NetworkPolicyOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/NoStackTraceTimeoutException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/RoleBindingOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/RoleOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/SecretOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/BuildConfigOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/ClusterRoleBindingOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/ConfigMapOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/DeploymentConfigOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/IngressOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/NodeOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/PodOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/ResourceDiff.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/RouteOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/ServiceOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/StorageClassOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/AbstractReadyResourceOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/AbstractWatchableResourceOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/operator/resource/PodDisruptionBudgetV1Beta1Operator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/process/ProcessHelper.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/MetricsAndLogging.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/ReconciliationException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/AbstractOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/Annotations.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/DefaultAdminClientProvider.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/InvalidConfigParameterException.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/MicrometerMetricsProvider.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/Operator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/OperatorWatcher.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/Reconciliation.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/ReconciliationLogger.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/common/Util.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/KubernetesVersion.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/operator-common/src/main/java/io/strimzi/operator/PlatformFeaturesAvailability.java...
Constructing Javadoc information...
Standard Doclet version 11.0.11
Building tree for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/AbstractOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/AdminClientProvider.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/Annotations.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/BackOff.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/DefaultAdminClientProvider.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/InvalidConfigParameterException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/InvalidConfigurationException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/MaxAttemptsExceededException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/MetricsAndLogging.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/MetricsProvider.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/MicrometerMetricsProvider.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/Operator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/PasswordGenerator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/Reconciliation.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/ReconciliationException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/ReconciliationLogger.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/Util.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlConfigurationParameters.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlEndpoints.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlGoals.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlLoadParameters.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlParameters.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlRebalanceKeys.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/CruiseControlUserTaskStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/Ca.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/ClientsCa.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/InvalidResourceException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/NodeUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/NoSuchResourceException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/StatusDiff.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/Labels.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/NamespaceAndName.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/OrderedProperties.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/ResourceVisitor.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/ResourceVisitor.Property.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/ResourceVisitor.Visitor.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/ValidationVisitor.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/KubernetesVersion.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/PlatformFeaturesAvailability.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/process/ProcessHelper.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/process/ProcessHelper.ProcessResult.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/AbstractJsonDiff.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/AbstractNonNamespacedResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/AbstractReadyResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/AbstractResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/AbstractScalableResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/AbstractWatchableResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/AbstractWatchableStatusedResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/BuildConfigOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/BuildOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ClusterRoleBindingOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ClusterRoleOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ConfigMapOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/CrdOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/DeploymentConfigOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/DeploymentOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/EndpointOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ImageStreamOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/IngressOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/IngressV1Beta1Operator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/NetworkPolicyOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/NodeOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/NoStackTraceTimeoutException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/PodDisruptionBudgetOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/PodDisruptionBudgetV1Beta1Operator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/PodOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/PvcOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ReconcileResult.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ReconcileResult.Created.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ReconcileResult.Noop.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ReconcileResult.Patched.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ResourceSupport.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/RoleBindingOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/RoleOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/RouteOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/SecretOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ServiceAccountOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/ServiceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/StatusUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/StorageClassOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/TimeoutException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/process/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/process/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/constant-values.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/serialized-form.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/class-use/InvalidResourceException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/class-use/NoSuchResourceException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/class-use/Ca.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/class-use/ClientsCa.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/class-use/NodeUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/class-use/StatusDiff.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/class-use/CruiseControlEndpoints.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/class-use/CruiseControlUserTaskStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/class-use/CruiseControlGoals.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/class-use/CruiseControlConfigurationParameters.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/class-use/CruiseControlLoadParameters.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/class-use/CruiseControlParameters.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/class-use/CruiseControlRebalanceKeys.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/AdminClientProvider.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/BackOff.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/InvalidConfigurationException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/MaxAttemptsExceededException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/MetricsProvider.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/PasswordGenerator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/class-use/NamespaceAndName.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/class-use/Labels.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/class-use/OrderedProperties.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/class-use/ResourceVisitor.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/class-use/ResourceVisitor.Property.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/class-use/ResourceVisitor.Visitor.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/class-use/ValidationVisitor.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/DeploymentOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/IngressV1Beta1Operator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/PodDisruptionBudgetOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/PvcOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ResourceSupport.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ServiceAccountOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/StatusUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/AbstractNonNamespacedResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/AbstractScalableResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ClusterRoleOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ReconcileResult.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ReconcileResult.Patched.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ReconcileResult.Created.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ReconcileResult.Noop.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/CrdOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/TimeoutException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/AbstractJsonDiff.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/AbstractResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/AbstractWatchableStatusedResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/BuildOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/EndpointOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ImageStreamOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/NetworkPolicyOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/NoStackTraceTimeoutException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/RoleBindingOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/RoleOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/SecretOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/BuildConfigOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ClusterRoleBindingOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ConfigMapOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/DeploymentConfigOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/IngressOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/NodeOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/PodOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/RouteOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/ServiceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/StorageClassOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/AbstractReadyResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/AbstractWatchableResourceOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/class-use/PodDisruptionBudgetV1Beta1Operator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/process/class-use/ProcessHelper.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/process/class-use/ProcessHelper.ProcessResult.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/MetricsAndLogging.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/ReconciliationException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/AbstractOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/Annotations.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/DefaultAdminClientProvider.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/InvalidConfigParameterException.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/MicrometerMetricsProvider.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/Operator.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/Reconciliation.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/ReconciliationLogger.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/class-use/Util.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/class-use/KubernetesVersion.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/class-use/PlatformFeaturesAvailability.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/model/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/cluster/operator/resource/cruisecontrol/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/model/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/operator/resource/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/io/strimzi/operator/common/process/package-use.html...
Building index for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/overview-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/index-all.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/allclasses-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/allpackages-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/deprecated-list.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/index.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/overview-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/operator-common/target/apidocs/help-doc.html...
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/operator-common/target/operator-common-0.29.0-SNAPSHOT-javadoc.jar
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:test-jar (default) @ operator-common ---
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:integration-test (default) @ operator-common ---
[WARNING] useSystemClassLoader setting has no effect when not forking
[WARNING] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:verify (default) @ operator-common ---
[INFO] 
[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (analyze) @ operator-common ---
[INFO] No dependency problems found
[INFO] 
[INFO] -----------------------< io.strimzi:systemtest >------------------------
[INFO] Building systemtest 0.29.0-SNAPSHOT                              [10/10]
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ systemtest ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ systemtest ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ systemtest ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/systemtest/target/jacoco.exec
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ systemtest ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 3 resources
[INFO] Copying 2 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:compile (default-compile) @ systemtest ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 147 source files to /home/cloud-user/strimzi-kafka-operator/systemtest/target/classes
[INFO] /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/BaseClientsFluentImpl.java: Some input files use unchecked or unsafe operations.
[INFO] /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/BaseClientsFluentImpl.java: Recompile with -Xlint:unchecked for details.
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ systemtest ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 32 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.8.1:testCompile (default-testCompile) @ systemtest ---
[INFO] Changes detected - recompiling the module!
[INFO] Compiling 67 source files to /home/cloud-user/strimzi-kafka-operator/systemtest/target/test-classes
[INFO] /home/cloud-user/strimzi-kafka-operator/systemtest/src/test/java/io/strimzi/systemtest/security/oauth/OauthPlainIsolatedST.java: Some input files use or override a deprecated API.
[INFO] /home/cloud-user/strimzi-kafka-operator/systemtest/src/test/java/io/strimzi/systemtest/security/oauth/OauthPlainIsolatedST.java: Recompile with -Xlint:deprecation for details.
[INFO] /home/cloud-user/strimzi-kafka-operator/systemtest/src/test/java/io/strimzi/systemtest/operators/CustomResourceStatusIsolatedST.java: /home/cloud-user/strimzi-kafka-operator/systemtest/src/test/java/io/strimzi/systemtest/operators/CustomResourceStatusIsolatedST.java uses unchecked or unsafe operations.
[INFO] /home/cloud-user/strimzi-kafka-operator/systemtest/src/test/java/io/strimzi/systemtest/operators/CustomResourceStatusIsolatedST.java: Recompile with -Xlint:unchecked for details.
[INFO] 
[INFO] --- maven-surefire-plugin:3.0.0-M5:test (default-test) @ systemtest ---
[INFO] 
[INFO] -------------------------------------------------------
[INFO]  T E S T S
[INFO] -------------------------------------------------------
[INFO] 
[INFO] Results:
[INFO] 
[INFO] Tests run: 0, Failures: 0, Errors: 0, Skipped: 0
[INFO] 
[WARNING] Corrupted STDOUT by directly writing to native stream in forked JVM 1. See FAQ web page and the dump file /home/cloud-user/strimzi-kafka-operator/systemtest/target/surefire-reports/2022-03-28T08-44-21_299-jvmRun1.dumpstream
[INFO] 
[INFO] --- maven-jar-plugin:3.1.0:jar (default-jar) @ systemtest ---
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/systemtest/target/systemtest-0.29.0-SNAPSHOT.jar
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:report (report) @ systemtest ---
[INFO] Skipping JaCoCo execution due to missing execution data file.
[INFO] 
[INFO] >>> maven-source-plugin:3.0.1:jar (attach-sources) > generate-sources @ systemtest >>>
[INFO] 
[INFO] --- maven-checkstyle-plugin:3.1.2:check (validate) @ systemtest ---
[INFO] Starting audit...
Audit done.
[INFO] You have 0 Checkstyle violations.
[INFO] 
[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (enforce-banned-dependencies) @ systemtest ---
[INFO] 
[INFO] --- jacoco-maven-plugin:0.7.9:prepare-agent (default) @ systemtest ---
[INFO] surefireArgLine set to -javaagent:/home/cloud-user/.m2/repository/org/jacoco/org.jacoco.agent/0.7.9/org.jacoco.agent-0.7.9-runtime.jar=destfile=/home/cloud-user/strimzi-kafka-operator/systemtest/target/jacoco.exec
[INFO] 
[INFO] <<< maven-source-plugin:3.0.1:jar (attach-sources) < generate-sources @ systemtest <<<
[INFO] 
[INFO] 
[INFO] --- maven-source-plugin:3.0.1:jar (attach-sources) @ systemtest ---
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/systemtest/target/systemtest-0.29.0-SNAPSHOT-sources.jar
[INFO] 
[INFO] --- maven-javadoc-plugin:3.1.0:jar (attach-javadocs) @ systemtest ---
[INFO] 
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/MultiNodeClusterOnly.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/MultiNodeClusterOnlyCondition.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/OpenShiftOnly.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/RequiredMinKubeApiVersion.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/RequiredMinKubeApiVersionCondition.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/IsolatedTest.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/ParallelNamespaceTest.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/IsolatedSuite.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/OpenShiftOnlyCondition.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/ParallelSuite.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/ParallelTest.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/StatefulSetTest.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/StatefulSetTestCondition.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/StrimziPodSetTest.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/annotations/StrimziPodSetTestCondition.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/cli/KafkaCmdClient.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/enums/DefaultNetworkPolicy.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/enums/ClusterOperatorInstallType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/enums/CustomResourceStatus.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/enums/OlmInstallationStrategy.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/enums/ClusterOperatorRBACType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/enums/DeploymentTypes.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/k8s/Events.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/clientproperties/ProducerProperties.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/clientproperties/AbstractKafkaClientProperties.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/clientproperties/ConsumerProperties.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/AbstractKafkaClient.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/KafkaClientOperations.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/clients/ClientArgument.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/clients/ClientArgumentMap.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/clients/ClientType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/clients/InternalKafkaClient.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/clients/VerifiableClient.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/externalClients/ExternalKafkaClient.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/internalClients/AdminClientOperations.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/internalClients/BaseClients.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/internalClients/BridgeClients.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/internalClients/KafkaAdminClients.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/internalClients/KafkaClients.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/internalClients/KafkaOauthClients.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/kafkaclients/internalClients/KafkaTracingClients.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/keycloak/KeycloakInstance.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/listeners/ExecutionListener.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/listeners/OrderTestSuites.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/logs/LogCollector.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/logs/TestExecutionWatcher.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/matchers/HasAllOfReasons.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/matchers/HasAnyOfReasons.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/matchers/HasNoneOfReasons.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/matchers/Matchers.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/matchers/LogHasNoUnexpectedErrors.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaBridgeResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaConnectResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaConnectorResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaMirrorMaker2Resource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaTopicResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaClientsResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaMirrorMakerResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaRebalanceResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/KafkaUserResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/crd/StrimziPodSetResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/operator/BundleResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/operator/specific/HelmResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/operator/specific/SpecificResourceType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/operator/specific/OlmResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/operator/SetupClusterOperator.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/ResourceCondition.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/ResourceItem.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/ResourceType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/ThrowableRunner.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/ServiceResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/ClusterOperatorCustomResourceDefinition.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/ClusterRoleBindingResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/ClusterRoleResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/ConfigMapResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/JobResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/NetworkPolicyResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/RoleBindingResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/RoleResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/SecretResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/ServiceAccountResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/ValidatingWebhookConfigurationResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/kubernetes/DeploymentResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/ComponentType.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/ResourceManager.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/draincleaner/DrainCleanerResource.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/draincleaner/SetupDrainCleaner.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/resources/ResourceOperation.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/security/CertAndKeyFiles.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/security/SystemTestCertAndKey.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/security/SystemTestCertAndKeyBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/security/SystemTestCertManager.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaBridgeUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaConnectUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaConnectorUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaMirrorMaker2Utils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaMirrorMakerUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaRebalanceUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUserUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kafkaUtils/KafkaTopicUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/controllers/ConfigMapUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/controllers/DeploymentUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/controllers/JobUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/controllers/StatefulSetUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/controllers/StrimziPodSetUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/objects/NamespaceUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/objects/NodeUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/objects/PersistentVolumeClaimUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/objects/PodUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/objects/SecretUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/kubeUtils/objects/ServiceUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/specific/BridgeUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/specific/CruiseControlUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/specific/JmxUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/specific/OlmUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/specific/TracingUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/specific/KeycloakUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/ClientUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/FileUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/HttpUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/RollingUpdateUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/StUtils.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/utils/TestKafkaVersion.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/interfaces/IndicativeSentences.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaBridgeTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaClientsTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaConnectTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaConnectorTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaMirrorMaker2Templates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaMirrorMakerTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaRebalanceTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaTopicTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/crd/KafkaUserTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/kubernetes/ServiceTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/kubernetes/ClusterRoleBindingTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/kubernetes/NetworkPolicyTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/templates/specific/ScraperTemplates.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/BeforeAllOnce.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/metrics/MetricsCollector.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/parallel/SuiteThreadController.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/parallel/TestSuiteNamespaceManager.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/storage/TestStorage.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/Constants.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/src/main/java/io/strimzi/systemtest/Environment.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaOauthClientsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaOauthClientsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaOauthClientsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/BridgeClientsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/BridgeClientsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/BridgeClientsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaTracingClientsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaTracingClientsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaTracingClientsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/BaseClientsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/BaseClientsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaClientsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaClientsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaClientsBuilder.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaAdminClientsFluent.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaAdminClientsFluentImpl.java...
Loading source file /home/cloud-user/strimzi-kafka-operator/systemtest/target/generated-sources/annotations/io/strimzi/systemtest/kafkaclients/internalClients/KafkaAdminClientsBuilder.java...
Constructing Javadoc information...
Standard Doclet version 11.0.11
Building tree for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/IsolatedSuite.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/IsolatedTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/MultiNodeClusterOnly.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/MultiNodeClusterOnlyCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/OpenShiftOnly.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/OpenShiftOnlyCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/ParallelNamespaceTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/ParallelSuite.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/ParallelTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/RequiredMinKubeApiVersion.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/RequiredMinKubeApiVersionCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/StatefulSetTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/StatefulSetTestCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/StrimziPodSetTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/StrimziPodSetTestCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/cli/KafkaCmdClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/AbstractKafkaClientProperties.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/AbstractKafkaClientProperties.KafkaClientPropertiesBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/ConsumerProperties.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/ConsumerProperties.ConsumerPropertiesBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/ProducerProperties.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/ProducerProperties.ProducerPropertiesBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/ClientArgument.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/ClientArgumentMap.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/ClientType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/InternalKafkaClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/InternalKafkaClient.Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/VerifiableClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/VerifiableClient.VerifiableClientBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/ConfigMapUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/DeploymentUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/JobUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/StatefulSetUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/StrimziPodSetUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/KafkaBridgeResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/KafkaClientsResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/KafkaConnectorResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/KafkaConnectResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/KafkaMirrorMaker2Resource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/KafkaMirrorMakerResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/KafkaRebalanceResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/KafkaResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/KafkaTopicResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/KafkaUserResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/StrimziPodSetResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/KafkaBridgeTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/KafkaClientsTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/KafkaConnectorTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/KafkaConnectTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/KafkaMirrorMaker2Templates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/KafkaMirrorMakerTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/KafkaRebalanceTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/KafkaTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/KafkaTopicTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/KafkaUserTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/draincleaner/DrainCleanerResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/draincleaner/SetupDrainCleaner.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/ClusterOperatorInstallType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/ClusterOperatorRBACType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/CustomResourceStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/DefaultNetworkPolicy.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/DeploymentTypes.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/OlmInstallationStrategy.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/externalClients/ExternalKafkaClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/externalClients/ExternalKafkaClient.Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/interfaces/IndicativeSentences.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/AdminClientOperations.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/BaseClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/BaseClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/BaseClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/BridgeClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/BridgeClientsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/BridgeClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/BridgeClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaAdminClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaAdminClientsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaAdminClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaAdminClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaClientsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaOauthClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaOauthClientsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaOauthClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaOauthClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaTracingClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaTracingClientsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaTracingClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/KafkaTracingClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/k8s/Events.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/AbstractKafkaClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/AbstractKafkaClient.Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/KafkaClientOperations.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/KafkaBridgeUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/KafkaConnectorUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/KafkaConnectUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/KafkaMirrorMaker2Utils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/KafkaMirrorMakerUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/KafkaRebalanceUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/KafkaTopicUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/KafkaUserUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/KafkaUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/keycloak/KeycloakInstance.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/ClusterOperatorCustomResourceDefinition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/ClusterRoleBindingResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/ClusterRoleResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/ConfigMapResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/DeploymentResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/JobResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/NetworkPolicyResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/RoleBindingResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/RoleResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/SecretResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/ServiceAccountResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/ServiceResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/ValidatingWebhookConfigurationResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/kubernetes/ClusterRoleBindingTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/kubernetes/NetworkPolicyTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/kubernetes/ServiceTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/listeners/ExecutionListener.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/listeners/OrderTestSuites.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/logs/LogCollector.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/logs/TestExecutionWatcher.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/HasAllOfReasons.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/HasAnyOfReasons.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/HasNoneOfReasons.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/LogHasNoUnexpectedErrors.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/Matchers.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/metrics/MetricsCollector.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/metrics/MetricsCollector.Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/NamespaceUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/NodeUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/PersistentVolumeClaimUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/PodUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/SecretUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/ServiceUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/BundleResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/BundleResource.BundleResourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/SetupClusterOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/SetupClusterOperator.SetupClusterOperatorBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/parallel/SuiteThreadController.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/parallel/TestSuiteNamespaceManager.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/ComponentType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/ResourceCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/ResourceItem.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/ResourceManager.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/ResourceOperation.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/ResourceType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/ThrowableRunner.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/CertAndKeyFiles.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/SystemTestCertAndKey.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/SystemTestCertAndKeyBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/SystemTestCertManager.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/specific/HelmResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/specific/OlmResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/specific/SpecificResourceType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/specific/ScraperTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/BridgeUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/CruiseControlUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/CruiseControlUtils.SupportedHttpMethods.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/CruiseControlUtils.SupportedSchemes.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/JmxUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/KeycloakUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/OlmUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/TracingUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/storage/TestStorage.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/BeforeAllOnce.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/Constants.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/Environment.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/ClientUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/FileUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/HttpUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/RollingUpdateUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/StUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/TestKafkaVersion.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/cli/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/cli/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/interfaces/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/interfaces/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/k8s/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/k8s/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/externalClients/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/externalClients/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/keycloak/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/keycloak/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/listeners/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/listeners/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/logs/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/logs/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/metrics/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/metrics/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/parallel/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/parallel/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/draincleaner/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/draincleaner/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/specific/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/specific/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/storage/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/storage/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/kubernetes/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/kubernetes/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/specific/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/specific/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/package-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/package-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/constant-values.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/MultiNodeClusterOnly.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/MultiNodeClusterOnlyCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/OpenShiftOnly.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/RequiredMinKubeApiVersion.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/RequiredMinKubeApiVersionCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/IsolatedTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/ParallelNamespaceTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/IsolatedSuite.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/OpenShiftOnlyCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/ParallelSuite.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/ParallelTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/StatefulSetTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/StatefulSetTestCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/StrimziPodSetTest.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/class-use/StrimziPodSetTestCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/cli/class-use/KafkaCmdClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/class-use/DefaultNetworkPolicy.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/class-use/ClusterOperatorInstallType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/class-use/CustomResourceStatus.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/class-use/OlmInstallationStrategy.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/class-use/ClusterOperatorRBACType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/class-use/DeploymentTypes.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/k8s/class-use/Events.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/class-use/ProducerProperties.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/class-use/ProducerProperties.ProducerPropertiesBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/class-use/AbstractKafkaClientProperties.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/class-use/AbstractKafkaClientProperties.KafkaClientPropertiesBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/class-use/ConsumerProperties.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/class-use/ConsumerProperties.ConsumerPropertiesBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/class-use/AbstractKafkaClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/class-use/AbstractKafkaClient.Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/class-use/KafkaClientOperations.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/class-use/ClientArgument.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/class-use/ClientArgumentMap.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/class-use/ClientType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/class-use/InternalKafkaClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/class-use/InternalKafkaClient.Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/class-use/VerifiableClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/class-use/VerifiableClient.VerifiableClientBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/externalClients/class-use/ExternalKafkaClient.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/externalClients/class-use/ExternalKafkaClient.Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/AdminClientOperations.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/BaseClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/BridgeClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaAdminClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaOauthClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaTracingClients.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/keycloak/class-use/KeycloakInstance.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/listeners/class-use/ExecutionListener.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/listeners/class-use/OrderTestSuites.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/logs/class-use/LogCollector.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/logs/class-use/TestExecutionWatcher.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/class-use/HasAllOfReasons.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/class-use/HasAnyOfReasons.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/class-use/HasNoneOfReasons.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/class-use/Matchers.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/class-use/LogHasNoUnexpectedErrors.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/KafkaBridgeResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/KafkaConnectResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/KafkaConnectorResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/KafkaMirrorMaker2Resource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/KafkaTopicResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/KafkaClientsResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/KafkaMirrorMakerResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/KafkaRebalanceResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/KafkaResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/KafkaUserResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/class-use/StrimziPodSetResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/class-use/BundleResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/class-use/BundleResource.BundleResourceBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/specific/class-use/HelmResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/specific/class-use/SpecificResourceType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/specific/class-use/OlmResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/class-use/SetupClusterOperator.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/class-use/SetupClusterOperator.SetupClusterOperatorBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/class-use/ResourceCondition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/class-use/ResourceItem.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/class-use/ResourceType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/class-use/ThrowableRunner.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/ServiceResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/ClusterOperatorCustomResourceDefinition.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/ClusterRoleBindingResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/ClusterRoleResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/ConfigMapResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/JobResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/NetworkPolicyResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/RoleBindingResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/RoleResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/SecretResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/ServiceAccountResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/ValidatingWebhookConfigurationResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/class-use/DeploymentResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/class-use/ComponentType.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/class-use/ResourceManager.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/draincleaner/class-use/DrainCleanerResource.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/draincleaner/class-use/SetupDrainCleaner.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/class-use/ResourceOperation.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/class-use/CertAndKeyFiles.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/class-use/SystemTestCertAndKey.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/class-use/SystemTestCertAndKeyBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/class-use/SystemTestCertManager.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/class-use/KafkaBridgeUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/class-use/KafkaConnectUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/class-use/KafkaConnectorUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/class-use/KafkaMirrorMaker2Utils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/class-use/KafkaMirrorMakerUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/class-use/KafkaRebalanceUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/class-use/KafkaUserUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/class-use/KafkaUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/class-use/KafkaTopicUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/class-use/ConfigMapUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/class-use/DeploymentUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/class-use/JobUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/class-use/StatefulSetUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/class-use/StrimziPodSetUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/class-use/NamespaceUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/class-use/NodeUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/class-use/PersistentVolumeClaimUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/class-use/PodUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/class-use/SecretUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/class-use/ServiceUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/class-use/BridgeUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/class-use/CruiseControlUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/class-use/CruiseControlUtils.SupportedSchemes.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/class-use/CruiseControlUtils.SupportedHttpMethods.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/class-use/JmxUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/class-use/OlmUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/class-use/TracingUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/class-use/KeycloakUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/class-use/ClientUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/class-use/FileUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/class-use/HttpUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/class-use/RollingUpdateUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/class-use/StUtils.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/class-use/TestKafkaVersion.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/interfaces/class-use/IndicativeSentences.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/class-use/KafkaBridgeTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/class-use/KafkaClientsTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/class-use/KafkaConnectTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/class-use/KafkaConnectorTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/class-use/KafkaMirrorMaker2Templates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/class-use/KafkaMirrorMakerTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/class-use/KafkaRebalanceTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/class-use/KafkaTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/class-use/KafkaTopicTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/class-use/KafkaUserTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/kubernetes/class-use/ServiceTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/kubernetes/class-use/ClusterRoleBindingTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/kubernetes/class-use/NetworkPolicyTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/specific/class-use/ScraperTemplates.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/class-use/BeforeAllOnce.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/metrics/class-use/MetricsCollector.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/metrics/class-use/MetricsCollector.Builder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/parallel/class-use/SuiteThreadController.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/parallel/class-use/TestSuiteNamespaceManager.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/storage/class-use/TestStorage.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/class-use/Constants.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/class-use/Environment.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaOauthClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaOauthClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaOauthClientsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/BridgeClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/BridgeClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/BridgeClientsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaTracingClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaTracingClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaTracingClientsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/BaseClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/BaseClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaClientsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaAdminClientsFluent.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaAdminClientsFluentImpl.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/class-use/KafkaAdminClientsBuilder.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/annotations/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/cli/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/enums/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/interfaces/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/k8s/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clientproperties/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/clients/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/externalClients/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/kafkaclients/internalClients/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/keycloak/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/listeners/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/logs/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/matchers/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/metrics/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/parallel/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/crd/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/draincleaner/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/kubernetes/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/resources/operator/specific/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/security/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/storage/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/crd/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/kubernetes/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/templates/specific/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kafkaUtils/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/controllers/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/kubeUtils/objects/package-use.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/io/strimzi/systemtest/utils/specific/package-use.html...
Building index for all the packages and classes...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/overview-tree.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/index-all.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/allclasses-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/allpackages-index.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/deprecated-list.html...
Building index for all classes...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/allclasses.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/index.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/overview-summary.html...
Generating /home/cloud-user/strimzi-kafka-operator/systemtest/target/apidocs/help-doc.html...
[INFO] Building jar: /home/cloud-user/strimzi-kafka-operator/systemtest/target/systemtest-0.29.0-SNAPSHOT-javadoc.jar
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:integration-test (default) @ systemtest ---
[WARNING] useSystemClassLoader setting has no effect when not forking
[WARNING] The parameter forkCount should likely not be 0, not forking a JVM for tests reduce test accuracy, ensure to have a <forkCount> >= 1.
2022-03-28 12:45:49 [main] INFO  [TestExecutionListener:29] =======================================================================
2022-03-28 12:45:49 [main] INFO  [TestExecutionListener:30] =======================================================================
2022-03-28 12:45:49 [main] INFO  [TestExecutionListener:31]                         Test run started
2022-03-28 12:45:49 [main] INFO  [TestExecutionListener:32] =======================================================================
2022-03-28 12:45:49 [main] INFO  [TestExecutionListener:33] =======================================================================
2022-03-28 12:45:49 [main] INFO  [TestExecutionListener:48] Following testclasses are selected for run:
2022-03-28 12:45:49 [main] INFO  [TestExecutionListener:51] -> io.strimzi.systemtest.bridge.HttpBridgeScramShaST
2022-03-28 12:45:49 [main] INFO  [TestExecutionListener:51] -> io.strimzi.systemtest.bridge.HttpBridgeTlsST
2022-03-28 12:45:49 [main] INFO  [TestExecutionListener:51] -> io.strimzi.systemtest.operators.topic.ThrottlingQuotaST
2022-03-28 12:45:49 [main] INFO  [TestExecutionListener:51] -> io.strimzi.systemtest.operators.topic.TopicST
2022-03-28 12:45:49 [main] INFO  [TestExecutionListener:51] -> io.strimzi.systemtest.operators.user.UserST
2022-03-28 12:45:49 [main] INFO  [TestExecutionListener:51] -> io.strimzi.systemtest.operators.ReconciliationST
2022-03-28 12:45:49 [main] INFO  [TestExecutionListener:51] -> io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST
2022-03-28 12:45:49 [main] INFO  [TestExecutionListener:52] =======================================================================
2022-03-28 12:45:49 [main] INFO  [TestExecutionListener:53] =======================================================================
[INFO] Running io.strimzi.systemtest.operators.user.UserST
[INFO] Running io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST
[INFO] Running io.strimzi.systemtest.operators.topic.TopicST
[INFO] Running io.strimzi.systemtest.bridge.HttpBridgeScramShaST
[INFO] Running io.strimzi.systemtest.bridge.HttpBridgeTlsST
[INFO] Running io.strimzi.systemtest.operators.topic.ThrottlingQuotaST
2022-03-28 12:45:50 [ForkJoinPool-1-worker-9] DEBUG [Environment:271] Json configuration is not provided or cannot be processed!
2022-03-28 12:45:50 [ForkJoinPool-1-worker-9] INFO  [Environment:219] Used environment variables:
2022-03-28 12:45:50 [ForkJoinPool-1-worker-9] INFO  [Environment:220] CONFIG: /home/cloud-user/strimzi-kafka-operator/systemtest/config.json
2022-03-28 12:45:50 [ForkJoinPool-1-worker-9] INFO  [Environment:221] STRIMZI_RBAC_SCOPE: CLUSTER
2022-03-28 12:45:50 [ForkJoinPool-1-worker-9] INFO  [Environment:221] OLM_APP_BUNDLE_PREFIX: strimzi-cluster-operator
2022-03-28 12:45:50 [ForkJoinPool-1-worker-9] INFO  [Environment:221] TEST_CLIENTS_VERSION: 0.2.0
2022-03-28 12:45:50 [ForkJoinPool-1-worker-9] INFO  [Environment:221] OLM_SOURCE_NAMESPACE: openshift-marketplace
2022-03-28 12:45:50 [ForkJoinPool-1-worker-9] INFO  [Environment:221] CLUSTER_OPERATOR_INSTALL_TYPE: BUNDLE
2022-03-28 12:45:50 [ForkJoinPool-1-worker-9] INFO  [Environment:221] STRIMZI_COMPONENTS_LOG_LEVEL: INFO
2022-03-28 12:45:50 [ForkJoinPool-1-worker-9] INFO  [Environment:221] SKIP_TEARDOWN: false
2022-03-28 12:45:50 [ForkJoinPool-1-worker-9] INFO  [Environment:221] LB_FINALIZERS: false
2022-03-28 12:45:50 [ForkJoinPool-1-worker-9] INFO  [Environment:221] OLM_OPERATOR_DEPLOYMENT_NAME: strimzi-cluster-operator
2022-03-28 12:45:50 [ForkJoinPool-1-worker-9] INFO  [Environment:221] DOCKER_ORG: strimzi
2022-03-28 12:45:50 [ForkJoinPool-1-worker-9] INFO  [Environment:221] TEST_LOG_DIR: /home/cloud-user/strimzi-kafka-operator/systemtest/../systemtest/target/logs/
2022-03-28 12:45:50 [ForkJoinPool-1-worker-9] INFO  [Environment:221] COMPONENTS_IMAGE_PULL_POLICY: IfNotPresent
2022-03-28 12:45:50 [ForkJoinPool-1-worker-9] INFO  [Environment:221] DOCKER_REGISTRY: quay.io
2022-03-28 12:45:50 [ForkJoinPool-1-worker-9] INFO  [Environment:221] TEST_CLIENT_IMAGE: quay.io/strimzi/test-client:latest-kafka-3.1.0
2022-03-28 12:45:50 [ForkJoinPool-1-worker-9] INFO  [Environment:221] SYSTEM_TEST_STRIMZI_IMAGE_PULL_SECRET: 
2022-03-28 12:45:50 [ForkJoinPool-1-worker-9] INFO  [Environment:221] TEST_ADMIN_IMAGE: quay.io/strimzi-test-clients/test-client-kafka-admin:0.2.0-kafka-3.1.0
2022-03-28 12:45:50 [ForkJoinPool-1-worker-9] INFO  [Environment:221] TEST_HTTP_PRODUCER_IMAGE: quay.io/strimzi-test-clients/test-client-http-producer:0.2.0
2022-03-28 12:45:50 [ForkJoinPool-1-worker-9] INFO  [Environment:221] OLM_OPERATOR_NAME: strimzi-kafka-operator
2022-03-28 12:45:50 [ForkJoinPool-1-worker-9] INFO  [Environment:221] DOCKER_TAG: latest
2022-03-28 12:45:50 [ForkJoinPool-1-worker-9] INFO  [Environment:221] OLM_SOURCE_NAME: community-operators
2022-03-28 12:45:50 [ForkJoinPool-1-worker-9] INFO  [Environment:221] STRIMZI_FEATURE_GATES: 
2022-03-28 12:45:50 [ForkJoinPool-1-worker-9] INFO  [Environment:221] CLIENTS_KAFKA_VERSION: 3.1.0
2022-03-28 12:45:50 [ForkJoinPool-1-worker-9] INFO  [Environment:221] TEST_HTTP_CONSUMER_IMAGE: quay.io/strimzi-test-clients/test-client-http-consumer:0.2.0
2022-03-28 12:45:50 [ForkJoinPool-1-worker-9] INFO  [Environment:221] STRIMZI_LOG_LEVEL: DEBUG
2022-03-28 12:45:50 [ForkJoinPool-1-worker-9] INFO  [Environment:221] ST_KAFKA_VERSION: 3.1.0
2022-03-28 12:45:50 [ForkJoinPool-1-worker-9] INFO  [Environment:221] OPERATOR_IMAGE_PULL_POLICY: Always
2022-03-28 12:45:50 [ForkJoinPool-1-worker-9] INFO  [Environment:221] DEFAULT_TO_DENY_NETWORK_POLICIES: true
2022-03-28 12:45:50 [ForkJoinPool-1-worker-9] INFO  [Environment:221] TEST_PRODUCER_IMAGE: quay.io/strimzi-test-clients/test-client-kafka-producer:0.2.0-kafka-3.1.0
2022-03-28 12:45:50 [ForkJoinPool-1-worker-9] INFO  [Environment:221] BRIDGE_IMAGE: latest-released
2022-03-28 12:45:50 [ForkJoinPool-1-worker-9] INFO  [Environment:221] TEST_STREAMS_IMAGE: quay.io/strimzi-test-clients/test-client-kafka-streams:0.2.0-kafka-3.1.0
2022-03-28 12:45:50 [ForkJoinPool-1-worker-9] INFO  [Environment:221] TEST_CONSUMER_IMAGE: quay.io/strimzi-test-clients/test-client-kafka-consumer:0.2.0-kafka-3.1.0
2022-03-28 12:45:50 [ForkJoinPool-1-worker-9] INFO  [Environment:221] OLM_OPERATOR_VERSION: 
2022-03-28 12:45:50 [ForkJoinPool-1-worker-11] DEBUG [BeforeAllOnce:51] ============================================================================
2022-03-28 12:45:50 [ForkJoinPool-1-worker-11] DEBUG [BeforeAllOnce:52] [io.strimzi.systemtest.operators.topic.ThrottlingQuotaST - Before Suite] - Setup Suite environment
2022-03-28 12:45:50 [ForkJoinPool-1-worker-11] DEBUG [Config:540] Trying to configure client from Kubernetes config...
2022-03-28 12:45:50 [ForkJoinPool-1-worker-11] DEBUG [Config:549] Found for Kubernetes config at: [/home/cloud-user/.kube/config].
2022-03-28 12:45:50 [ForkJoinPool-1-worker-11] DEBUG [Config:540] Trying to configure client from Kubernetes config...
2022-03-28 12:45:50 [ForkJoinPool-1-worker-11] DEBUG [Config:549] Found for Kubernetes config at: [/home/cloud-user/.kube/config].
2022-03-28 12:45:50 [ForkJoinPool-1-worker-11] DEBUG [KubeCluster:80] Cluster minikube is not installed!
2022-03-28 12:45:50 [ForkJoinPool-1-worker-11] DEBUG [KubeCluster:71] Cluster kubectl is installed
2022-03-28 12:45:50 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - kubectl cluster-info
2022-03-28 12:45:52 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: kubectl cluster-info
2022-03-28 12:45:52 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:45:52 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - kubectl api-resources
2022-03-28 12:45:52 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: kubectl api-resources
2022-03-28 12:45:52 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:45:52 [ForkJoinPool-1-worker-11] DEBUG [KubeCluster:77] Cluster kubectl is not running!
2022-03-28 12:45:52 [ForkJoinPool-1-worker-11] DEBUG [KubeCluster:71] Cluster oc is installed
2022-03-28 12:45:52 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc status -n default
2022-03-28 12:45:54 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc status -n default
2022-03-28 12:45:54 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:45:54 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc api-resources
[INFO] Running io.strimzi.systemtest.operators.ReconciliationST
2022-03-28 12:45:54 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc api-resources
2022-03-28 12:45:54 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:45:54 [ForkJoinPool-1-worker-11] DEBUG [KubeCluster:73] Cluster oc is running
2022-03-28 12:45:54 [ForkJoinPool-1-worker-11] INFO  [KubeCluster:87] Using cluster: oc
2022-03-28 12:45:54 [ForkJoinPool-1-worker-11] INFO  [KubeClusterResource:60] Cluster default namespace is 'default'
2022-03-28 12:45:54 [ForkJoinPool-1-worker-11] INFO  [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@27798af4
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-03-28 12:45:55 [ForkJoinPool-1-worker-11] DEBUG [SetupClusterOperator:198] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@12a2b997, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@27798af4, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='*', bindingsNamespaces=[infra-namespace], operationTimeout=300000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-03-28 12:45:55 [ForkJoinPool-1-worker-11] INFO  [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-03-28 12:45:56 [ForkJoinPool-1-worker-11] INFO  [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-03-28 12:45:56 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Namespace infra-namespace
2022-03-28 12:45:56 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace default get Namespace infra-namespace -o json
2022-03-28 12:45:57 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace default get Namespace infra-namespace -o json
2022-03-28 12:45:57 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:45:57 [ForkJoinPool-1-worker-11] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c26,c0",
            "openshift.io/sa.scc.supplemental-groups": "1000650000/10000",
            "openshift.io/sa.scc.uid-range": "1000650000/10000"
        },
        "creationTimestamp": "2022-03-28T12:45:52Z",
        "labels": {
            "kubernetes.io/metadata.name": "infra-namespace"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T12:45:52Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T12:45:52Z"
            }
        ],
        "name": "infra-namespace",
        "resourceVersion": "42941",
        "uid": "60012aa3-624b-439c-ad15-10b9705754e6"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 12:45:57 [ForkJoinPool-1-worker-11] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace]}
2022-03-28 12:45:57 [ForkJoinPool-1-worker-11] INFO  [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-28 12:45:57 [ForkJoinPool-1-worker-11] DEBUG [SetupClusterOperator:478] Installation resource type: ServiceAccount
2022-03-28 12:45:57 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-28 12:45:57 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-28 12:45:57 [ForkJoinPool-1-worker-11] DEBUG [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-28 12:45:57 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-28 12:45:57 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-28 12:45:57 [ForkJoinPool-1-worker-11] DEBUG [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-28 12:45:57 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-28 12:45:57 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-28 12:45:57 [ForkJoinPool-1-worker-11] DEBUG [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-28 12:45:57 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-28 12:45:58 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-28 12:45:58 [ForkJoinPool-1-worker-11] DEBUG [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-28 12:45:58 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-28 12:45:58 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-28 12:45:58 [ForkJoinPool-1-worker-11] DEBUG [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-28 12:45:58 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-28 12:45:58 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-28 12:45:58 [ForkJoinPool-1-worker-11] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 12:45:58 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-28 12:45:59 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-28 12:45:59 [ForkJoinPool-1-worker-11] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 12:45:59 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-28 12:45:59 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-28 12:46:00 [ForkJoinPool-1-worker-11] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 12:46:00 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-28 12:46:00 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-28 12:46:00 [ForkJoinPool-1-worker-11] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 12:46:00 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-28 12:46:00 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-28 12:46:01 [ForkJoinPool-1-worker-11] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 12:46:01 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-28 12:46:01 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-28 12:46:01 [ForkJoinPool-1-worker-11] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 12:46:01 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-28 12:46:01 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-28 12:46:01 [ForkJoinPool-1-worker-11] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 12:46:01 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-28 12:46:02 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-28 12:46:02 [ForkJoinPool-1-worker-11] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 12:46:02 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-28 12:46:02 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-28 12:46:02 [ForkJoinPool-1-worker-11] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 12:46:02 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-28 12:46:02 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-28 12:46:03 [ForkJoinPool-1-worker-11] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 12:46:03 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-28 12:46:03 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-28 12:46:03 [ForkJoinPool-1-worker-11] DEBUG [SetupClusterOperator:478] Installation resource type: ConfigMap
2022-03-28 12:46:03 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-28 12:46:03 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-28 12:46:03 [ForkJoinPool-1-worker-11] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 12:46:03 [ForkJoinPool-1-worker-11] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-03-28 12:46:03 [ForkJoinPool-1-worker-11] INFO  [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/cloud-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-28 12:46:03 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-28 12:46:03 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-28 12:46:03 [ForkJoinPool-1-worker-11] INFO  [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/cloud-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-28 12:46:03 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-28 12:46:03 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-28 12:46:04 [ForkJoinPool-1-worker-11] INFO  [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/cloud-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-28 12:46:04 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-28 12:46:04 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-28 12:46:04 [ForkJoinPool-1-worker-11] INFO  [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/cloud-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-28 12:46:04 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-28 12:46:04 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-28 12:46:04 [ForkJoinPool-1-worker-11] INFO  [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/cloud-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-28 12:46:04 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-28 12:46:04 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-28 12:46:04 [ForkJoinPool-1-worker-11] INFO  [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-28 12:46:04 [ForkJoinPool-1-worker-11] INFO  [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-28 12:46:04 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-28 12:46:05 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-28 12:46:05 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-28 12:46:05 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-28 12:46:05 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-28 12:46:05 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-28 12:46:05 [ForkJoinPool-1-worker-11] INFO  [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-28 12:46:05 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-28 12:46:06 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (479887ms till timeout)
2022-03-28 12:46:07 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (478773ms till timeout)
2022-03-28 12:46:08 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (477655ms till timeout)
2022-03-28 12:46:09 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (476539ms till timeout)
2022-03-28 12:46:10 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (475425ms till timeout)
2022-03-28 12:46:11 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (474308ms till timeout)
2022-03-28 12:46:12 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (473194ms till timeout)
2022-03-28 12:46:13 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (472081ms till timeout)
2022-03-28 12:46:15 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (470967ms till timeout)
2022-03-28 12:46:16 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (469844ms till timeout)
2022-03-28 12:46:17 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (468720ms till timeout)
2022-03-28 12:46:18 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (467604ms till timeout)
2022-03-28 12:46:19 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (466485ms till timeout)
2022-03-28 12:46:20 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (465367ms till timeout)
2022-03-28 12:46:21 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (464248ms till timeout)
2022-03-28 12:46:22 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (463132ms till timeout)
2022-03-28 12:46:23 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (462019ms till timeout)
2022-03-28 12:46:25 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (460908ms till timeout)
2022-03-28 12:46:26 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (459796ms till timeout)
2022-03-28 12:46:27 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (458684ms till timeout)
2022-03-28 12:46:28 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (457569ms till timeout)
2022-03-28 12:46:29 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (456454ms till timeout)
2022-03-28 12:46:30 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (455334ms till timeout)
2022-03-28 12:46:31 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (454221ms till timeout)
2022-03-28 12:46:32 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (453108ms till timeout)
2022-03-28 12:46:33 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (451990ms till timeout)
2022-03-28 12:46:35 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (450877ms till timeout)
2022-03-28 12:46:36 [ForkJoinPool-1-worker-11] INFO  [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-28 12:46:36 [ForkJoinPool-1-worker-11] INFO  [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-03-28 12:46:36 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-28 12:46:36 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-hfqtf not ready: strimzi-cluster-operator)
2022-03-28 12:46:36 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-hfqtf are ready
2022-03-28 12:46:36 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599865ms till timeout)
2022-03-28 12:46:37 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-hfqtf not ready: strimzi-cluster-operator)
2022-03-28 12:46:37 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-hfqtf are ready
2022-03-28 12:46:37 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598753ms till timeout)
2022-03-28 12:46:38 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-hfqtf not ready: strimzi-cluster-operator)
2022-03-28 12:46:38 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-hfqtf are ready
2022-03-28 12:46:38 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597637ms till timeout)
2022-03-28 12:46:39 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-hfqtf not ready: strimzi-cluster-operator)
2022-03-28 12:46:39 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-hfqtf are ready
2022-03-28 12:46:39 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596521ms till timeout)
2022-03-28 12:46:40 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-hfqtf not ready: strimzi-cluster-operator)
2022-03-28 12:46:40 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-hfqtf are ready
2022-03-28 12:46:40 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595406ms till timeout)
2022-03-28 12:46:42 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-hfqtf not ready: strimzi-cluster-operator)
2022-03-28 12:46:42 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-hfqtf are ready
2022-03-28 12:46:42 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594282ms till timeout)
2022-03-28 12:46:43 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-hfqtf not ready: strimzi-cluster-operator)
2022-03-28 12:46:43 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-hfqtf are ready
2022-03-28 12:46:43 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593166ms till timeout)
2022-03-28 12:46:44 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-hfqtf not ready: strimzi-cluster-operator)
2022-03-28 12:46:44 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-hfqtf are ready
2022-03-28 12:46:44 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592049ms till timeout)
2022-03-28 12:46:45 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-hfqtf not ready: strimzi-cluster-operator)
2022-03-28 12:46:45 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-hfqtf are ready
2022-03-28 12:46:45 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590934ms till timeout)
2022-03-28 12:46:46 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-hfqtf not ready: strimzi-cluster-operator)
2022-03-28 12:46:46 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-hfqtf are ready
2022-03-28 12:46:46 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (589819ms till timeout)
2022-03-28 12:46:47 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-hfqtf not ready: strimzi-cluster-operator)
2022-03-28 12:46:47 [ForkJoinPool-1-worker-11] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-hfqtf are ready
2022-03-28 12:46:47 [ForkJoinPool-1-worker-11] INFO  [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-03-28 12:46:47 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:666] ============================================================================
2022-03-28 12:46:47 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:666] ============================================================================
2022-03-28 12:46:47 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:667] [operators.user.UserST - Before All] - Setup test suite environment
2022-03-28 12:46:47 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:667] [operators.topic.TopicST - Before All] - Setup test suite environment
2022-03-28 12:46:47 [ForkJoinPool-1-worker-11] DEBUG [AbstractST:666] ============================================================================
2022-03-28 12:46:47 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:69] [operators.user.UserST] - Adding parallel suite: UserST
2022-03-28 12:46:47 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:666] ============================================================================
2022-03-28 12:46:47 [ForkJoinPool-1-worker-11] DEBUG [AbstractST:667] [operators.topic.ThrottlingQuotaST - Before All] - Setup test suite environment
2022-03-28 12:46:47 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:73] [operators.user.UserST] - Parallel suites count: 1
2022-03-28 12:46:47 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:667] [bridge.HttpBridgeTlsST - Before All] - Setup test suite environment
2022-03-28 12:46:47 [ForkJoinPool-1-worker-11] DEBUG [SuiteThreadController:69] [operators.topic.ThrottlingQuotaST] - Adding parallel suite: ThrottlingQuotaST
2022-03-28 12:46:47 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:69] [operators.topic.TopicST] - Adding parallel suite: TopicST
2022-03-28 12:46:47 [ForkJoinPool-1-worker-15] DEBUG [AbstractST:666] ============================================================================
2022-03-28 12:46:47 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:666] ============================================================================
2022-03-28 12:46:47 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:666] ============================================================================
2022-03-28 12:46:47 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:73] [operators.topic.TopicST] - Parallel suites count: 3
2022-03-28 12:46:47 [ForkJoinPool-1-worker-15] DEBUG [AbstractST:667] [operators.ReconciliationST - Before All] - Setup test suite environment
2022-03-28 12:46:47 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:667] [cruisecontrol.CruiseControlConfigurationST - Before All] - Setup test suite environment
2022-03-28 12:46:47 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:667] [bridge.HttpBridgeScramShaST - Before All] - Setup test suite environment
2022-03-28 12:46:47 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:184] TopicST suite now can proceed its execution
2022-03-28 12:46:47 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:69] [operators.ReconciliationST] - Adding parallel suite: ReconciliationST
2022-03-28 12:46:47 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:69] [cruisecontrol.CruiseControlConfigurationST] - Adding parallel suite: CruiseControlConfigurationST
2022-03-28 12:46:47 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:73] [operators.ReconciliationST] - Parallel suites count: 4
2022-03-28 12:46:47 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:69] [bridge.HttpBridgeScramShaST] - Adding parallel suite: HttpBridgeScramShaST
2022-03-28 12:46:47 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:73] [cruisecontrol.CruiseControlConfigurationST] - Parallel suites count: 5
2022-03-28 12:46:47 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:184] ReconciliationST suite now can proceed its execution
2022-03-28 12:46:47 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:73] [bridge.HttpBridgeScramShaST] - Parallel suites count: 6
2022-03-28 12:46:47 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:184] CruiseControlConfigurationST suite now can proceed its execution
2022-03-28 12:46:47 [ForkJoinPool-1-worker-15] DEBUG [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-28 12:46:47 [ForkJoinPool-1-worker-3] DEBUG [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-28 12:46:47 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:184] HttpBridgeScramShaST suite now can proceed its execution
2022-03-28 12:46:47 [ForkJoinPool-1-worker-15] DEBUG [TestSuiteNamespaceManager:129] Test suite `ReconciliationST` creates these additional namespaces:[reconciliation-st]
2022-03-28 12:46:47 [ForkJoinPool-1-worker-3] DEBUG [TestSuiteNamespaceManager:129] Test suite `CruiseControlConfigurationST` creates these additional namespaces:[cruise-control-configuration-st]
2022-03-28 12:46:47 [ForkJoinPool-1-worker-7] DEBUG [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-28 12:46:47 [ForkJoinPool-1-worker-7] DEBUG [TestSuiteNamespaceManager:129] Test suite `HttpBridgeScramShaST` creates these additional namespaces:[http-bridge-scram-sha-st]
2022-03-28 12:46:47 [ForkJoinPool-1-worker-5] DEBUG [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-28 12:46:47 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:184] UserST suite now can proceed its execution
2022-03-28 12:46:47 [ForkJoinPool-1-worker-11] DEBUG [SuiteThreadController:73] [operators.topic.ThrottlingQuotaST] - Parallel suites count: 2
2022-03-28 12:46:47 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:69] [bridge.HttpBridgeTlsST] - Adding parallel suite: HttpBridgeTlsST
2022-03-28 12:46:47 [ForkJoinPool-1-worker-11] DEBUG [SuiteThreadController:184] ThrottlingQuotaST suite now can proceed its execution
2022-03-28 12:46:47 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:73] [bridge.HttpBridgeTlsST] - Parallel suites count: 7
2022-03-28 12:46:47 [ForkJoinPool-1-worker-13] DEBUG [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-28 12:46:47 [ForkJoinPool-1-worker-13] DEBUG [TestSuiteNamespaceManager:129] Test suite `UserST` creates these additional namespaces:[user-st]
2022-03-28 12:46:47 [ForkJoinPool-1-worker-5] DEBUG [TestSuiteNamespaceManager:129] Test suite `TopicST` creates these additional namespaces:[topic-st]
2022-03-28 12:46:47 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:159] [HttpBridgeTlsST] moved to the WaitZone, because current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:46:47 [ForkJoinPool-1-worker-11] DEBUG [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-28 12:46:47 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:46:47 [ForkJoinPool-1-worker-11] DEBUG [TestSuiteNamespaceManager:129] Test suite `ThrottlingQuotaST` creates these additional namespaces:[throttling-quota-st]
2022-03-28 12:46:47 [ForkJoinPool-1-worker-13] INFO  [KubeClusterResource:156] Creating Namespace: user-st
2022-03-28 12:46:47 [ForkJoinPool-1-worker-15] INFO  [KubeClusterResource:156] Creating Namespace: reconciliation-st
2022-03-28 12:46:47 [ForkJoinPool-1-worker-3] INFO  [KubeClusterResource:156] Creating Namespace: cruise-control-configuration-st
2022-03-28 12:46:47 [ForkJoinPool-1-worker-7] INFO  [KubeClusterResource:156] Creating Namespace: http-bridge-scram-sha-st
2022-03-28 12:46:47 [ForkJoinPool-1-worker-11] INFO  [KubeClusterResource:156] Creating Namespace: throttling-quota-st
2022-03-28 12:46:47 [ForkJoinPool-1-worker-5] INFO  [KubeClusterResource:156] Creating Namespace: topic-st
2022-03-28 12:46:48 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Namespace user-st
2022-03-28 12:46:48 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace infra-namespace get Namespace user-st -o json
2022-03-28 12:46:48 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Namespace cruise-control-configuration-st
2022-03-28 12:46:48 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace infra-namespace get Namespace cruise-control-configuration-st -o json
2022-03-28 12:46:48 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Namespace throttling-quota-st
2022-03-28 12:46:48 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace infra-namespace get Namespace throttling-quota-st -o json
2022-03-28 12:46:48 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Namespace http-bridge-scram-sha-st
2022-03-28 12:46:48 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace infra-namespace get Namespace http-bridge-scram-sha-st -o json
2022-03-28 12:46:48 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Namespace reconciliation-st
2022-03-28 12:46:48 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace infra-namespace get Namespace reconciliation-st -o json
2022-03-28 12:46:48 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Namespace topic-st
2022-03-28 12:46:48 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace infra-namespace get Namespace topic-st -o json
2022-03-28 12:46:48 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace infra-namespace get Namespace user-st -o json
2022-03-28 12:46:48 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 12:46:48 [ForkJoinPool-1-worker-13] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c26,c5",
            "openshift.io/sa.scc.supplemental-groups": "1000660000/10000",
            "openshift.io/sa.scc.uid-range": "1000660000/10000"
        },
        "creationTimestamp": "2022-03-28T12:46:43Z",
        "labels": {
            "kubernetes.io/metadata.name": "user-st"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T12:46:43Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T12:46:43Z"
            }
        ],
        "name": "user-st",
        "resourceVersion": "43417",
        "uid": "02073346-92b0-4729-ad75-431e44f0661a"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 12:46:48 [ForkJoinPool-1-worker-13] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st]}
2022-03-28 12:46:48 [ForkJoinPool-1-worker-13] INFO  [KubeClusterResource:82] Client use Namespace: user-st
2022-03-28 12:46:48 [ForkJoinPool-1-worker-13] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=user-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 12:46:48 [ForkJoinPool-1-worker-13] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: user-st
2022-03-28 12:46:48 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace infra-namespace get Namespace cruise-control-configuration-st -o json
2022-03-28 12:46:48 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:46:48 [ForkJoinPool-1-worker-3] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c26,c10",
            "openshift.io/sa.scc.supplemental-groups": "1000670000/10000",
            "openshift.io/sa.scc.uid-range": "1000670000/10000"
        },
        "creationTimestamp": "2022-03-28T12:46:43Z",
        "labels": {
            "kubernetes.io/metadata.name": "cruise-control-configuration-st"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T12:46:43Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T12:46:43Z"
            }
        ],
        "name": "cruise-control-configuration-st",
        "resourceVersion": "43456",
        "uid": "474bc2bc-8fc0-42c8-88b3-d8771b960b94"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 12:46:48 [ForkJoinPool-1-worker-3] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st]}
2022-03-28 12:46:48 [ForkJoinPool-1-worker-3] INFO  [KubeClusterResource:82] Client use Namespace: cruise-control-configuration-st
2022-03-28 12:46:48 [ForkJoinPool-1-worker-3] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=cruise-control-configuration-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 12:46:48 [ForkJoinPool-1-worker-3] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: cruise-control-configuration-st
2022-03-28 12:46:48 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace infra-namespace get Namespace throttling-quota-st -o json
2022-03-28 12:46:48 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:46:48 [ForkJoinPool-1-worker-11] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c26,c15",
            "openshift.io/sa.scc.supplemental-groups": "1000680000/10000",
            "openshift.io/sa.scc.uid-range": "1000680000/10000"
        },
        "creationTimestamp": "2022-03-28T12:46:43Z",
        "labels": {
            "kubernetes.io/metadata.name": "throttling-quota-st"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T12:46:43Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T12:46:43Z"
            }
        ],
        "name": "throttling-quota-st",
        "resourceVersion": "43488",
        "uid": "6399d8df-f637-4a1b-b026-a7f71c81b8b7"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 12:46:48 [ForkJoinPool-1-worker-11] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st]}
2022-03-28 12:46:48 [ForkJoinPool-1-worker-11] INFO  [KubeClusterResource:82] Client use Namespace: throttling-quota-st
2022-03-28 12:46:48 [ForkJoinPool-1-worker-11] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=throttling-quota-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 12:46:48 [ForkJoinPool-1-worker-11] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: throttling-quota-st
2022-03-28 12:46:48 [ForkJoinPool-1-worker-11] INFO  [ThrottlingQuotaST:304] Deploying shared Kafka across all test cases in throttling-quota-st namespace
2022-03-28 12:46:48 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:23] ############################################################################
2022-03-28 12:46:48 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testCapacityFile-STARTED
2022-03-28 12:46:48 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:658] ============================================================================
2022-03-28 12:46:48 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:659] [cruisecontrol.CruiseControlConfigurationST - Before Each] - Setup test case environment
2022-03-28 12:46:48 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:77] [cruisecontrol.CruiseControlConfigurationST] - Adding parallel test: testCapacityFile
2022-03-28 12:46:48 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:81] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 1
2022-03-28 12:46:48 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:230] testCapacityFile test now can proceed its execution
2022-03-28 12:46:48 [ForkJoinPool-1-worker-3] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 12:46:48 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace infra-namespace get Namespace topic-st -o json
2022-03-28 12:46:48 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 12:46:48 [ForkJoinPool-1-worker-5] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c26,c20",
            "openshift.io/sa.scc.supplemental-groups": "1000690000/10000",
            "openshift.io/sa.scc.uid-range": "1000690000/10000"
        },
        "creationTimestamp": "2022-03-28T12:46:43Z",
        "labels": {
            "kubernetes.io/metadata.name": "topic-st"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T12:46:43Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T12:46:43Z"
            }
        ],
        "name": "topic-st",
        "resourceVersion": "43511",
        "uid": "3ef6675c-b5a1-4a1c-ba94-e997f1db44a1"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 12:46:48 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace infra-namespace get Namespace reconciliation-st -o json
2022-03-28 12:46:48 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 12:46:48 [ForkJoinPool-1-worker-15] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c27,c4",
            "openshift.io/sa.scc.supplemental-groups": "1000710000/10000",
            "openshift.io/sa.scc.uid-range": "1000710000/10000"
        },
        "creationTimestamp": "2022-03-28T12:46:43Z",
        "labels": {
            "kubernetes.io/metadata.name": "reconciliation-st"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T12:46:43Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T12:46:43Z"
            }
        ],
        "name": "reconciliation-st",
        "resourceVersion": "43538",
        "uid": "cb458e22-59ec-4759-a587-9aceb5afaa6b"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 12:46:48 [ForkJoinPool-1-worker-5] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st]}
2022-03-28 12:46:48 [ForkJoinPool-1-worker-5] INFO  [KubeClusterResource:82] Client use Namespace: topic-st
2022-03-28 12:46:48 [ForkJoinPool-1-worker-15] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st]}
2022-03-28 12:46:48 [ForkJoinPool-1-worker-15] INFO  [KubeClusterResource:82] Client use Namespace: reconciliation-st
2022-03-28 12:46:48 [ForkJoinPool-1-worker-5] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=topic-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 12:46:48 [ForkJoinPool-1-worker-15] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=reconciliation-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 12:46:48 [ForkJoinPool-1-worker-5] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: topic-st
2022-03-28 12:46:48 [ForkJoinPool-1-worker-15] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: reconciliation-st
2022-03-28 12:46:48 [ForkJoinPool-1-worker-5] INFO  [TopicST:491] Deploying shared Kafka across all test cases in topic-st namespace
2022-03-28 12:46:48 [ForkJoinPool-1-worker-15] INFO  [TestSeparator:23] ############################################################################
2022-03-28 12:46:48 [ForkJoinPool-1-worker-15] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.ReconciliationST.testPauseReconciliationInKafkaRebalanceAndTopic-STARTED
2022-03-28 12:46:48 [ForkJoinPool-1-worker-15] DEBUG [AbstractST:658] ============================================================================
2022-03-28 12:46:48 [ForkJoinPool-1-worker-15] DEBUG [AbstractST:659] [operators.ReconciliationST - Before Each] - Setup test case environment
2022-03-28 12:46:48 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:77] [operators.ReconciliationST] - Adding parallel test: testPauseReconciliationInKafkaRebalanceAndTopic
2022-03-28 12:46:48 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:81] [operators.ReconciliationST] - Parallel test count: 2
2022-03-28 12:46:48 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:230] testPauseReconciliationInKafkaRebalanceAndTopic test now can proceed its execution
2022-03-28 12:46:48 [ForkJoinPool-1-worker-3] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testCapacityFile=my-cluster-39ee3f62}
2022-03-28 12:46:48 [ForkJoinPool-1-worker-3] TRACE [AbstractST:607] USERS_NAME_MAP: {testCapacityFile=my-user-597365340-661017603}
2022-03-28 12:46:48 [ForkJoinPool-1-worker-3] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testCapacityFile=my-topic-1390944843-775067696}
2022-03-28 12:46:48 [ForkJoinPool-1-worker-3] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testCapacityFile=my-cluster-39ee3f62-kafka-clients}
2022-03-28 12:46:48 [ForkJoinPool-1-worker-3] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-0 for test case:testCapacityFile
2022-03-28 12:46:48 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace infra-namespace get Namespace http-bridge-scram-sha-st -o json
2022-03-28 12:46:48 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 12:46:48 [ForkJoinPool-1-worker-7] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c26,c25",
            "openshift.io/sa.scc.supplemental-groups": "1000700000/10000",
            "openshift.io/sa.scc.uid-range": "1000700000/10000"
        },
        "creationTimestamp": "2022-03-28T12:46:43Z",
        "labels": {
            "kubernetes.io/metadata.name": "http-bridge-scram-sha-st"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T12:46:43Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T12:46:43Z"
            }
        ],
        "name": "http-bridge-scram-sha-st",
        "resourceVersion": "43522",
        "uid": "2ff313a3-6f3a-4083-8963-475c5cfb64e4"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 12:46:48 [ForkJoinPool-1-worker-7] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st]}
2022-03-28 12:46:48 [ForkJoinPool-1-worker-7] INFO  [KubeClusterResource:82] Client use Namespace: http-bridge-scram-sha-st
2022-03-28 12:46:48 [ForkJoinPool-1-worker-7] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=http-bridge-scram-sha-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 12:46:48 [ForkJoinPool-1-worker-7] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: http-bridge-scram-sha-st
2022-03-28 12:46:48 [ForkJoinPool-1-worker-7] INFO  [HttpBridgeScramShaST:123] Deploy Kafka and KafkaBridge before tests
2022-03-28 12:46:48 [ForkJoinPool-1-worker-3] INFO  [KubeClusterResource:156] Creating Namespace: namespace-0
2022-03-28 12:46:48 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Namespace namespace-0
2022-03-28 12:46:48 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace http-bridge-scram-sha-st get Namespace namespace-0 -o json
2022-03-28 12:46:48 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update Kafka user-cluster-name in namespace user-st
2022-03-28 12:46:48 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update Kafka topic-cluster-name in namespace topic-st
2022-03-28 12:46:48 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update Kafka http-bridge-scram-sha-cluster-name in namespace http-bridge-scram-sha-st
2022-03-28 12:46:48 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update Kafka quota-cluster in namespace throttling-quota-st
2022-03-28 12:46:49 [ForkJoinPool-1-worker-13] WARN  [VersionUsageUtils:60] The client is using resource type 'kafkas' with unstable version 'v1beta2'
2022-03-28 12:46:49 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:quota-cluster
2022-03-28 12:46:49 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:user-cluster-name
2022-03-28 12:46:49 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:topic-cluster-name
2022-03-28 12:46:49 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:http-bridge-scram-sha-cluster-name
2022-03-28 12:46:49 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:433] Wait for Kafka: quota-cluster will have desired state: Ready
2022-03-28 12:46:49 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Kafka: quota-cluster will have desired state: Ready
2022-03-28 12:46:49 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace http-bridge-scram-sha-st get Namespace namespace-0 -o json
2022-03-28 12:46:49 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:46:49 [ForkJoinPool-1-worker-3] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c27,c9",
            "openshift.io/sa.scc.supplemental-groups": "1000720000/10000",
            "openshift.io/sa.scc.uid-range": "1000720000/10000"
        },
        "creationTimestamp": "2022-03-28T12:46:44Z",
        "labels": {
            "kubernetes.io/metadata.name": "namespace-0"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T12:46:44Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T12:46:44Z"
            }
        ],
        "name": "namespace-0",
        "resourceVersion": "43629",
        "uid": "42355d5e-0d43-4bf0-82bd-01d9bd39cfa5"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 12:46:49 [ForkJoinPool-1-worker-3] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@3e255cd9=[namespace-0]}
2022-03-28 12:46:49 [ForkJoinPool-1-worker-3] INFO  [KubeClusterResource:82] Client use Namespace: namespace-0
2022-03-28 12:46:49 [ForkJoinPool-1-worker-3] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-0, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 12:46:49 [ForkJoinPool-1-worker-3] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-0
2022-03-28 12:46:49 [ForkJoinPool-1-worker-15] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 12:46:49 [ForkJoinPool-1-worker-15] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d, testCapacityFile=my-cluster-39ee3f62}
2022-03-28 12:46:49 [ForkJoinPool-1-worker-15] TRACE [AbstractST:607] USERS_NAME_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-user-467839358-1504804463, testCapacityFile=my-user-597365340-661017603}
2022-03-28 12:46:49 [ForkJoinPool-1-worker-15] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-712754859-1910580676, testCapacityFile=my-topic-1390944843-775067696}
2022-03-28 12:46:49 [ForkJoinPool-1-worker-15] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d-kafka-clients, testCapacityFile=my-cluster-39ee3f62-kafka-clients}
2022-03-28 12:46:49 [ForkJoinPool-1-worker-15] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-1 for test case:testPauseReconciliationInKafkaRebalanceAndTopic
2022-03-28 12:46:49 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-39ee3f62 in namespace namespace-0
2022-03-28 12:46:49 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:164] Using Namespace: namespace-0
2022-03-28 12:46:49 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:433] Wait for Kafka: topic-cluster-name will have desired state: Ready
2022-03-28 12:46:49 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Kafka: topic-cluster-name will have desired state: Ready
2022-03-28 12:46:49 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:433] Wait for Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready
2022-03-28 12:46:49 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:433] Wait for Kafka: user-cluster-name will have desired state: Ready
2022-03-28 12:46:49 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready
2022-03-28 12:46:49 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Kafka: user-cluster-name will have desired state: Ready
2022-03-28 12:46:49 [ForkJoinPool-1-worker-15] INFO  [KubeClusterResource:156] Creating Namespace: namespace-1
2022-03-28 12:46:49 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (839784ms till timeout)
2022-03-28 12:46:49 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-39ee3f62
2022-03-28 12:46:49 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (839791ms till timeout)
2022-03-28 12:46:49 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (839788ms till timeout)
2022-03-28 12:46:49 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (839789ms till timeout)
2022-03-28 12:46:49 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-39ee3f62 will have desired state: Ready
2022-03-28 12:46:49 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-39ee3f62 will have desired state: Ready
2022-03-28 12:46:49 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Namespace namespace-1
2022-03-28 12:46:49 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-0 get Namespace namespace-1 -o json
2022-03-28 12:46:50 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1319886ms till timeout)
2022-03-28 12:46:50 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-0 get Namespace namespace-1 -o json
2022-03-28 12:46:50 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 12:46:50 [ForkJoinPool-1-worker-15] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c27,c14",
            "openshift.io/sa.scc.supplemental-groups": "1000730000/10000",
            "openshift.io/sa.scc.uid-range": "1000730000/10000"
        },
        "creationTimestamp": "2022-03-28T12:46:45Z",
        "labels": {
            "kubernetes.io/metadata.name": "namespace-1"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T12:46:45Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T12:46:45Z"
            }
        ],
        "name": "namespace-1",
        "resourceVersion": "43675",
        "uid": "da51c9ca-dc1c-4c84-8b24-3d6069123092"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 12:46:50 [ForkJoinPool-1-worker-15] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-1], io.strimzi.test.logs.CollectorElement@3e255cd9=[namespace-0]}
2022-03-28 12:46:50 [ForkJoinPool-1-worker-15] INFO  [KubeClusterResource:82] Client use Namespace: namespace-1
2022-03-28 12:46:50 [ForkJoinPool-1-worker-15] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-1, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 12:46:50 [ForkJoinPool-1-worker-15] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-1
2022-03-28 12:46:50 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-30bb976d in namespace namespace-1
2022-03-28 12:46:50 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:164] Using Namespace: namespace-1
2022-03-28 12:46:50 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-30bb976d
2022-03-28 12:46:50 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-30bb976d will have desired state: Ready
2022-03-28 12:46:50 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-30bb976d will have desired state: Ready
2022-03-28 12:46:50 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (838603ms till timeout)
2022-03-28 12:46:50 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (838606ms till timeout)
2022-03-28 12:46:50 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (838605ms till timeout)
2022-03-28 12:46:50 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1319781ms till timeout)
2022-03-28 12:46:51 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (838499ms till timeout)
2022-03-28 12:46:51 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1318774ms till timeout)
2022-03-28 12:46:51 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (837486ms till timeout)
2022-03-28 12:46:52 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (837491ms till timeout)
2022-03-28 12:46:52 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (837491ms till timeout)
2022-03-28 12:46:52 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1318666ms till timeout)
2022-03-28 12:46:52 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (837379ms till timeout)
2022-03-28 12:46:52 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1317588ms till timeout)
2022-03-28 12:46:52 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:46:53 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (836375ms till timeout)
2022-03-28 12:46:53 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1317555ms till timeout)
2022-03-28 12:46:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (836378ms till timeout)
2022-03-28 12:46:53 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (836378ms till timeout)
2022-03-28 12:46:53 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (836266ms till timeout)
2022-03-28 12:46:53 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1316475ms till timeout)
2022-03-28 12:46:54 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (835258ms till timeout)
2022-03-28 12:46:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (835264ms till timeout)
2022-03-28 12:46:54 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (835264ms till timeout)
2022-03-28 12:46:54 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1316440ms till timeout)
2022-03-28 12:46:54 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (835153ms till timeout)
2022-03-28 12:46:54 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1315362ms till timeout)
2022-03-28 12:46:55 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (834142ms till timeout)
2022-03-28 12:46:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (834147ms till timeout)
2022-03-28 12:46:55 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1315323ms till timeout)
2022-03-28 12:46:55 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (834146ms till timeout)
2022-03-28 12:46:55 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (834037ms till timeout)
2022-03-28 12:46:55 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1314246ms till timeout)
2022-03-28 12:46:56 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (833028ms till timeout)
2022-03-28 12:46:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (833031ms till timeout)
2022-03-28 12:46:56 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1314207ms till timeout)
2022-03-28 12:46:56 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (833030ms till timeout)
2022-03-28 12:46:56 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (832919ms till timeout)
2022-03-28 12:46:56 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1313125ms till timeout)
2022-03-28 12:46:57 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (831917ms till timeout)
2022-03-28 12:46:57 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (831919ms till timeout)
2022-03-28 12:46:57 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:46:57 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (831811ms till timeout)
2022-03-28 12:46:57 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1312986ms till timeout)
2022-03-28 12:46:57 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (831701ms till timeout)
2022-03-28 12:46:57 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1311909ms till timeout)
2022-03-28 12:46:58 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (830803ms till timeout)
2022-03-28 12:46:58 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (830807ms till timeout)
2022-03-28 12:46:58 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1311873ms till timeout)
2022-03-28 12:46:58 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (830696ms till timeout)
2022-03-28 12:46:58 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (830587ms till timeout)
2022-03-28 12:46:59 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1310797ms till timeout)
2022-03-28 12:46:59 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (829691ms till timeout)
2022-03-28 12:46:59 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (829695ms till timeout)
2022-03-28 12:46:59 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1310760ms till timeout)
2022-03-28 12:47:00 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (829476ms till timeout)
2022-03-28 12:47:00 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (829474ms till timeout)
2022-03-28 12:47:00 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1309685ms till timeout)
2022-03-28 12:47:00 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (828578ms till timeout)
2022-03-28 12:47:01 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (828580ms till timeout)
2022-03-28 12:47:01 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1309647ms till timeout)
2022-03-28 12:47:01 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (828362ms till timeout)
2022-03-28 12:47:01 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (828361ms till timeout)
2022-03-28 12:47:01 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1308570ms till timeout)
2022-03-28 12:47:02 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (827463ms till timeout)
2022-03-28 12:47:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (827466ms till timeout)
2022-03-28 12:47:02 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1308524ms till timeout)
2022-03-28 12:47:02 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (827239ms till timeout)
2022-03-28 12:47:02 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (827236ms till timeout)
2022-03-28 12:47:02 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1307444ms till timeout)
2022-03-28 12:47:02 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:47:03 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (826349ms till timeout)
2022-03-28 12:47:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (826291ms till timeout)
2022-03-28 12:47:03 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1307355ms till timeout)
2022-03-28 12:47:03 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (826066ms till timeout)
2022-03-28 12:47:03 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (826067ms till timeout)
2022-03-28 12:47:03 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1306274ms till timeout)
2022-03-28 12:47:04 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (825237ms till timeout)
2022-03-28 12:47:04 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (825178ms till timeout)
2022-03-28 12:47:04 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1306241ms till timeout)
2022-03-28 12:47:04 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (824954ms till timeout)
2022-03-28 12:47:04 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (824840ms till timeout)
2022-03-28 12:47:04 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1305157ms till timeout)
2022-03-28 12:47:05 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (824124ms till timeout)
2022-03-28 12:47:05 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (824066ms till timeout)
2022-03-28 12:47:05 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1305131ms till timeout)
2022-03-28 12:47:05 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (823843ms till timeout)
2022-03-28 12:47:05 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (823719ms till timeout)
2022-03-28 12:47:05 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1303927ms till timeout)
2022-03-28 12:47:06 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (823013ms till timeout)
2022-03-28 12:47:06 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (822955ms till timeout)
2022-03-28 12:47:06 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1304020ms till timeout)
2022-03-28 12:47:06 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (822730ms till timeout)
2022-03-28 12:47:06 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (822607ms till timeout)
2022-03-28 12:47:07 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1302815ms till timeout)
2022-03-28 12:47:07 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (821900ms till timeout)
2022-03-28 12:47:07 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:47:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (821843ms till timeout)
2022-03-28 12:47:07 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1302909ms till timeout)
2022-03-28 12:47:07 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (821619ms till timeout)
2022-03-28 12:47:08 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (821495ms till timeout)
2022-03-28 12:47:08 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1301703ms till timeout)
2022-03-28 12:47:08 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (820788ms till timeout)
2022-03-28 12:47:08 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (820732ms till timeout)
2022-03-28 12:47:08 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1301797ms till timeout)
2022-03-28 12:47:09 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (820507ms till timeout)
2022-03-28 12:47:09 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (820384ms till timeout)
2022-03-28 12:47:09 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1300593ms till timeout)
2022-03-28 12:47:09 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (819676ms till timeout)
2022-03-28 12:47:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (819620ms till timeout)
2022-03-28 12:47:10 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1300687ms till timeout)
2022-03-28 12:47:10 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (819397ms till timeout)
2022-03-28 12:47:10 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (819272ms till timeout)
2022-03-28 12:47:10 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1299480ms till timeout)
2022-03-28 12:47:10 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (818565ms till timeout)
2022-03-28 12:47:11 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (818510ms till timeout)
2022-03-28 12:47:11 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1299576ms till timeout)
2022-03-28 12:47:11 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (818286ms till timeout)
2022-03-28 12:47:11 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (818160ms till timeout)
2022-03-28 12:47:11 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1298370ms till timeout)
2022-03-28 12:47:12 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (817452ms till timeout)
2022-03-28 12:47:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (817398ms till timeout)
2022-03-28 12:47:12 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1298465ms till timeout)
2022-03-28 12:47:12 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (817173ms till timeout)
2022-03-28 12:47:12 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (817049ms till timeout)
2022-03-28 12:47:12 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1297258ms till timeout)
2022-03-28 12:47:12 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:47:13 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (816341ms till timeout)
2022-03-28 12:47:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (816288ms till timeout)
2022-03-28 12:47:13 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1297352ms till timeout)
2022-03-28 12:47:13 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (816062ms till timeout)
2022-03-28 12:47:13 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (815938ms till timeout)
2022-03-28 12:47:13 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1296147ms till timeout)
2022-03-28 12:47:14 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (815227ms till timeout)
2022-03-28 12:47:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (815176ms till timeout)
2022-03-28 12:47:14 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1296242ms till timeout)
2022-03-28 12:47:14 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (814951ms till timeout)
2022-03-28 12:47:14 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (814827ms till timeout)
2022-03-28 12:47:14 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1295035ms till timeout)
2022-03-28 12:47:15 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (814115ms till timeout)
2022-03-28 12:47:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (814066ms till timeout)
2022-03-28 12:47:15 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1295131ms till timeout)
2022-03-28 12:47:15 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (813838ms till timeout)
2022-03-28 12:47:15 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (813716ms till timeout)
2022-03-28 12:47:15 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1293925ms till timeout)
2022-03-28 12:47:16 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (813002ms till timeout)
2022-03-28 12:47:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (812954ms till timeout)
2022-03-28 12:47:16 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1294020ms till timeout)
2022-03-28 12:47:16 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (812726ms till timeout)
2022-03-28 12:47:16 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (812603ms till timeout)
2022-03-28 12:47:17 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1292812ms till timeout)
2022-03-28 12:47:17 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (811891ms till timeout)
2022-03-28 12:47:17 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:47:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (811843ms till timeout)
2022-03-28 12:47:17 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1292909ms till timeout)
2022-03-28 12:47:17 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (811614ms till timeout)
2022-03-28 12:47:18 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (811493ms till timeout)
2022-03-28 12:47:18 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1291701ms till timeout)
2022-03-28 12:47:18 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (810779ms till timeout)
2022-03-28 12:47:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (810731ms till timeout)
2022-03-28 12:47:18 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1291798ms till timeout)
2022-03-28 12:47:19 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (810503ms till timeout)
2022-03-28 12:47:19 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (810381ms till timeout)
2022-03-28 12:47:19 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1290589ms till timeout)
2022-03-28 12:47:19 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (809667ms till timeout)
2022-03-28 12:47:19 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (809620ms till timeout)
2022-03-28 12:47:20 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1290686ms till timeout)
2022-03-28 12:47:20 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (809391ms till timeout)
2022-03-28 12:47:20 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (809269ms till timeout)
2022-03-28 12:47:20 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1289478ms till timeout)
2022-03-28 12:47:20 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (808554ms till timeout)
2022-03-28 12:47:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (808506ms till timeout)
2022-03-28 12:47:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1289574ms till timeout)
2022-03-28 12:47:21 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (808280ms till timeout)
2022-03-28 12:47:21 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (808156ms till timeout)
2022-03-28 12:47:21 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1288365ms till timeout)
2022-03-28 12:47:22 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (807442ms till timeout)
2022-03-28 12:47:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (807396ms till timeout)
2022-03-28 12:47:22 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1288463ms till timeout)
2022-03-28 12:47:22 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (807168ms till timeout)
2022-03-28 12:47:22 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (807045ms till timeout)
2022-03-28 12:47:22 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1287255ms till timeout)
2022-03-28 12:47:22 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:47:23 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (806328ms till timeout)
2022-03-28 12:47:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (806283ms till timeout)
2022-03-28 12:47:23 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1287350ms till timeout)
2022-03-28 12:47:23 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (806057ms till timeout)
2022-03-28 12:47:23 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (805927ms till timeout)
2022-03-28 12:47:23 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1286135ms till timeout)
2022-03-28 12:47:24 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (805215ms till timeout)
2022-03-28 12:47:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (805172ms till timeout)
2022-03-28 12:47:24 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1286239ms till timeout)
2022-03-28 12:47:24 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (804945ms till timeout)
2022-03-28 12:47:24 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (804816ms till timeout)
2022-03-28 12:47:24 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1285025ms till timeout)
2022-03-28 12:47:25 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (804103ms till timeout)
2022-03-28 12:47:25 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (804060ms till timeout)
2022-03-28 12:47:25 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1285127ms till timeout)
2022-03-28 12:47:25 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (803835ms till timeout)
2022-03-28 12:47:25 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (803703ms till timeout)
2022-03-28 12:47:25 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1283911ms till timeout)
2022-03-28 12:47:26 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (802989ms till timeout)
2022-03-28 12:47:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (802949ms till timeout)
2022-03-28 12:47:26 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1284015ms till timeout)
2022-03-28 12:47:26 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (802722ms till timeout)
2022-03-28 12:47:26 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (802592ms till timeout)
2022-03-28 12:47:27 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1282800ms till timeout)
2022-03-28 12:47:27 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (801877ms till timeout)
2022-03-28 12:47:27 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:47:27 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (801837ms till timeout)
2022-03-28 12:47:27 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1282904ms till timeout)
2022-03-28 12:47:27 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (801612ms till timeout)
2022-03-28 12:47:28 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (801479ms till timeout)
2022-03-28 12:47:28 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1281687ms till timeout)
2022-03-28 12:47:28 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (800766ms till timeout)
2022-03-28 12:47:28 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (800727ms till timeout)
2022-03-28 12:47:28 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1281792ms till timeout)
2022-03-28 12:47:29 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (800500ms till timeout)
2022-03-28 12:47:29 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (800369ms till timeout)
2022-03-28 12:47:29 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1280576ms till timeout)
2022-03-28 12:47:29 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (799654ms till timeout)
2022-03-28 12:47:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (799616ms till timeout)
2022-03-28 12:47:30 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1280681ms till timeout)
2022-03-28 12:47:30 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (799389ms till timeout)
2022-03-28 12:47:30 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (799256ms till timeout)
2022-03-28 12:47:30 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1279465ms till timeout)
2022-03-28 12:47:30 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (798543ms till timeout)
2022-03-28 12:47:31 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (798505ms till timeout)
2022-03-28 12:47:31 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1279570ms till timeout)
2022-03-28 12:47:31 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (798278ms till timeout)
2022-03-28 12:47:31 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (798146ms till timeout)
2022-03-28 12:47:31 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1278353ms till timeout)
2022-03-28 12:47:32 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (797428ms till timeout)
2022-03-28 12:47:32 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (797394ms till timeout)
2022-03-28 12:47:32 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1278460ms till timeout)
2022-03-28 12:47:32 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (797165ms till timeout)
2022-03-28 12:47:32 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (797035ms till timeout)
2022-03-28 12:47:32 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1277243ms till timeout)
2022-03-28 12:47:32 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:47:33 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (796317ms till timeout)
2022-03-28 12:47:33 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (796283ms till timeout)
2022-03-28 12:47:33 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1277349ms till timeout)
2022-03-28 12:47:33 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (796055ms till timeout)
2022-03-28 12:47:33 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (795925ms till timeout)
2022-03-28 12:47:33 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1276132ms till timeout)
2022-03-28 12:47:34 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (795203ms till timeout)
2022-03-28 12:47:34 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (795172ms till timeout)
2022-03-28 12:47:34 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1276234ms till timeout)
2022-03-28 12:47:34 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (794944ms till timeout)
2022-03-28 12:47:34 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (794813ms till timeout)
2022-03-28 12:47:34 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1275022ms till timeout)
2022-03-28 12:47:35 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (794090ms till timeout)
2022-03-28 12:47:35 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (794061ms till timeout)
2022-03-28 12:47:35 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1275121ms till timeout)
2022-03-28 12:47:35 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (793834ms till timeout)
2022-03-28 12:47:35 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (793703ms till timeout)
2022-03-28 12:47:35 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1273911ms till timeout)
2022-03-28 12:47:36 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (792976ms till timeout)
2022-03-28 12:47:36 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (792950ms till timeout)
2022-03-28 12:47:36 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1274010ms till timeout)
2022-03-28 12:47:36 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (792718ms till timeout)
2022-03-28 12:47:36 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (792591ms till timeout)
2022-03-28 12:47:37 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1272800ms till timeout)
2022-03-28 12:47:37 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (791864ms till timeout)
2022-03-28 12:47:37 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:47:37 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (791838ms till timeout)
2022-03-28 12:47:37 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1272898ms till timeout)
2022-03-28 12:47:37 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (791608ms till timeout)
2022-03-28 12:47:38 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (791481ms till timeout)
2022-03-28 12:47:38 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1271689ms till timeout)
2022-03-28 12:47:38 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (790752ms till timeout)
2022-03-28 12:47:38 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (790727ms till timeout)
2022-03-28 12:47:38 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1271787ms till timeout)
2022-03-28 12:47:39 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (790264ms till timeout)
2022-03-28 12:47:39 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (790262ms till timeout)
2022-03-28 12:47:39 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1270571ms till timeout)
2022-03-28 12:47:39 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (789640ms till timeout)
2022-03-28 12:47:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (789614ms till timeout)
2022-03-28 12:47:40 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1270675ms till timeout)
2022-03-28 12:47:40 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (789152ms till timeout)
2022-03-28 12:47:40 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (789042ms till timeout)
2022-03-28 12:47:40 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1269358ms till timeout)
2022-03-28 12:47:40 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (788528ms till timeout)
2022-03-28 12:47:41 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (788504ms till timeout)
2022-03-28 12:47:41 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1269565ms till timeout)
2022-03-28 12:47:41 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (788040ms till timeout)
2022-03-28 12:47:41 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1268246ms till timeout)
2022-03-28 12:47:41 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (787928ms till timeout)
2022-03-28 12:47:42 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (787418ms till timeout)
2022-03-28 12:47:42 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (787392ms till timeout)
2022-03-28 12:47:42 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1268453ms till timeout)
2022-03-28 12:47:42 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (786929ms till timeout)
2022-03-28 12:47:42 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:47:42 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (786817ms till timeout)
2022-03-28 12:47:42 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1267025ms till timeout)
2022-03-28 12:47:43 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (786305ms till timeout)
2022-03-28 12:47:43 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (786282ms till timeout)
2022-03-28 12:47:43 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1267342ms till timeout)
2022-03-28 12:47:43 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (785818ms till timeout)
2022-03-28 12:47:43 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (785703ms till timeout)
2022-03-28 12:47:43 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1265912ms till timeout)
2022-03-28 12:47:44 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (785194ms till timeout)
2022-03-28 12:47:44 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (785170ms till timeout)
2022-03-28 12:47:44 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1266231ms till timeout)
2022-03-28 12:47:44 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (784707ms till timeout)
2022-03-28 12:47:44 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (784591ms till timeout)
2022-03-28 12:47:45 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1264796ms till timeout)
2022-03-28 12:47:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (783955ms till timeout)
2022-03-28 12:47:45 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (783843ms till timeout)
2022-03-28 12:47:45 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1265119ms till timeout)
2022-03-28 12:47:45 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (783594ms till timeout)
2022-03-28 12:47:46 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (783480ms till timeout)
2022-03-28 12:47:46 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1263686ms till timeout)
2022-03-28 12:47:46 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (782842ms till timeout)
2022-03-28 12:47:46 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1263910ms till timeout)
2022-03-28 12:47:46 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (782518ms till timeout)
2022-03-28 12:47:47 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (782483ms till timeout)
2022-03-28 12:47:47 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (782368ms till timeout)
2022-03-28 12:47:47 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1262573ms till timeout)
2022-03-28 12:47:47 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:47:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (781729ms till timeout)
2022-03-28 12:47:47 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1262785ms till timeout)
2022-03-28 12:47:48 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (781385ms till timeout)
2022-03-28 12:47:48 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (781370ms till timeout)
2022-03-28 12:47:48 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (781250ms till timeout)
2022-03-28 12:47:48 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1261460ms till timeout)
2022-03-28 12:47:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (780615ms till timeout)
2022-03-28 12:47:49 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1261673ms till timeout)
2022-03-28 12:47:49 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (780274ms till timeout)
2022-03-28 12:47:49 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (780260ms till timeout)
2022-03-28 12:47:49 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (780136ms till timeout)
2022-03-28 12:47:49 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1260342ms till timeout)
2022-03-28 12:47:50 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (779473ms till timeout)
2022-03-28 12:47:50 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1260534ms till timeout)
2022-03-28 12:47:50 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (779138ms till timeout)
2022-03-28 12:47:50 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (779141ms till timeout)
2022-03-28 12:47:50 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (779025ms till timeout)
2022-03-28 12:47:50 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1259230ms till timeout)
2022-03-28 12:47:51 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (778359ms till timeout)
2022-03-28 12:47:51 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1259421ms till timeout)
2022-03-28 12:47:51 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (778025ms till timeout)
2022-03-28 12:47:51 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (778028ms till timeout)
2022-03-28 12:47:51 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (777912ms till timeout)
2022-03-28 12:47:51 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1258116ms till timeout)
2022-03-28 12:47:52 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (777248ms till timeout)
2022-03-28 12:47:52 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1258310ms till timeout)
2022-03-28 12:47:52 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (776913ms till timeout)
2022-03-28 12:47:52 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (776915ms till timeout)
2022-03-28 12:47:52 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:47:52 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (776794ms till timeout)
2022-03-28 12:47:52 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1257002ms till timeout)
2022-03-28 12:47:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (776134ms till timeout)
2022-03-28 12:47:53 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1257198ms till timeout)
2022-03-28 12:47:53 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (775801ms till timeout)
2022-03-28 12:47:53 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (775804ms till timeout)
2022-03-28 12:47:53 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (775681ms till timeout)
2022-03-28 12:47:54 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1255889ms till timeout)
2022-03-28 12:47:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (775023ms till timeout)
2022-03-28 12:47:54 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1256086ms till timeout)
2022-03-28 12:47:54 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (774678ms till timeout)
2022-03-28 12:47:54 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (774680ms till timeout)
2022-03-28 12:47:55 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (774568ms till timeout)
2022-03-28 12:47:55 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1254772ms till timeout)
2022-03-28 12:47:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (773911ms till timeout)
2022-03-28 12:47:55 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1254974ms till timeout)
2022-03-28 12:47:55 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (773567ms till timeout)
2022-03-28 12:47:56 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (773569ms till timeout)
2022-03-28 12:47:56 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (773456ms till timeout)
2022-03-28 12:47:56 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1253549ms till timeout)
2022-03-28 12:47:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (772799ms till timeout)
2022-03-28 12:47:56 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1253862ms till timeout)
2022-03-28 12:47:57 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (772455ms till timeout)
2022-03-28 12:47:57 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (772454ms till timeout)
2022-03-28 12:47:57 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (772343ms till timeout)
2022-03-28 12:47:57 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1252439ms till timeout)
2022-03-28 12:47:57 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:47:57 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (771687ms till timeout)
2022-03-28 12:47:58 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1252707ms till timeout)
2022-03-28 12:47:58 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (771314ms till timeout)
2022-03-28 12:47:58 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (771322ms till timeout)
2022-03-28 12:47:58 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (771214ms till timeout)
2022-03-28 12:47:58 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1251325ms till timeout)
2022-03-28 12:47:59 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (770574ms till timeout)
2022-03-28 12:47:59 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1251597ms till timeout)
2022-03-28 12:47:59 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (770198ms till timeout)
2022-03-28 12:47:59 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (770201ms till timeout)
2022-03-28 12:47:59 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (770091ms till timeout)
2022-03-28 12:47:59 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1250212ms till timeout)
2022-03-28 12:48:00 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (769461ms till timeout)
2022-03-28 12:48:00 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1250486ms till timeout)
2022-03-28 12:48:00 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (769087ms till timeout)
2022-03-28 12:48:00 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (769090ms till timeout)
2022-03-28 12:48:00 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (768978ms till timeout)
2022-03-28 12:48:00 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1249099ms till timeout)
2022-03-28 12:48:01 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (768350ms till timeout)
2022-03-28 12:48:01 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1249374ms till timeout)
2022-03-28 12:48:01 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (767975ms till timeout)
2022-03-28 12:48:01 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (767979ms till timeout)
2022-03-28 12:48:01 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (767867ms till timeout)
2022-03-28 12:48:01 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1247986ms till timeout)
2022-03-28 12:48:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (767237ms till timeout)
2022-03-28 12:48:02 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1248257ms till timeout)
2022-03-28 12:48:02 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (766861ms till timeout)
2022-03-28 12:48:02 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:48:02 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (766864ms till timeout)
2022-03-28 12:48:02 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (766752ms till timeout)
2022-03-28 12:48:03 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1246875ms till timeout)
2022-03-28 12:48:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (766126ms till timeout)
2022-03-28 12:48:03 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1247145ms till timeout)
2022-03-28 12:48:03 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (765748ms till timeout)
2022-03-28 12:48:03 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (765753ms till timeout)
2022-03-28 12:48:03 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (765642ms till timeout)
2022-03-28 12:48:04 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1245762ms till timeout)
2022-03-28 12:48:04 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (765012ms till timeout)
2022-03-28 12:48:04 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1246033ms till timeout)
2022-03-28 12:48:04 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (764637ms till timeout)
2022-03-28 12:48:04 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (764640ms till timeout)
2022-03-28 12:48:05 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (764528ms till timeout)
2022-03-28 12:48:05 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1244651ms till timeout)
2022-03-28 12:48:05 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (763901ms till timeout)
2022-03-28 12:48:05 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1244922ms till timeout)
2022-03-28 12:48:05 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (763524ms till timeout)
2022-03-28 12:48:06 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (763528ms till timeout)
2022-03-28 12:48:06 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (763417ms till timeout)
2022-03-28 12:48:06 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1243539ms till timeout)
2022-03-28 12:48:06 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (762787ms till timeout)
2022-03-28 12:48:06 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1243811ms till timeout)
2022-03-28 12:48:07 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (762414ms till timeout)
2022-03-28 12:48:07 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (762416ms till timeout)
2022-03-28 12:48:07 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (762304ms till timeout)
2022-03-28 12:48:07 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1242427ms till timeout)
2022-03-28 12:48:07 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:48:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (761676ms till timeout)
2022-03-28 12:48:08 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1242699ms till timeout)
2022-03-28 12:48:08 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (761302ms till timeout)
2022-03-28 12:48:08 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (761227ms till timeout)
2022-03-28 12:48:08 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (761117ms till timeout)
2022-03-28 12:48:08 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1241315ms till timeout)
2022-03-28 12:48:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (760563ms till timeout)
2022-03-28 12:48:09 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1241588ms till timeout)
2022-03-28 12:48:09 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (760191ms till timeout)
2022-03-28 12:48:09 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (760115ms till timeout)
2022-03-28 12:48:09 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (760003ms till timeout)
2022-03-28 12:48:09 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1240204ms till timeout)
2022-03-28 12:48:10 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (759452ms till timeout)
2022-03-28 12:48:10 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1240476ms till timeout)
2022-03-28 12:48:10 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (759078ms till timeout)
2022-03-28 12:48:10 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (759005ms till timeout)
2022-03-28 12:48:10 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (758892ms till timeout)
2022-03-28 12:48:10 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1239092ms till timeout)
2022-03-28 12:48:11 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (758337ms till timeout)
2022-03-28 12:48:11 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1239361ms till timeout)
2022-03-28 12:48:11 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (757965ms till timeout)
2022-03-28 12:48:11 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (757892ms till timeout)
2022-03-28 12:48:12 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (757361ms till timeout)
2022-03-28 12:48:12 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1237678ms till timeout)
2022-03-28 12:48:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (757226ms till timeout)
2022-03-28 12:48:12 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1238248ms till timeout)
2022-03-28 12:48:12 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (756850ms till timeout)
2022-03-28 12:48:12 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:48:12 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (756779ms till timeout)
2022-03-28 12:48:13 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (756250ms till timeout)
2022-03-28 12:48:13 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1236459ms till timeout)
2022-03-28 12:48:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (756034ms till timeout)
2022-03-28 12:48:13 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1237101ms till timeout)
2022-03-28 12:48:13 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (755706ms till timeout)
2022-03-28 12:48:13 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (755667ms till timeout)
2022-03-28 12:48:14 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (755137ms till timeout)
2022-03-28 12:48:14 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1235347ms till timeout)
2022-03-28 12:48:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (754923ms till timeout)
2022-03-28 12:48:14 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1235989ms till timeout)
2022-03-28 12:48:14 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (754591ms till timeout)
2022-03-28 12:48:15 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (754557ms till timeout)
2022-03-28 12:48:15 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (754026ms till timeout)
2022-03-28 12:48:15 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1234235ms till timeout)
2022-03-28 12:48:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (753810ms till timeout)
2022-03-28 12:48:15 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1234875ms till timeout)
2022-03-28 12:48:15 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (753479ms till timeout)
2022-03-28 12:48:16 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (753445ms till timeout)
2022-03-28 12:48:16 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (752914ms till timeout)
2022-03-28 12:48:16 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1233123ms till timeout)
2022-03-28 12:48:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (752699ms till timeout)
2022-03-28 12:48:16 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1233762ms till timeout)
2022-03-28 12:48:17 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (752365ms till timeout)
2022-03-28 12:48:17 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (752334ms till timeout)
2022-03-28 12:48:17 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:48:17 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (751804ms till timeout)
2022-03-28 12:48:17 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1232013ms till timeout)
2022-03-28 12:48:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (751586ms till timeout)
2022-03-28 12:48:18 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1232651ms till timeout)
2022-03-28 12:48:18 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (751255ms till timeout)
2022-03-28 12:48:18 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (751223ms till timeout)
2022-03-28 12:48:18 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (750692ms till timeout)
2022-03-28 12:48:18 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1230902ms till timeout)
2022-03-28 12:48:19 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (750476ms till timeout)
2022-03-28 12:48:19 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1231540ms till timeout)
2022-03-28 12:48:19 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (750136ms till timeout)
2022-03-28 12:48:19 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (750113ms till timeout)
2022-03-28 12:48:19 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (749580ms till timeout)
2022-03-28 12:48:20 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1229789ms till timeout)
2022-03-28 12:48:20 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (749362ms till timeout)
2022-03-28 12:48:20 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1230430ms till timeout)
2022-03-28 12:48:20 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (749027ms till timeout)
2022-03-28 12:48:20 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (749002ms till timeout)
2022-03-28 12:48:21 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (748468ms till timeout)
2022-03-28 12:48:21 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1228677ms till timeout)
2022-03-28 12:48:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (748253ms till timeout)
2022-03-28 12:48:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1229318ms till timeout)
2022-03-28 12:48:21 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (747913ms till timeout)
2022-03-28 12:48:21 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (747891ms till timeout)
2022-03-28 12:48:22 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (747356ms till timeout)
2022-03-28 12:48:22 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1227565ms till timeout)
2022-03-28 12:48:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (747140ms till timeout)
2022-03-28 12:48:22 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1228207ms till timeout)
2022-03-28 12:48:22 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (746802ms till timeout)
2022-03-28 12:48:22 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:48:22 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (746780ms till timeout)
2022-03-28 12:48:23 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (746243ms till timeout)
2022-03-28 12:48:23 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1226453ms till timeout)
2022-03-28 12:48:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (746030ms till timeout)
2022-03-28 12:48:23 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1227097ms till timeout)
2022-03-28 12:48:23 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (745691ms till timeout)
2022-03-28 12:48:23 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (745669ms till timeout)
2022-03-28 12:48:24 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (745131ms till timeout)
2022-03-28 12:48:24 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1225340ms till timeout)
2022-03-28 12:48:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (744914ms till timeout)
2022-03-28 12:48:24 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1225981ms till timeout)
2022-03-28 12:48:24 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (744581ms till timeout)
2022-03-28 12:48:25 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (744558ms till timeout)
2022-03-28 12:48:25 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (744018ms till timeout)
2022-03-28 12:48:25 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1224228ms till timeout)
2022-03-28 12:48:25 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (743804ms till timeout)
2022-03-28 12:48:25 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1224869ms till timeout)
2022-03-28 12:48:25 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (743470ms till timeout)
2022-03-28 12:48:26 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (743447ms till timeout)
2022-03-28 12:48:26 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (742906ms till timeout)
2022-03-28 12:48:26 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1223115ms till timeout)
2022-03-28 12:48:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (742690ms till timeout)
2022-03-28 12:48:26 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1223758ms till timeout)
2022-03-28 12:48:27 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (742360ms till timeout)
2022-03-28 12:48:27 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (742336ms till timeout)
2022-03-28 12:48:27 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:48:27 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (741794ms till timeout)
2022-03-28 12:48:27 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1222001ms till timeout)
2022-03-28 12:48:28 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (741567ms till timeout)
2022-03-28 12:48:28 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1222634ms till timeout)
2022-03-28 12:48:28 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (741234ms till timeout)
2022-03-28 12:48:28 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (741226ms till timeout)
2022-03-28 12:48:28 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (740683ms till timeout)
2022-03-28 12:48:29 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1220889ms till timeout)
2022-03-28 12:48:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (740455ms till timeout)
2022-03-28 12:48:29 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1221523ms till timeout)
2022-03-28 12:48:29 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (740123ms till timeout)
2022-03-28 12:48:29 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (740114ms till timeout)
2022-03-28 12:48:30 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (739564ms till timeout)
2022-03-28 12:48:30 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1219774ms till timeout)
2022-03-28 12:48:30 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (739345ms till timeout)
2022-03-28 12:48:30 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1220412ms till timeout)
2022-03-28 12:48:30 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (739013ms till timeout)
2022-03-28 12:48:30 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (739004ms till timeout)
2022-03-28 12:48:31 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (738451ms till timeout)
2022-03-28 12:48:31 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1218659ms till timeout)
2022-03-28 12:48:31 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (738235ms till timeout)
2022-03-28 12:48:31 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1219302ms till timeout)
2022-03-28 12:48:31 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (737901ms till timeout)
2022-03-28 12:48:31 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (737893ms till timeout)
2022-03-28 12:48:32 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (737340ms till timeout)
2022-03-28 12:48:32 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1217547ms till timeout)
2022-03-28 12:48:32 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (737123ms till timeout)
2022-03-28 12:48:32 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1218188ms till timeout)
2022-03-28 12:48:32 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:48:32 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (736791ms till timeout)
2022-03-28 12:48:32 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (736782ms till timeout)
2022-03-28 12:48:33 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (736228ms till timeout)
2022-03-28 12:48:33 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1216435ms till timeout)
2022-03-28 12:48:33 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (736011ms till timeout)
2022-03-28 12:48:33 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1217078ms till timeout)
2022-03-28 12:48:33 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (735669ms till timeout)
2022-03-28 12:48:33 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (735669ms till timeout)
2022-03-28 12:48:34 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (735116ms till timeout)
2022-03-28 12:48:34 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1215325ms till timeout)
2022-03-28 12:48:34 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (734900ms till timeout)
2022-03-28 12:48:34 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1215967ms till timeout)
2022-03-28 12:48:34 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (734558ms till timeout)
2022-03-28 12:48:35 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (734560ms till timeout)
2022-03-28 12:48:35 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (734005ms till timeout)
2022-03-28 12:48:35 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1214213ms till timeout)
2022-03-28 12:48:35 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (733790ms till timeout)
2022-03-28 12:48:35 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1214857ms till timeout)
2022-03-28 12:48:36 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (733447ms till timeout)
2022-03-28 12:48:36 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (733448ms till timeout)
2022-03-28 12:48:36 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (732894ms till timeout)
2022-03-28 12:48:36 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1213102ms till timeout)
2022-03-28 12:48:36 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (732677ms till timeout)
2022-03-28 12:48:37 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1213743ms till timeout)
2022-03-28 12:48:37 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (732336ms till timeout)
2022-03-28 12:48:37 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (732338ms till timeout)
2022-03-28 12:48:37 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:48:37 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (731783ms till timeout)
2022-03-28 12:48:37 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1211990ms till timeout)
2022-03-28 12:48:38 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (731566ms till timeout)
2022-03-28 12:48:38 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1212632ms till timeout)
2022-03-28 12:48:38 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (731225ms till timeout)
2022-03-28 12:48:38 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (731227ms till timeout)
2022-03-28 12:48:38 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (730672ms till timeout)
2022-03-28 12:48:39 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1210881ms till timeout)
2022-03-28 12:48:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (730456ms till timeout)
2022-03-28 12:48:39 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1211521ms till timeout)
2022-03-28 12:48:39 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (730115ms till timeout)
2022-03-28 12:48:39 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (730117ms till timeout)
2022-03-28 12:48:40 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (729559ms till timeout)
2022-03-28 12:48:40 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1209767ms till timeout)
2022-03-28 12:48:40 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (729343ms till timeout)
2022-03-28 12:48:40 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1210412ms till timeout)
2022-03-28 12:48:40 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (729004ms till timeout)
2022-03-28 12:48:40 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (729005ms till timeout)
2022-03-28 12:48:41 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (728448ms till timeout)
2022-03-28 12:48:41 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1208657ms till timeout)
2022-03-28 12:48:41 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (728231ms till timeout)
2022-03-28 12:48:41 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1209297ms till timeout)
2022-03-28 12:48:41 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (727895ms till timeout)
2022-03-28 12:48:41 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (727895ms till timeout)
2022-03-28 12:48:42 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (727334ms till timeout)
2022-03-28 12:48:42 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1207541ms till timeout)
2022-03-28 12:48:42 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (727117ms till timeout)
2022-03-28 12:48:42 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1208185ms till timeout)
2022-03-28 12:48:42 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:48:42 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (726784ms till timeout)
2022-03-28 12:48:42 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (726783ms till timeout)
2022-03-28 12:48:43 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (726223ms till timeout)
2022-03-28 12:48:43 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1206432ms till timeout)
2022-03-28 12:48:43 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (726006ms till timeout)
2022-03-28 12:48:43 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1207073ms till timeout)
2022-03-28 12:48:43 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (725674ms till timeout)
2022-03-28 12:48:43 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (725672ms till timeout)
2022-03-28 12:48:44 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (725112ms till timeout)
2022-03-28 12:48:44 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1205320ms till timeout)
2022-03-28 12:48:44 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (724896ms till timeout)
2022-03-28 12:48:44 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1205963ms till timeout)
2022-03-28 12:48:44 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (724562ms till timeout)
2022-03-28 12:48:45 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (724559ms till timeout)
2022-03-28 12:48:45 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (724001ms till timeout)
2022-03-28 12:48:45 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1204210ms till timeout)
2022-03-28 12:48:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (723785ms till timeout)
2022-03-28 12:48:45 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1204852ms till timeout)
2022-03-28 12:48:46 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (723452ms till timeout)
2022-03-28 12:48:46 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (723449ms till timeout)
2022-03-28 12:48:46 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (722889ms till timeout)
2022-03-28 12:48:46 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1203097ms till timeout)
2022-03-28 12:48:46 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:444] Kafka: http-bridge-scram-sha-cluster-name is in desired state: Ready
2022-03-28 12:48:46 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update KafkaUser my-user-716724139-1524027490 in namespace http-bridge-scram-sha-st
2022-03-28 12:48:46 [ForkJoinPool-1-worker-7] WARN  [VersionUsageUtils:60] The client is using resource type 'kafkausers' with unstable version 'v1beta2'
2022-03-28 12:48:47 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1203740ms till timeout)
2022-03-28 12:48:47 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-716724139-1524027490
2022-03-28 12:48:47 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (722237ms till timeout)
2022-03-28 12:48:47 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (722240ms till timeout)
2022-03-28 12:48:47 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-716724139-1524027490 will have desired state: Ready
2022-03-28 12:48:47 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-716724139-1524027490 will have desired state: Ready
2022-03-28 12:48:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaUser: my-user-716724139-1524027490 will have desired state: Ready not ready, will try again in 1000 ms (179890ms till timeout)
2022-03-28 12:48:47 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:48:47 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (721779ms till timeout)
2022-03-28 12:48:47 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1201986ms till timeout)
2022-03-28 12:48:48 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1202628ms till timeout)
2022-03-28 12:48:48 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (721128ms till timeout)
2022-03-28 12:48:48 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (721130ms till timeout)
2022-03-28 12:48:48 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:444] KafkaUser: my-user-716724139-1524027490 is in desired state: Ready
2022-03-28 12:48:48 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (720668ms till timeout)
2022-03-28 12:48:49 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1200663ms till timeout)
2022-03-28 12:48:49 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1201419ms till timeout)
2022-03-28 12:48:49 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update Deployment http-bridge-scram-sha-st-shared-kafka-clients in namespace http-bridge-scram-sha-st
2022-03-28 12:48:49 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (720007ms till timeout)
2022-03-28 12:48:49 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:http-bridge-scram-sha-st-shared-kafka-clients
2022-03-28 12:48:49 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (719926ms till timeout)
2022-03-28 12:48:49 [ForkJoinPool-1-worker-7] INFO  [DeploymentUtils:161] Wait for Deployment: http-bridge-scram-sha-st-shared-kafka-clients will be ready
2022-03-28 12:48:49 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Wait for Deployment: http-bridge-scram-sha-st-shared-kafka-clients will be ready
2022-03-28 12:48:50 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (719468ms till timeout)
2022-03-28 12:48:50 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Wait for Deployment: http-bridge-scram-sha-st-shared-kafka-clients will be ready not ready, will try again in 1000 ms (479776ms till timeout)
2022-03-28 12:48:50 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1199553ms till timeout)
2022-03-28 12:48:50 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1200305ms till timeout)
2022-03-28 12:48:50 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (718896ms till timeout)
2022-03-28 12:48:50 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (718815ms till timeout)
2022-03-28 12:48:51 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (718357ms till timeout)
2022-03-28 12:48:51 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Wait for Deployment: http-bridge-scram-sha-st-shared-kafka-clients will be ready not ready, will try again in 1000 ms (478664ms till timeout)
2022-03-28 12:48:51 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1198444ms till timeout)
2022-03-28 12:48:51 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1199193ms till timeout)
2022-03-28 12:48:51 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (717785ms till timeout)
2022-03-28 12:48:51 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (717705ms till timeout)
2022-03-28 12:48:52 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (717244ms till timeout)
2022-03-28 12:48:52 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Wait for Deployment: http-bridge-scram-sha-st-shared-kafka-clients will be ready not ready, will try again in 1000 ms (477553ms till timeout)
2022-03-28 12:48:52 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1197333ms till timeout)
2022-03-28 12:48:52 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1198083ms till timeout)
2022-03-28 12:48:52 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:48:52 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (716675ms till timeout)
2022-03-28 12:48:52 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (716594ms till timeout)
2022-03-28 12:48:53 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (716132ms till timeout)
2022-03-28 12:48:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Wait for Deployment: http-bridge-scram-sha-st-shared-kafka-clients will be ready not ready, will try again in 1000 ms (476443ms till timeout)
2022-03-28 12:48:53 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1196221ms till timeout)
2022-03-28 12:48:53 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1196973ms till timeout)
2022-03-28 12:48:53 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Kafka: quota-cluster will have desired state: Ready not ready, will try again in 1000 ms (715562ms till timeout)
2022-03-28 12:48:54 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (715483ms till timeout)
2022-03-28 12:48:54 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (715021ms till timeout)
2022-03-28 12:48:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Wait for Deployment: http-bridge-scram-sha-st-shared-kafka-clients will be ready not ready, will try again in 1000 ms (475334ms till timeout)
2022-03-28 12:48:54 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1195112ms till timeout)
2022-03-28 12:48:54 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1195862ms till timeout)
2022-03-28 12:48:55 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:444] Kafka: quota-cluster is in desired state: Ready
2022-03-28 12:48:55 [ForkJoinPool-1-worker-11] INFO  [TestSeparator:23] ############################################################################
2022-03-28 12:48:55 [ForkJoinPool-1-worker-11] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testThrottlingQuotasCreateTopic-STARTED
2022-03-28 12:48:55 [ForkJoinPool-1-worker-11] DEBUG [AbstractST:658] ============================================================================
2022-03-28 12:48:55 [ForkJoinPool-1-worker-11] DEBUG [AbstractST:659] [operators.topic.ThrottlingQuotaST - Before Each] - Setup test case environment
2022-03-28 12:48:55 [ForkJoinPool-1-worker-11] DEBUG [SuiteThreadController:77] [operators.topic.ThrottlingQuotaST] - Adding parallel test: testThrottlingQuotasCreateTopic
2022-03-28 12:48:55 [ForkJoinPool-1-worker-11] DEBUG [SuiteThreadController:81] [operators.topic.ThrottlingQuotaST] - Parallel test count: 3
2022-03-28 12:48:55 [ForkJoinPool-1-worker-11] DEBUG [SuiteThreadController:230] testThrottlingQuotasCreateTopic test now can proceed its execution
2022-03-28 12:48:55 [ForkJoinPool-1-worker-11] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 12:48:55 [ForkJoinPool-1-worker-11] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2, testCapacityFile=my-cluster-39ee3f62}
2022-03-28 12:48:55 [ForkJoinPool-1-worker-11] TRACE [AbstractST:607] USERS_NAME_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-user-467839358-1504804463, testThrottlingQuotasCreateTopic=my-user-896662438-260898887, testCapacityFile=my-user-597365340-661017603}
2022-03-28 12:48:55 [ForkJoinPool-1-worker-11] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-712754859-1910580676, testThrottlingQuotasCreateTopic=my-topic-1532506723-1472611013, testCapacityFile=my-topic-1390944843-775067696}
2022-03-28 12:48:55 [ForkJoinPool-1-worker-11] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2-kafka-clients, testCapacityFile=my-cluster-39ee3f62-kafka-clients}
2022-03-28 12:48:55 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update KafkaUser my-user-896662438-260898887 in namespace throttling-quota-st
2022-03-28 12:48:55 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-896662438-260898887
2022-03-28 12:48:55 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (714331ms till timeout)
2022-03-28 12:48:55 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-896662438-260898887 will have desired state: Ready
2022-03-28 12:48:55 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-896662438-260898887 will have desired state: Ready
2022-03-28 12:48:55 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] KafkaUser: my-user-896662438-260898887 will have desired state: Ready not ready, will try again in 1000 ms (179891ms till timeout)
2022-03-28 12:48:55 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (713911ms till timeout)
2022-03-28 12:48:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Wait for Deployment: http-bridge-scram-sha-st-shared-kafka-clients will be ready not ready, will try again in 1000 ms (474220ms till timeout)
2022-03-28 12:48:55 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1194001ms till timeout)
2022-03-28 12:48:56 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1194751ms till timeout)
2022-03-28 12:48:56 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: user-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (713211ms till timeout)
2022-03-28 12:48:56 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:444] KafkaUser: my-user-896662438-260898887 is in desired state: Ready
2022-03-28 12:48:56 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update Job create-admin-my-cluster-0690c7b2-kafka-clients in namespace throttling-quota-st
2022-03-28 12:48:56 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (712769ms till timeout)
2022-03-28 12:48:56 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:create-admin-my-cluster-0690c7b2-kafka-clients
2022-03-28 12:48:57 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Wait for Deployment: http-bridge-scram-sha-st-shared-kafka-clients will be ready not ready, will try again in 1000 ms (472923ms till timeout)
2022-03-28 12:48:57 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1192820ms till timeout)
2022-03-28 12:48:57 [ForkJoinPool-1-worker-11] INFO  [JobUtils:81] Waiting for job: create-admin-my-cluster-0690c7b2-kafka-clients will be in active state
2022-03-28 12:48:57 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1193569ms till timeout)
2022-03-28 12:48:57 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 12:48:57 [ForkJoinPool-1-worker-11] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 12:48:57 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 12:48:57 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:444] Kafka: user-cluster-name is in desired state: Ready
2022-03-28 12:48:57 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:23] ############################################################################
2022-03-28 12:48:57 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testCreatingUsersWithSecretPrefix-STARTED
2022-03-28 12:48:57 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:658] ============================================================================
2022-03-28 12:48:57 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:659] [operators.user.UserST - Before Each] - Setup test case environment
2022-03-28 12:48:57 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:77] [operators.user.UserST] - Adding parallel test: testCreatingUsersWithSecretPrefix
2022-03-28 12:48:57 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:81] [operators.user.UserST] - Parallel test count: 4
2022-03-28 12:48:57 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:230] testCreatingUsersWithSecretPrefix test now can proceed its execution
2022-03-28 12:48:57 [ForkJoinPool-1-worker-13] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 12:48:57 [ForkJoinPool-1-worker-13] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2, testCapacityFile=my-cluster-39ee3f62, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5}
2022-03-28 12:48:57 [ForkJoinPool-1-worker-13] TRACE [AbstractST:607] USERS_NAME_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-user-467839358-1504804463, testThrottlingQuotasCreateTopic=my-user-896662438-260898887, testCapacityFile=my-user-597365340-661017603, testCreatingUsersWithSecretPrefix=my-user-1928899390-1444689439}
2022-03-28 12:48:57 [ForkJoinPool-1-worker-13] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-712754859-1910580676, testThrottlingQuotasCreateTopic=my-topic-1532506723-1472611013, testCapacityFile=my-topic-1390944843-775067696, testCreatingUsersWithSecretPrefix=my-topic-2033928996-1209892355}
2022-03-28 12:48:57 [ForkJoinPool-1-worker-13] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2-kafka-clients, testCapacityFile=my-cluster-39ee3f62-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5-kafka-clients}
2022-03-28 12:48:57 [ForkJoinPool-1-worker-13] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-2 for test case:testCreatingUsersWithSecretPrefix
2022-03-28 12:48:57 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (299774ms till timeout)
2022-03-28 12:48:57 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:48:57 [ForkJoinPool-1-worker-13] INFO  [KubeClusterResource:156] Creating Namespace: namespace-2
2022-03-28 12:48:57 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Namespace namespace-2
2022-03-28 12:48:57 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Namespace namespace-2 -o json
2022-03-28 12:48:57 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: topic-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (711612ms till timeout)
2022-03-28 12:48:58 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1191707ms till timeout)
2022-03-28 12:48:58 [ForkJoinPool-1-worker-7] INFO  [DeploymentUtils:168] Deployment: http-bridge-scram-sha-st-shared-kafka-clients is ready
2022-03-28 12:48:58 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Namespace namespace-2 -o json
2022-03-28 12:48:58 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 12:48:58 [ForkJoinPool-1-worker-13] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c27,c19",
            "openshift.io/sa.scc.supplemental-groups": "1000740000/10000",
            "openshift.io/sa.scc.uid-range": "1000740000/10000"
        },
        "creationTimestamp": "2022-03-28T12:48:53Z",
        "labels": {
            "kubernetes.io/metadata.name": "namespace-2"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T12:48:53Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T12:48:53Z"
            }
        ],
        "name": "namespace-2",
        "resourceVersion": "45779",
        "uid": "1f131f87-4278-4c50-b5eb-d4f234b1bc6f"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 12:48:58 [ForkJoinPool-1-worker-13] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-1], io.strimzi.test.logs.CollectorElement@c45aa4b1=[namespace-2], io.strimzi.test.logs.CollectorElement@3e255cd9=[namespace-0]}
2022-03-28 12:48:58 [ForkJoinPool-1-worker-13] INFO  [KubeClusterResource:82] Client use Namespace: namespace-2
2022-03-28 12:48:58 [ForkJoinPool-1-worker-13] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-2, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 12:48:58 [ForkJoinPool-1-worker-13] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-2
2022-03-28 12:48:58 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-c525f5e5 in namespace namespace-2
2022-03-28 12:48:58 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:164] Using Namespace: namespace-2
2022-03-28 12:48:58 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1192344ms till timeout)
2022-03-28 12:48:58 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (298574ms till timeout)
2022-03-28 12:48:58 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-c525f5e5
2022-03-28 12:48:58 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update KafkaBridge http-bridge-scram-sha-cluster-name in namespace http-bridge-scram-sha-st
2022-03-28 12:48:58 [ForkJoinPool-1-worker-7] WARN  [VersionUsageUtils:60] The client is using resource type 'kafkabridges' with unstable version 'v1beta2'
2022-03-28 12:48:58 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-c525f5e5 will have desired state: Ready
2022-03-28 12:48:58 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-c525f5e5 will have desired state: Ready
2022-03-28 12:48:59 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaBridge:http-bridge-scram-sha-cluster-name
2022-03-28 12:48:59 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:444] Kafka: topic-cluster-name is in desired state: Ready
2022-03-28 12:48:59 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (839779ms till timeout)
2022-03-28 12:48:59 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:433] Wait for KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready
2022-03-28 12:48:59 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready
2022-03-28 12:48:59 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1190507ms till timeout)
2022-03-28 12:48:59 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (479773ms till timeout)
2022-03-28 12:48:59 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1191149ms till timeout)
2022-03-28 12:48:59 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (297462ms till timeout)
2022-03-28 12:49:00 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (838665ms till timeout)
2022-03-28 12:49:00 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1189397ms till timeout)
2022-03-28 12:49:00 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (478662ms till timeout)
2022-03-28 12:49:00 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1190039ms till timeout)
2022-03-28 12:49:01 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (296349ms till timeout)
2022-03-28 12:49:01 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (837553ms till timeout)
2022-03-28 12:49:01 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1188285ms till timeout)
2022-03-28 12:49:01 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (477552ms till timeout)
2022-03-28 12:49:01 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1188929ms till timeout)
2022-03-28 12:49:02 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (295232ms till timeout)
2022-03-28 12:49:02 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (836439ms till timeout)
2022-03-28 12:49:02 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:49:02 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1187174ms till timeout)
2022-03-28 12:49:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (476438ms till timeout)
2022-03-28 12:49:02 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1187814ms till timeout)
2022-03-28 12:49:03 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (294120ms till timeout)
2022-03-28 12:49:03 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (835325ms till timeout)
2022-03-28 12:49:03 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1186064ms till timeout)
2022-03-28 12:49:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (475326ms till timeout)
2022-03-28 12:49:04 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1186700ms till timeout)
2022-03-28 12:49:04 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (293007ms till timeout)
2022-03-28 12:49:04 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (834213ms till timeout)
2022-03-28 12:49:04 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1184954ms till timeout)
2022-03-28 12:49:05 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (474212ms till timeout)
2022-03-28 12:49:05 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1185589ms till timeout)
2022-03-28 12:49:05 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (291894ms till timeout)
2022-03-28 12:49:05 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (833100ms till timeout)
2022-03-28 12:49:06 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1183841ms till timeout)
2022-03-28 12:49:06 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (473102ms till timeout)
2022-03-28 12:49:06 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1184478ms till timeout)
2022-03-28 12:49:06 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (290781ms till timeout)
2022-03-28 12:49:06 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (831989ms till timeout)
2022-03-28 12:49:07 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1182726ms till timeout)
2022-03-28 12:49:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (471992ms till timeout)
2022-03-28 12:49:07 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1183366ms till timeout)
2022-03-28 12:49:07 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:49:07 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (289667ms till timeout)
2022-03-28 12:49:08 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (830877ms till timeout)
2022-03-28 12:49:08 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1181616ms till timeout)
2022-03-28 12:49:08 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (470881ms till timeout)
2022-03-28 12:49:08 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1182246ms till timeout)
2022-03-28 12:49:08 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (288551ms till timeout)
2022-03-28 12:49:09 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (829764ms till timeout)
2022-03-28 12:49:09 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1180506ms till timeout)
2022-03-28 12:49:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (469770ms till timeout)
2022-03-28 12:49:09 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1181136ms till timeout)
2022-03-28 12:49:09 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (287437ms till timeout)
2022-03-28 12:49:10 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (828653ms till timeout)
2022-03-28 12:49:10 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1179395ms till timeout)
2022-03-28 12:49:10 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (468661ms till timeout)
2022-03-28 12:49:10 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1180026ms till timeout)
2022-03-28 12:49:11 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (286324ms till timeout)
2022-03-28 12:49:11 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (827542ms till timeout)
2022-03-28 12:49:11 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1178284ms till timeout)
2022-03-28 12:49:11 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (467547ms till timeout)
2022-03-28 12:49:11 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1178916ms till timeout)
2022-03-28 12:49:12 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (285211ms till timeout)
2022-03-28 12:49:12 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (826429ms till timeout)
2022-03-28 12:49:12 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:49:12 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1177173ms till timeout)
2022-03-28 12:49:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (466433ms till timeout)
2022-03-28 12:49:12 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1177805ms till timeout)
2022-03-28 12:49:13 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (284099ms till timeout)
2022-03-28 12:49:13 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (825315ms till timeout)
2022-03-28 12:49:13 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1176063ms till timeout)
2022-03-28 12:49:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (465322ms till timeout)
2022-03-28 12:49:14 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1176696ms till timeout)
2022-03-28 12:49:14 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (282966ms till timeout)
2022-03-28 12:49:14 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (824203ms till timeout)
2022-03-28 12:49:14 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1174952ms till timeout)
2022-03-28 12:49:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (464212ms till timeout)
2022-03-28 12:49:15 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1175586ms till timeout)
2022-03-28 12:49:15 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (281851ms till timeout)
2022-03-28 12:49:15 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (823092ms till timeout)
2022-03-28 12:49:16 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-39ee3f62 will have desired state: Ready not ready, will try again in 1000 ms (1173838ms till timeout)
2022-03-28 12:49:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (463103ms till timeout)
2022-03-28 12:49:16 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1174475ms till timeout)
2022-03-28 12:49:16 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (280739ms till timeout)
2022-03-28 12:49:16 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (821978ms till timeout)
2022-03-28 12:49:17 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:444] Kafka: my-cluster-39ee3f62 is in desired state: Ready
2022-03-28 12:49:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (461992ms till timeout)
2022-03-28 12:49:17 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:49:17 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (279457ms till timeout)
2022-03-28 12:49:17 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-30bb976d will have desired state: Ready not ready, will try again in 1000 ms (1172802ms till timeout)
2022-03-28 12:49:17 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-0 exec my-cluster-39ee3f62-cruise-control-9b5767dd8-227qv -- /bin/bash -c cat /tmp/capacity.json
2022-03-28 12:49:18 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (820867ms till timeout)
2022-03-28 12:49:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (460880ms till timeout)
2022-03-28 12:49:19 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (278343ms till timeout)
2022-03-28 12:49:19 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:444] Kafka: my-cluster-30bb976d is in desired state: Ready
2022-03-28 12:49:19 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:155] Create/Update KafkaTopic my-topic-712754859-1910580676 in namespace namespace-2
2022-03-28 12:49:19 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:164] Using Namespace: namespace-1
2022-03-28 12:49:19 [ForkJoinPool-1-worker-15] WARN  [VersionUsageUtils:60] The client is using resource type 'kafkatopics' with unstable version 'v1beta2'
2022-03-28 12:49:19 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-0 exec my-cluster-39ee3f62-cruise-control-9b5767dd8-227qv -- /bin/bash -c cat /tmp/capacity.json
2022-03-28 12:49:19 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 12:49:19 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:23] ############################################################################
2022-03-28 12:49:19 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testTlsExternalUserWithQuotas-STARTED
2022-03-28 12:49:19 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:658] ============================================================================
2022-03-28 12:49:19 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:659] [operators.user.UserST - Before Each] - Setup test case environment
2022-03-28 12:49:19 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:77] [operators.user.UserST] - Adding parallel test: testTlsExternalUserWithQuotas
2022-03-28 12:49:19 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:81] [operators.user.UserST] - Parallel test count: 5
2022-03-28 12:49:19 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:230] testTlsExternalUserWithQuotas test now can proceed its execution
2022-03-28 12:49:19 [ForkJoinPool-1-worker-1] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 12:49:19 [ForkJoinPool-1-worker-1] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d, testTlsExternalUserWithQuotas=my-cluster-3e2e11de, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2, testCapacityFile=my-cluster-39ee3f62, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5}
2022-03-28 12:49:19 [ForkJoinPool-1-worker-1] TRACE [AbstractST:607] USERS_NAME_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-user-467839358-1504804463, testTlsExternalUserWithQuotas=my-user-1786174319-1274634874, testThrottlingQuotasCreateTopic=my-user-896662438-260898887, testCapacityFile=my-user-597365340-661017603, testCreatingUsersWithSecretPrefix=my-user-1928899390-1444689439}
2022-03-28 12:49:19 [ForkJoinPool-1-worker-1] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-712754859-1910580676, testTlsExternalUserWithQuotas=my-topic-593679216-1309035072, testThrottlingQuotasCreateTopic=my-topic-1532506723-1472611013, testCapacityFile=my-topic-1390944843-775067696, testCreatingUsersWithSecretPrefix=my-topic-2033928996-1209892355}
2022-03-28 12:49:19 [ForkJoinPool-1-worker-1] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-3e2e11de-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2-kafka-clients, testCapacityFile=my-cluster-39ee3f62-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5-kafka-clients}
2022-03-28 12:49:19 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:155] Create/Update KafkaUser my-user-1786174319-1274634874 in namespace user-st
2022-03-28 12:49:19 [ForkJoinPool-1-worker-3] INFO  [CruiseControlConfigurationST:80] We got only one configuration of broker-capacities
2022-03-28 12:49:19 [ForkJoinPool-1-worker-3] INFO  [CruiseControlConfigurationST:83] Verifying cruise control configuration.
2022-03-28 12:49:19 [ForkJoinPool-1-worker-3] INFO  [CruiseControlConfigurationST:92] Verifying default cruise control capacities
2022-03-28 12:49:19 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:674] ============================================================================
2022-03-28 12:49:19 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:675] [cruisecontrol.CruiseControlConfigurationST - After Each] - Clean up after test
2022-03-28 12:49:19 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:344] ############################################################################
2022-03-28 12:49:19 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:348] Delete all resources for testCapacityFile
2022-03-28 12:49:19 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:241] Delete of Kafka my-cluster-39ee3f62 in namespace namespace-0
2022-03-28 12:49:19 [ForkJoinPool-1-worker-3] INFO  [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-0, for cruise control Kafka cluster my-cluster-39ee3f62
2022-03-28 12:49:19 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (819665ms till timeout)
2022-03-28 12:49:19 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-712754859-1910580676
2022-03-28 12:49:19 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-1786174319-1274634874
2022-03-28 12:49:19 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (459771ms till timeout)
2022-03-28 12:49:19 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-712754859-1910580676 will have desired state: Ready
2022-03-28 12:49:19 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-1786174319-1274634874 will have desired state: Ready
2022-03-28 12:49:19 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-1786174319-1274634874 will have desired state: Ready
2022-03-28 12:49:19 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-712754859-1910580676 will have desired state: Ready
2022-03-28 12:49:19 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:444] KafkaUser: my-user-1786174319-1274634874 is in desired state: Ready
2022-03-28 12:49:19 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:444] KafkaTopic: my-topic-712754859-1910580676 is in desired state: Ready
2022-03-28 12:49:19 [ForkJoinPool-1-worker-15] INFO  [ReconciliationST:147] Adding pause annotation into KafkaTopic resource and changing replication factor
2022-03-28 12:49:19 [ForkJoinPool-1-worker-1] DEBUG [UserST:274] Command for kafka-configs.sh bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=my-user-1786174319-1274634874
2022-03-28 12:49:19 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=my-user-1786174319-1274634874
2022-03-28 12:49:20 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-712754859-1910580676 will have desired state: ReconciliationPaused
2022-03-28 12:49:20 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-712754859-1910580676 will have desired state: ReconciliationPaused
2022-03-28 12:49:20 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (276833ms till timeout)
2022-03-28 12:49:20 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (818370ms till timeout)
2022-03-28 12:49:20 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-39ee3f62
2022-03-28 12:49:20 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic: my-topic-712754859-1910580676 will have desired state: ReconciliationPaused not ready, will try again in 1000 ms (179790ms till timeout)
2022-03-28 12:49:20 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (458555ms till timeout)
2022-03-28 12:49:20 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:369] ############################################################################
2022-03-28 12:49:20 [ForkJoinPool-1-worker-3] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-0 for test case:testCapacityFile
2022-03-28 12:49:20 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Namespace namespace-0 removal
2022-03-28 12:49:20 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:21 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:21 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:49:21 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (479553ms till timeout)
2022-03-28 12:49:21 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (275509ms till timeout)
2022-03-28 12:49:22 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:444] KafkaTopic: my-topic-712754859-1910580676 is in desired state: ReconciliationPaused
2022-03-28 12:49:22 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (816942ms till timeout)
2022-03-28 12:49:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (457264ms till timeout)
2022-03-28 12:49:22 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-1 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:49:22 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:22 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:49:22 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:22 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:49:22 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (478114ms till timeout)
2022-03-28 12:49:23 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (274187ms till timeout)
2022-03-28 12:49:23 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (815724ms till timeout)
2022-03-28 12:49:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (456046ms till timeout)
2022-03-28 12:49:23 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:24 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:24 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:49:24 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (476668ms till timeout)
2022-03-28 12:49:24 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (272865ms till timeout)
2022-03-28 12:49:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (454724ms till timeout)
2022-03-28 12:49:24 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (814402ms till timeout)
2022-03-28 12:49:25 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:25 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:25 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:49:25 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (475239ms till timeout)
2022-03-28 12:49:25 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (271544ms till timeout)
2022-03-28 12:49:25 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (813082ms till timeout)
2022-03-28 12:49:25 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (453404ms till timeout)
2022-03-28 12:49:25 [ForkJoinPool-1-worker-1] INFO  [Exec:417] Command: oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=my-user-1786174319-1274634874
2022-03-28 12:49:25 [ForkJoinPool-1-worker-1] INFO  [Exec:417] Return code: 0
2022-03-28 12:49:26 [ForkJoinPool-1-worker-1] INFO  [KafkaUserUtils:62] Waiting for KafkaUser deletion my-user-1786174319-1274634874
2022-03-28 12:49:26 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for KafkaUser deletion my-user-1786174319-1274634874
2022-03-28 12:49:26 [ForkJoinPool-1-worker-1] INFO  [KafkaUserUtils:75] KafkaUser my-user-1786174319-1274634874 deleted
2022-03-28 12:49:26 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for all KafkaUser CN=my-user-1786174319-1274634874 attributes will be cleaned
2022-03-28 12:49:26 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=my-user-1786174319-1274634874
2022-03-28 12:49:26 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:27 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (270222ms till timeout)
2022-03-28 12:49:27 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (452081ms till timeout)
2022-03-28 12:49:27 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (811759ms till timeout)
2022-03-28 12:49:27 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:27 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:49:27 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (473780ms till timeout)
2022-03-28 12:49:27 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:49:28 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:28 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-1 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:49:28 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 12:49:28 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for KafkaTopic's spec will be stable
2022-03-28 12:49:28 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-2 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:49:28 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (268899ms till timeout)
2022-03-28 12:49:28 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (810437ms till timeout)
2022-03-28 12:49:28 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (450758ms till timeout)
2022-03-28 12:49:28 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:28 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:49:28 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (472324ms till timeout)
2022-03-28 12:49:28 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-2 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:49:28 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:49:28 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:49:28 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:49:28 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:49:28 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (179446ms till timeout)
2022-03-28 12:49:29 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (809324ms till timeout)
2022-03-28 12:49:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (449646ms till timeout)
2022-03-28 12:49:29 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:29 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-2 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:49:29 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (267467ms till timeout)
2022-03-28 12:49:30 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:30 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:49:30 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (470870ms till timeout)
2022-03-28 12:49:30 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-2 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:49:30 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:49:30 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:49:30 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:49:30 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:49:30 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (177900ms till timeout)
2022-03-28 12:49:30 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (808212ms till timeout)
2022-03-28 12:49:30 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (448424ms till timeout)
2022-03-28 12:49:31 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:31 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (266127ms till timeout)
2022-03-28 12:49:31 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-2 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:49:31 [ForkJoinPool-1-worker-1] INFO  [Exec:417] Command: oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=my-user-1786174319-1274634874
2022-03-28 12:49:31 [ForkJoinPool-1-worker-1] INFO  [Exec:417] Return code: 0
2022-03-28 12:49:31 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:674] ============================================================================
2022-03-28 12:49:31 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:675] [operators.user.UserST - After Each] - Clean up after test
2022-03-28 12:49:31 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:344] ############################################################################
2022-03-28 12:49:31 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:348] Delete all resources for testTlsExternalUserWithQuotas
2022-03-28 12:49:31 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:241] Delete of KafkaUser my-user-1786174319-1274634874 in namespace user-st
2022-03-28 12:49:31 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1786174319-1274634874
2022-03-28 12:49:31 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:31 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:49:31 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (469443ms till timeout)
2022-03-28 12:49:31 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:369] ############################################################################
2022-03-28 12:49:31 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:267] testTlsExternalUserWithQuotas - Notifies waiting test cases:[testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testTlsExternalUserWithQuotas] to and randomly select one to start execution
2022-03-28 12:49:31 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:93] [operators.user.UserST] - Removing parallel test: testTlsExternalUserWithQuotas
2022-03-28 12:49:31 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:97] [operators.user.UserST] - Parallel test count: 4
2022-03-28 12:49:31 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testTlsExternalUserWithQuotas-FINISHED
2022-03-28 12:49:31 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:30] ############################################################################
2022-03-28 12:49:31 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:23] ############################################################################
2022-03-28 12:49:31 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testUserTemplate-STARTED
2022-03-28 12:49:31 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:658] ============================================================================
2022-03-28 12:49:31 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:659] [operators.user.UserST - Before Each] - Setup test case environment
2022-03-28 12:49:31 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:77] [operators.user.UserST] - Adding parallel test: testUserTemplate
2022-03-28 12:49:31 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:81] [operators.user.UserST] - Parallel test count: 5
2022-03-28 12:49:31 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:230] testUserTemplate test now can proceed its execution
2022-03-28 12:49:31 [ForkJoinPool-1-worker-1] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 12:49:31 [ForkJoinPool-1-worker-1] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d, testTlsExternalUserWithQuotas=my-cluster-3e2e11de, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2, testCapacityFile=my-cluster-39ee3f62, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5, testUserTemplate=my-cluster-157df94e}
2022-03-28 12:49:31 [ForkJoinPool-1-worker-1] TRACE [AbstractST:607] USERS_NAME_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-user-467839358-1504804463, testTlsExternalUserWithQuotas=my-user-1786174319-1274634874, testThrottlingQuotasCreateTopic=my-user-896662438-260898887, testCapacityFile=my-user-597365340-661017603, testCreatingUsersWithSecretPrefix=my-user-1928899390-1444689439, testUserTemplate=my-user-1253881534-190261188}
2022-03-28 12:49:31 [ForkJoinPool-1-worker-1] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-712754859-1910580676, testTlsExternalUserWithQuotas=my-topic-593679216-1309035072, testThrottlingQuotasCreateTopic=my-topic-1532506723-1472611013, testCapacityFile=my-topic-1390944843-775067696, testCreatingUsersWithSecretPrefix=my-topic-2033928996-1209892355, testUserTemplate=my-topic-515111334-943904347}
2022-03-28 12:49:31 [ForkJoinPool-1-worker-1] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-3e2e11de-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2-kafka-clients, testCapacityFile=my-cluster-39ee3f62-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5-kafka-clients, testUserTemplate=my-cluster-157df94e-kafka-clients}
2022-03-28 12:49:31 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:155] Create/Update KafkaUser my-user-1253881534-190261188 in namespace user-st
2022-03-28 12:49:31 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-1253881534-190261188
2022-03-28 12:49:31 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-2 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:49:31 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:49:31 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:49:31 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:49:31 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:49:31 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (176357ms till timeout)
2022-03-28 12:49:31 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (807050ms till timeout)
2022-03-28 12:49:32 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (447263ms till timeout)
2022-03-28 12:49:32 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-1253881534-190261188 will have desired state: Ready
2022-03-28 12:49:32 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-1253881534-190261188 will have desired state: Ready
2022-03-28 12:49:32 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] KafkaUser: my-user-1253881534-190261188 will have desired state: Ready not ready, will try again in 1000 ms (179889ms till timeout)
2022-03-28 12:49:32 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (264906ms till timeout)
2022-03-28 12:49:32 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:32 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:49:32 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-2 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:49:32 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:32 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:49:32 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (468015ms till timeout)
2022-03-28 12:49:33 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (805938ms till timeout)
2022-03-28 12:49:33 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (446153ms till timeout)
2022-03-28 12:49:33 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:444] KafkaUser: my-user-1253881534-190261188 is in desired state: Ready
2022-03-28 12:49:33 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:674] ============================================================================
2022-03-28 12:49:33 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:675] [operators.user.UserST - After Each] - Clean up after test
2022-03-28 12:49:33 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:344] ############################################################################
2022-03-28 12:49:33 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:348] Delete all resources for testUserTemplate
2022-03-28 12:49:33 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:241] Delete of KafkaUser my-user-1253881534-190261188 in namespace user-st
2022-03-28 12:49:33 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-2 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:49:33 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:49:33 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:49:33 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:49:33 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:49:33 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (174839ms till timeout)
2022-03-28 12:49:33 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1253881534-190261188
2022-03-28 12:49:33 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (263601ms till timeout)
2022-03-28 12:49:33 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:369] ############################################################################
2022-03-28 12:49:33 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:267] testUserTemplate - Notifies waiting test cases:[testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testTlsExternalUserWithQuotas, testUserTemplate] to and randomly select one to start execution
2022-03-28 12:49:33 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:93] [operators.user.UserST] - Removing parallel test: testUserTemplate
2022-03-28 12:49:33 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:97] [operators.user.UserST] - Parallel test count: 4
2022-03-28 12:49:33 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testUserTemplate-FINISHED
2022-03-28 12:49:33 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:30] ############################################################################
2022-03-28 12:49:33 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:23] ############################################################################
2022-03-28 12:49:33 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testUpdateUser-STARTED
2022-03-28 12:49:33 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:658] ============================================================================
2022-03-28 12:49:33 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:659] [operators.user.UserST - Before Each] - Setup test case environment
2022-03-28 12:49:33 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:77] [operators.user.UserST] - Adding parallel test: testUpdateUser
2022-03-28 12:49:33 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:81] [operators.user.UserST] - Parallel test count: 5
2022-03-28 12:49:33 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:230] testUpdateUser test now can proceed its execution
2022-03-28 12:49:33 [ForkJoinPool-1-worker-1] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 12:49:33 [ForkJoinPool-1-worker-1] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d, testTlsExternalUserWithQuotas=my-cluster-3e2e11de, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2, testCapacityFile=my-cluster-39ee3f62, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5, testUpdateUser=my-cluster-e16ccf16, testUserTemplate=my-cluster-157df94e}
2022-03-28 12:49:33 [ForkJoinPool-1-worker-1] TRACE [AbstractST:607] USERS_NAME_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-user-467839358-1504804463, testTlsExternalUserWithQuotas=my-user-1786174319-1274634874, testThrottlingQuotasCreateTopic=my-user-896662438-260898887, testCapacityFile=my-user-597365340-661017603, testCreatingUsersWithSecretPrefix=my-user-1928899390-1444689439, testUpdateUser=my-user-1694542041-806176635, testUserTemplate=my-user-1253881534-190261188}
2022-03-28 12:49:33 [ForkJoinPool-1-worker-1] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-712754859-1910580676, testTlsExternalUserWithQuotas=my-topic-593679216-1309035072, testThrottlingQuotasCreateTopic=my-topic-1532506723-1472611013, testCapacityFile=my-topic-1390944843-775067696, testCreatingUsersWithSecretPrefix=my-topic-2033928996-1209892355, testUpdateUser=my-topic-1239518257-2056139809, testUserTemplate=my-topic-515111334-943904347}
2022-03-28 12:49:33 [ForkJoinPool-1-worker-1] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-3e2e11de-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2-kafka-clients, testCapacityFile=my-cluster-39ee3f62-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5-kafka-clients, testUpdateUser=my-cluster-e16ccf16-kafka-clients, testUserTemplate=my-cluster-157df94e-kafka-clients}
2022-03-28 12:49:33 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:155] Create/Update KafkaUser my-user-1694542041-806176635 in namespace user-st
2022-03-28 12:49:33 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-1694542041-806176635
2022-03-28 12:49:33 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:34 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-1694542041-806176635 will have desired state: Ready
2022-03-28 12:49:34 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-1694542041-806176635 will have desired state: Ready
2022-03-28 12:49:34 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (804788ms till timeout)
2022-03-28 12:49:34 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (445001ms till timeout)
2022-03-28 12:49:34 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] KafkaUser: my-user-1694542041-806176635 will have desired state: Ready not ready, will try again in 1000 ms (179782ms till timeout)
2022-03-28 12:49:34 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-2 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:49:34 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:34 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:49:34 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (466588ms till timeout)
2022-03-28 12:49:34 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-2 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:49:34 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:49:34 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:49:34 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:49:34 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:49:34 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (173335ms till timeout)
2022-03-28 12:49:35 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (262278ms till timeout)
2022-03-28 12:49:35 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (803675ms till timeout)
2022-03-28 12:49:35 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:444] KafkaUser: my-user-1694542041-806176635 is in desired state: Ready
2022-03-28 12:49:35 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (443888ms till timeout)
2022-03-28 12:49:35 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:35 [ForkJoinPool-1-worker-1] DEBUG [CompiledPath:93] Evaluating path: $['data']['ca.crt']
2022-03-28 12:49:35 [ForkJoinPool-1-worker-1] DEBUG [CompiledPath:93] Evaluating path: $['data']['user.crt']
2022-03-28 12:49:35 [ForkJoinPool-1-worker-1] DEBUG [CompiledPath:93] Evaluating path: $['data']['user.key']
2022-03-28 12:49:35 [ForkJoinPool-1-worker-1] DEBUG [CompiledPath:93] Evaluating path: $['metadata']['name']
2022-03-28 12:49:35 [ForkJoinPool-1-worker-1] DEBUG [CompiledPath:93] Evaluating path: $['metadata']['namespace']
2022-03-28 12:49:35 [ForkJoinPool-1-worker-1] DEBUG [CompiledPath:93] Evaluating path: $['metadata']['name']
2022-03-28 12:49:35 [ForkJoinPool-1-worker-1] DEBUG [CompiledPath:93] Evaluating path: $['metadata']['namespace']
2022-03-28 12:49:35 [ForkJoinPool-1-worker-1] DEBUG [CompiledPath:93] Evaluating path: $['spec']['authentication']['type']
2022-03-28 12:49:35 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:35 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:49:35 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (465150ms till timeout)
2022-03-28 12:49:35 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-2 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:49:36 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for increase observation generation from 1 for user my-user-1694542041-806176635
2022-03-28 12:49:36 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (261059ms till timeout)
2022-03-28 12:49:36 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] increase observation generation from 1 for user my-user-1694542041-806176635 not ready, will try again in 1000 ms (179793ms till timeout)
2022-03-28 12:49:36 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-2 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:49:36 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:49:36 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:49:36 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:49:36 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:49:36 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (171821ms till timeout)
2022-03-28 12:49:36 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (802489ms till timeout)
2022-03-28 12:49:36 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (442702ms till timeout)
2022-03-28 12:49:36 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:37 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:37 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:49:37 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (463713ms till timeout)
2022-03-28 12:49:37 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-2 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:49:37 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (259841ms till timeout)
2022-03-28 12:49:37 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (801375ms till timeout)
2022-03-28 12:49:37 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (441592ms till timeout)
2022-03-28 12:49:37 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:49:37 [ForkJoinPool-1-worker-1] INFO  [SecretUtils:46] Waiting for Secret my-user-1694542041-806176635
2022-03-28 12:49:37 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Expected secret my-user-1694542041-806176635 exists
2022-03-28 12:49:37 [ForkJoinPool-1-worker-1] INFO  [SecretUtils:50] Secret my-user-1694542041-806176635 created
2022-03-28 12:49:37 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-1694542041-806176635 will have desired state: Ready
2022-03-28 12:49:37 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-1694542041-806176635 will have desired state: Ready
2022-03-28 12:49:37 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-2 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:49:37 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:49:37 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:49:37 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:49:37 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:49:37 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (170304ms till timeout)
2022-03-28 12:49:38 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:444] KafkaUser: my-user-1694542041-806176635 is in desired state: Ready
2022-03-28 12:49:38 [ForkJoinPool-1-worker-1] DEBUG [CompiledPath:93] Evaluating path: $['data']['password']
2022-03-28 12:49:38 [ForkJoinPool-1-worker-1] DEBUG [CompiledPath:93] Evaluating path: $['metadata']['name']
2022-03-28 12:49:38 [ForkJoinPool-1-worker-1] DEBUG [CompiledPath:93] Evaluating path: $['metadata']['namespace']
2022-03-28 12:49:38 [ForkJoinPool-1-worker-1] DEBUG [CompiledPath:93] Evaluating path: $['spec']['authentication']['type']
2022-03-28 12:49:38 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:38 [ForkJoinPool-1-worker-1] INFO  [KafkaUserUtils:62] Waiting for KafkaUser deletion my-user-1694542041-806176635
2022-03-28 12:49:38 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for KafkaUser deletion my-user-1694542041-806176635
2022-03-28 12:49:38 [ForkJoinPool-1-worker-1] INFO  [KafkaUserUtils:75] KafkaUser my-user-1694542041-806176635 deleted
2022-03-28 12:49:38 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:674] ============================================================================
2022-03-28 12:49:38 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:675] [operators.user.UserST - After Each] - Clean up after test
2022-03-28 12:49:38 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:344] ############################################################################
2022-03-28 12:49:38 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:348] Delete all resources for testUpdateUser
2022-03-28 12:49:38 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:241] Delete of KafkaUser my-user-1694542041-806176635 in namespace user-st
2022-03-28 12:49:38 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1694542041-806176635
2022-03-28 12:49:38 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (258724ms till timeout)
2022-03-28 12:49:38 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:38 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:49:38 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (462288ms till timeout)
2022-03-28 12:49:38 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:369] ############################################################################
2022-03-28 12:49:38 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (440479ms till timeout)
2022-03-28 12:49:38 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:267] testUpdateUser - Notifies waiting test cases:[testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testTlsExternalUserWithQuotas, testUserTemplate, testUpdateUser] to and randomly select one to start execution
2022-03-28 12:49:38 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:93] [operators.user.UserST] - Removing parallel test: testUpdateUser
2022-03-28 12:49:38 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:97] [operators.user.UserST] - Parallel test count: 4
2022-03-28 12:49:38 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (800155ms till timeout)
2022-03-28 12:49:38 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testUpdateUser-FINISHED
2022-03-28 12:49:38 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:30] ############################################################################
2022-03-28 12:49:38 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:23] ############################################################################
2022-03-28 12:49:38 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testTlsExternalUser-STARTED
2022-03-28 12:49:38 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:658] ============================================================================
2022-03-28 12:49:38 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:659] [operators.user.UserST - Before Each] - Setup test case environment
2022-03-28 12:49:38 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:77] [operators.user.UserST] - Adding parallel test: testTlsExternalUser
2022-03-28 12:49:38 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:81] [operators.user.UserST] - Parallel test count: 5
2022-03-28 12:49:38 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:230] testTlsExternalUser test now can proceed its execution
2022-03-28 12:49:38 [ForkJoinPool-1-worker-1] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 12:49:38 [ForkJoinPool-1-worker-1] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d, testTlsExternalUserWithQuotas=my-cluster-3e2e11de, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2, testCapacityFile=my-cluster-39ee3f62, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5, testUpdateUser=my-cluster-e16ccf16, testTlsExternalUser=my-cluster-965e9d1e, testUserTemplate=my-cluster-157df94e}
2022-03-28 12:49:38 [ForkJoinPool-1-worker-1] TRACE [AbstractST:607] USERS_NAME_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-user-467839358-1504804463, testTlsExternalUserWithQuotas=my-user-1786174319-1274634874, testThrottlingQuotasCreateTopic=my-user-896662438-260898887, testCapacityFile=my-user-597365340-661017603, testCreatingUsersWithSecretPrefix=my-user-1928899390-1444689439, testUpdateUser=my-user-1694542041-806176635, testTlsExternalUser=my-user-1295334081-1602801897, testUserTemplate=my-user-1253881534-190261188}
2022-03-28 12:49:38 [ForkJoinPool-1-worker-1] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-712754859-1910580676, testTlsExternalUserWithQuotas=my-topic-593679216-1309035072, testThrottlingQuotasCreateTopic=my-topic-1532506723-1472611013, testCapacityFile=my-topic-1390944843-775067696, testCreatingUsersWithSecretPrefix=my-topic-2033928996-1209892355, testUpdateUser=my-topic-1239518257-2056139809, testTlsExternalUser=my-topic-476529121-306489993, testUserTemplate=my-topic-515111334-943904347}
2022-03-28 12:49:38 [ForkJoinPool-1-worker-1] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-3e2e11de-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2-kafka-clients, testCapacityFile=my-cluster-39ee3f62-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5-kafka-clients, testUpdateUser=my-cluster-e16ccf16-kafka-clients, testTlsExternalUser=my-cluster-965e9d1e-kafka-clients, testUserTemplate=my-cluster-157df94e-kafka-clients}
2022-03-28 12:49:38 [ForkJoinPool-1-worker-1] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-3 for test case:testTlsExternalUser
2022-03-28 12:49:38 [ForkJoinPool-1-worker-1] INFO  [KubeClusterResource:156] Creating Namespace: namespace-3
2022-03-28 12:49:38 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-2 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:49:39 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Namespace namespace-3
2022-03-28 12:49:39 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace namespace-3 -o json
2022-03-28 12:49:39 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace namespace-3 -o json
2022-03-28 12:49:39 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 12:49:39 [ForkJoinPool-1-worker-1] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c27,c24",
            "openshift.io/sa.scc.supplemental-groups": "1000750000/10000",
            "openshift.io/sa.scc.uid-range": "1000750000/10000"
        },
        "creationTimestamp": "2022-03-28T12:49:34Z",
        "labels": {
            "kubernetes.io/metadata.name": "namespace-3"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T12:49:34Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T12:49:34Z"
            }
        ],
        "name": "namespace-3",
        "resourceVersion": "46738",
        "uid": "baf562a0-7541-461c-9eed-e21032786c18"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 12:49:39 [ForkJoinPool-1-worker-1] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@5c7379cb=[namespace-3], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-1], io.strimzi.test.logs.CollectorElement@c45aa4b1=[namespace-2], io.strimzi.test.logs.CollectorElement@3e255cd9=[namespace-0]}
2022-03-28 12:49:39 [ForkJoinPool-1-worker-1] INFO  [KubeClusterResource:82] Client use Namespace: namespace-3
2022-03-28 12:49:39 [ForkJoinPool-1-worker-1] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-3, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 12:49:39 [ForkJoinPool-1-worker-1] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-3
2022-03-28 12:49:39 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-2 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:49:39 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:49:39 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:49:39 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:49:39 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:49:39 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (168757ms till timeout)
2022-03-28 12:49:39 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-965e9d1e in namespace namespace-3
2022-03-28 12:49:39 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:164] Using Namespace: namespace-3
2022-03-28 12:49:39 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-965e9d1e
2022-03-28 12:49:39 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:39 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-965e9d1e will have desired state: Ready
2022-03-28 12:49:39 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-965e9d1e will have desired state: Ready
2022-03-28 12:49:39 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (257465ms till timeout)
2022-03-28 12:49:39 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (798992ms till timeout)
2022-03-28 12:49:40 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaBridge: http-bridge-scram-sha-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (439206ms till timeout)
2022-03-28 12:49:40 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (839666ms till timeout)
2022-03-28 12:49:40 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:40 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:49:40 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (460850ms till timeout)
2022-03-28 12:49:40 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-3 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:49:41 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-3 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:49:41 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:49:41 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:49:41 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:49:41 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:49:41 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (167224ms till timeout)
2022-03-28 12:49:41 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:41 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (256141ms till timeout)
2022-03-28 12:49:41 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (797679ms till timeout)
2022-03-28 12:49:41 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (838459ms till timeout)
2022-03-28 12:49:41 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:444] KafkaBridge: http-bridge-scram-sha-cluster-name is in desired state: Ready
2022-03-28 12:49:41 [ForkJoinPool-1-worker-7] INFO  [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-03-28 12:49:41 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:23] ############################################################################
2022-03-28 12:49:41 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeScramShaST.testReceiveSimpleMessageTlsScramSha-STARTED
2022-03-28 12:49:41 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:658] ============================================================================
2022-03-28 12:49:41 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:659] [bridge.HttpBridgeScramShaST - Before Each] - Setup test case environment
2022-03-28 12:49:41 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:77] [bridge.HttpBridgeScramShaST] - Adding parallel test: testReceiveSimpleMessageTlsScramSha
2022-03-28 12:49:41 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:81] [bridge.HttpBridgeScramShaST] - Parallel test count: 6
2022-03-28 12:49:41 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:230] testReceiveSimpleMessageTlsScramSha test now can proceed its execution
2022-03-28 12:49:41 [ForkJoinPool-1-worker-7] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 12:49:41 [ForkJoinPool-1-worker-7] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d, testTlsExternalUserWithQuotas=my-cluster-3e2e11de, testReceiveSimpleMessageTlsScramSha=my-cluster-455915b9, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2, testCapacityFile=my-cluster-39ee3f62, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5, testUpdateUser=my-cluster-e16ccf16, testTlsExternalUser=my-cluster-965e9d1e, testUserTemplate=my-cluster-157df94e}
2022-03-28 12:49:41 [ForkJoinPool-1-worker-7] TRACE [AbstractST:607] USERS_NAME_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-user-467839358-1504804463, testTlsExternalUserWithQuotas=my-user-1786174319-1274634874, testReceiveSimpleMessageTlsScramSha=my-user-1329598231-1183867861, testThrottlingQuotasCreateTopic=my-user-896662438-260898887, testCapacityFile=my-user-597365340-661017603, testCreatingUsersWithSecretPrefix=my-user-1928899390-1444689439, testUpdateUser=my-user-1694542041-806176635, testTlsExternalUser=my-user-1295334081-1602801897, testUserTemplate=my-user-1253881534-190261188}
2022-03-28 12:49:41 [ForkJoinPool-1-worker-7] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-712754859-1910580676, testTlsExternalUserWithQuotas=my-topic-593679216-1309035072, testReceiveSimpleMessageTlsScramSha=my-topic-170116874-1830928683, testThrottlingQuotasCreateTopic=my-topic-1532506723-1472611013, testCapacityFile=my-topic-1390944843-775067696, testCreatingUsersWithSecretPrefix=my-topic-2033928996-1209892355, testUpdateUser=my-topic-1239518257-2056139809, testTlsExternalUser=my-topic-476529121-306489993, testUserTemplate=my-topic-515111334-943904347}
2022-03-28 12:49:41 [ForkJoinPool-1-worker-7] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-3e2e11de-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-455915b9-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2-kafka-clients, testCapacityFile=my-cluster-39ee3f62-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5-kafka-clients, testUpdateUser=my-cluster-e16ccf16-kafka-clients, testTlsExternalUser=my-cluster-965e9d1e-kafka-clients, testUserTemplate=my-cluster-157df94e-kafka-clients}
2022-03-28 12:49:41 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update KafkaTopic my-topic-1741595665-1144540447 in namespace http-bridge-scram-sha-st
2022-03-28 12:49:41 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1741595665-1144540447
2022-03-28 12:49:41 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-1741595665-1144540447 will have desired state: Ready
2022-03-28 12:49:41 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-1741595665-1144540447 will have desired state: Ready
2022-03-28 12:49:41 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:41 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:49:41 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (459440ms till timeout)
2022-03-28 12:49:41 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic: my-topic-1741595665-1144540447 will have desired state: Ready not ready, will try again in 1000 ms (179891ms till timeout)
2022-03-28 12:49:42 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-3 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:49:42 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (796566ms till timeout)
2022-03-28 12:49:42 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (837234ms till timeout)
2022-03-28 12:49:42 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:42 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-3 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:49:42 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:49:42 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:49:42 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:49:42 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:49:42 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (165648ms till timeout)
2022-03-28 12:49:42 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:49:42 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (254704ms till timeout)
2022-03-28 12:49:42 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:444] KafkaTopic: my-topic-1741595665-1144540447 is in desired state: Ready
2022-03-28 12:49:42 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update Job consumer-491886873 in namespace http-bridge-scram-sha-st
2022-03-28 12:49:42 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:consumer-491886873
2022-03-28 12:49:42 [ForkJoinPool-1-worker-7] INFO  [JobUtils:81] Waiting for job: consumer-491886873 will be in active state
2022-03-28 12:49:42 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 12:49:42 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:42 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:49:42 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (457998ms till timeout)
2022-03-28 12:49:43 [ForkJoinPool-1-worker-7] INFO  [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-03-28 12:49:43 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update Job producer-1263865700 in namespace http-bridge-scram-sha-st
2022-03-28 12:49:43 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:producer-1263865700
2022-03-28 12:49:43 [ForkJoinPool-1-worker-7] INFO  [JobUtils:81] Waiting for job: producer-1263865700 will be in active state
2022-03-28 12:49:43 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 12:49:43 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (795418ms till timeout)
2022-03-28 12:49:43 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-3 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:49:43 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (836090ms till timeout)
2022-03-28 12:49:43 [ForkJoinPool-1-worker-7] INFO  [ClientUtils:61] Waiting till producer producer-1263865700 and consumer consumer-491886873 finish
2022-03-28 12:49:43 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for clients finished
2022-03-28 12:49:43 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (219889ms till timeout)
2022-03-28 12:49:43 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (253441ms till timeout)
2022-03-28 12:49:43 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:44 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-3 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:49:44 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:49:44 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:49:44 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:49:44 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:49:44 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (164075ms till timeout)
2022-03-28 12:49:44 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:44 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:49:44 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (456555ms till timeout)
2022-03-28 12:49:44 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (794306ms till timeout)
2022-03-28 12:49:44 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (834978ms till timeout)
2022-03-28 12:49:44 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (218776ms till timeout)
2022-03-28 12:49:45 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-3 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:49:45 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (252099ms till timeout)
2022-03-28 12:49:45 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:45 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-3 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:49:45 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:49:45 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:49:45 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:49:45 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:49:45 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (162542ms till timeout)
2022-03-28 12:49:45 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (793194ms till timeout)
2022-03-28 12:49:45 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (833868ms till timeout)
2022-03-28 12:49:45 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:45 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:49:45 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (455112ms till timeout)
2022-03-28 12:49:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (217666ms till timeout)
2022-03-28 12:49:46 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (250776ms till timeout)
2022-03-28 12:49:46 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-3 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:49:46 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (792081ms till timeout)
2022-03-28 12:49:46 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:46 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (832755ms till timeout)
2022-03-28 12:49:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (216554ms till timeout)
2022-03-28 12:49:47 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-3 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:49:47 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:49:47 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:49:47 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:49:47 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:49:47 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (160995ms till timeout)
2022-03-28 12:49:47 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:47 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:49:47 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (453651ms till timeout)
2022-03-28 12:49:47 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:49:47 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (249452ms till timeout)
2022-03-28 12:49:47 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (790970ms till timeout)
2022-03-28 12:49:48 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (831644ms till timeout)
2022-03-28 12:49:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (215445ms till timeout)
2022-03-28 12:49:48 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-3 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:49:48 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:48 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-3 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:49:48 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:49:48 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:49:48 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:49:48 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:49:48 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (159375ms till timeout)
2022-03-28 12:49:48 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:48 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:49:48 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (452042ms till timeout)
2022-03-28 12:49:49 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (248131ms till timeout)
2022-03-28 12:49:49 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (789668ms till timeout)
2022-03-28 12:49:49 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (830446ms till timeout)
2022-03-28 12:49:49 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (214254ms till timeout)
2022-03-28 12:49:49 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-3 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:49:49 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:50 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:50 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:49:50 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (450565ms till timeout)
2022-03-28 12:49:50 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-3 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:49:50 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:49:50 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:49:50 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:49:50 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:49:50 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (157792ms till timeout)
2022-03-28 12:49:50 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (246808ms till timeout)
2022-03-28 12:49:50 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (829120ms till timeout)
2022-03-28 12:49:50 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (788337ms till timeout)
2022-03-28 12:49:50 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (213028ms till timeout)
2022-03-28 12:49:51 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:51 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-3 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:49:51 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:51 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:49:51 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (449079ms till timeout)
2022-03-28 12:49:51 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (245476ms till timeout)
2022-03-28 12:49:51 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (211708ms till timeout)
2022-03-28 12:49:51 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (787012ms till timeout)
2022-03-28 12:49:51 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (827794ms till timeout)
2022-03-28 12:49:52 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-3 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:49:52 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:49:52 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:49:52 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:49:52 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:49:52 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (156220ms till timeout)
2022-03-28 12:49:52 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:49:52 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:53 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-3 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:49:53 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (244147ms till timeout)
2022-03-28 12:49:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (210380ms till timeout)
2022-03-28 12:49:53 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (826466ms till timeout)
2022-03-28 12:49:53 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (785684ms till timeout)
2022-03-28 12:49:53 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:53 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:49:53 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (447631ms till timeout)
2022-03-28 12:49:53 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-3 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:49:53 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:49:53 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:49:53 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:49:53 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:49:53 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (154684ms till timeout)
2022-03-28 12:49:54 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:54 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-3 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:49:54 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (242827ms till timeout)
2022-03-28 12:49:54 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (825145ms till timeout)
2022-03-28 12:49:54 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (784363ms till timeout)
2022-03-28 12:49:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (209057ms till timeout)
2022-03-28 12:49:54 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:54 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:49:54 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (446157ms till timeout)
2022-03-28 12:49:55 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-3 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:49:55 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:49:55 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:49:55 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:49:55 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:49:55 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (153152ms till timeout)
2022-03-28 12:49:55 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:55 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (241503ms till timeout)
2022-03-28 12:49:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (207735ms till timeout)
2022-03-28 12:49:55 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (823822ms till timeout)
2022-03-28 12:49:55 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (783040ms till timeout)
2022-03-28 12:49:56 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-3 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:49:56 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:56 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:49:56 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-0 removal not ready, will try again in 1000 ms (444706ms till timeout)
2022-03-28 12:49:56 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-3 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:49:56 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:49:56 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:49:56 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:49:56 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:49:56 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (151606ms till timeout)
2022-03-28 12:49:57 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (240180ms till timeout)
2022-03-28 12:49:57 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (822499ms till timeout)
2022-03-28 12:49:57 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (206411ms till timeout)
2022-03-28 12:49:57 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (781715ms till timeout)
2022-03-28 12:49:57 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:57 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-3 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:49:57 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:49:57 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-2 get Namespace namespace-0 -o yaml
2022-03-28 12:49:57 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 1
2022-03-28 12:49:57 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 12:49:57 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-0" not found
2022-03-28 12:49:57 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] ======STDERR END======
2022-03-28 12:49:57 [ForkJoinPool-1-worker-3] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@5c7379cb=[namespace-3], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-1], io.strimzi.test.logs.CollectorElement@c45aa4b1=[namespace-2], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 12:49:57 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:267] testCapacityFile - Notifies waiting test cases:[testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testTlsExternalUserWithQuotas, testUserTemplate, testUpdateUser, testTlsExternalUser, testReceiveSimpleMessageTlsScramSha] to and randomly select one to start execution
2022-03-28 12:49:57 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:93] [cruisecontrol.CruiseControlConfigurationST] - Removing parallel test: testCapacityFile
2022-03-28 12:49:57 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:97] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 5
2022-03-28 12:49:57 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testCapacityFile-FINISHED
2022-03-28 12:49:57 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:30] ############################################################################
2022-03-28 12:49:57 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:23] ############################################################################
2022-03-28 12:49:57 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods-STARTED
2022-03-28 12:49:57 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:658] ============================================================================
2022-03-28 12:49:57 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:659] [cruisecontrol.CruiseControlConfigurationST - Before Each] - Setup test case environment
2022-03-28 12:49:57 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:77] [cruisecontrol.CruiseControlConfigurationST] - Adding parallel test: testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods
2022-03-28 12:49:57 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:81] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 6
2022-03-28 12:49:57 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:230] testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods test now can proceed its execution
2022-03-28 12:49:57 [ForkJoinPool-1-worker-3] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 12:49:57 [ForkJoinPool-1-worker-3] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d, testTlsExternalUserWithQuotas=my-cluster-3e2e11de, testReceiveSimpleMessageTlsScramSha=my-cluster-455915b9, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2, testCapacityFile=my-cluster-39ee3f62, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5, testUpdateUser=my-cluster-e16ccf16, testTlsExternalUser=my-cluster-965e9d1e, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-31feb41b, testUserTemplate=my-cluster-157df94e}
2022-03-28 12:49:57 [ForkJoinPool-1-worker-3] TRACE [AbstractST:607] USERS_NAME_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-user-467839358-1504804463, testTlsExternalUserWithQuotas=my-user-1786174319-1274634874, testReceiveSimpleMessageTlsScramSha=my-user-1329598231-1183867861, testThrottlingQuotasCreateTopic=my-user-896662438-260898887, testCapacityFile=my-user-597365340-661017603, testCreatingUsersWithSecretPrefix=my-user-1928899390-1444689439, testUpdateUser=my-user-1694542041-806176635, testTlsExternalUser=my-user-1295334081-1602801897, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2100837352-1014792902, testUserTemplate=my-user-1253881534-190261188}
2022-03-28 12:49:57 [ForkJoinPool-1-worker-3] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-712754859-1910580676, testTlsExternalUserWithQuotas=my-topic-593679216-1309035072, testReceiveSimpleMessageTlsScramSha=my-topic-170116874-1830928683, testThrottlingQuotasCreateTopic=my-topic-1532506723-1472611013, testCapacityFile=my-topic-1390944843-775067696, testCreatingUsersWithSecretPrefix=my-topic-2033928996-1209892355, testUpdateUser=my-topic-1239518257-2056139809, testTlsExternalUser=my-topic-476529121-306489993, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1084162749-1576061083, testUserTemplate=my-topic-515111334-943904347}
2022-03-28 12:49:57 [ForkJoinPool-1-worker-3] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-3e2e11de-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-455915b9-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2-kafka-clients, testCapacityFile=my-cluster-39ee3f62-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5-kafka-clients, testUpdateUser=my-cluster-e16ccf16-kafka-clients, testTlsExternalUser=my-cluster-965e9d1e-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-31feb41b-kafka-clients, testUserTemplate=my-cluster-157df94e-kafka-clients}
2022-03-28 12:49:57 [ForkJoinPool-1-worker-3] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-4 for test case:testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods
2022-03-28 12:49:57 [ForkJoinPool-1-worker-3] INFO  [KubeClusterResource:156] Creating Namespace: namespace-4
2022-03-28 12:49:57 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Namespace namespace-4
2022-03-28 12:49:57 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-3 get Namespace namespace-4 -o json
2022-03-28 12:49:58 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-3 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:49:58 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:49:58 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:49:58 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:49:58 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:49:58 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (150072ms till timeout)
2022-03-28 12:49:58 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-3 get Namespace namespace-4 -o json
2022-03-28 12:49:58 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:49:58 [ForkJoinPool-1-worker-3] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c28,c2",
            "openshift.io/sa.scc.supplemental-groups": "1000760000/10000",
            "openshift.io/sa.scc.uid-range": "1000760000/10000"
        },
        "creationTimestamp": "2022-03-28T12:49:53Z",
        "labels": {
            "kubernetes.io/metadata.name": "namespace-4"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T12:49:53Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T12:49:53Z"
            }
        ],
        "name": "namespace-4",
        "resourceVersion": "47130",
        "uid": "695d8c99-75d7-412a-8037-ebe028cd5544"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 12:49:58 [ForkJoinPool-1-worker-3] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@3881d5f2=[namespace-4], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@5c7379cb=[namespace-3], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-1], io.strimzi.test.logs.CollectorElement@c45aa4b1=[namespace-2], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 12:49:58 [ForkJoinPool-1-worker-3] INFO  [KubeClusterResource:82] Client use Namespace: namespace-4
2022-03-28 12:49:58 [ForkJoinPool-1-worker-3] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-4, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 12:49:58 [ForkJoinPool-1-worker-3] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-4
2022-03-28 12:49:58 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-31feb41b in namespace namespace-4
2022-03-28 12:49:58 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:164] Using Namespace: namespace-4
2022-03-28 12:49:58 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (238858ms till timeout)
2022-03-28 12:49:58 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (821177ms till timeout)
2022-03-28 12:49:58 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (780395ms till timeout)
2022-03-28 12:49:58 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-31feb41b
2022-03-28 12:49:58 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (204981ms till timeout)
2022-03-28 12:49:58 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-31feb41b will have desired state: Ready
2022-03-28 12:49:58 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-31feb41b will have desired state: Ready
2022-03-28 12:49:58 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1319889ms till timeout)
2022-03-28 12:49:59 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:49:59 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:49:59 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:49:59 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:49:59 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:49:59 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:49:59 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (148531ms till timeout)
2022-03-28 12:49:59 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (237508ms till timeout)
2022-03-28 12:49:59 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (819827ms till timeout)
2022-03-28 12:49:59 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (779045ms till timeout)
2022-03-28 12:49:59 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1318780ms till timeout)
2022-03-28 12:50:00 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (203544ms till timeout)
2022-03-28 12:50:00 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:01 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:01 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:50:01 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:50:01 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:50:01 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:50:01 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (147017ms till timeout)
2022-03-28 12:50:01 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (236186ms till timeout)
2022-03-28 12:50:01 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1317547ms till timeout)
2022-03-28 12:50:01 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (818505ms till timeout)
2022-03-28 12:50:01 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (777723ms till timeout)
2022-03-28 12:50:01 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (202306ms till timeout)
2022-03-28 12:50:02 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:02 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (234862ms till timeout)
2022-03-28 12:50:02 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1316224ms till timeout)
2022-03-28 12:50:02 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (817182ms till timeout)
2022-03-28 12:50:02 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (776400ms till timeout)
2022-03-28 12:50:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (200985ms till timeout)
2022-03-28 12:50:02 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:50:02 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:02 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:50:02 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:50:02 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:50:02 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:50:02 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (145463ms till timeout)
2022-03-28 12:50:03 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:03 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (233540ms till timeout)
2022-03-28 12:50:03 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1314902ms till timeout)
2022-03-28 12:50:03 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (775078ms till timeout)
2022-03-28 12:50:03 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (815860ms till timeout)
2022-03-28 12:50:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (199663ms till timeout)
2022-03-28 12:50:04 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:04 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:50:04 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:50:04 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:50:04 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:50:04 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (143943ms till timeout)
2022-03-28 12:50:05 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (232219ms till timeout)
2022-03-28 12:50:05 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (814539ms till timeout)
2022-03-28 12:50:05 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (773757ms till timeout)
2022-03-28 12:50:05 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1313581ms till timeout)
2022-03-28 12:50:05 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:05 [ForkJoinPool-1-worker-7] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment producer-1263865700 deletion
2022-03-28 12:50:05 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for ReplicaSet producer-1263865700 to be deleted
2022-03-28 12:50:05 [ForkJoinPool-1-worker-7] DEBUG [JobUtils:40] Job producer-1263865700 was deleted
2022-03-28 12:50:05 [ForkJoinPool-1-worker-7] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment consumer-491886873 deletion
2022-03-28 12:50:05 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for ReplicaSet consumer-491886873 to be deleted
2022-03-28 12:50:05 [ForkJoinPool-1-worker-7] DEBUG [JobUtils:40] Job consumer-491886873 was deleted
2022-03-28 12:50:05 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:674] ============================================================================
2022-03-28 12:50:05 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:675] [bridge.HttpBridgeScramShaST - After Each] - Clean up after test
2022-03-28 12:50:05 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:344] ############################################################################
2022-03-28 12:50:05 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:348] Delete all resources for testReceiveSimpleMessageTlsScramSha
2022-03-28 12:50:05 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of Job consumer-491886873 in namespace http-bridge-scram-sha-st
2022-03-28 12:50:05 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:05 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:50:05 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:50:05 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:50:05 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:50:05 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (142391ms till timeout)
2022-03-28 12:50:05 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:consumer-491886873
2022-03-28 12:50:05 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of Job producer-1263865700 in namespace http-bridge-scram-sha-st
2022-03-28 12:50:06 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:producer-1263865700
2022-03-28 12:50:06 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of KafkaTopic my-topic-1741595665-1144540447 in namespace http-bridge-scram-sha-st
2022-03-28 12:50:06 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (230999ms till timeout)
2022-03-28 12:50:06 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (813303ms till timeout)
2022-03-28 12:50:06 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (772522ms till timeout)
2022-03-28 12:50:06 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1741595665-1144540447
2022-03-28 12:50:06 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1312341ms till timeout)
2022-03-28 12:50:06 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:369] ############################################################################
2022-03-28 12:50:06 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:267] testReceiveSimpleMessageTlsScramSha - Notifies waiting test cases:[testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testTlsExternalUserWithQuotas, testUserTemplate, testUpdateUser, testTlsExternalUser, testReceiveSimpleMessageTlsScramSha, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods] to and randomly select one to start execution
2022-03-28 12:50:06 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:93] [bridge.HttpBridgeScramShaST] - Removing parallel test: testReceiveSimpleMessageTlsScramSha
2022-03-28 12:50:06 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:97] [bridge.HttpBridgeScramShaST] - Parallel test count: 5
2022-03-28 12:50:06 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeScramShaST.testReceiveSimpleMessageTlsScramSha-FINISHED
2022-03-28 12:50:06 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:30] ############################################################################
2022-03-28 12:50:06 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:23] ############################################################################
2022-03-28 12:50:06 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeScramShaST.testSendSimpleMessageTlsScramSha-STARTED
2022-03-28 12:50:06 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:658] ============================================================================
2022-03-28 12:50:06 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:659] [bridge.HttpBridgeScramShaST - Before Each] - Setup test case environment
2022-03-28 12:50:06 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:77] [bridge.HttpBridgeScramShaST] - Adding parallel test: testSendSimpleMessageTlsScramSha
2022-03-28 12:50:06 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:81] [bridge.HttpBridgeScramShaST] - Parallel test count: 6
2022-03-28 12:50:06 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:230] testSendSimpleMessageTlsScramSha test now can proceed its execution
2022-03-28 12:50:06 [ForkJoinPool-1-worker-7] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 12:50:06 [ForkJoinPool-1-worker-7] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d, testTlsExternalUserWithQuotas=my-cluster-3e2e11de, testReceiveSimpleMessageTlsScramSha=my-cluster-455915b9, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2, testCapacityFile=my-cluster-39ee3f62, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5, testUpdateUser=my-cluster-e16ccf16, testTlsExternalUser=my-cluster-965e9d1e, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-31feb41b, testSendSimpleMessageTlsScramSha=my-cluster-27773859, testUserTemplate=my-cluster-157df94e}
2022-03-28 12:50:06 [ForkJoinPool-1-worker-7] TRACE [AbstractST:607] USERS_NAME_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-user-467839358-1504804463, testTlsExternalUserWithQuotas=my-user-1786174319-1274634874, testReceiveSimpleMessageTlsScramSha=my-user-1329598231-1183867861, testThrottlingQuotasCreateTopic=my-user-896662438-260898887, testCapacityFile=my-user-597365340-661017603, testCreatingUsersWithSecretPrefix=my-user-1928899390-1444689439, testUpdateUser=my-user-1694542041-806176635, testTlsExternalUser=my-user-1295334081-1602801897, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2100837352-1014792902, testSendSimpleMessageTlsScramSha=my-user-507582460-1003875464, testUserTemplate=my-user-1253881534-190261188}
2022-03-28 12:50:06 [ForkJoinPool-1-worker-7] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-712754859-1910580676, testTlsExternalUserWithQuotas=my-topic-593679216-1309035072, testReceiveSimpleMessageTlsScramSha=my-topic-170116874-1830928683, testThrottlingQuotasCreateTopic=my-topic-1532506723-1472611013, testCapacityFile=my-topic-1390944843-775067696, testCreatingUsersWithSecretPrefix=my-topic-2033928996-1209892355, testUpdateUser=my-topic-1239518257-2056139809, testTlsExternalUser=my-topic-476529121-306489993, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1084162749-1576061083, testSendSimpleMessageTlsScramSha=my-topic-62182003-1963040205, testUserTemplate=my-topic-515111334-943904347}
2022-03-28 12:50:06 [ForkJoinPool-1-worker-7] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-3e2e11de-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-455915b9-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2-kafka-clients, testCapacityFile=my-cluster-39ee3f62-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5-kafka-clients, testUpdateUser=my-cluster-e16ccf16-kafka-clients, testTlsExternalUser=my-cluster-965e9d1e-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-31feb41b-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-27773859-kafka-clients, testUserTemplate=my-cluster-157df94e-kafka-clients}
2022-03-28 12:50:06 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update KafkaTopic my-topic-1924076743-1829262453 in namespace http-bridge-scram-sha-st
2022-03-28 12:50:06 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1924076743-1829262453
2022-03-28 12:50:06 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:06 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-1924076743-1829262453 will have desired state: Ready
2022-03-28 12:50:06 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-1924076743-1829262453 will have desired state: Ready
2022-03-28 12:50:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] KafkaTopic: my-topic-1924076743-1829262453 will have desired state: Ready not ready, will try again in 1000 ms (179891ms till timeout)
2022-03-28 12:50:07 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:07 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:50:07 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:50:07 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:50:07 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:50:07 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (140863ms till timeout)
2022-03-28 12:50:07 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (229781ms till timeout)
2022-03-28 12:50:07 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (812096ms till timeout)
2022-03-28 12:50:07 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1311138ms till timeout)
2022-03-28 12:50:07 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (771313ms till timeout)
2022-03-28 12:50:07 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:50:08 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:444] KafkaTopic: my-topic-1924076743-1829262453 is in desired state: Ready
2022-03-28 12:50:08 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update Job producer-756913591 in namespace http-bridge-scram-sha-st
2022-03-28 12:50:08 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:producer-756913591
2022-03-28 12:50:08 [ForkJoinPool-1-worker-7] INFO  [JobUtils:81] Waiting for job: producer-756913591 will be in active state
2022-03-28 12:50:08 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 12:50:08 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:08 [ForkJoinPool-1-worker-7] INFO  [ClientUtils:76] Waiting for producer/consumer:producer-756913591 to finished
2022-03-28 12:50:08 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 12:50:08 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job producer-756913591 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:50:03Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:50:08 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (219766ms till timeout)
2022-03-28 12:50:08 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (770127ms till timeout)
2022-03-28 12:50:08 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1309950ms till timeout)
2022-03-28 12:50:08 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (810908ms till timeout)
2022-03-28 12:50:08 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:08 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:50:08 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:50:08 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:50:08 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:50:08 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (139342ms till timeout)
2022-03-28 12:50:08 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (228471ms till timeout)
2022-03-28 12:50:09 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job producer-756913591 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:50:03Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:50:09 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (218547ms till timeout)
2022-03-28 12:50:10 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (768910ms till timeout)
2022-03-28 12:50:10 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (809690ms till timeout)
2022-03-28 12:50:10 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1308732ms till timeout)
2022-03-28 12:50:10 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (227155ms till timeout)
2022-03-28 12:50:10 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:10 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:50:10 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:50:10 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:50:10 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:50:10 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (137828ms till timeout)
2022-03-28 12:50:11 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job producer-756913591 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:50:03Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:50:11 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (767801ms till timeout)
2022-03-28 12:50:11 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (808475ms till timeout)
2022-03-28 12:50:11 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (217221ms till timeout)
2022-03-28 12:50:11 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1307516ms till timeout)
2022-03-28 12:50:11 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:11 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (225937ms till timeout)
2022-03-28 12:50:11 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:11 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:50:11 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:50:11 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:50:11 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:50:11 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (136301ms till timeout)
2022-03-28 12:50:12 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (766688ms till timeout)
2022-03-28 12:50:12 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1306402ms till timeout)
2022-03-28 12:50:12 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (807359ms till timeout)
2022-03-28 12:50:12 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job producer-756913591 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:50:03Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:50:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (215995ms till timeout)
2022-03-28 12:50:12 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:50:12 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (224714ms till timeout)
2022-03-28 12:50:12 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:13 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (765574ms till timeout)
2022-03-28 12:50:13 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:13 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:50:13 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:50:13 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:50:13 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:50:13 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (134758ms till timeout)
2022-03-28 12:50:13 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (806248ms till timeout)
2022-03-28 12:50:13 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1305289ms till timeout)
2022-03-28 12:50:13 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job producer-756913591 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:50:03Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:50:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (214776ms till timeout)
2022-03-28 12:50:13 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (223489ms till timeout)
2022-03-28 12:50:14 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:14 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-c525f5e5 will have desired state: Ready not ready, will try again in 1000 ms (764462ms till timeout)
2022-03-28 12:50:14 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (805134ms till timeout)
2022-03-28 12:50:14 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1304176ms till timeout)
2022-03-28 12:50:14 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job producer-756913591 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:50:03Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:50:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (213551ms till timeout)
2022-03-28 12:50:15 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:15 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:50:15 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:50:15 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:50:15 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:50:15 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (133214ms till timeout)
2022-03-28 12:50:15 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (222266ms till timeout)
2022-03-28 12:50:15 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:444] Kafka: my-cluster-c525f5e5 is in desired state: Ready
2022-03-28 12:50:15 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update KafkaTopic my-topic-2033928996-1209892355 in namespace namespace-4
2022-03-28 12:50:15 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:164] Using Namespace: namespace-2
2022-03-28 12:50:15 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (804023ms till timeout)
2022-03-28 12:50:15 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-2033928996-1209892355
2022-03-28 12:50:15 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1302953ms till timeout)
2022-03-28 12:50:15 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-2033928996-1209892355 will have desired state: Ready
2022-03-28 12:50:15 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-2033928996-1209892355 will have desired state: Ready
2022-03-28 12:50:16 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:16 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job producer-756913591 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:50:03Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:50:16 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:444] KafkaTopic: my-topic-2033928996-1209892355 is in desired state: Ready
2022-03-28 12:50:16 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update KafkaUser encrypted-leopold in namespace namespace-4
2022-03-28 12:50:16 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:164] Using Namespace: namespace-2
2022-03-28 12:50:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (212224ms till timeout)
2022-03-28 12:50:16 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:encrypted-leopold
2022-03-28 12:50:16 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (220940ms till timeout)
2022-03-28 12:50:16 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:433] Wait for KafkaUser: encrypted-leopold will have desired state: Ready
2022-03-28 12:50:16 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaUser: encrypted-leopold will have desired state: Ready
2022-03-28 12:50:16 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:16 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:50:16 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:50:16 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:50:16 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:50:16 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (131678ms till timeout)
2022-03-28 12:50:16 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] KafkaUser: encrypted-leopold will have desired state: Ready not ready, will try again in 1000 ms (179891ms till timeout)
2022-03-28 12:50:16 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (802912ms till timeout)
2022-03-28 12:50:16 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1301844ms till timeout)
2022-03-28 12:50:17 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job producer-756913591 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:50:03Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:50:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (211004ms till timeout)
2022-03-28 12:50:17 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:17 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:50:17 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (219723ms till timeout)
2022-03-28 12:50:17 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:444] KafkaUser: encrypted-leopold is in desired state: Ready
2022-03-28 12:50:17 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update KafkaUser scramed-leopold in namespace namespace-4
2022-03-28 12:50:17 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:164] Using Namespace: namespace-2
2022-03-28 12:50:17 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:scramed-leopold
2022-03-28 12:50:17 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (801800ms till timeout)
2022-03-28 12:50:18 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:433] Wait for KafkaUser: scramed-leopold will have desired state: Ready
2022-03-28 12:50:18 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaUser: scramed-leopold will have desired state: Ready
2022-03-28 12:50:18 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:18 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:50:18 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:50:18 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:50:18 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:50:18 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (130119ms till timeout)
2022-03-28 12:50:18 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1300626ms till timeout)
2022-03-28 12:50:18 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:444] KafkaUser: scramed-leopold is in desired state: Ready
2022-03-28 12:50:18 [ForkJoinPool-1-worker-13] INFO  [UserST:346] Deploying KafkaClients pod for TLS listener
2022-03-28 12:50:18 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update Deployment my-cluster-c525f5e5-tls-kafka-clients in namespace namespace-2
2022-03-28 12:50:18 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:164] Using Namespace: namespace-2
2022-03-28 12:50:18 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job producer-756913591 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:50:03Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:50:18 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-c525f5e5-tls-kafka-clients
2022-03-28 12:50:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (209663ms till timeout)
2022-03-28 12:50:18 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (218493ms till timeout)
2022-03-28 12:50:19 [ForkJoinPool-1-worker-13] INFO  [DeploymentUtils:161] Wait for Deployment: my-cluster-c525f5e5-tls-kafka-clients will be ready
2022-03-28 12:50:19 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Wait for Deployment: my-cluster-c525f5e5-tls-kafka-clients will be ready
2022-03-28 12:50:19 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:19 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (800589ms till timeout)
2022-03-28 12:50:19 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: my-cluster-c525f5e5-tls-kafka-clients will be ready not ready, will try again in 1000 ms (479784ms till timeout)
2022-03-28 12:50:19 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1299415ms till timeout)
2022-03-28 12:50:19 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:19 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:50:19 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:50:19 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:50:19 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:50:19 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (128567ms till timeout)
2022-03-28 12:50:19 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job producer-756913591 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:50:03Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:50:20 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (217269ms till timeout)
2022-03-28 12:50:20 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (208333ms till timeout)
2022-03-28 12:50:20 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (799478ms till timeout)
2022-03-28 12:50:20 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: my-cluster-c525f5e5-tls-kafka-clients will be ready not ready, will try again in 1000 ms (478671ms till timeout)
2022-03-28 12:50:20 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1298302ms till timeout)
2022-03-28 12:50:20 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:21 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:21 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:50:21 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:50:21 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:50:21 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:50:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (127020ms till timeout)
2022-03-28 12:50:21 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (216051ms till timeout)
2022-03-28 12:50:21 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job producer-756913591 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:50:03Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:50:21 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (798263ms till timeout)
2022-03-28 12:50:21 [ForkJoinPool-1-worker-13] INFO  [DeploymentUtils:168] Deployment: my-cluster-c525f5e5-tls-kafka-clients is ready
2022-03-28 12:50:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (206901ms till timeout)
2022-03-28 12:50:21 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1297090ms till timeout)
2022-03-28 12:50:21 [ForkJoinPool-1-worker-13] INFO  [UserST:350] Deploying KafkaClients pod for PLAIN listener
2022-03-28 12:50:22 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update Deployment my-cluster-c525f5e5-plain-kafka-clients in namespace namespace-2
2022-03-28 12:50:22 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:164] Using Namespace: namespace-2
2022-03-28 12:50:22 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:22 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-c525f5e5-plain-kafka-clients
2022-03-28 12:50:22 [ForkJoinPool-1-worker-13] INFO  [DeploymentUtils:161] Wait for Deployment: my-cluster-c525f5e5-plain-kafka-clients will be ready
2022-03-28 12:50:22 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Wait for Deployment: my-cluster-c525f5e5-plain-kafka-clients will be ready
2022-03-28 12:50:22 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (214833ms till timeout)
2022-03-28 12:50:22 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: my-cluster-c525f5e5-plain-kafka-clients will be ready not ready, will try again in 1000 ms (479782ms till timeout)
2022-03-28 12:50:22 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (797046ms till timeout)
2022-03-28 12:50:22 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:50:22 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:22 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:50:22 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:50:22 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:50:22 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:50:22 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (125472ms till timeout)
2022-03-28 12:50:22 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job producer-756913591 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:50:03Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:50:22 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1295872ms till timeout)
2022-03-28 12:50:23 [ForkJoinPool-1-worker-7] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment producer-756913591 deletion
2022-03-28 12:50:23 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for ReplicaSet producer-756913591 to be deleted
2022-03-28 12:50:23 [ForkJoinPool-1-worker-7] DEBUG [JobUtils:40] Job producer-756913591 was deleted
2022-03-28 12:50:23 [ForkJoinPool-1-worker-7] INFO  [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-03-28 12:50:23 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update Job consumer-1273345896 in namespace http-bridge-scram-sha-st
2022-03-28 12:50:23 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:consumer-1273345896
2022-03-28 12:50:23 [ForkJoinPool-1-worker-7] INFO  [JobUtils:81] Waiting for job: consumer-1273345896 will be in active state
2022-03-28 12:50:23 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 12:50:23 [ForkJoinPool-1-worker-7] INFO  [ClientUtils:76] Waiting for producer/consumer:consumer-1273345896 to finished
2022-03-28 12:50:23 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 12:50:23 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:23 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (213620ms till timeout)
2022-03-28 12:50:23 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: my-cluster-c525f5e5-plain-kafka-clients will be ready not ready, will try again in 1000 ms (478568ms till timeout)
2022-03-28 12:50:23 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (795834ms till timeout)
2022-03-28 12:50:23 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job consumer-1273345896 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:50:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:50:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (219671ms till timeout)
2022-03-28 12:50:24 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1294657ms till timeout)
2022-03-28 12:50:24 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:24 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:50:24 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:50:24 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:50:24 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:50:24 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (123943ms till timeout)
2022-03-28 12:50:25 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (212403ms till timeout)
2022-03-28 12:50:25 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: my-cluster-c525f5e5-plain-kafka-clients will be ready not ready, will try again in 1000 ms (477422ms till timeout)
2022-03-28 12:50:25 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (794688ms till timeout)
2022-03-28 12:50:25 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job consumer-1273345896 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:50:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:50:25 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1293545ms till timeout)
2022-03-28 12:50:25 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:25 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (218344ms till timeout)
2022-03-28 12:50:25 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:25 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:50:25 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:50:25 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:50:25 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:50:25 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (122395ms till timeout)
2022-03-28 12:50:26 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (211185ms till timeout)
2022-03-28 12:50:26 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: my-cluster-c525f5e5-plain-kafka-clients will be ready not ready, will try again in 1000 ms (476135ms till timeout)
2022-03-28 12:50:26 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (793401ms till timeout)
2022-03-28 12:50:26 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1292336ms till timeout)
2022-03-28 12:50:26 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job consumer-1273345896 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:50:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:50:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (217021ms till timeout)
2022-03-28 12:50:26 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:27 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:27 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:50:27 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:50:27 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:50:27 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:50:27 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (120799ms till timeout)
2022-03-28 12:50:27 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (209966ms till timeout)
2022-03-28 12:50:27 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (792182ms till timeout)
2022-03-28 12:50:27 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: my-cluster-c525f5e5-plain-kafka-clients will be ready not ready, will try again in 1000 ms (474916ms till timeout)
2022-03-28 12:50:27 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1291214ms till timeout)
2022-03-28 12:50:27 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:50:27 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job consumer-1273345896 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:50:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:50:27 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (215803ms till timeout)
2022-03-28 12:50:28 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:28 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (208746ms till timeout)
2022-03-28 12:50:28 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (790961ms till timeout)
2022-03-28 12:50:28 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: my-cluster-c525f5e5-plain-kafka-clients will be ready not ready, will try again in 1000 ms (473695ms till timeout)
2022-03-28 12:50:28 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1290000ms till timeout)
2022-03-28 12:50:28 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job consumer-1273345896 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:50:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:50:29 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:29 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:50:29 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:50:29 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:50:29 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:50:29 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (119190ms till timeout)
2022-03-28 12:50:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (214556ms till timeout)
2022-03-28 12:50:29 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (207528ms till timeout)
2022-03-28 12:50:29 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Wait for Deployment: my-cluster-c525f5e5-plain-kafka-clients will be ready not ready, will try again in 1000 ms (472477ms till timeout)
2022-03-28 12:50:29 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1288785ms till timeout)
2022-03-28 12:50:29 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (789742ms till timeout)
2022-03-28 12:50:30 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:30 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job consumer-1273345896 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:50:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:50:30 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (213336ms till timeout)
2022-03-28 12:50:30 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:30 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:50:30 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:50:30 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:50:30 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:50:30 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (117636ms till timeout)
2022-03-28 12:50:31 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (206307ms till timeout)
2022-03-28 12:50:31 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (788524ms till timeout)
2022-03-28 12:50:31 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1287565ms till timeout)
2022-03-28 12:50:31 [ForkJoinPool-1-worker-13] INFO  [DeploymentUtils:168] Deployment: my-cluster-c525f5e5-plain-kafka-clients is ready
2022-03-28 12:50:31 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job consumer-1273345896 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:50:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:50:31 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:31 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (212010ms till timeout)
2022-03-28 12:50:31 [ForkJoinPool-1-worker-13] INFO  [UserST:357] Checking if user secrets with secret prefixes exists
2022-03-28 12:50:31 [ForkJoinPool-1-worker-13] INFO  [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-03-28 12:50:31 [ForkJoinPool-1-worker-13] INFO  [UserST:373] Checking if TLS user is able to send messages
2022-03-28 12:50:31 [ForkJoinPool-1-worker-13] DEBUG [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@33805dcc, which are set.
2022-03-28 12:50:31 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:123] Starting verifiableClient tls producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@536754a1, messages=[], arguments=[--topic, my-topic-1741595665-1144540447, --max-messages, 100, USER=top_secret_encrypted_leopold, --bootstrap-server, my-cluster-c525f5e5-kafka-bootstrap.namespace-2.svc:9093], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-c525f5e5-tls-kafka-clients-688ff47cfd-7knwk', podNamespace='namespace-2', bootstrapServer='my-cluster-c525f5e5-kafka-bootstrap.namespace-2.svc:9093', topicName='my-topic-1741595665-1144540447', maxMessages=100, kafkaUsername='top-secret-encrypted-leopold', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@33805dcc}
2022-03-28 12:50:31 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:124] Producing 100 messages to my-cluster-c525f5e5-kafka-bootstrap.namespace-2.svc:9093:my-topic-1741595665-1144540447 from pod my-cluster-c525f5e5-tls-kafka-clients-688ff47cfd-7knwk
2022-03-28 12:50:31 [ForkJoinPool-1-worker-13] INFO  [VerifiableClient:192] Client command: oc exec my-cluster-c525f5e5-tls-kafka-clients-688ff47cfd-7knwk -n namespace-2 -- /opt/kafka/producer.sh --topic my-topic-1741595665-1144540447 --max-messages 100 USER=top_secret_encrypted_leopold --bootstrap-server my-cluster-c525f5e5-kafka-bootstrap.namespace-2.svc:9093
2022-03-28 12:50:31 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc exec my-cluster-c525f5e5-tls-kafka-clients-688ff47cfd-7knwk -n namespace-2 -- /opt/kafka/producer.sh --topic my-topic-1741595665-1144540447 --max-messages 100 USER=top_secret_encrypted_leopold --bootstrap-server my-cluster-c525f5e5-kafka-bootstrap.namespace-2.svc:9093
2022-03-28 12:50:32 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:32 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:50:32 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:50:32 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:50:32 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:50:32 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (116106ms till timeout)
2022-03-28 12:50:32 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (205090ms till timeout)
2022-03-28 12:50:32 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (787407ms till timeout)
2022-03-28 12:50:32 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1286449ms till timeout)
2022-03-28 12:50:32 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:50:32 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job consumer-1273345896 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:50:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:50:32 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (210790ms till timeout)
2022-03-28 12:50:33 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:33 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (203766ms till timeout)
2022-03-28 12:50:33 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (786085ms till timeout)
2022-03-28 12:50:33 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1285127ms till timeout)
2022-03-28 12:50:33 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:33 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:50:33 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:50:33 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:50:33 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:50:33 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (114576ms till timeout)
2022-03-28 12:50:34 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job consumer-1273345896 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:50:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:50:34 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (209569ms till timeout)
2022-03-28 12:50:34 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:34 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (202437ms till timeout)
2022-03-28 12:50:34 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1283798ms till timeout)
2022-03-28 12:50:34 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (784755ms till timeout)
2022-03-28 12:50:35 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:35 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:50:35 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:50:35 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:50:35 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:50:35 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (113062ms till timeout)
2022-03-28 12:50:35 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job consumer-1273345896 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:50:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:50:35 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (208349ms till timeout)
2022-03-28 12:50:36 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:36 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (201115ms till timeout)
2022-03-28 12:50:36 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (783434ms till timeout)
2022-03-28 12:50:36 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1282476ms till timeout)
2022-03-28 12:50:36 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job consumer-1273345896 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:50:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:50:36 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (207129ms till timeout)
2022-03-28 12:50:36 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:36 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:50:36 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:50:36 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:50:36 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:50:36 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (111554ms till timeout)
2022-03-28 12:50:37 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (199791ms till timeout)
2022-03-28 12:50:37 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (782110ms till timeout)
2022-03-28 12:50:37 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1281152ms till timeout)
2022-03-28 12:50:37 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job consumer-1273345896 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:50:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:50:37 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:37 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:50:37 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (205910ms till timeout)
2022-03-28 12:50:38 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:127] Producer finished correctly: true
2022-03-28 12:50:38 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:131] Producer produced 100 messages
2022-03-28 12:50:38 [ForkJoinPool-1-worker-13] DEBUG [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6b37e57b, which are set.
2022-03-28 12:50:38 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:191] Starting verifiableClient tls consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@74b02498, messages=[], arguments=[--topic, my-topic-1741595665-1144540447, --max-messages, 100, USER=top_secret_encrypted_leopold, --group-id, my-consumer-group-1187770955, --group-instance-id, instance1584668258, --bootstrap-server, my-cluster-c525f5e5-kafka-bootstrap.namespace-2.svc:9093], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-c525f5e5-tls-kafka-clients-688ff47cfd-7knwk', podNamespace='namespace-2', bootstrapServer='my-cluster-c525f5e5-kafka-bootstrap.namespace-2.svc:9093', topicName='my-topic-1741595665-1144540447', maxMessages=100, kafkaUsername='top-secret-encrypted-leopold', consumerGroupName='my-consumer-group-1187770955', consumerInstanceId='instance1584668258', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@6b37e57b}
2022-03-28 12:50:38 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:192] Consuming 100 messages from my-cluster-c525f5e5-kafka-bootstrap.namespace-2.svc:9093:my-topic-1741595665-1144540447 from pod my-cluster-c525f5e5-tls-kafka-clients-688ff47cfd-7knwk
2022-03-28 12:50:38 [ForkJoinPool-1-worker-13] INFO  [VerifiableClient:192] Client command: oc exec my-cluster-c525f5e5-tls-kafka-clients-688ff47cfd-7knwk -n namespace-2 -- /opt/kafka/consumer.sh --topic my-topic-1741595665-1144540447 --max-messages 100 USER=top_secret_encrypted_leopold --group-id my-consumer-group-1187770955 --group-instance-id instance1584668258 --bootstrap-server my-cluster-c525f5e5-kafka-bootstrap.namespace-2.svc:9093
2022-03-28 12:50:38 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc exec my-cluster-c525f5e5-tls-kafka-clients-688ff47cfd-7knwk -n namespace-2 -- /opt/kafka/consumer.sh --topic my-topic-1741595665-1144540447 --max-messages 100 USER=top_secret_encrypted_leopold --group-id my-consumer-group-1187770955 --group-instance-id instance1584668258 --bootstrap-server my-cluster-c525f5e5-kafka-bootstrap.namespace-2.svc:9093
2022-03-28 12:50:38 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:38 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:50:38 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:50:38 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:50:38 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:50:38 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (110041ms till timeout)
2022-03-28 12:50:38 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (198468ms till timeout)
2022-03-28 12:50:38 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (780787ms till timeout)
2022-03-28 12:50:38 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1279829ms till timeout)
2022-03-28 12:50:38 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job consumer-1273345896 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:50:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:50:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (204622ms till timeout)
2022-03-28 12:50:39 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:39 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:39 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:50:39 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:50:39 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:50:39 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:50:39 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (108508ms till timeout)
2022-03-28 12:50:40 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (779668ms till timeout)
2022-03-28 12:50:40 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (197139ms till timeout)
2022-03-28 12:50:40 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1278500ms till timeout)
2022-03-28 12:50:40 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job consumer-1273345896 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:50:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:50:40 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (203297ms till timeout)
2022-03-28 12:50:40 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:41 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (778556ms till timeout)
2022-03-28 12:50:41 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:41 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:50:41 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:50:41 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:50:41 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:50:41 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (106959ms till timeout)
2022-03-28 12:50:41 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (195816ms till timeout)
2022-03-28 12:50:41 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1277177ms till timeout)
2022-03-28 12:50:41 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job consumer-1273345896 in namespace http-bridge-scram-sha-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:50:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:50:41 [ForkJoinPool-1-worker-7] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment consumer-1273345896 deletion
2022-03-28 12:50:41 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for ReplicaSet consumer-1273345896 to be deleted
2022-03-28 12:50:41 [ForkJoinPool-1-worker-7] DEBUG [JobUtils:40] Job consumer-1273345896 was deleted
2022-03-28 12:50:41 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:674] ============================================================================
2022-03-28 12:50:41 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:675] [bridge.HttpBridgeScramShaST - After Each] - Clean up after test
2022-03-28 12:50:41 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:344] ############################################################################
2022-03-28 12:50:41 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:348] Delete all resources for testSendSimpleMessageTlsScramSha
2022-03-28 12:50:41 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of Job producer-756913591 in namespace http-bridge-scram-sha-st
2022-03-28 12:50:42 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:producer-756913591
2022-03-28 12:50:42 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of Job consumer-1273345896 in namespace http-bridge-scram-sha-st
2022-03-28 12:50:42 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:consumer-1273345896
2022-03-28 12:50:42 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:42 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (777362ms till timeout)
2022-03-28 12:50:42 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of KafkaTopic my-topic-1924076743-1829262453 in namespace http-bridge-scram-sha-st
2022-03-28 12:50:42 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1924076743-1829262453
2022-03-28 12:50:42 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:50:42 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:369] ############################################################################
2022-03-28 12:50:42 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:267] testSendSimpleMessageTlsScramSha - Notifies waiting test cases:[testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testTlsExternalUserWithQuotas, testUserTemplate, testUpdateUser, testTlsExternalUser, testReceiveSimpleMessageTlsScramSha, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testSendSimpleMessageTlsScramSha] to and randomly select one to start execution
2022-03-28 12:50:42 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:93] [bridge.HttpBridgeScramShaST] - Removing parallel test: testSendSimpleMessageTlsScramSha
2022-03-28 12:50:42 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:97] [bridge.HttpBridgeScramShaST] - Parallel test count: 5
2022-03-28 12:50:42 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeScramShaST.testSendSimpleMessageTlsScramSha-FINISHED
2022-03-28 12:50:42 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:30] ############################################################################
2022-03-28 12:50:42 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:689] ============================================================================
2022-03-28 12:50:42 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:690] [bridge.HttpBridgeScramShaST - After All] - Clean up after test suite
2022-03-28 12:50:42 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:344] ############################################################################
2022-03-28 12:50:42 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:348] Delete all resources for HttpBridgeScramShaST
2022-03-28 12:50:42 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of Deployment http-bridge-scram-sha-st-shared-kafka-clients in namespace http-bridge-scram-sha-st
2022-03-28 12:50:42 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:42 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:50:42 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:50:42 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:50:42 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:50:42 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (105428ms till timeout)
2022-03-28 12:50:42 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1275959ms till timeout)
2022-03-28 12:50:42 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (194492ms till timeout)
2022-03-28 12:50:42 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:http-bridge-scram-sha-st-shared-kafka-clients
2022-03-28 12:50:43 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-scram-sha-st-shared-kafka-clients not ready, will try again in 10000 ms (479664ms till timeout)
2022-03-28 12:50:43 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (776250ms till timeout)
2022-03-28 12:50:43 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:43 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1274849ms till timeout)
2022-03-28 12:50:44 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (193270ms till timeout)
2022-03-28 12:50:44 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:44 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:50:44 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:50:44 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:50:44 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:50:44 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (103860ms till timeout)
2022-03-28 12:50:44 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (775141ms till timeout)
2022-03-28 12:50:45 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1273737ms till timeout)
2022-03-28 12:50:45 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:45 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (191948ms till timeout)
2022-03-28 12:50:45 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (774012ms till timeout)
2022-03-28 12:50:45 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:45 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:50:45 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:50:45 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:50:45 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:50:45 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (102279ms till timeout)
2022-03-28 12:50:46 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1272626ms till timeout)
2022-03-28 12:50:46 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (190626ms till timeout)
2022-03-28 12:50:46 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (772900ms till timeout)
2022-03-28 12:50:46 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:47 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1271514ms till timeout)
2022-03-28 12:50:47 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:47 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:50:47 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:50:47 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:50:47 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:50:47 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (100694ms till timeout)
2022-03-28 12:50:47 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:50:47 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:195] Consumer finished correctly: true
2022-03-28 12:50:47 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:198] Consumer consumed 100 messages
2022-03-28 12:50:47 [ForkJoinPool-1-worker-13] INFO  [UserST:386] Checking if SCRAM-SHA user is able to send messages
2022-03-28 12:50:48 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (189304ms till timeout)
2022-03-28 12:50:48 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (771621ms till timeout)
2022-03-28 12:50:48 [ForkJoinPool-1-worker-13] DEBUG [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@30c166f5, which are set.
2022-03-28 12:50:48 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@5a635f16, messages=[], arguments=[--topic, my-topic-1741595665-1144540447, --max-messages, 100, USER=top_secret_scramed_leopold, --bootstrap-server, my-cluster-c525f5e5-kafka-bootstrap.namespace-2.svc:9092], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-c525f5e5-plain-kafka-clients-5b57b99d55-vkfsx', podNamespace='namespace-2', bootstrapServer='my-cluster-c525f5e5-kafka-bootstrap.namespace-2.svc:9092', topicName='my-topic-1741595665-1144540447', maxMessages=100, kafkaUsername='top-secret-scramed-leopold', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@30c166f5}
2022-03-28 12:50:48 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:94] Producing 100 messages to my-cluster-c525f5e5-kafka-bootstrap.namespace-2.svc:9092:my-topic-1741595665-1144540447 from pod my-cluster-c525f5e5-plain-kafka-clients-5b57b99d55-vkfsx
2022-03-28 12:50:48 [ForkJoinPool-1-worker-13] INFO  [VerifiableClient:192] Client command: oc exec my-cluster-c525f5e5-plain-kafka-clients-5b57b99d55-vkfsx -n namespace-2 -- /opt/kafka/producer.sh --topic my-topic-1741595665-1144540447 --max-messages 100 USER=top_secret_scramed_leopold --bootstrap-server my-cluster-c525f5e5-kafka-bootstrap.namespace-2.svc:9092
2022-03-28 12:50:48 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc exec my-cluster-c525f5e5-plain-kafka-clients-5b57b99d55-vkfsx -n namespace-2 -- /opt/kafka/producer.sh --topic my-topic-1741595665-1144540447 --max-messages 100 USER=top_secret_scramed_leopold --bootstrap-server my-cluster-c525f5e5-kafka-bootstrap.namespace-2.svc:9092
2022-03-28 12:50:48 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1270402ms till timeout)
2022-03-28 12:50:48 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:49 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:49 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:50:49 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:50:49 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:50:49 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:50:49 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (99145ms till timeout)
2022-03-28 12:50:49 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (187981ms till timeout)
2022-03-28 12:50:49 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (770297ms till timeout)
2022-03-28 12:50:49 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1269290ms till timeout)
2022-03-28 12:50:50 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:50 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:50 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:50:50 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:50:50 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:50:50 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:50:50 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (97525ms till timeout)
2022-03-28 12:50:50 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (186659ms till timeout)
2022-03-28 12:50:50 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1268020ms till timeout)
2022-03-28 12:50:50 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (768977ms till timeout)
2022-03-28 12:50:51 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:52 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (185336ms till timeout)
2022-03-28 12:50:52 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1266697ms till timeout)
2022-03-28 12:50:52 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (767654ms till timeout)
2022-03-28 12:50:52 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:52 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:50:52 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:50:52 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:50:52 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:50:52 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (95943ms till timeout)
2022-03-28 12:50:52 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:97] Producer finished correctly: true
2022-03-28 12:50:52 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:101] Producer produced 100 messages
2022-03-28 12:50:52 [ForkJoinPool-1-worker-13] DEBUG [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@eab751c, which are set.
2022-03-28 12:50:52 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@7bca14b5, messages=[], arguments=[--topic, my-topic-1741595665-1144540447, --max-messages, 100, USER=top_secret_scramed_leopold, --group-id, my-consumer-group-1187770955, --group-instance-id, instance2012792764, --bootstrap-server, my-cluster-c525f5e5-kafka-bootstrap.namespace-2.svc:9092], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-c525f5e5-plain-kafka-clients-5b57b99d55-vkfsx', podNamespace='namespace-2', bootstrapServer='my-cluster-c525f5e5-kafka-bootstrap.namespace-2.svc:9092', topicName='my-topic-1741595665-1144540447', maxMessages=100, kafkaUsername='top-secret-scramed-leopold', consumerGroupName='my-consumer-group-1187770955', consumerInstanceId='instance2012792764', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@eab751c}
2022-03-28 12:50:52 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:157] Consuming 100 messages from my-cluster-c525f5e5-kafka-bootstrap.namespace-2.svc:9092#my-topic-1741595665-1144540447 from pod my-cluster-c525f5e5-plain-kafka-clients-5b57b99d55-vkfsx
2022-03-28 12:50:52 [ForkJoinPool-1-worker-13] INFO  [VerifiableClient:192] Client command: oc exec my-cluster-c525f5e5-plain-kafka-clients-5b57b99d55-vkfsx -n namespace-2 -- /opt/kafka/consumer.sh --topic my-topic-1741595665-1144540447 --max-messages 100 USER=top_secret_scramed_leopold --group-id my-consumer-group-1187770955 --group-instance-id instance2012792764 --bootstrap-server my-cluster-c525f5e5-kafka-bootstrap.namespace-2.svc:9092
2022-03-28 12:50:52 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc exec my-cluster-c525f5e5-plain-kafka-clients-5b57b99d55-vkfsx -n namespace-2 -- /opt/kafka/consumer.sh --topic my-topic-1741595665-1144540447 --max-messages 100 USER=top_secret_scramed_leopold --group-id my-consumer-group-1187770955 --group-instance-id instance2012792764 --bootstrap-server my-cluster-c525f5e5-kafka-bootstrap.namespace-2.svc:9092
2022-03-28 12:50:52 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:50:53 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:53 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1265372ms till timeout)
2022-03-28 12:50:53 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-965e9d1e will have desired state: Ready not ready, will try again in 1000 ms (766330ms till timeout)
2022-03-28 12:50:53 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (184010ms till timeout)
2022-03-28 12:50:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-scram-sha-st-shared-kafka-clients not ready, will try again in 10000 ms (469201ms till timeout)
2022-03-28 12:50:53 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:53 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:50:53 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:50:53 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:50:53 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:50:53 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (94429ms till timeout)
2022-03-28 12:50:54 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:444] Kafka: my-cluster-965e9d1e is in desired state: Ready
2022-03-28 12:50:54 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:155] Create/Update KafkaUser my-user-1295334081-1602801897 in namespace namespace-3
2022-03-28 12:50:54 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:164] Using Namespace: namespace-3
2022-03-28 12:50:54 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1264152ms till timeout)
2022-03-28 12:50:54 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (182649ms till timeout)
2022-03-28 12:50:54 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-1295334081-1602801897
2022-03-28 12:50:54 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:54 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-1295334081-1602801897 will have desired state: Ready
2022-03-28 12:50:54 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-1295334081-1602801897 will have desired state: Ready
2022-03-28 12:50:54 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] KafkaUser: my-user-1295334081-1602801897 will have desired state: Ready not ready, will try again in 1000 ms (179890ms till timeout)
2022-03-28 12:50:55 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:55 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:50:55 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:50:55 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:50:55 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:50:55 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (92888ms till timeout)
2022-03-28 12:50:55 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1263041ms till timeout)
2022-03-28 12:50:56 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (181326ms till timeout)
2022-03-28 12:50:56 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:444] KafkaUser: my-user-1295334081-1602801897 is in desired state: Ready
2022-03-28 12:50:56 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-1295334081-1602801897 will have desired state: Ready
2022-03-28 12:50:56 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-1295334081-1602801897 will have desired state: Ready
2022-03-28 12:50:56 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:56 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:444] KafkaUser: my-user-1295334081-1602801897 is in desired state: Ready
2022-03-28 12:50:56 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:674] ============================================================================
2022-03-28 12:50:56 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:675] [operators.user.UserST - After Each] - Clean up after test
2022-03-28 12:50:56 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:344] ############################################################################
2022-03-28 12:50:56 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:348] Delete all resources for testTlsExternalUser
2022-03-28 12:50:56 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:241] Delete of KafkaUser my-user-1295334081-1602801897 in namespace namespace-3
2022-03-28 12:50:56 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1295334081-1602801897
2022-03-28 12:50:56 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1261907ms till timeout)
2022-03-28 12:50:56 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:56 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:50:56 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:50:56 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:50:56 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:50:56 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (91317ms till timeout)
2022-03-28 12:50:56 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:241] Delete of Kafka my-cluster-965e9d1e in namespace namespace-3
2022-03-28 12:50:57 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-965e9d1e
2022-03-28 12:50:57 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (180087ms till timeout)
2022-03-28 12:50:57 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:369] ############################################################################
2022-03-28 12:50:57 [ForkJoinPool-1-worker-1] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-3 for test case:testTlsExternalUser
2022-03-28 12:50:57 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Namespace namespace-3 removal
2022-03-28 12:50:57 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace namespace-3 -o yaml
2022-03-28 12:50:57 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:50:57 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:57 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-4 get Namespace namespace-3 -o yaml
2022-03-28 12:50:57 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 12:50:57 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (479530ms till timeout)
2022-03-28 12:50:57 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1260793ms till timeout)
2022-03-28 12:50:58 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:58 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:50:58 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:50:58 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:50:58 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:50:58 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (89781ms till timeout)
2022-03-28 12:50:58 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (178763ms till timeout)
2022-03-28 12:50:58 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace namespace-3 -o yaml
2022-03-28 12:50:59 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1259680ms till timeout)
2022-03-28 12:50:59 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-4 get Namespace namespace-3 -o yaml
2022-03-28 12:50:59 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 12:50:59 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (478088ms till timeout)
2022-03-28 12:50:59 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:50:59 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (177431ms till timeout)
2022-03-28 12:51:00 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:00 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:51:00 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:51:00 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:51:00 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:51:00 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (88182ms till timeout)
2022-03-28 12:51:00 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1258568ms till timeout)
2022-03-28 12:51:00 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace namespace-3 -o yaml
2022-03-28 12:51:00 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-4 get Namespace namespace-3 -o yaml
2022-03-28 12:51:00 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 12:51:00 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (476566ms till timeout)
2022-03-28 12:51:01 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:01 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (176105ms till timeout)
2022-03-28 12:51:01 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1257454ms till timeout)
2022-03-28 12:51:01 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:01 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:51:01 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:51:01 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:51:01 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:51:01 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (86627ms till timeout)
2022-03-28 12:51:01 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace namespace-3 -o yaml
2022-03-28 12:51:02 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-4 get Namespace namespace-3 -o yaml
2022-03-28 12:51:02 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 12:51:02 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (475012ms till timeout)
2022-03-28 12:51:02 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:02 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1256118ms till timeout)
2022-03-28 12:51:02 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (174756ms till timeout)
2022-03-28 12:51:02 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:51:03 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:03 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:51:03 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:51:03 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:51:03 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:51:03 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (85111ms till timeout)
2022-03-28 12:51:03 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace namespace-3 -o yaml
2022-03-28 12:51:03 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1255007ms till timeout)
2022-03-28 12:51:03 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-4 get Namespace namespace-3 -o yaml
2022-03-28 12:51:03 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 12:51:03 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (473581ms till timeout)
2022-03-28 12:51:04 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (173327ms till timeout)
2022-03-28 12:51:04 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:04 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-scram-sha-st-shared-kafka-clients not ready, will try again in 10000 ms (458624ms till timeout)
2022-03-28 12:51:04 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:04 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:51:04 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:51:04 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:51:04 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:51:04 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (83599ms till timeout)
2022-03-28 12:51:04 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1253895ms till timeout)
2022-03-28 12:51:04 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace namespace-3 -o yaml
2022-03-28 12:51:05 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-4 get Namespace namespace-3 -o yaml
2022-03-28 12:51:05 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 12:51:05 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (472131ms till timeout)
2022-03-28 12:51:05 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (172005ms till timeout)
2022-03-28 12:51:05 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:05 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1252781ms till timeout)
2022-03-28 12:51:06 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:06 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:51:06 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:51:06 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:51:06 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:51:06 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (82060ms till timeout)
2022-03-28 12:51:06 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace namespace-3 -o yaml
2022-03-28 12:51:06 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (170683ms till timeout)
2022-03-28 12:51:06 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-4 get Namespace namespace-3 -o yaml
2022-03-28 12:51:06 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 12:51:06 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (470644ms till timeout)
2022-03-28 12:51:07 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1251672ms till timeout)
2022-03-28 12:51:07 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:07 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:07 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:51:07 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:51:07 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:51:07 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:51:07 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (80544ms till timeout)
2022-03-28 12:51:07 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:51:07 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace namespace-3 -o yaml
2022-03-28 12:51:08 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (169361ms till timeout)
2022-03-28 12:51:08 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1250559ms till timeout)
2022-03-28 12:51:08 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-4 get Namespace namespace-3 -o yaml
2022-03-28 12:51:08 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 12:51:08 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (469235ms till timeout)
2022-03-28 12:51:08 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:09 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace namespace-3 -o yaml
2022-03-28 12:51:09 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:09 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:51:09 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:51:09 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:51:09 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:51:09 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (78972ms till timeout)
2022-03-28 12:51:09 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (168038ms till timeout)
2022-03-28 12:51:09 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1249399ms till timeout)
2022-03-28 12:51:09 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-4 get Namespace namespace-3 -o yaml
2022-03-28 12:51:09 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 12:51:09 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (467812ms till timeout)
2022-03-28 12:51:10 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:10 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace namespace-3 -o yaml
2022-03-28 12:51:10 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (166716ms till timeout)
2022-03-28 12:51:10 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1248077ms till timeout)
2022-03-28 12:51:10 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:10 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:51:10 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:51:10 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:51:10 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:51:10 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (77427ms till timeout)
2022-03-28 12:51:11 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-4 get Namespace namespace-3 -o yaml
2022-03-28 12:51:11 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 12:51:11 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (466368ms till timeout)
2022-03-28 12:51:11 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:12 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (165392ms till timeout)
2022-03-28 12:51:12 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1246754ms till timeout)
2022-03-28 12:51:12 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace namespace-3 -o yaml
2022-03-28 12:51:12 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:12 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:51:12 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:51:12 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:51:12 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:51:12 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (75896ms till timeout)
2022-03-28 12:51:12 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-4 get Namespace namespace-3 -o yaml
2022-03-28 12:51:12 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 12:51:12 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (464940ms till timeout)
2022-03-28 12:51:12 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:51:13 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:13 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (164064ms till timeout)
2022-03-28 12:51:13 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1245426ms till timeout)
2022-03-28 12:51:13 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace namespace-3 -o yaml
2022-03-28 12:51:13 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:13 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:51:13 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:51:13 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:51:13 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:51:13 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (74311ms till timeout)
2022-03-28 12:51:14 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-4 get Namespace namespace-3 -o yaml
2022-03-28 12:51:14 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 12:51:14 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (463457ms till timeout)
2022-03-28 12:51:14 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of KafkaBridge http-bridge-scram-sha-cluster-name in namespace http-bridge-scram-sha-st
2022-03-28 12:51:14 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1244249ms till timeout)
2022-03-28 12:51:14 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (162739ms till timeout)
2022-03-28 12:51:14 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaBridge:http-bridge-scram-sha-cluster-name
2022-03-28 12:51:14 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:14 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of KafkaUser my-user-716724139-1524027490 in namespace http-bridge-scram-sha-st
2022-03-28 12:51:15 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace namespace-3 -o yaml
2022-03-28 12:51:15 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-716724139-1524027490
2022-03-28 12:51:15 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of Kafka http-bridge-scram-sha-cluster-name in namespace http-bridge-scram-sha-st
2022-03-28 12:51:15 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:http-bridge-scram-sha-cluster-name
2022-03-28 12:51:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:http-bridge-scram-sha-cluster-name not ready, will try again in 10000 ms (839874ms till timeout)
2022-03-28 12:51:15 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-4 get Namespace namespace-3 -o yaml
2022-03-28 12:51:15 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 12:51:15 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (461956ms till timeout)
2022-03-28 12:51:15 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:15 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:51:15 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:51:15 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:51:15 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:51:15 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (72705ms till timeout)
2022-03-28 12:51:15 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1243134ms till timeout)
2022-03-28 12:51:15 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (161612ms till timeout)
2022-03-28 12:51:16 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace namespace-3 -o yaml
2022-03-28 12:51:16 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:16 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1242022ms till timeout)
2022-03-28 12:51:16 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-4 get Namespace namespace-3 -o yaml
2022-03-28 12:51:16 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 12:51:16 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (460482ms till timeout)
2022-03-28 12:51:17 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:17 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:51:17 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:51:17 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:51:17 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:51:17 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (71171ms till timeout)
2022-03-28 12:51:17 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (160289ms till timeout)
2022-03-28 12:51:17 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:51:17 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1240909ms till timeout)
2022-03-28 12:51:17 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:160] Consumer finished correctly: true
2022-03-28 12:51:17 [ForkJoinPool-1-worker-13] INFO  [InternalKafkaClient:163] Consumer consumed 100 messages
2022-03-28 12:51:17 [ForkJoinPool-1-worker-13] INFO  [UserST:392] Checking owner reference - if the secret will be deleted when we delete KafkaUser
2022-03-28 12:51:17 [ForkJoinPool-1-worker-13] INFO  [UserST:394] Deleting KafkaUser:encrypted-leopold
2022-03-28 12:51:17 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace namespace-3 -o yaml
2022-03-28 12:51:18 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:18 [ForkJoinPool-1-worker-13] INFO  [KafkaUserUtils:62] Waiting for KafkaUser deletion encrypted-leopold
2022-03-28 12:51:18 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaUser deletion encrypted-leopold
2022-03-28 12:51:18 [ForkJoinPool-1-worker-13] INFO  [KafkaUserUtils:75] KafkaUser encrypted-leopold deleted
2022-03-28 12:51:18 [ForkJoinPool-1-worker-13] INFO  [UserST:398] Deleting KafkaUser:scramed-leopold
2022-03-28 12:51:18 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-4 get Namespace namespace-3 -o yaml
2022-03-28 12:51:18 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 12:51:18 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (459018ms till timeout)
2022-03-28 12:51:18 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (158894ms till timeout)
2022-03-28 12:51:18 [ForkJoinPool-1-worker-13] INFO  [KafkaUserUtils:62] Waiting for KafkaUser deletion scramed-leopold
2022-03-28 12:51:18 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaUser deletion scramed-leopold
2022-03-28 12:51:18 [ForkJoinPool-1-worker-13] INFO  [KafkaUserUtils:75] KafkaUser scramed-leopold deleted
2022-03-28 12:51:18 [ForkJoinPool-1-worker-13] INFO  [UserST:402] Checking if secrets are deleted
2022-03-28 12:51:18 [ForkJoinPool-1-worker-13] INFO  [SecretUtils:54] Waiting for Secret deletion top-secret-encrypted-leopold
2022-03-28 12:51:18 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Expected secret top-secret-encrypted-leopold deleted
2022-03-28 12:51:18 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:18 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:51:18 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:51:18 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:51:18 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:51:18 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (69567ms till timeout)
2022-03-28 12:51:18 [ForkJoinPool-1-worker-13] INFO  [SecretUtils:58] Secret top-secret-encrypted-leopold deleted
2022-03-28 12:51:18 [ForkJoinPool-1-worker-13] INFO  [SecretUtils:54] Waiting for Secret deletion top-secret-scramed-leopold
2022-03-28 12:51:18 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Expected secret top-secret-scramed-leopold deleted
2022-03-28 12:51:18 [ForkJoinPool-1-worker-13] INFO  [SecretUtils:58] Secret top-secret-scramed-leopold deleted
2022-03-28 12:51:19 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1239704ms till timeout)
2022-03-28 12:51:19 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:674] ============================================================================
2022-03-28 12:51:19 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:675] [operators.user.UserST - After Each] - Clean up after test
2022-03-28 12:51:19 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:344] ############################################################################
2022-03-28 12:51:19 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:348] Delete all resources for testCreatingUsersWithSecretPrefix
2022-03-28 12:51:19 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of KafkaUser scramed-leopold in namespace namespace-2
2022-03-28 12:51:19 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:scramed-leopold
2022-03-28 12:51:19 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of Deployment my-cluster-c525f5e5-plain-kafka-clients in namespace namespace-2
2022-03-28 12:51:19 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace namespace-3 -o yaml
2022-03-28 12:51:19 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-c525f5e5-plain-kafka-clients
2022-03-28 12:51:19 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:19 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (157568ms till timeout)
2022-03-28 12:51:19 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-4 get Namespace namespace-3 -o yaml
2022-03-28 12:51:19 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 12:51:19 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (457549ms till timeout)
2022-03-28 12:51:19 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-c525f5e5-plain-kafka-clients not ready, will try again in 10000 ms (479561ms till timeout)
2022-03-28 12:51:20 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1238594ms till timeout)
2022-03-28 12:51:20 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:20 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:51:20 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:51:20 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:51:20 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:51:20 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (68008ms till timeout)
2022-03-28 12:51:20 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace namespace-3 -o yaml
2022-03-28 12:51:21 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (156350ms till timeout)
2022-03-28 12:51:21 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:21 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1237481ms till timeout)
2022-03-28 12:51:21 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-4 get Namespace namespace-3 -o yaml
2022-03-28 12:51:21 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 12:51:21 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (456066ms till timeout)
2022-03-28 12:51:21 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:21 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:51:21 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:51:21 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:51:21 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:51:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (66454ms till timeout)
2022-03-28 12:51:22 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (155133ms till timeout)
2022-03-28 12:51:22 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace namespace-3 -o yaml
2022-03-28 12:51:22 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1236365ms till timeout)
2022-03-28 12:51:22 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:51:22 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:22 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-4 get Namespace namespace-3 -o yaml
2022-03-28 12:51:22 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 12:51:22 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (454612ms till timeout)
2022-03-28 12:51:23 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:23 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:51:23 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:51:23 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:51:23 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:51:23 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (64929ms till timeout)
2022-03-28 12:51:23 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (153697ms till timeout)
2022-03-28 12:51:23 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1235058ms till timeout)
2022-03-28 12:51:23 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace namespace-3 -o yaml
2022-03-28 12:51:24 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-4 get Namespace namespace-3 -o yaml
2022-03-28 12:51:24 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 12:51:24 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (453190ms till timeout)
2022-03-28 12:51:24 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:24 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:24 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:51:24 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:51:24 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:51:24 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:51:24 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (63396ms till timeout)
2022-03-28 12:51:25 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (152375ms till timeout)
2022-03-28 12:51:25 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1233737ms till timeout)
2022-03-28 12:51:25 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace namespace-3 -o yaml
2022-03-28 12:51:25 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:369] ############################################################################
2022-03-28 12:51:25 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Namespace http-bridge-scram-sha-st removal
2022-03-28 12:51:25 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 12:51:25 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-4 get Namespace namespace-3 -o yaml
2022-03-28 12:51:25 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 12:51:25 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-3 removal not ready, will try again in 1000 ms (451758ms till timeout)
2022-03-28 12:51:25 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:26 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-4 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 12:51:26 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 12:51:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (479572ms till timeout)
2022-03-28 12:51:26 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (151055ms till timeout)
2022-03-28 12:51:26 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1232416ms till timeout)
2022-03-28 12:51:26 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:26 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:51:26 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:51:26 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:51:26 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:51:26 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (61844ms till timeout)
2022-03-28 12:51:26 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace namespace-3 -o yaml
2022-03-28 12:51:27 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 12:51:27 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-4 get Namespace namespace-3 -o yaml
2022-03-28 12:51:27 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 1
2022-03-28 12:51:27 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 12:51:27 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-3" not found
2022-03-28 12:51:27 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] ======STDERR END======
2022-03-28 12:51:27 [ForkJoinPool-1-worker-1] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@3881d5f2=[namespace-4], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[http-bridge-scram-sha-st], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-1], io.strimzi.test.logs.CollectorElement@c45aa4b1=[namespace-2], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 12:51:27 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:267] testTlsExternalUser - Notifies waiting test cases:[testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testTlsExternalUserWithQuotas, testUserTemplate, testUpdateUser, testTlsExternalUser, testReceiveSimpleMessageTlsScramSha, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testSendSimpleMessageTlsScramSha] to and randomly select one to start execution
2022-03-28 12:51:27 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:93] [operators.user.UserST] - Removing parallel test: testTlsExternalUser
2022-03-28 12:51:27 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:97] [operators.user.UserST] - Parallel test count: 4
2022-03-28 12:51:27 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testTlsExternalUser-FINISHED
2022-03-28 12:51:27 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:30] ############################################################################
2022-03-28 12:51:27 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:23] ############################################################################
2022-03-28 12:51:27 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testUserWithNameMoreThan64Chars-STARTED
2022-03-28 12:51:27 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:658] ============================================================================
2022-03-28 12:51:27 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:659] [operators.user.UserST - Before Each] - Setup test case environment
2022-03-28 12:51:27 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:77] [operators.user.UserST] - Adding parallel test: testUserWithNameMoreThan64Chars
2022-03-28 12:51:27 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:81] [operators.user.UserST] - Parallel test count: 5
2022-03-28 12:51:27 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:230] testUserWithNameMoreThan64Chars test now can proceed its execution
2022-03-28 12:51:27 [ForkJoinPool-1-worker-1] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 12:51:27 [ForkJoinPool-1-worker-1] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d, testTlsExternalUserWithQuotas=my-cluster-3e2e11de, testUserWithNameMoreThan64Chars=my-cluster-ed39a269, testReceiveSimpleMessageTlsScramSha=my-cluster-455915b9, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2, testCapacityFile=my-cluster-39ee3f62, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5, testUpdateUser=my-cluster-e16ccf16, testTlsExternalUser=my-cluster-965e9d1e, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-31feb41b, testSendSimpleMessageTlsScramSha=my-cluster-27773859, testUserTemplate=my-cluster-157df94e}
2022-03-28 12:51:27 [ForkJoinPool-1-worker-1] TRACE [AbstractST:607] USERS_NAME_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-user-467839358-1504804463, testTlsExternalUserWithQuotas=my-user-1786174319-1274634874, testUserWithNameMoreThan64Chars=my-user-1048816470-1103470031, testReceiveSimpleMessageTlsScramSha=my-user-1329598231-1183867861, testThrottlingQuotasCreateTopic=my-user-896662438-260898887, testCapacityFile=my-user-597365340-661017603, testCreatingUsersWithSecretPrefix=my-user-1928899390-1444689439, testUpdateUser=my-user-1694542041-806176635, testTlsExternalUser=my-user-1295334081-1602801897, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2100837352-1014792902, testSendSimpleMessageTlsScramSha=my-user-507582460-1003875464, testUserTemplate=my-user-1253881534-190261188}
2022-03-28 12:51:27 [ForkJoinPool-1-worker-1] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-712754859-1910580676, testTlsExternalUserWithQuotas=my-topic-593679216-1309035072, testUserWithNameMoreThan64Chars=my-topic-1603054043-1686316417, testReceiveSimpleMessageTlsScramSha=my-topic-170116874-1830928683, testThrottlingQuotasCreateTopic=my-topic-1532506723-1472611013, testCapacityFile=my-topic-1390944843-775067696, testCreatingUsersWithSecretPrefix=my-topic-2033928996-1209892355, testUpdateUser=my-topic-1239518257-2056139809, testTlsExternalUser=my-topic-476529121-306489993, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1084162749-1576061083, testSendSimpleMessageTlsScramSha=my-topic-62182003-1963040205, testUserTemplate=my-topic-515111334-943904347}
2022-03-28 12:51:27 [ForkJoinPool-1-worker-1] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-3e2e11de-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-ed39a269-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-455915b9-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2-kafka-clients, testCapacityFile=my-cluster-39ee3f62-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5-kafka-clients, testUpdateUser=my-cluster-e16ccf16-kafka-clients, testTlsExternalUser=my-cluster-965e9d1e-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-31feb41b-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-27773859-kafka-clients, testUserTemplate=my-cluster-157df94e-kafka-clients}
2022-03-28 12:51:27 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:155] Create/Update KafkaUser user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq in namespace user-st
2022-03-28 12:51:27 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq
2022-03-28 12:51:27 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:27 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:433] Wait for KafkaUser: user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq will have desired state: Ready
2022-03-28 12:51:27 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for KafkaUser: user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq will have desired state: Ready
2022-03-28 12:51:27 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-4 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 12:51:27 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 12:51:27 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (478122ms till timeout)
2022-03-28 12:51:27 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1231244ms till timeout)
2022-03-28 12:51:27 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:51:27 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (149673ms till timeout)
2022-03-28 12:51:27 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:444] KafkaUser: user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq is in desired state: Ready
2022-03-28 12:51:27 [ForkJoinPool-1-worker-1] INFO  [KafkaUserUtils:90] Wait until KafkaUser user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq status is available
2022-03-28 12:51:27 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for KafkaUser user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq status is available
2022-03-28 12:51:27 [ForkJoinPool-1-worker-1] INFO  [KafkaUserUtils:95] KafkaUser user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq status is available
2022-03-28 12:51:27 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:27 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:51:27 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:51:27 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:51:27 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:51:27 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (60316ms till timeout)
2022-03-28 12:51:27 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:155] Create/Update KafkaUser sasl-userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef in namespace user-st
2022-03-28 12:51:28 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:sasl-userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef
2022-03-28 12:51:28 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:433] Wait for KafkaUser: sasl-userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef will have desired state: Ready
2022-03-28 12:51:28 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for KafkaUser: sasl-userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef will have desired state: Ready
2022-03-28 12:51:28 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:444] KafkaUser: sasl-userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef is in desired state: Ready
2022-03-28 12:51:28 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:155] Create/Update KafkaUser userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdefghijk in namespace user-st
2022-03-28 12:51:28 [ForkJoinPool-1-worker-1] INFO  [KafkaUserUtils:90] Wait until KafkaUser userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdefghijk status is available
2022-03-28 12:51:28 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for KafkaUser userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdefghijk status is available
2022-03-28 12:51:28 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 12:51:28 [ForkJoinPool-1-worker-1] INFO  [KafkaUserUtils:95] KafkaUser userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdefghijk status is available
2022-03-28 12:51:28 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1230129ms till timeout)
2022-03-28 12:51:28 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:674] ============================================================================
2022-03-28 12:51:28 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:675] [operators.user.UserST - After Each] - Clean up after test
2022-03-28 12:51:28 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:344] ############################################################################
2022-03-28 12:51:28 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:348] Delete all resources for testUserWithNameMoreThan64Chars
2022-03-28 12:51:28 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:241] Delete of KafkaUser sasl-userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef in namespace user-st
2022-03-28 12:51:28 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (148543ms till timeout)
2022-03-28 12:51:28 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:28 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-4 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 12:51:28 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 12:51:28 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (476683ms till timeout)
2022-03-28 12:51:28 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:sasl-userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdef
2022-03-28 12:51:29 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:241] Delete of KafkaUser userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdefghijk in namespace user-st
2022-03-28 12:51:29 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:userabcdefghijklmnopqrstuvxyzabcdefghijklmnopqrstuvxyzabcdefghijk
2022-03-28 12:51:29 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:241] Delete of KafkaUser user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq in namespace user-st
2022-03-28 12:51:29 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:user-with-correct-nameabcdefghijklmnopqrstuvxyzabcdefghijklmnopq
2022-03-28 12:51:29 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:29 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:51:29 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:51:29 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:51:29 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:51:29 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (58795ms till timeout)
2022-03-28 12:51:29 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:369] ############################################################################
2022-03-28 12:51:29 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:267] testUserWithNameMoreThan64Chars - Notifies waiting test cases:[testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testTlsExternalUserWithQuotas, testUserTemplate, testUpdateUser, testTlsExternalUser, testReceiveSimpleMessageTlsScramSha, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testSendSimpleMessageTlsScramSha, testUserWithNameMoreThan64Chars] to and randomly select one to start execution
2022-03-28 12:51:29 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:93] [operators.user.UserST] - Removing parallel test: testUserWithNameMoreThan64Chars
2022-03-28 12:51:29 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:97] [operators.user.UserST] - Parallel test count: 4
2022-03-28 12:51:29 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testUserWithNameMoreThan64Chars-FINISHED
2022-03-28 12:51:29 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:30] ############################################################################
2022-03-28 12:51:29 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:23] ############################################################################
2022-03-28 12:51:29 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testTlsUserWithQuotas-STARTED
2022-03-28 12:51:29 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:658] ============================================================================
2022-03-28 12:51:29 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:659] [operators.user.UserST - Before Each] - Setup test case environment
2022-03-28 12:51:29 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:77] [operators.user.UserST] - Adding parallel test: testTlsUserWithQuotas
2022-03-28 12:51:29 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:81] [operators.user.UserST] - Parallel test count: 5
2022-03-28 12:51:29 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:230] testTlsUserWithQuotas test now can proceed its execution
2022-03-28 12:51:29 [ForkJoinPool-1-worker-1] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 12:51:29 [ForkJoinPool-1-worker-1] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-455915b9, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5, testUpdateUser=my-cluster-e16ccf16, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d, testTlsExternalUserWithQuotas=my-cluster-3e2e11de, testUserWithNameMoreThan64Chars=my-cluster-ed39a269, testTlsUserWithQuotas=my-cluster-3b191485, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2, testCapacityFile=my-cluster-39ee3f62, testTlsExternalUser=my-cluster-965e9d1e, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-31feb41b, testSendSimpleMessageTlsScramSha=my-cluster-27773859, testUserTemplate=my-cluster-157df94e}
2022-03-28 12:51:29 [ForkJoinPool-1-worker-1] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-1329598231-1183867861, testCreatingUsersWithSecretPrefix=my-user-1928899390-1444689439, testUpdateUser=my-user-1694542041-806176635, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-467839358-1504804463, testTlsExternalUserWithQuotas=my-user-1786174319-1274634874, testUserWithNameMoreThan64Chars=my-user-1048816470-1103470031, testTlsUserWithQuotas=my-user-420001054-1296284129, testThrottlingQuotasCreateTopic=my-user-896662438-260898887, testCapacityFile=my-user-597365340-661017603, testTlsExternalUser=my-user-1295334081-1602801897, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2100837352-1014792902, testSendSimpleMessageTlsScramSha=my-user-507582460-1003875464, testUserTemplate=my-user-1253881534-190261188}
2022-03-28 12:51:29 [ForkJoinPool-1-worker-1] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-170116874-1830928683, testCreatingUsersWithSecretPrefix=my-topic-2033928996-1209892355, testUpdateUser=my-topic-1239518257-2056139809, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-712754859-1910580676, testTlsExternalUserWithQuotas=my-topic-593679216-1309035072, testUserWithNameMoreThan64Chars=my-topic-1603054043-1686316417, testTlsUserWithQuotas=my-topic-1752299353-1315959033, testThrottlingQuotasCreateTopic=my-topic-1532506723-1472611013, testCapacityFile=my-topic-1390944843-775067696, testTlsExternalUser=my-topic-476529121-306489993, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1084162749-1576061083, testSendSimpleMessageTlsScramSha=my-topic-62182003-1963040205, testUserTemplate=my-topic-515111334-943904347}
2022-03-28 12:51:29 [ForkJoinPool-1-worker-1] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-455915b9-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5-kafka-clients, testUpdateUser=my-cluster-e16ccf16-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-3e2e11de-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-ed39a269-kafka-clients, testTlsUserWithQuotas=my-cluster-3b191485-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2-kafka-clients, testCapacityFile=my-cluster-39ee3f62-kafka-clients, testTlsExternalUser=my-cluster-965e9d1e-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-31feb41b-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-27773859-kafka-clients, testUserTemplate=my-cluster-157df94e-kafka-clients}
2022-03-28 12:51:29 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:155] Create/Update KafkaUser encrypted-arnost in namespace user-st
2022-03-28 12:51:29 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:encrypted-arnost
2022-03-28 12:51:29 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1229005ms till timeout)
2022-03-28 12:51:29 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:433] Wait for KafkaUser: encrypted-arnost will have desired state: Ready
2022-03-28 12:51:29 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for KafkaUser: encrypted-arnost will have desired state: Ready
2022-03-28 12:51:29 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 12:51:29 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (147422ms till timeout)
2022-03-28 12:51:30 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] KafkaUser: encrypted-arnost will have desired state: Ready not ready, will try again in 1000 ms (179782ms till timeout)
2022-03-28 12:51:30 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-c525f5e5-plain-kafka-clients not ready, will try again in 10000 ms (469202ms till timeout)
2022-03-28 12:51:30 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:30 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-4 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 12:51:30 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 12:51:30 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (475148ms till timeout)
2022-03-28 12:51:30 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1227891ms till timeout)
2022-03-28 12:51:31 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (146203ms till timeout)
2022-03-28 12:51:31 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:444] KafkaUser: encrypted-arnost is in desired state: Ready
2022-03-28 12:51:31 [ForkJoinPool-1-worker-1] DEBUG [UserST:274] Command for kafka-configs.sh bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=encrypted-arnost
2022-03-28 12:51:31 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=encrypted-arnost
2022-03-28 12:51:31 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 12:51:31 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1226778ms till timeout)
2022-03-28 12:51:32 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (144881ms till timeout)
2022-03-28 12:51:32 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:51:33 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1225667ms till timeout)
2022-03-28 12:51:33 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (143555ms till timeout)
2022-03-28 12:51:34 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1224546ms till timeout)
2022-03-28 12:51:35 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (142231ms till timeout)
2022-03-28 12:51:35 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1223435ms till timeout)
2022-03-28 12:51:36 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:36 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:51:36 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:51:36 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:51:36 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:51:36 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (52122ms till timeout)
2022-03-28 12:51:36 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (140907ms till timeout)
2022-03-28 12:51:36 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-31feb41b will have desired state: Ready not ready, will try again in 1000 ms (1222268ms till timeout)
2022-03-28 12:51:37 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-4 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 12:51:37 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 12:51:37 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace http-bridge-scram-sha-st removal not ready, will try again in 1000 ms (468612ms till timeout)
2022-03-28 12:51:37 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:37 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:51:37 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:37 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:51:37 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:51:37 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:51:37 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:51:37 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (50453ms till timeout)
2022-03-28 12:51:37 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (139584ms till timeout)
2022-03-28 12:51:37 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:444] Kafka: my-cluster-31feb41b is in desired state: Ready
2022-03-28 12:51:38 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 12:51:38 [ForkJoinPool-1-worker-3] INFO  [CruiseControlConfigurationST:157] Changing the broker capacity of the cruise control
2022-03-28 12:51:38 [ForkJoinPool-1-worker-3] INFO  [CruiseControlConfigurationST:168] Verifying that CC pod is rolling, because of change size of disk
2022-03-28 12:51:38 [ForkJoinPool-1-worker-3] INFO  [DeploymentUtils:136] Waiting for Deployment my-cluster-31feb41b-cruise-control rolling update
2022-03-28 12:51:38 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Deployment my-cluster-31feb41b-cruise-control rolling update in namespace:namespace-4
2022-03-28 12:51:38 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-31feb41b-cruise-control-799978bc54-q58sp=73e1002d-1930-48bf-855d-d34e3786adc0}
2022-03-28 12:51:38 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-31feb41b-cruise-control-799978bc54-q58sp=73e1002d-1930-48bf-855d-d34e3786adc0}
2022-03-28 12:51:38 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-31feb41b-cruise-control-799978bc54-q58sp=73e1002d-1930-48bf-855d-d34e3786adc0}
2022-03-28 12:51:38 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Deployment my-cluster-31feb41b-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (599771ms till timeout)
2022-03-28 12:51:38 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:38 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (138468ms till timeout)
2022-03-28 12:51:40 [ForkJoinPool-1-worker-1] INFO  [Exec:417] Command: oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=encrypted-arnost
2022-03-28 12:51:40 [ForkJoinPool-1-worker-1] INFO  [Exec:417] Return code: 0
2022-03-28 12:51:40 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (137148ms till timeout)
2022-03-28 12:51:40 [ForkJoinPool-1-worker-1] INFO  [KafkaUserUtils:62] Waiting for KafkaUser deletion encrypted-arnost
2022-03-28 12:51:40 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for KafkaUser deletion encrypted-arnost
2022-03-28 12:51:40 [ForkJoinPool-1-worker-1] INFO  [KafkaUserUtils:75] KafkaUser encrypted-arnost deleted
2022-03-28 12:51:40 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for all KafkaUser CN=encrypted-arnost attributes will be cleaned
2022-03-28 12:51:40 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=encrypted-arnost
2022-03-28 12:51:40 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-c525f5e5-plain-kafka-clients not ready, will try again in 10000 ms (458733ms till timeout)
2022-03-28 12:51:41 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (135932ms till timeout)
2022-03-28 12:51:42 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:164] HttpBridgeTlsST is waiting to proceed with execution but current thread exceed maximum of allowed test suites in parallel. (7/6)
2022-03-28 12:51:42 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (134614ms till timeout)
2022-03-28 12:51:43 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-4 get Namespace http-bridge-scram-sha-st -o yaml
2022-03-28 12:51:43 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 1
2022-03-28 12:51:43 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 12:51:43 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Error from server (NotFound): namespaces "http-bridge-scram-sha-st" not found
2022-03-28 12:51:43 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] ======STDERR END======
2022-03-28 12:51:43 [ForkJoinPool-1-worker-7] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@3881d5f2=[namespace-4], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-1], io.strimzi.test.logs.CollectorElement@c45aa4b1=[namespace-2], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 12:51:43 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:254] HttpBridgeScramShaST - Notifies waiting test suites:[UserST, TopicST, ReconciliationST, CruiseControlConfigurationST, HttpBridgeScramShaST, ThrottlingQuotaST, HttpBridgeTlsST] to and randomly select one to start execution
2022-03-28 12:51:43 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:85] [bridge.HttpBridgeScramShaST] - Removing parallel suite: HttpBridgeScramShaST
2022-03-28 12:51:43 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:89] [bridge.HttpBridgeScramShaST] - Parallel suites count: 6
[INFO] Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 353.658 s - in io.strimzi.systemtest.bridge.HttpBridgeScramShaST
2022-03-28 12:51:43 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:23] ############################################################################
2022-03-28 12:51:43 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationFileIsCreated-STARTED
2022-03-28 12:51:43 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:658] ============================================================================
2022-03-28 12:51:43 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:659] [cruisecontrol.CruiseControlConfigurationST - Before Each] - Setup test case environment
2022-03-28 12:51:43 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:77] [cruisecontrol.CruiseControlConfigurationST] - Adding parallel test: testConfigurationFileIsCreated
2022-03-28 12:51:43 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:81] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 6
2022-03-28 12:51:43 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:230] testConfigurationFileIsCreated test now can proceed its execution
2022-03-28 12:51:43 [ForkJoinPool-1-worker-7] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 12:51:43 [ForkJoinPool-1-worker-7] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-455915b9, testConfigurationFileIsCreated=my-cluster-020ceaf8, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5, testUpdateUser=my-cluster-e16ccf16, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d, testTlsExternalUserWithQuotas=my-cluster-3e2e11de, testUserWithNameMoreThan64Chars=my-cluster-ed39a269, testTlsUserWithQuotas=my-cluster-3b191485, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2, testCapacityFile=my-cluster-39ee3f62, testTlsExternalUser=my-cluster-965e9d1e, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-31feb41b, testSendSimpleMessageTlsScramSha=my-cluster-27773859, testUserTemplate=my-cluster-157df94e}
2022-03-28 12:51:43 [ForkJoinPool-1-worker-7] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-1329598231-1183867861, testConfigurationFileIsCreated=my-user-1609977058-1196012647, testCreatingUsersWithSecretPrefix=my-user-1928899390-1444689439, testUpdateUser=my-user-1694542041-806176635, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-467839358-1504804463, testTlsExternalUserWithQuotas=my-user-1786174319-1274634874, testUserWithNameMoreThan64Chars=my-user-1048816470-1103470031, testTlsUserWithQuotas=my-user-420001054-1296284129, testThrottlingQuotasCreateTopic=my-user-896662438-260898887, testCapacityFile=my-user-597365340-661017603, testTlsExternalUser=my-user-1295334081-1602801897, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2100837352-1014792902, testSendSimpleMessageTlsScramSha=my-user-507582460-1003875464, testUserTemplate=my-user-1253881534-190261188}
2022-03-28 12:51:43 [ForkJoinPool-1-worker-7] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-170116874-1830928683, testConfigurationFileIsCreated=my-topic-155202447-1921897899, testCreatingUsersWithSecretPrefix=my-topic-2033928996-1209892355, testUpdateUser=my-topic-1239518257-2056139809, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-712754859-1910580676, testTlsExternalUserWithQuotas=my-topic-593679216-1309035072, testUserWithNameMoreThan64Chars=my-topic-1603054043-1686316417, testTlsUserWithQuotas=my-topic-1752299353-1315959033, testThrottlingQuotasCreateTopic=my-topic-1532506723-1472611013, testCapacityFile=my-topic-1390944843-775067696, testTlsExternalUser=my-topic-476529121-306489993, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1084162749-1576061083, testSendSimpleMessageTlsScramSha=my-topic-62182003-1963040205, testUserTemplate=my-topic-515111334-943904347}
2022-03-28 12:51:43 [ForkJoinPool-1-worker-7] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-455915b9-kafka-clients, testConfigurationFileIsCreated=my-cluster-020ceaf8-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5-kafka-clients, testUpdateUser=my-cluster-e16ccf16-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-3e2e11de-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-ed39a269-kafka-clients, testTlsUserWithQuotas=my-cluster-3b191485-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2-kafka-clients, testCapacityFile=my-cluster-39ee3f62-kafka-clients, testTlsExternalUser=my-cluster-965e9d1e-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-31feb41b-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-27773859-kafka-clients, testUserTemplate=my-cluster-157df94e-kafka-clients}
2022-03-28 12:51:43 [ForkJoinPool-1-worker-7] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-5 for test case:testConfigurationFileIsCreated
2022-03-28 12:51:43 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-31feb41b-cruise-control-799978bc54-q58sp=73e1002d-1930-48bf-855d-d34e3786adc0}
2022-03-28 12:51:43 [ForkJoinPool-1-worker-7] INFO  [KubeClusterResource:156] Creating Namespace: namespace-5
2022-03-28 12:51:43 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Namespace namespace-5
2022-03-28 12:51:43 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace namespace-5 -o json
2022-03-28 12:51:43 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-31feb41b-cruise-control-799978bc54-q58sp=73e1002d-1930-48bf-855d-d34e3786adc0, my-cluster-31feb41b-cruise-control-fb9768f77-9scrm=c1499226-7064-4e21-bc93-8ab8ede85e86}
2022-03-28 12:51:43 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-31feb41b-cruise-control-799978bc54-q58sp=73e1002d-1930-48bf-855d-d34e3786adc0}
2022-03-28 12:51:43 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Deployment my-cluster-31feb41b-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (594409ms till timeout)
2022-03-28 12:51:44 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (133313ms till timeout)
2022-03-28 12:51:44 [ForkJoinPool-1-worker-1] INFO  [Exec:417] Command: oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user CN=encrypted-arnost
2022-03-28 12:51:44 [ForkJoinPool-1-worker-1] INFO  [Exec:417] Return code: 0
2022-03-28 12:51:44 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:674] ============================================================================
2022-03-28 12:51:44 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:675] [operators.user.UserST - After Each] - Clean up after test
2022-03-28 12:51:44 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:344] ############################################################################
2022-03-28 12:51:44 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:348] Delete all resources for testTlsUserWithQuotas
2022-03-28 12:51:44 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:241] Delete of KafkaUser encrypted-arnost in namespace user-st
2022-03-28 12:51:44 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:44 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:51:44 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:51:44 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:51:44 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:51:44 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (43874ms till timeout)
2022-03-28 12:51:44 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:encrypted-arnost
2022-03-28 12:51:44 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:369] ############################################################################
2022-03-28 12:51:44 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:267] testTlsUserWithQuotas - Notifies waiting test cases:[testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testTlsExternalUserWithQuotas, testUserTemplate, testUpdateUser, testTlsExternalUser, testReceiveSimpleMessageTlsScramSha, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testSendSimpleMessageTlsScramSha, testUserWithNameMoreThan64Chars, testTlsUserWithQuotas, testConfigurationFileIsCreated] to and randomly select one to start execution
2022-03-28 12:51:44 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:93] [operators.user.UserST] - Removing parallel test: testTlsUserWithQuotas
2022-03-28 12:51:44 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:97] [operators.user.UserST] - Parallel test count: 5
2022-03-28 12:51:44 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testTlsUserWithQuotas-FINISHED
2022-03-28 12:51:44 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:30] ############################################################################
2022-03-28 12:51:44 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:23] ############################################################################
2022-03-28 12:51:44 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.user.UserST.testScramUserWithQuotas-STARTED
2022-03-28 12:51:44 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:658] ============================================================================
2022-03-28 12:51:44 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:659] [operators.user.UserST - Before Each] - Setup test case environment
2022-03-28 12:51:44 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:77] [operators.user.UserST] - Adding parallel test: testScramUserWithQuotas
2022-03-28 12:51:44 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:81] [operators.user.UserST] - Parallel test count: 6
2022-03-28 12:51:44 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:230] testScramUserWithQuotas test now can proceed its execution
2022-03-28 12:51:45 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:45 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (131995ms till timeout)
2022-03-28 12:51:46 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (130675ms till timeout)
2022-03-28 12:51:47 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:184] HttpBridgeTlsST suite now can proceed its execution
2022-03-28 12:51:47 [ForkJoinPool-1-worker-9] DEBUG [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-28 12:51:47 [ForkJoinPool-1-worker-9] DEBUG [TestSuiteNamespaceManager:129] Test suite `HttpBridgeTlsST` creates these additional namespaces:[http-bridge-tls-st]
2022-03-28 12:51:47 [ForkJoinPool-1-worker-9] INFO  [KubeClusterResource:156] Creating Namespace: http-bridge-tls-st
2022-03-28 12:51:48 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (129281ms till timeout)
2022-03-28 12:51:48 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Namespace http-bridge-tls-st
2022-03-28 12:51:48 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace namespace-4 get Namespace http-bridge-tls-st -o json
2022-03-28 12:51:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace namespace-4 get Namespace http-bridge-tls-st -o json
2022-03-28 12:51:48 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 12:51:48 [ForkJoinPool-1-worker-9] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c28,c12",
            "openshift.io/sa.scc.supplemental-groups": "1000780000/10000",
            "openshift.io/sa.scc.uid-range": "1000780000/10000"
        },
        "creationTimestamp": "2022-03-28T12:51:43Z",
        "labels": {
            "kubernetes.io/metadata.name": "http-bridge-tls-st"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T12:51:43Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T12:51:43Z"
            }
        ],
        "name": "http-bridge-tls-st",
        "resourceVersion": "49332",
        "uid": "41a56884-c74b-40fc-ad86-73ebafe26a28"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 12:51:48 [ForkJoinPool-1-worker-9] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[namespace-2], io.strimzi.test.logs.CollectorElement@3881d5f2=[namespace-4], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-1], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 12:51:48 [ForkJoinPool-1-worker-9] INFO  [KubeClusterResource:82] Client use Namespace: http-bridge-tls-st
2022-03-28 12:51:48 [ForkJoinPool-1-worker-9] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=http-bridge-tls-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 12:51:48 [ForkJoinPool-1-worker-9] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: http-bridge-tls-st
2022-03-28 12:51:48 [ForkJoinPool-1-worker-9] INFO  [HttpBridgeTlsST:129] Deploy Kafka and KafkaBridge before tests
2022-03-28 12:51:48 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update Kafka http-bridge-tls-cluster-name in namespace http-bridge-tls-st
2022-03-28 12:51:48 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:http-bridge-tls-cluster-name
2022-03-28 12:51:48 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:433] Wait for Kafka: http-bridge-tls-cluster-name will have desired state: Ready
2022-03-28 12:51:48 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Kafka: http-bridge-tls-cluster-name will have desired state: Ready
2022-03-28 12:51:48 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-31feb41b-cruise-control-799978bc54-q58sp=73e1002d-1930-48bf-855d-d34e3786adc0}
2022-03-28 12:51:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (839891ms till timeout)
2022-03-28 12:51:49 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (128054ms till timeout)
2022-03-28 12:51:49 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-4 get Namespace namespace-5 -o json
2022-03-28 12:51:49 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 12:51:49 [ForkJoinPool-1-worker-7] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c28,c7",
            "openshift.io/sa.scc.supplemental-groups": "1000770000/10000",
            "openshift.io/sa.scc.uid-range": "1000770000/10000"
        },
        "creationTimestamp": "2022-03-28T12:51:39Z",
        "labels": {
            "kubernetes.io/metadata.name": "namespace-5"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T12:51:39Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T12:51:39Z"
            }
        ],
        "name": "namespace-5",
        "resourceVersion": "49265",
        "uid": "1662756d-f798-44a2-830b-26e3dc132822"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 12:51:49 [ForkJoinPool-1-worker-7] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[namespace-2], io.strimzi.test.logs.CollectorElement@aec142ef=[namespace-5], io.strimzi.test.logs.CollectorElement@3881d5f2=[namespace-4], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-1], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 12:51:49 [ForkJoinPool-1-worker-7] INFO  [KubeClusterResource:82] Client use Namespace: namespace-5
2022-03-28 12:51:49 [ForkJoinPool-1-worker-7] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-5, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 12:51:49 [ForkJoinPool-1-worker-7] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-5
2022-03-28 12:51:49 [ForkJoinPool-1-worker-1] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 12:51:49 [ForkJoinPool-1-worker-1] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testScramUserWithQuotas=my-cluster-90c82d82, testReceiveSimpleMessageTlsScramSha=my-cluster-455915b9, testConfigurationFileIsCreated=my-cluster-020ceaf8, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5, testUpdateUser=my-cluster-e16ccf16, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d, testTlsExternalUserWithQuotas=my-cluster-3e2e11de, testUserWithNameMoreThan64Chars=my-cluster-ed39a269, testTlsUserWithQuotas=my-cluster-3b191485, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2, testCapacityFile=my-cluster-39ee3f62, testTlsExternalUser=my-cluster-965e9d1e, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-31feb41b, testSendSimpleMessageTlsScramSha=my-cluster-27773859, testUserTemplate=my-cluster-157df94e}
2022-03-28 12:51:49 [ForkJoinPool-1-worker-1] TRACE [AbstractST:607] USERS_NAME_MAP: {testScramUserWithQuotas=my-user-1857980152-2145707698, testReceiveSimpleMessageTlsScramSha=my-user-1329598231-1183867861, testConfigurationFileIsCreated=my-user-1609977058-1196012647, testCreatingUsersWithSecretPrefix=my-user-1928899390-1444689439, testUpdateUser=my-user-1694542041-806176635, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-467839358-1504804463, testTlsExternalUserWithQuotas=my-user-1786174319-1274634874, testUserWithNameMoreThan64Chars=my-user-1048816470-1103470031, testTlsUserWithQuotas=my-user-420001054-1296284129, testThrottlingQuotasCreateTopic=my-user-896662438-260898887, testCapacityFile=my-user-597365340-661017603, testTlsExternalUser=my-user-1295334081-1602801897, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2100837352-1014792902, testSendSimpleMessageTlsScramSha=my-user-507582460-1003875464, testUserTemplate=my-user-1253881534-190261188}
2022-03-28 12:51:49 [ForkJoinPool-1-worker-1] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testScramUserWithQuotas=my-topic-315112689-1043284277, testReceiveSimpleMessageTlsScramSha=my-topic-170116874-1830928683, testConfigurationFileIsCreated=my-topic-155202447-1921897899, testCreatingUsersWithSecretPrefix=my-topic-2033928996-1209892355, testUpdateUser=my-topic-1239518257-2056139809, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-712754859-1910580676, testTlsExternalUserWithQuotas=my-topic-593679216-1309035072, testUserWithNameMoreThan64Chars=my-topic-1603054043-1686316417, testTlsUserWithQuotas=my-topic-1752299353-1315959033, testThrottlingQuotasCreateTopic=my-topic-1532506723-1472611013, testCapacityFile=my-topic-1390944843-775067696, testTlsExternalUser=my-topic-476529121-306489993, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1084162749-1576061083, testSendSimpleMessageTlsScramSha=my-topic-62182003-1963040205, testUserTemplate=my-topic-515111334-943904347}
2022-03-28 12:51:49 [ForkJoinPool-1-worker-1] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testScramUserWithQuotas=my-cluster-90c82d82-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-455915b9-kafka-clients, testConfigurationFileIsCreated=my-cluster-020ceaf8-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5-kafka-clients, testUpdateUser=my-cluster-e16ccf16-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-3e2e11de-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-ed39a269-kafka-clients, testTlsUserWithQuotas=my-cluster-3b191485-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2-kafka-clients, testCapacityFile=my-cluster-39ee3f62-kafka-clients, testTlsExternalUser=my-cluster-965e9d1e-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-31feb41b-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-27773859-kafka-clients, testUserTemplate=my-cluster-157df94e-kafka-clients}
2022-03-28 12:51:49 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:155] Create/Update KafkaUser scramed-arnost in namespace user-st
2022-03-28 12:51:49 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-020ceaf8 in namespace namespace-5
2022-03-28 12:51:49 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:164] Using Namespace: namespace-5
2022-03-28 12:51:49 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-31feb41b-cruise-control-799978bc54-q58sp=73e1002d-1930-48bf-855d-d34e3786adc0, my-cluster-31feb41b-cruise-control-fb9768f77-9scrm=c1499226-7064-4e21-bc93-8ab8ede85e86}
2022-03-28 12:51:49 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-31feb41b-cruise-control-799978bc54-q58sp=73e1002d-1930-48bf-855d-d34e3786adc0}
2022-03-28 12:51:49 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Deployment my-cluster-31feb41b-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (588951ms till timeout)
2022-03-28 12:51:49 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:scramed-arnost
2022-03-28 12:51:49 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-020ceaf8
2022-03-28 12:51:49 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:433] Wait for KafkaUser: scramed-arnost will have desired state: Ready
2022-03-28 12:51:49 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for KafkaUser: scramed-arnost will have desired state: Ready
2022-03-28 12:51:49 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-020ceaf8 will have desired state: Ready
2022-03-28 12:51:49 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-020ceaf8 will have desired state: Ready
2022-03-28 12:51:49 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:444] KafkaUser: scramed-arnost is in desired state: Ready
2022-03-28 12:51:50 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1319787ms till timeout)
2022-03-28 12:51:50 [ForkJoinPool-1-worker-1] DEBUG [UserST:274] Command for kafka-configs.sh bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user scramed-arnost
2022-03-28 12:51:50 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user scramed-arnost
2022-03-28 12:51:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (838681ms till timeout)
2022-03-28 12:51:50 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (126941ms till timeout)
2022-03-28 12:51:50 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of Deployment my-cluster-c525f5e5-tls-kafka-clients in namespace namespace-2
2022-03-28 12:51:50 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-4 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:50 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:51:50 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:51:50 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:51:50 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:51:50 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (37278ms till timeout)
2022-03-28 12:51:51 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-c525f5e5-tls-kafka-clients
2022-03-28 12:51:51 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1318595ms till timeout)
2022-03-28 12:51:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (837488ms till timeout)
2022-03-28 12:51:51 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-c525f5e5-tls-kafka-clients not ready, will try again in 10000 ms (479459ms till timeout)
2022-03-28 12:51:51 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (125647ms till timeout)
2022-03-28 12:51:51 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-5 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:52 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1317482ms till timeout)
2022-03-28 12:51:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (836379ms till timeout)
2022-03-28 12:51:52 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-5 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:52 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:51:52 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:51:52 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:51:52 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:51:52 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (35612ms till timeout)
2022-03-28 12:51:53 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (124328ms till timeout)
2022-03-28 12:51:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1316368ms till timeout)
2022-03-28 12:51:53 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-5 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (835268ms till timeout)
2022-03-28 12:51:54 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (123009ms till timeout)
2022-03-28 12:51:54 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-31feb41b-cruise-control-799978bc54-q58sp=73e1002d-1930-48bf-855d-d34e3786adc0}
2022-03-28 12:51:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1315255ms till timeout)
2022-03-28 12:51:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (834147ms till timeout)
2022-03-28 12:51:54 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-31feb41b-cruise-control-799978bc54-q58sp=73e1002d-1930-48bf-855d-d34e3786adc0, my-cluster-31feb41b-cruise-control-fb9768f77-9scrm=c1499226-7064-4e21-bc93-8ab8ede85e86}
2022-03-28 12:51:54 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-31feb41b-cruise-control-799978bc54-q58sp=73e1002d-1930-48bf-855d-d34e3786adc0}
2022-03-28 12:51:54 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Deployment my-cluster-31feb41b-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (583513ms till timeout)
2022-03-28 12:51:55 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (121690ms till timeout)
2022-03-28 12:51:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1314097ms till timeout)
2022-03-28 12:51:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (833036ms till timeout)
2022-03-28 12:51:57 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (120371ms till timeout)
2022-03-28 12:51:57 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1312779ms till timeout)
2022-03-28 12:51:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (831885ms till timeout)
2022-03-28 12:51:58 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (119048ms till timeout)
2022-03-28 12:51:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (830564ms till timeout)
2022-03-28 12:51:58 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1311454ms till timeout)
2022-03-28 12:51:59 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-5 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:51:59 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:51:59 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:51:59 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:51:59 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:51:59 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (29010ms till timeout)
2022-03-28 12:51:59 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (117727ms till timeout)
2022-03-28 12:51:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (829244ms till timeout)
2022-03-28 12:51:59 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1310134ms till timeout)
2022-03-28 12:51:59 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-31feb41b-cruise-control-799978bc54-q58sp=73e1002d-1930-48bf-855d-d34e3786adc0}
2022-03-28 12:52:00 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-31feb41b-cruise-control-799978bc54-q58sp=73e1002d-1930-48bf-855d-d34e3786adc0, my-cluster-31feb41b-cruise-control-fb9768f77-9scrm=c1499226-7064-4e21-bc93-8ab8ede85e86}
2022-03-28 12:52:00 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-31feb41b-cruise-control-799978bc54-q58sp=73e1002d-1930-48bf-855d-d34e3786adc0}
2022-03-28 12:52:00 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Deployment my-cluster-31feb41b-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (578288ms till timeout)
2022-03-28 12:52:00 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-5 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:52:00 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-5 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:52:00 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:52:00 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:52:00 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:52:00 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:52:00 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (27484ms till timeout)
2022-03-28 12:52:01 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (116408ms till timeout)
2022-03-28 12:52:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (827925ms till timeout)
2022-03-28 12:52:01 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1308816ms till timeout)
2022-03-28 12:52:01 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-5 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:52:02 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-c525f5e5-tls-kafka-clients not ready, will try again in 10000 ms (468986ms till timeout)
2022-03-28 12:52:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (826800ms till timeout)
2022-03-28 12:52:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1307691ms till timeout)
2022-03-28 12:52:02 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (115178ms till timeout)
2022-03-28 12:52:02 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-5 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:52:02 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:52:02 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:52:02 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:52:02 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:52:02 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (25849ms till timeout)
2022-03-28 12:52:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (825689ms till timeout)
2022-03-28 12:52:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1306580ms till timeout)
2022-03-28 12:52:03 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-5 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:52:03 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (113956ms till timeout)
2022-03-28 12:52:03 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-5 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:52:03 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:52:03 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:52:03 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:52:03 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:52:03 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (24298ms till timeout)
2022-03-28 12:52:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (824579ms till timeout)
2022-03-28 12:52:04 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1305468ms till timeout)
2022-03-28 12:52:04 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (112638ms till timeout)
2022-03-28 12:52:04 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-5 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:52:05 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-31feb41b-cruise-control-799978bc54-q58sp=73e1002d-1930-48bf-855d-d34e3786adc0}
2022-03-28 12:52:05 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-31feb41b-cruise-control-799978bc54-q58sp=73e1002d-1930-48bf-855d-d34e3786adc0, my-cluster-31feb41b-cruise-control-fb9768f77-9scrm=c1499226-7064-4e21-bc93-8ab8ede85e86}
2022-03-28 12:52:05 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-31feb41b-cruise-control-799978bc54-q58sp=73e1002d-1930-48bf-855d-d34e3786adc0}
2022-03-28 12:52:05 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Deployment my-cluster-31feb41b-cruise-control rolling update in namespace:namespace-4 not ready, will try again in 5000 ms (573062ms till timeout)
2022-03-28 12:52:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (823470ms till timeout)
2022-03-28 12:52:05 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-5 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:52:05 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:52:05 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:52:05 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:52:05 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:52:05 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (22733ms till timeout)
2022-03-28 12:52:05 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1304253ms till timeout)
2022-03-28 12:52:05 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (111423ms till timeout)
2022-03-28 12:52:06 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-5 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:52:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (822359ms till timeout)
2022-03-28 12:52:06 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1303142ms till timeout)
2022-03-28 12:52:06 [ForkJoinPool-1-worker-1] INFO  [Exec:417] Command: oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user scramed-arnost
2022-03-28 12:52:06 [ForkJoinPool-1-worker-1] INFO  [Exec:417] Return code: 0
2022-03-28 12:52:07 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-5 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:52:07 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:52:07 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:52:07 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:52:07 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:52:07 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (21168ms till timeout)
2022-03-28 12:52:07 [ForkJoinPool-1-worker-1] INFO  [KafkaUserUtils:62] Waiting for KafkaUser deletion scramed-arnost
2022-03-28 12:52:07 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for KafkaUser deletion scramed-arnost
2022-03-28 12:52:07 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (110117ms till timeout)
2022-03-28 12:52:07 [ForkJoinPool-1-worker-1] INFO  [KafkaUserUtils:75] KafkaUser scramed-arnost deleted
2022-03-28 12:52:07 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for all KafkaUser scramed-arnost attributes will be cleaned
2022-03-28 12:52:07 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user scramed-arnost
2022-03-28 12:52:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (821248ms till timeout)
2022-03-28 12:52:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1302032ms till timeout)
2022-03-28 12:52:08 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-5 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:52:08 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-5 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:52:08 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:52:08 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:52:08 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:52:08 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:52:08 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (19632ms till timeout)
2022-03-28 12:52:08 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (108798ms till timeout)
2022-03-28 12:52:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (820139ms till timeout)
2022-03-28 12:52:08 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1300922ms till timeout)
2022-03-28 12:52:09 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-5 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:52:09 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (107477ms till timeout)
2022-03-28 12:52:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (818994ms till timeout)
2022-03-28 12:52:10 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1299778ms till timeout)
2022-03-28 12:52:10 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-5 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:52:10 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:52:10 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:52:10 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:52:10 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:52:10 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (18105ms till timeout)
2022-03-28 12:52:10 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-31feb41b-cruise-control-799978bc54-q58sp=73e1002d-1930-48bf-855d-d34e3786adc0}
2022-03-28 12:52:10 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-31feb41b-cruise-control-fb9768f77-9scrm=c1499226-7064-4e21-bc93-8ab8ede85e86}
2022-03-28 12:52:10 [ForkJoinPool-1-worker-3] DEBUG [DeploymentUtils:119] All pods seem to have rolled
2022-03-28 12:52:10 [ForkJoinPool-1-worker-3] INFO  [DeploymentUtils:161] Wait for Deployment: my-cluster-31feb41b-cruise-control will be ready
2022-03-28 12:52:10 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Wait for Deployment: my-cluster-31feb41b-cruise-control will be ready
2022-03-28 12:52:10 [ForkJoinPool-1-worker-3] INFO  [DeploymentUtils:168] Deployment: my-cluster-31feb41b-cruise-control is ready
2022-03-28 12:52:10 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-31feb41b, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-31feb41b-cruise-control}, additionalProperties={})to be ready
2022-03-28 12:52:10 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-31feb41b-cruise-control-fb9768f77-9scrm not ready: cruise-control)
2022-03-28 12:52:10 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-31feb41b-cruise-control-fb9768f77-9scrm not ready: tls-sidecar)
2022-03-28 12:52:10 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-31feb41b-cruise-control-fb9768f77-9scrm are ready
2022-03-28 12:52:10 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-31feb41b, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-31feb41b-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599889ms till timeout)
2022-03-28 12:52:11 [ForkJoinPool-1-worker-1] INFO  [Exec:417] Command: oc --namespace user-st exec user-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-configs.sh --bootstrap-server localhost:9092 --describe --user scramed-arnost
2022-03-28 12:52:11 [ForkJoinPool-1-worker-1] INFO  [Exec:417] Return code: 0
2022-03-28 12:52:11 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:674] ============================================================================
2022-03-28 12:52:11 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:675] [operators.user.UserST - After Each] - Clean up after test
2022-03-28 12:52:11 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:344] ############################################################################
2022-03-28 12:52:11 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:348] Delete all resources for testScramUserWithQuotas
2022-03-28 12:52:11 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:241] Delete of KafkaUser scramed-arnost in namespace user-st
2022-03-28 12:52:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (817880ms till timeout)
2022-03-28 12:52:11 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-5 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:52:11 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:scramed-arnost
2022-03-28 12:52:11 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1298661ms till timeout)
2022-03-28 12:52:11 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (106149ms till timeout)
2022-03-28 12:52:11 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:369] ############################################################################
2022-03-28 12:52:11 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:267] testScramUserWithQuotas - Notifies waiting test cases:[testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testTlsExternalUserWithQuotas, testUserTemplate, testUpdateUser, testTlsExternalUser, testReceiveSimpleMessageTlsScramSha, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testSendSimpleMessageTlsScramSha, testUserWithNameMoreThan64Chars, testTlsUserWithQuotas, testConfigurationFileIsCreated, testScramUserWithQuotas] to and randomly select one to start execution
2022-03-28 12:52:11 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:93] [operators.user.UserST] - Removing parallel test: testScramUserWithQuotas
2022-03-28 12:52:11 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:97] [operators.user.UserST] - Parallel test count: 5
2022-03-28 12:52:11 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testScramUserWithQuotas-FINISHED
2022-03-28 12:52:11 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:30] ############################################################################
2022-03-28 12:52:11 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:241] Delete of KafkaTopic my-topic-2033928996-1209892355 in namespace namespace-2
2022-03-28 12:52:11 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-2033928996-1209892355
2022-03-28 12:52:11 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:241] Delete of KafkaUser encrypted-leopold in namespace namespace-2
2022-03-28 12:52:11 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:encrypted-leopold
2022-03-28 12:52:11 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-5 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:52:11 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:52:11 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:52:11 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:52:11 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:52:11 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (16577ms till timeout)
2022-03-28 12:52:11 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:241] Delete of Kafka my-cluster-c525f5e5 in namespace namespace-2
2022-03-28 12:52:11 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-c525f5e5
2022-03-28 12:52:11 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-c525f5e5 not ready, will try again in 10000 ms (839889ms till timeout)
2022-03-28 12:52:12 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-31feb41b-cruise-control-fb9768f77-9scrm not ready: cruise-control)
2022-03-28 12:52:12 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-31feb41b-cruise-control-fb9768f77-9scrm not ready: tls-sidecar)
2022-03-28 12:52:12 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-31feb41b-cruise-control-fb9768f77-9scrm are ready
2022-03-28 12:52:12 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-31feb41b, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-31feb41b-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598740ms till timeout)
2022-03-28 12:52:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (816653ms till timeout)
2022-03-28 12:52:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1297433ms till timeout)
2022-03-28 12:52:12 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (105017ms till timeout)
2022-03-28 12:52:12 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-c525f5e5-tls-kafka-clients not ready, will try again in 10000 ms (458512ms till timeout)
2022-03-28 12:52:12 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-5 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:52:13 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-31feb41b-cruise-control-fb9768f77-9scrm not ready: cruise-control)
2022-03-28 12:52:13 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-31feb41b-cruise-control-fb9768f77-9scrm not ready: tls-sidecar)
2022-03-28 12:52:13 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-31feb41b-cruise-control-fb9768f77-9scrm are ready
2022-03-28 12:52:13 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-31feb41b, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-31feb41b-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597628ms till timeout)
2022-03-28 12:52:13 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-5 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:52:13 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:52:13 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:52:13 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:52:13 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:52:13 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (15057ms till timeout)
2022-03-28 12:52:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (815543ms till timeout)
2022-03-28 12:52:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1296325ms till timeout)
2022-03-28 12:52:13 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (103701ms till timeout)
2022-03-28 12:52:14 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-5 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:52:14 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-31feb41b-cruise-control-fb9768f77-9scrm not ready: cruise-control)
2022-03-28 12:52:14 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-31feb41b-cruise-control-fb9768f77-9scrm not ready: tls-sidecar)
2022-03-28 12:52:14 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-31feb41b-cruise-control-fb9768f77-9scrm are ready
2022-03-28 12:52:14 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-31feb41b, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-31feb41b-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596516ms till timeout)
2022-03-28 12:52:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (814432ms till timeout)
2022-03-28 12:52:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1295214ms till timeout)
2022-03-28 12:52:14 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-5 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:52:14 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:52:14 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:52:14 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:52:14 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:52:14 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (13498ms till timeout)
2022-03-28 12:52:14 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (102484ms till timeout)
2022-03-28 12:52:15 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-31feb41b-cruise-control-fb9768f77-9scrm not ready: cruise-control)
2022-03-28 12:52:15 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-31feb41b-cruise-control-fb9768f77-9scrm not ready: tls-sidecar)
2022-03-28 12:52:15 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-31feb41b-cruise-control-fb9768f77-9scrm are ready
2022-03-28 12:52:15 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-31feb41b, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-31feb41b-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595405ms till timeout)
2022-03-28 12:52:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (813324ms till timeout)
2022-03-28 12:52:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1294106ms till timeout)
2022-03-28 12:52:15 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-5 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:52:16 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (101268ms till timeout)
2022-03-28 12:52:16 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-5 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:52:16 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:52:16 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:52:16 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:52:16 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:52:16 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (11941ms till timeout)
2022-03-28 12:52:16 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-31feb41b-cruise-control-fb9768f77-9scrm not ready: cruise-control)
2022-03-28 12:52:16 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-31feb41b-cruise-control-fb9768f77-9scrm not ready: tls-sidecar)
2022-03-28 12:52:16 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-31feb41b-cruise-control-fb9768f77-9scrm are ready
2022-03-28 12:52:16 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-31feb41b, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-31feb41b-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594291ms till timeout)
2022-03-28 12:52:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (812213ms till timeout)
2022-03-28 12:52:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1292994ms till timeout)
2022-03-28 12:52:17 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-5 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:52:17 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (99948ms till timeout)
2022-03-28 12:52:17 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-31feb41b-cruise-control-fb9768f77-9scrm not ready: cruise-control)
2022-03-28 12:52:17 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-31feb41b-cruise-control-fb9768f77-9scrm not ready: tls-sidecar)
2022-03-28 12:52:17 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-31feb41b-cruise-control-fb9768f77-9scrm are ready
2022-03-28 12:52:17 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-31feb41b, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-31feb41b-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593176ms till timeout)
2022-03-28 12:52:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (811103ms till timeout)
2022-03-28 12:52:17 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-5 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:52:17 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:52:17 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:52:17 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:52:17 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:52:17 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (10368ms till timeout)
2022-03-28 12:52:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1291884ms till timeout)
2022-03-28 12:52:18 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (98622ms till timeout)
2022-03-28 12:52:18 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-31feb41b-cruise-control-fb9768f77-9scrm not ready: cruise-control)
2022-03-28 12:52:18 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-31feb41b-cruise-control-fb9768f77-9scrm not ready: tls-sidecar)
2022-03-28 12:52:18 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-31feb41b-cruise-control-fb9768f77-9scrm are ready
2022-03-28 12:52:18 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-31feb41b, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-31feb41b-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591993ms till timeout)
2022-03-28 12:52:18 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-5 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:52:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (809993ms till timeout)
2022-03-28 12:52:19 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1290773ms till timeout)
2022-03-28 12:52:19 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-5 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:52:19 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:52:19 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:52:19 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:52:19 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:52:19 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (8823ms till timeout)
2022-03-28 12:52:20 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (97302ms till timeout)
2022-03-28 12:52:20 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-31feb41b-cruise-control-fb9768f77-9scrm not ready: cruise-control)
2022-03-28 12:52:20 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-31feb41b-cruise-control-fb9768f77-9scrm not ready: tls-sidecar)
2022-03-28 12:52:20 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-31feb41b-cruise-control-fb9768f77-9scrm are ready
2022-03-28 12:52:20 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-31feb41b, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-31feb41b-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590673ms till timeout)
2022-03-28 12:52:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (808815ms till timeout)
2022-03-28 12:52:20 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1289602ms till timeout)
2022-03-28 12:52:20 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-5 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:52:20 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-5 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:52:20 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:52:20 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:52:20 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:52:20 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:52:20 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (7304ms till timeout)
2022-03-28 12:52:21 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (95983ms till timeout)
2022-03-28 12:52:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (807499ms till timeout)
2022-03-28 12:52:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1288390ms till timeout)
2022-03-28 12:52:21 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-31feb41b-cruise-control-fb9768f77-9scrm not ready: cruise-control)
2022-03-28 12:52:21 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-31feb41b-cruise-control-fb9768f77-9scrm not ready: tls-sidecar)
2022-03-28 12:52:21 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-31feb41b-cruise-control-fb9768f77-9scrm are ready
2022-03-28 12:52:21 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-31feb41b, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-31feb41b-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (589354ms till timeout)
2022-03-28 12:52:21 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-5 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:52:22 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:23] ############################################################################
2022-03-28 12:52:22 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testDeployAndUnDeployCruiseControl-STARTED
2022-03-28 12:52:22 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:658] ============================================================================
2022-03-28 12:52:22 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:659] [cruisecontrol.CruiseControlConfigurationST - Before Each] - Setup test case environment
2022-03-28 12:52:22 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:77] [cruisecontrol.CruiseControlConfigurationST] - Adding parallel test: testDeployAndUnDeployCruiseControl
2022-03-28 12:52:22 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:81] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 6
2022-03-28 12:52:22 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:230] testDeployAndUnDeployCruiseControl test now can proceed its execution
2022-03-28 12:52:22 [ForkJoinPool-1-worker-1] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 12:52:22 [ForkJoinPool-1-worker-1] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testScramUserWithQuotas=my-cluster-90c82d82, testReceiveSimpleMessageTlsScramSha=my-cluster-455915b9, testConfigurationFileIsCreated=my-cluster-020ceaf8, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5, testUpdateUser=my-cluster-e16ccf16, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d, testTlsExternalUserWithQuotas=my-cluster-3e2e11de, testUserWithNameMoreThan64Chars=my-cluster-ed39a269, testTlsUserWithQuotas=my-cluster-3b191485, testDeployAndUnDeployCruiseControl=my-cluster-534609d0, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2, testCapacityFile=my-cluster-39ee3f62, testTlsExternalUser=my-cluster-965e9d1e, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-31feb41b, testSendSimpleMessageTlsScramSha=my-cluster-27773859, testUserTemplate=my-cluster-157df94e}
2022-03-28 12:52:22 [ForkJoinPool-1-worker-1] TRACE [AbstractST:607] USERS_NAME_MAP: {testScramUserWithQuotas=my-user-1857980152-2145707698, testReceiveSimpleMessageTlsScramSha=my-user-1329598231-1183867861, testConfigurationFileIsCreated=my-user-1609977058-1196012647, testCreatingUsersWithSecretPrefix=my-user-1928899390-1444689439, testUpdateUser=my-user-1694542041-806176635, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-467839358-1504804463, testTlsExternalUserWithQuotas=my-user-1786174319-1274634874, testUserWithNameMoreThan64Chars=my-user-1048816470-1103470031, testTlsUserWithQuotas=my-user-420001054-1296284129, testDeployAndUnDeployCruiseControl=my-user-475346616-1293712526, testThrottlingQuotasCreateTopic=my-user-896662438-260898887, testCapacityFile=my-user-597365340-661017603, testTlsExternalUser=my-user-1295334081-1602801897, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2100837352-1014792902, testSendSimpleMessageTlsScramSha=my-user-507582460-1003875464, testUserTemplate=my-user-1253881534-190261188}
2022-03-28 12:52:22 [ForkJoinPool-1-worker-1] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testScramUserWithQuotas=my-topic-315112689-1043284277, testReceiveSimpleMessageTlsScramSha=my-topic-170116874-1830928683, testConfigurationFileIsCreated=my-topic-155202447-1921897899, testCreatingUsersWithSecretPrefix=my-topic-2033928996-1209892355, testUpdateUser=my-topic-1239518257-2056139809, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-712754859-1910580676, testTlsExternalUserWithQuotas=my-topic-593679216-1309035072, testUserWithNameMoreThan64Chars=my-topic-1603054043-1686316417, testTlsUserWithQuotas=my-topic-1752299353-1315959033, testDeployAndUnDeployCruiseControl=my-topic-1815491150-540070233, testThrottlingQuotasCreateTopic=my-topic-1532506723-1472611013, testCapacityFile=my-topic-1390944843-775067696, testTlsExternalUser=my-topic-476529121-306489993, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1084162749-1576061083, testSendSimpleMessageTlsScramSha=my-topic-62182003-1963040205, testUserTemplate=my-topic-515111334-943904347}
2022-03-28 12:52:22 [ForkJoinPool-1-worker-1] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testScramUserWithQuotas=my-cluster-90c82d82-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-455915b9-kafka-clients, testConfigurationFileIsCreated=my-cluster-020ceaf8-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5-kafka-clients, testUpdateUser=my-cluster-e16ccf16-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-3e2e11de-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-ed39a269-kafka-clients, testTlsUserWithQuotas=my-cluster-3b191485-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-534609d0-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2-kafka-clients, testCapacityFile=my-cluster-39ee3f62-kafka-clients, testTlsExternalUser=my-cluster-965e9d1e-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-31feb41b-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-27773859-kafka-clients, testUserTemplate=my-cluster-157df94e-kafka-clients}
2022-03-28 12:52:22 [ForkJoinPool-1-worker-1] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-6 for test case:testDeployAndUnDeployCruiseControl
2022-03-28 12:52:22 [ForkJoinPool-1-worker-1] INFO  [KubeClusterResource:156] Creating Namespace: namespace-6
2022-03-28 12:52:22 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Namespace namespace-6
2022-03-28 12:52:22 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-5 get Namespace namespace-6 -o json
2022-03-28 12:52:22 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-5 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:52:22 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:52:22 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:52:22 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:52:22 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:52:22 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (5765ms till timeout)
2022-03-28 12:52:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (806390ms till timeout)
2022-03-28 12:52:22 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-5 get Namespace namespace-6 -o json
2022-03-28 12:52:22 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 12:52:22 [ForkJoinPool-1-worker-1] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c28,c17",
            "openshift.io/sa.scc.supplemental-groups": "1000790000/10000",
            "openshift.io/sa.scc.uid-range": "1000790000/10000"
        },
        "creationTimestamp": "2022-03-28T12:52:17Z",
        "labels": {
            "kubernetes.io/metadata.name": "namespace-6"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T12:52:17Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T12:52:17Z"
            }
        ],
        "name": "namespace-6",
        "resourceVersion": "50128",
        "uid": "eb7419f0-a285-41b4-a1fb-f396c180b49a"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 12:52:22 [ForkJoinPool-1-worker-1] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[namespace-2], io.strimzi.test.logs.CollectorElement@aec142ef=[namespace-5], io.strimzi.test.logs.CollectorElement@3881d5f2=[namespace-4], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-1], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-6], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 12:52:22 [ForkJoinPool-1-worker-1] INFO  [KubeClusterResource:82] Client use Namespace: namespace-6
2022-03-28 12:52:22 [ForkJoinPool-1-worker-1] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-6, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 12:52:22 [ForkJoinPool-1-worker-1] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-6
2022-03-28 12:52:22 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-534609d0 in namespace namespace-6
2022-03-28 12:52:22 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:164] Using Namespace: namespace-6
2022-03-28 12:52:22 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (94666ms till timeout)
2022-03-28 12:52:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1287073ms till timeout)
2022-03-28 12:52:22 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:369] ############################################################################
2022-03-28 12:52:22 [ForkJoinPool-1-worker-13] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-2 for test case:testCreatingUsersWithSecretPrefix
2022-03-28 12:52:22 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-31feb41b-cruise-control-fb9768f77-9scrm not ready: cruise-control)
2022-03-28 12:52:22 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-31feb41b-cruise-control-fb9768f77-9scrm not ready: tls-sidecar)
2022-03-28 12:52:22 [ForkJoinPool-1-worker-3] DEBUG [PodUtils:106] Pods my-cluster-31feb41b-cruise-control-fb9768f77-9scrm are ready
2022-03-28 12:52:22 [ForkJoinPool-1-worker-3] INFO  [DeploymentUtils:141] Deployment my-cluster-31feb41b-cruise-control rolling update finished
2022-03-28 12:52:22 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-534609d0
2022-03-28 12:52:22 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Namespace namespace-2 removal
2022-03-28 12:52:22 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 12:52:23 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-534609d0 will have desired state: Ready
2022-03-28 12:52:23 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-534609d0 will have desired state: Ready
2022-03-28 12:52:23 [ForkJoinPool-1-worker-3] INFO  [CruiseControlConfigurationST:171] Verifying that Kafka pods did not roll
2022-03-28 12:52:23 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Waiting for stability of rolling update will be not triggered
2022-03-28 12:52:23 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:23 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1319784ms till timeout)
2022-03-28 12:52:23 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:23 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:23 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:52:23 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 50
2022-03-28 12:52:23 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (299784ms till timeout)
2022-03-28 12:52:23 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 12:52:23 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 12:52:23 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (479554ms till timeout)
2022-03-28 12:52:23 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-6 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:52:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (805280ms till timeout)
2022-03-28 12:52:23 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (93551ms till timeout)
2022-03-28 12:52:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1285853ms till timeout)
2022-03-28 12:52:24 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-6 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:52:24 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:52:24 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:52:24 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:52:24 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:52:24 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (4211ms till timeout)
2022-03-28 12:52:24 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1318674ms till timeout)
2022-03-28 12:52:24 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:24 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 12:52:24 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:24 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:24 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:52:24 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 49
2022-03-28 12:52:24 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (298674ms till timeout)
2022-03-28 12:52:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (804171ms till timeout)
2022-03-28 12:52:24 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 12:52:24 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 12:52:24 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (478133ms till timeout)
2022-03-28 12:52:25 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-6 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:52:25 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (92333ms till timeout)
2022-03-28 12:52:25 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1284739ms till timeout)
2022-03-28 12:52:25 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:25 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1317562ms till timeout)
2022-03-28 12:52:25 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-6 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:52:25 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:52:25 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:52:25 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:52:25 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:52:25 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (2681ms till timeout)
2022-03-28 12:52:25 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:25 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:25 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:52:25 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 48
2022-03-28 12:52:25 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (297560ms till timeout)
2022-03-28 12:52:25 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 12:52:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (803062ms till timeout)
2022-03-28 12:52:26 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 12:52:26 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 12:52:26 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (476708ms till timeout)
2022-03-28 12:52:26 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (91117ms till timeout)
2022-03-28 12:52:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1283522ms till timeout)
2022-03-28 12:52:26 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-6 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:52:26 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1316452ms till timeout)
2022-03-28 12:52:26 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:26 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:26 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:26 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:52:26 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 47
2022-03-28 12:52:26 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (296445ms till timeout)
2022-03-28 12:52:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (801929ms till timeout)
2022-03-28 12:52:27 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-6 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:52:27 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:52:27 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:52:27 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:52:27 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:52:27 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (1115ms till timeout)
2022-03-28 12:52:27 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 12:52:27 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (89896ms till timeout)
2022-03-28 12:52:27 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1282302ms till timeout)
2022-03-28 12:52:27 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 12:52:27 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 12:52:27 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (475239ms till timeout)
2022-03-28 12:52:27 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1315344ms till timeout)
2022-03-28 12:52:27 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:27 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:27 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:27 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:52:27 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 46
2022-03-28 12:52:27 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (295334ms till timeout)
2022-03-28 12:52:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (800819ms till timeout)
2022-03-28 12:52:28 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-6 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:52:28 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Failed to exec command: oc --namespace namespace-6 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092
2022-03-28 12:52:28 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 1
2022-03-28 12:52:28 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR START=======
2022-03-28 12:52:28 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found
2022-03-28 12:52:28 [ForkJoinPool-1-worker-15] INFO  [Exec:417] ======STDERR END======
2022-03-28 12:52:28 [ForkJoinPool-1-worker-15] ERROR [TestUtils:162] Exception waiting for KafkaTopic's spec will be stable, `oc --namespace namespace-6 exec my-cluster-30bb976d-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-712754859-1910580676 --describe --bootstrap-server my-cluster-30bb976d-kafka-bootstrap:9092` got status code 1 and stderr:
------
Error from server (NotFound): pods "my-cluster-30bb976d-kafka-0" not found

------
and stdout:
------

------
2022-03-28 12:52:28 [ForkJoinPool-1-worker-15] ERROR [TestExecutionWatcher:28] ReconciliationST - Exception Timeout after 180000 ms waiting for KafkaTopic's spec will be stable has been thrown in @Test. Going to collect logs from components.
2022-03-28 12:52:28 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 12:52:28 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (88677ms till timeout)
2022-03-28 12:52:28 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1281080ms till timeout)
2022-03-28 12:52:28 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:28 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1314187ms till timeout)
2022-03-28 12:52:29 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:29 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:29 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:52:29 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 45
2022-03-28 12:52:29 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (294034ms till timeout)
2022-03-28 12:52:29 [ForkJoinPool-1-worker-15] DEBUG [LogCollector:158] [reconciliation-st] adding to all namespaces, which should be collected: [infra-namespace, reconciliation-st, namespace-1]
2022-03-28 12:52:29 [ForkJoinPool-1-worker-15] INFO  [LogCollector:252] Collecting events in Namespace infra-namespace
2022-03-28 12:52:29 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace infra-namespace get events
2022-03-28 12:52:29 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 12:52:29 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 12:52:29 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (473804ms till timeout)
2022-03-28 12:52:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (799710ms till timeout)
2022-03-28 12:52:29 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace infra-namespace get events
2022-03-28 12:52:29 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 12:52:29 [ForkJoinPool-1-worker-15] INFO  [LogCollector:259] Collecting ConfigMaps in Namespace infra-namespace
2022-03-28 12:52:29 [ForkJoinPool-1-worker-15] INFO  [LogCollector:217] Collecting logs for Pod(s) in Namespace infra-namespace
2022-03-28 12:52:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1279971ms till timeout)
2022-03-28 12:52:29 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (87557ms till timeout)
2022-03-28 12:52:29 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1313068ms till timeout)
2022-03-28 12:52:30 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:30 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 12:52:30 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:30 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:30 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:52:30 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 44
2022-03-28 12:52:30 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (292826ms till timeout)
2022-03-28 12:52:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (798511ms till timeout)
2022-03-28 12:52:30 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 12:52:30 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 12:52:30 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (472393ms till timeout)
2022-03-28 12:52:30 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1278861ms till timeout)
2022-03-28 12:52:30 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace infra-namespace describe pod strimzi-cluster-operator-5dc4cd9447-hfqtf
2022-03-28 12:52:31 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (86330ms till timeout)
2022-03-28 12:52:31 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1311855ms till timeout)
2022-03-28 12:52:31 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:31 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:31 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:31 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:52:31 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 43
2022-03-28 12:52:31 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (291712ms till timeout)
2022-03-28 12:52:31 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 12:52:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (797394ms till timeout)
2022-03-28 12:52:31 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace infra-namespace describe pod strimzi-cluster-operator-5dc4cd9447-hfqtf
2022-03-28 12:52:31 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 12:52:31 [ForkJoinPool-1-worker-15] INFO  [LogCollector:266] Collecting Deployments in Namespace infra-namespace
2022-03-28 12:52:31 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace infra-namespace get Deployment -o yaml
2022-03-28 12:52:31 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 12:52:31 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 12:52:31 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (470961ms till timeout)
2022-03-28 12:52:32 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1277750ms till timeout)
2022-03-28 12:52:32 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (85214ms till timeout)
2022-03-28 12:52:32 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace infra-namespace get Deployment -o yaml
2022-03-28 12:52:32 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 12:52:32 [ForkJoinPool-1-worker-15] INFO  [LogCollector:271] Collecting StatefulSets in Namespace infra-namespace
2022-03-28 12:52:32 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace infra-namespace get StatefulSet -o yaml
2022-03-28 12:52:32 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1310730ms till timeout)
2022-03-28 12:52:32 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:32 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:32 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:32 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:52:32 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 42
2022-03-28 12:52:32 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (290601ms till timeout)
2022-03-28 12:52:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (796284ms till timeout)
2022-03-28 12:52:32 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace infra-namespace get StatefulSet -o yaml
2022-03-28 12:52:32 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 12:52:32 [ForkJoinPool-1-worker-15] INFO  [LogCollector:276] Collecting ReplicaSets in Namespace infra-namespace
2022-03-28 12:52:32 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace infra-namespace get replicaset -o yaml
2022-03-28 12:52:32 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 12:52:33 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace infra-namespace get replicaset -o yaml
2022-03-28 12:52:33 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 12:52:33 [ForkJoinPool-1-worker-15] INFO  [LogCollector:281] Collecting Strimzi in Namespace infra-namespace
2022-03-28 12:52:33 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc get strimzi -o yaml -n infra-namespace
2022-03-28 12:52:33 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1276640ms till timeout)
2022-03-28 12:52:33 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (84095ms till timeout)
2022-03-28 12:52:33 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 12:52:33 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 12:52:33 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (469548ms till timeout)
2022-03-28 12:52:33 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1309611ms till timeout)
2022-03-28 12:52:33 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:33 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:33 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:33 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:52:33 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 41
2022-03-28 12:52:33 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (289487ms till timeout)
2022-03-28 12:52:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (795170ms till timeout)
2022-03-28 12:52:33 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Failed to exec command: oc get strimzi -o yaml -n infra-namespace
2022-03-28 12:52:33 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 1
2022-03-28 12:52:33 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 12:52:33 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] error: the server doesn't have a resource type "strimzi"
2022-03-28 12:52:33 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] ======STDERR END======
2022-03-28 12:52:33 [ForkJoinPool-1-worker-15] INFO  [LogCollector:287] Collecting cluster status
2022-03-28 12:52:33 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc describe nodes
2022-03-28 12:52:34 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1275530ms till timeout)
2022-03-28 12:52:34 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 12:52:34 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (82979ms till timeout)
2022-03-28 12:52:34 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1308495ms till timeout)
2022-03-28 12:52:34 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:34 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:34 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:34 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:52:34 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 40
2022-03-28 12:52:34 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (288376ms till timeout)
2022-03-28 12:52:34 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 12:52:34 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 12:52:34 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-2 removal not ready, will try again in 1000 ms (468124ms till timeout)
2022-03-28 12:52:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (794060ms till timeout)
2022-03-28 12:52:35 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1274420ms till timeout)
2022-03-28 12:52:35 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (81861ms till timeout)
2022-03-28 12:52:35 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1307377ms till timeout)
2022-03-28 12:52:35 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:35 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 12:52:35 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:35 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:35 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:52:35 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 39
2022-03-28 12:52:35 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (287264ms till timeout)
2022-03-28 12:52:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (792949ms till timeout)
2022-03-28 12:52:36 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-6 get Namespace namespace-2 -o yaml
2022-03-28 12:52:36 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 1
2022-03-28 12:52:36 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 12:52:36 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-2" not found
2022-03-28 12:52:36 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] ======STDERR END======
2022-03-28 12:52:36 [ForkJoinPool-1-worker-13] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[user-st], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@aec142ef=[namespace-5], io.strimzi.test.logs.CollectorElement@3881d5f2=[namespace-4], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[infra-namespace, reconciliation-st, namespace-1], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-6], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 12:52:36 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:267] testCreatingUsersWithSecretPrefix - Notifies waiting test cases:[testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testTlsExternalUserWithQuotas, testUserTemplate, testUpdateUser, testTlsExternalUser, testReceiveSimpleMessageTlsScramSha, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testSendSimpleMessageTlsScramSha, testUserWithNameMoreThan64Chars, testTlsUserWithQuotas, testConfigurationFileIsCreated, testScramUserWithQuotas, testDeployAndUnDeployCruiseControl] to and randomly select one to start execution
2022-03-28 12:52:36 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:93] [operators.user.UserST] - Removing parallel test: testCreatingUsersWithSecretPrefix
2022-03-28 12:52:36 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:97] [operators.user.UserST] - Parallel test count: 5
2022-03-28 12:52:36 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.user.UserST.testCreatingUsersWithSecretPrefix-FINISHED
2022-03-28 12:52:36 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:30] ############################################################################
2022-03-28 12:52:36 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:689] ============================================================================
2022-03-28 12:52:36 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:690] [operators.user.UserST - After All] - Clean up after test suite
2022-03-28 12:52:36 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:344] ############################################################################
2022-03-28 12:52:36 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:348] Delete all resources for UserST
2022-03-28 12:52:36 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of Kafka user-cluster-name in namespace user-st
2022-03-28 12:52:36 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:user-cluster-name
2022-03-28 12:52:36 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:user-cluster-name not ready, will try again in 10000 ms (839889ms till timeout)
2022-03-28 12:52:36 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1273231ms till timeout)
2022-03-28 12:52:36 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (80697ms till timeout)
2022-03-28 12:52:36 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1306212ms till timeout)
2022-03-28 12:52:36 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:36 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:36 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:36 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:52:36 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 38
2022-03-28 12:52:36 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (286151ms till timeout)
2022-03-28 12:52:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (791833ms till timeout)
2022-03-28 12:52:37 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1272104ms till timeout)
2022-03-28 12:52:37 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (79478ms till timeout)
2022-03-28 12:52:37 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1305094ms till timeout)
2022-03-28 12:52:37 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:38 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:38 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:38 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:52:38 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 37
2022-03-28 12:52:38 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (285041ms till timeout)
2022-03-28 12:52:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (790723ms till timeout)
2022-03-28 12:52:38 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc describe nodes
2022-03-28 12:52:38 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 12:52:38 [ForkJoinPool-1-worker-15] INFO  [LogCollector:252] Collecting events in Namespace reconciliation-st
2022-03-28 12:52:38 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace reconciliation-st get events
2022-03-28 12:52:38 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1270994ms till timeout)
2022-03-28 12:52:38 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace reconciliation-st get events
2022-03-28 12:52:38 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 12:52:38 [ForkJoinPool-1-worker-15] INFO  [LogCollector:259] Collecting ConfigMaps in Namespace reconciliation-st
2022-03-28 12:52:39 [ForkJoinPool-1-worker-15] INFO  [LogCollector:217] Collecting logs for Pod(s) in Namespace reconciliation-st
2022-03-28 12:52:39 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:39 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1303888ms till timeout)
2022-03-28 12:52:39 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (78162ms till timeout)
2022-03-28 12:52:39 [ForkJoinPool-1-worker-15] INFO  [LogCollector:266] Collecting Deployments in Namespace reconciliation-st
2022-03-28 12:52:39 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace reconciliation-st get Deployment -o yaml
2022-03-28 12:52:39 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:39 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:39 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:52:39 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 36
2022-03-28 12:52:39 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (283886ms till timeout)
2022-03-28 12:52:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (789575ms till timeout)
2022-03-28 12:52:39 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace reconciliation-st get Deployment -o yaml
2022-03-28 12:52:39 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 12:52:39 [ForkJoinPool-1-worker-15] INFO  [LogCollector:271] Collecting StatefulSets in Namespace reconciliation-st
2022-03-28 12:52:39 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace reconciliation-st get StatefulSet -o yaml
2022-03-28 12:52:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1269883ms till timeout)
2022-03-28 12:52:40 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace reconciliation-st get StatefulSet -o yaml
2022-03-28 12:52:40 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 12:52:40 [ForkJoinPool-1-worker-15] INFO  [LogCollector:276] Collecting ReplicaSets in Namespace reconciliation-st
2022-03-28 12:52:40 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace reconciliation-st get replicaset -o yaml
2022-03-28 12:52:40 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1302779ms till timeout)
2022-03-28 12:52:40 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:40 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (76948ms till timeout)
2022-03-28 12:52:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (788461ms till timeout)
2022-03-28 12:52:40 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace reconciliation-st get replicaset -o yaml
2022-03-28 12:52:40 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 12:52:40 [ForkJoinPool-1-worker-15] INFO  [LogCollector:281] Collecting Strimzi in Namespace reconciliation-st
2022-03-28 12:52:40 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc get strimzi -o yaml -n reconciliation-st
2022-03-28 12:52:40 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:40 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:40 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:52:40 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 35
2022-03-28 12:52:40 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (282571ms till timeout)
2022-03-28 12:52:41 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1268772ms till timeout)
2022-03-28 12:52:41 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1301669ms till timeout)
2022-03-28 12:52:41 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:41 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (75733ms till timeout)
2022-03-28 12:52:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (787245ms till timeout)
2022-03-28 12:52:41 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:41 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:41 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:52:41 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 34
2022-03-28 12:52:41 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (281355ms till timeout)
2022-03-28 12:52:42 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1267661ms till timeout)
2022-03-28 12:52:42 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc get strimzi -o yaml -n reconciliation-st
2022-03-28 12:52:42 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 12:52:42 [ForkJoinPool-1-worker-15] INFO  [LogCollector:287] Collecting cluster status
2022-03-28 12:52:42 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc describe nodes
2022-03-28 12:52:42 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1300558ms till timeout)
2022-03-28 12:52:42 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:42 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (74414ms till timeout)
2022-03-28 12:52:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (785931ms till timeout)
2022-03-28 12:52:42 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:42 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:42 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:52:42 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 33
2022-03-28 12:52:43 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (280139ms till timeout)
2022-03-28 12:52:43 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1266552ms till timeout)
2022-03-28 12:52:43 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1299448ms till timeout)
2022-03-28 12:52:44 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:44 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (73198ms till timeout)
2022-03-28 12:52:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (784710ms till timeout)
2022-03-28 12:52:44 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:44 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:44 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:52:44 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 32
2022-03-28 12:52:44 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (278821ms till timeout)
2022-03-28 12:52:44 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1265442ms till timeout)
2022-03-28 12:52:44 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1298339ms till timeout)
2022-03-28 12:52:45 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:45 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (71879ms till timeout)
2022-03-28 12:52:45 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:45 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:45 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:52:45 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 31
2022-03-28 12:52:45 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (277564ms till timeout)
2022-03-28 12:52:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (783292ms till timeout)
2022-03-28 12:52:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1264183ms till timeout)
2022-03-28 12:52:45 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1297228ms till timeout)
2022-03-28 12:52:46 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc describe nodes
2022-03-28 12:52:46 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 12:52:46 [ForkJoinPool-1-worker-15] INFO  [LogCollector:252] Collecting events in Namespace namespace-1
2022-03-28 12:52:46 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-1 get events
2022-03-28 12:52:46 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:46 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:369] ############################################################################
2022-03-28 12:52:46 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:46 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:46 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:52:46 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 30
2022-03-28 12:52:46 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (276347ms till timeout)
2022-03-28 12:52:46 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (70518ms till timeout)
2022-03-28 12:52:46 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Namespace user-st removal
2022-03-28 12:52:46 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace user-st -o yaml
2022-03-28 12:52:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (782034ms till timeout)
2022-03-28 12:52:46 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1262924ms till timeout)
2022-03-28 12:52:46 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1296117ms till timeout)
2022-03-28 12:52:47 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-1 get events
2022-03-28 12:52:47 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 12:52:47 [ForkJoinPool-1-worker-15] INFO  [LogCollector:259] Collecting ConfigMaps in Namespace namespace-1
2022-03-28 12:52:47 [ForkJoinPool-1-worker-15] INFO  [LogCollector:217] Collecting logs for Pod(s) in Namespace namespace-1
2022-03-28 12:52:47 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace user-st -o yaml
2022-03-28 12:52:47 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 12:52:47 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (479548ms till timeout)
2022-03-28 12:52:47 [ForkJoinPool-1-worker-15] DEBUG [LogCollector:208] Collecting logs for TestCase: testPauseReconciliationInKafkaRebalanceAndTopic, and Pod: my-cluster-30bb976d-cruise-control-798647c8b9-qwgs6
2022-03-28 12:52:47 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:47 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:47 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:47 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:52:47 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 29
2022-03-28 12:52:47 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (275234ms till timeout)
2022-03-28 12:52:47 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-1 describe pod my-cluster-30bb976d-cruise-control-798647c8b9-qwgs6
2022-03-28 12:52:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (780921ms till timeout)
2022-03-28 12:52:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1261811ms till timeout)
2022-03-28 12:52:48 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (69394ms till timeout)
2022-03-28 12:52:48 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1294912ms till timeout)
2022-03-28 12:52:48 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace user-st -o yaml
2022-03-28 12:52:48 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-1 describe pod my-cluster-30bb976d-cruise-control-798647c8b9-qwgs6
2022-03-28 12:52:48 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 12:52:48 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-1 describe pod my-cluster-30bb976d-cruise-control-798647c8b9-qwgs6
2022-03-28 12:52:48 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace user-st -o yaml
2022-03-28 12:52:48 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 12:52:48 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (478060ms till timeout)
2022-03-28 12:52:48 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:49 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:49 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:49 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:52:49 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 28
2022-03-28 12:52:49 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (274123ms till timeout)
2022-03-28 12:52:49 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1260697ms till timeout)
2022-03-28 12:52:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (779805ms till timeout)
2022-03-28 12:52:49 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1293800ms till timeout)
2022-03-28 12:52:49 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (68169ms till timeout)
2022-03-28 12:52:49 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-1 describe pod my-cluster-30bb976d-cruise-control-798647c8b9-qwgs6
2022-03-28 12:52:49 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 12:52:49 [ForkJoinPool-1-worker-15] DEBUG [LogCollector:208] Collecting logs for TestCase: testPauseReconciliationInKafkaRebalanceAndTopic, and Pod: my-cluster-30bb976d-entity-operator-59d45bf857-8p8zl
2022-03-28 12:52:49 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-1 describe pod my-cluster-30bb976d-entity-operator-59d45bf857-8p8zl
2022-03-28 12:52:49 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace user-st -o yaml
2022-03-28 12:52:50 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:50 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:50 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:50 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:52:50 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 27
2022-03-28 12:52:50 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (273012ms till timeout)
2022-03-28 12:52:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (778691ms till timeout)
2022-03-28 12:52:50 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1259582ms till timeout)
2022-03-28 12:52:50 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace user-st -o yaml
2022-03-28 12:52:50 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 12:52:50 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (476600ms till timeout)
2022-03-28 12:52:50 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1292684ms till timeout)
2022-03-28 12:52:50 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-1 describe pod my-cluster-30bb976d-entity-operator-59d45bf857-8p8zl
2022-03-28 12:52:50 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 12:52:50 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (66953ms till timeout)
2022-03-28 12:52:50 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-1 describe pod my-cluster-30bb976d-entity-operator-59d45bf857-8p8zl
2022-03-28 12:52:51 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:51 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:51 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:51 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:52:51 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 26
2022-03-28 12:52:51 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (271898ms till timeout)
2022-03-28 12:52:51 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace user-st -o yaml
2022-03-28 12:52:51 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1258473ms till timeout)
2022-03-28 12:52:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (777579ms till timeout)
2022-03-28 12:52:51 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-1 describe pod my-cluster-30bb976d-entity-operator-59d45bf857-8p8zl
2022-03-28 12:52:51 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 12:52:51 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1291574ms till timeout)
2022-03-28 12:52:51 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-1 describe pod my-cluster-30bb976d-entity-operator-59d45bf857-8p8zl
2022-03-28 12:52:51 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (65732ms till timeout)
2022-03-28 12:52:51 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace user-st -o yaml
2022-03-28 12:52:51 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 12:52:51 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace user-st removal not ready, will try again in 1000 ms (475093ms till timeout)
2022-03-28 12:52:52 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:52 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-1 describe pod my-cluster-30bb976d-entity-operator-59d45bf857-8p8zl
2022-03-28 12:52:52 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 12:52:52 [ForkJoinPool-1-worker-15] DEBUG [LogCollector:208] Collecting logs for TestCase: testPauseReconciliationInKafkaRebalanceAndTopic, and Pod: my-cluster-30bb976d-kafka-0
2022-03-28 12:52:52 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:52 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:52 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:52:52 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 25
2022-03-28 12:52:52 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (270776ms till timeout)
2022-03-28 12:52:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (776459ms till timeout)
2022-03-28 12:52:52 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1257349ms till timeout)
2022-03-28 12:52:52 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1290246ms till timeout)
2022-03-28 12:52:52 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-1 describe pod my-cluster-30bb976d-kafka-0
2022-03-28 12:52:52 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (64618ms till timeout)
2022-03-28 12:52:52 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace user-st -o yaml
2022-03-28 12:52:53 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-6 get Namespace user-st -o yaml
2022-03-28 12:52:53 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 1
2022-03-28 12:52:53 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 12:52:53 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Error from server (NotFound): namespaces "user-st" not found
2022-03-28 12:52:53 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] ======STDERR END======
2022-03-28 12:52:53 [ForkJoinPool-1-worker-13] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@aec142ef=[namespace-5], io.strimzi.test.logs.CollectorElement@3881d5f2=[namespace-4], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[infra-namespace, reconciliation-st, namespace-1], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-6], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 12:52:53 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:254] UserST - Notifies waiting test suites:[UserST, TopicST, ReconciliationST, CruiseControlConfigurationST, HttpBridgeScramShaST, ThrottlingQuotaST] to and randomly select one to start execution
2022-03-28 12:52:53 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:85] [operators.user.UserST] - Removing parallel suite: UserST
2022-03-28 12:52:53 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:89] [operators.user.UserST] - Parallel suites count: 5
[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 423.393 s - in io.strimzi.systemtest.operators.user.UserST
2022-03-28 12:52:53 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:23] ############################################################################
2022-03-28 12:52:53 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationPerformanceOptions-STARTED
2022-03-28 12:52:53 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:658] ============================================================================
2022-03-28 12:52:53 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:659] [cruisecontrol.CruiseControlConfigurationST - Before Each] - Setup test case environment
2022-03-28 12:52:53 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:77] [cruisecontrol.CruiseControlConfigurationST] - Adding parallel test: testConfigurationPerformanceOptions
2022-03-28 12:52:53 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:81] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 6
2022-03-28 12:52:53 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:230] testConfigurationPerformanceOptions test now can proceed its execution
2022-03-28 12:52:53 [ForkJoinPool-1-worker-13] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 12:52:53 [ForkJoinPool-1-worker-13] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testScramUserWithQuotas=my-cluster-90c82d82, testReceiveSimpleMessageTlsScramSha=my-cluster-455915b9, testConfigurationFileIsCreated=my-cluster-020ceaf8, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5, testUpdateUser=my-cluster-e16ccf16, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d, testTlsExternalUserWithQuotas=my-cluster-3e2e11de, testUserWithNameMoreThan64Chars=my-cluster-ed39a269, testTlsUserWithQuotas=my-cluster-3b191485, testDeployAndUnDeployCruiseControl=my-cluster-534609d0, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2, testCapacityFile=my-cluster-39ee3f62, testConfigurationPerformanceOptions=my-cluster-bc0ed00a, testTlsExternalUser=my-cluster-965e9d1e, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-31feb41b, testSendSimpleMessageTlsScramSha=my-cluster-27773859, testUserTemplate=my-cluster-157df94e}
2022-03-28 12:52:53 [ForkJoinPool-1-worker-13] TRACE [AbstractST:607] USERS_NAME_MAP: {testScramUserWithQuotas=my-user-1857980152-2145707698, testReceiveSimpleMessageTlsScramSha=my-user-1329598231-1183867861, testConfigurationFileIsCreated=my-user-1609977058-1196012647, testCreatingUsersWithSecretPrefix=my-user-1928899390-1444689439, testUpdateUser=my-user-1694542041-806176635, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-467839358-1504804463, testTlsExternalUserWithQuotas=my-user-1786174319-1274634874, testUserWithNameMoreThan64Chars=my-user-1048816470-1103470031, testTlsUserWithQuotas=my-user-420001054-1296284129, testDeployAndUnDeployCruiseControl=my-user-475346616-1293712526, testThrottlingQuotasCreateTopic=my-user-896662438-260898887, testCapacityFile=my-user-597365340-661017603, testConfigurationPerformanceOptions=my-user-722474509-712960391, testTlsExternalUser=my-user-1295334081-1602801897, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2100837352-1014792902, testSendSimpleMessageTlsScramSha=my-user-507582460-1003875464, testUserTemplate=my-user-1253881534-190261188}
2022-03-28 12:52:53 [ForkJoinPool-1-worker-13] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testScramUserWithQuotas=my-topic-315112689-1043284277, testReceiveSimpleMessageTlsScramSha=my-topic-170116874-1830928683, testConfigurationFileIsCreated=my-topic-155202447-1921897899, testCreatingUsersWithSecretPrefix=my-topic-2033928996-1209892355, testUpdateUser=my-topic-1239518257-2056139809, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-712754859-1910580676, testTlsExternalUserWithQuotas=my-topic-593679216-1309035072, testUserWithNameMoreThan64Chars=my-topic-1603054043-1686316417, testTlsUserWithQuotas=my-topic-1752299353-1315959033, testDeployAndUnDeployCruiseControl=my-topic-1815491150-540070233, testThrottlingQuotasCreateTopic=my-topic-1532506723-1472611013, testCapacityFile=my-topic-1390944843-775067696, testConfigurationPerformanceOptions=my-topic-1891296149-770116276, testTlsExternalUser=my-topic-476529121-306489993, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1084162749-1576061083, testSendSimpleMessageTlsScramSha=my-topic-62182003-1963040205, testUserTemplate=my-topic-515111334-943904347}
2022-03-28 12:52:53 [ForkJoinPool-1-worker-13] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testScramUserWithQuotas=my-cluster-90c82d82-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-455915b9-kafka-clients, testConfigurationFileIsCreated=my-cluster-020ceaf8-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5-kafka-clients, testUpdateUser=my-cluster-e16ccf16-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-3e2e11de-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-ed39a269-kafka-clients, testTlsUserWithQuotas=my-cluster-3b191485-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-534609d0-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2-kafka-clients, testCapacityFile=my-cluster-39ee3f62-kafka-clients, testConfigurationPerformanceOptions=my-cluster-bc0ed00a-kafka-clients, testTlsExternalUser=my-cluster-965e9d1e-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-31feb41b-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-27773859-kafka-clients, testUserTemplate=my-cluster-157df94e-kafka-clients}
2022-03-28 12:52:53 [ForkJoinPool-1-worker-13] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-7 for test case:testConfigurationPerformanceOptions
2022-03-28 12:52:53 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:53 [ForkJoinPool-1-worker-13] INFO  [KubeClusterResource:156] Creating Namespace: namespace-7
2022-03-28 12:52:53 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:53 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:53 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:52:53 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 24
2022-03-28 12:52:53 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (269639ms till timeout)
2022-03-28 12:52:53 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-1 describe pod my-cluster-30bb976d-kafka-0
2022-03-28 12:52:53 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 12:52:53 [ForkJoinPool-1-worker-15] DEBUG [LogCollector:208] Collecting logs for TestCase: testPauseReconciliationInKafkaRebalanceAndTopic, and Pod: my-cluster-30bb976d-kafka-1
2022-03-28 12:52:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (775322ms till timeout)
2022-03-28 12:52:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1256213ms till timeout)
2022-03-28 12:52:53 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Namespace namespace-7
2022-03-28 12:52:53 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-6 get Namespace namespace-7 -o json
2022-03-28 12:52:53 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-1 describe pod my-cluster-30bb976d-kafka-1
2022-03-28 12:52:53 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1289137ms till timeout)
2022-03-28 12:52:54 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (63404ms till timeout)
2022-03-28 12:52:54 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-6 get Namespace namespace-7 -o json
2022-03-28 12:52:54 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 12:52:54 [ForkJoinPool-1-worker-13] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c28,c22",
            "openshift.io/sa.scc.supplemental-groups": "1000800000/10000",
            "openshift.io/sa.scc.uid-range": "1000800000/10000"
        },
        "creationTimestamp": "2022-03-28T12:52:49Z",
        "labels": {
            "kubernetes.io/metadata.name": "namespace-7"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T12:52:49Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T12:52:49Z"
            }
        ],
        "name": "namespace-7",
        "resourceVersion": "51034",
        "uid": "024f9029-863e-46a3-a2ce-d98c3cafaf2d"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 12:52:54 [ForkJoinPool-1-worker-13] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@aec142ef=[namespace-5], io.strimzi.test.logs.CollectorElement@f851b6c3=[namespace-7], io.strimzi.test.logs.CollectorElement@3881d5f2=[namespace-4], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[infra-namespace, reconciliation-st, namespace-1], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-6], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 12:52:54 [ForkJoinPool-1-worker-13] INFO  [KubeClusterResource:82] Client use Namespace: namespace-7
2022-03-28 12:52:54 [ForkJoinPool-1-worker-13] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-7, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 12:52:54 [ForkJoinPool-1-worker-13] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-7
2022-03-28 12:52:54 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-bc0ed00a in namespace namespace-7
2022-03-28 12:52:54 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:164] Using Namespace: namespace-7
2022-03-28 12:52:54 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-bc0ed00a
2022-03-28 12:52:54 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-bc0ed00a will have desired state: Ready
2022-03-28 12:52:54 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-bc0ed00a will have desired state: Ready
2022-03-28 12:52:54 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1319892ms till timeout)
2022-03-28 12:52:54 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:54 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:54 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:54 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:52:54 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 23
2022-03-28 12:52:54 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (268524ms till timeout)
2022-03-28 12:52:54 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-1 describe pod my-cluster-30bb976d-kafka-1
2022-03-28 12:52:54 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 12:52:54 [ForkJoinPool-1-worker-15] DEBUG [LogCollector:208] Collecting logs for TestCase: testPauseReconciliationInKafkaRebalanceAndTopic, and Pod: my-cluster-30bb976d-kafka-2
2022-03-28 12:52:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (774209ms till timeout)
2022-03-28 12:52:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1255100ms till timeout)
2022-03-28 12:52:54 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-1 describe pod my-cluster-30bb976d-kafka-2
2022-03-28 12:52:55 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1288025ms till timeout)
2022-03-28 12:52:55 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (62291ms till timeout)
2022-03-28 12:52:55 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1318784ms till timeout)
2022-03-28 12:52:55 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:55 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-1 describe pod my-cluster-30bb976d-kafka-2
2022-03-28 12:52:55 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 12:52:55 [ForkJoinPool-1-worker-15] DEBUG [LogCollector:208] Collecting logs for TestCase: testPauseReconciliationInKafkaRebalanceAndTopic, and Pod: my-cluster-30bb976d-zookeeper-0
2022-03-28 12:52:55 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:55 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:55 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:52:55 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 22
2022-03-28 12:52:55 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (267412ms till timeout)
2022-03-28 12:52:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1253987ms till timeout)
2022-03-28 12:52:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (773094ms till timeout)
2022-03-28 12:52:55 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-1 describe pod my-cluster-30bb976d-zookeeper-0
2022-03-28 12:52:56 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1286914ms till timeout)
2022-03-28 12:52:56 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (61172ms till timeout)
2022-03-28 12:52:56 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1317675ms till timeout)
2022-03-28 12:52:56 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-1 describe pod my-cluster-30bb976d-zookeeper-0
2022-03-28 12:52:56 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 12:52:56 [ForkJoinPool-1-worker-15] DEBUG [LogCollector:208] Collecting logs for TestCase: testPauseReconciliationInKafkaRebalanceAndTopic, and Pod: my-cluster-30bb976d-zookeeper-1
2022-03-28 12:52:56 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:56 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:56 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:56 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:52:56 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 21
2022-03-28 12:52:56 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (266243ms till timeout)
2022-03-28 12:52:56 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-1 describe pod my-cluster-30bb976d-zookeeper-1
2022-03-28 12:52:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1252829ms till timeout)
2022-03-28 12:52:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (771937ms till timeout)
2022-03-28 12:52:57 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1285802ms till timeout)
2022-03-28 12:52:57 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (60060ms till timeout)
2022-03-28 12:52:57 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-1 describe pod my-cluster-30bb976d-zookeeper-1
2022-03-28 12:52:57 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 12:52:57 [ForkJoinPool-1-worker-15] DEBUG [LogCollector:208] Collecting logs for TestCase: testPauseReconciliationInKafkaRebalanceAndTopic, and Pod: my-cluster-30bb976d-zookeeper-2
2022-03-28 12:52:57 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1316565ms till timeout)
2022-03-28 12:52:57 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:57 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-1 describe pod my-cluster-30bb976d-zookeeper-2
2022-03-28 12:52:58 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:58 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:58 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:52:58 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 20
2022-03-28 12:52:58 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (265125ms till timeout)
2022-03-28 12:52:58 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1251699ms till timeout)
2022-03-28 12:52:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (770806ms till timeout)
2022-03-28 12:52:58 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1284691ms till timeout)
2022-03-28 12:52:58 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (58945ms till timeout)
2022-03-28 12:52:58 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-1 describe pod my-cluster-30bb976d-zookeeper-2
2022-03-28 12:52:58 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 12:52:58 [ForkJoinPool-1-worker-15] INFO  [LogCollector:266] Collecting Deployments in Namespace namespace-1
2022-03-28 12:52:58 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-1 get Deployment -o yaml
2022-03-28 12:52:58 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1315456ms till timeout)
2022-03-28 12:52:59 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:59 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:59 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:52:59 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:52:59 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 19
2022-03-28 12:52:59 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (264009ms till timeout)
2022-03-28 12:52:59 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-1 get Deployment -o yaml
2022-03-28 12:52:59 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 12:52:59 [ForkJoinPool-1-worker-15] INFO  [LogCollector:271] Collecting StatefulSets in Namespace namespace-1
2022-03-28 12:52:59 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-1 get StatefulSet -o yaml
2022-03-28 12:52:59 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1250584ms till timeout)
2022-03-28 12:52:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (769691ms till timeout)
2022-03-28 12:52:59 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1283581ms till timeout)
2022-03-28 12:52:59 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (57830ms till timeout)
2022-03-28 12:52:59 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-1 get StatefulSet -o yaml
2022-03-28 12:52:59 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 12:52:59 [ForkJoinPool-1-worker-15] INFO  [LogCollector:276] Collecting ReplicaSets in Namespace namespace-1
2022-03-28 12:52:59 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-1 get replicaset -o yaml
2022-03-28 12:52:59 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1314347ms till timeout)
2022-03-28 12:53:00 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:00 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:00 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:00 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:53:00 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 18
2022-03-28 12:53:00 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (262894ms till timeout)
2022-03-28 12:53:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Kafka: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (768577ms till timeout)
2022-03-28 12:53:00 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1249468ms till timeout)
2022-03-28 12:53:00 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-1 get replicaset -o yaml
2022-03-28 12:53:00 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:00 [ForkJoinPool-1-worker-15] INFO  [LogCollector:281] Collecting Strimzi in Namespace namespace-1
2022-03-28 12:53:00 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc get strimzi -o yaml -n namespace-1
2022-03-28 12:53:00 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1282472ms till timeout)
2022-03-28 12:53:00 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (56685ms till timeout)
2022-03-28 12:53:01 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1313213ms till timeout)
2022-03-28 12:53:01 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:01 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:01 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:01 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:53:01 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 17
2022-03-28 12:53:01 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (261782ms till timeout)
2022-03-28 12:53:01 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:444] Kafka: http-bridge-tls-cluster-name is in desired state: Ready
2022-03-28 12:53:01 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update KafkaUser my-user-1621717414-1679128198 in namespace http-bridge-tls-st
2022-03-28 12:53:01 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1248356ms till timeout)
2022-03-28 12:53:01 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-1621717414-1679128198
2022-03-28 12:53:01 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1281341ms till timeout)
2022-03-28 12:53:01 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-1621717414-1679128198 will have desired state: Ready
2022-03-28 12:53:01 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-1621717414-1679128198 will have desired state: Ready
2022-03-28 12:53:01 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (55498ms till timeout)
2022-03-28 12:53:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] KafkaUser: my-user-1621717414-1679128198 will have desired state: Ready not ready, will try again in 1000 ms (179767ms till timeout)
2022-03-28 12:53:02 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1312103ms till timeout)
2022-03-28 12:53:02 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:02 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc get strimzi -o yaml -n namespace-1
2022-03-28 12:53:02 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:02 [ForkJoinPool-1-worker-15] INFO  [LogCollector:287] Collecting cluster status
2022-03-28 12:53:02 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc describe nodes
2022-03-28 12:53:02 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:02 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:02 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:53:02 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 16
2022-03-28 12:53:02 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (260671ms till timeout)
2022-03-28 12:53:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1247245ms till timeout)
2022-03-28 12:53:02 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1280233ms till timeout)
2022-03-28 12:53:03 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (54377ms till timeout)
2022-03-28 12:53:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] KafkaUser: my-user-1621717414-1679128198 will have desired state: Ready not ready, will try again in 1000 ms (178658ms till timeout)
2022-03-28 12:53:03 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1310994ms till timeout)
2022-03-28 12:53:03 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:03 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:03 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:03 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:53:03 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 15
2022-03-28 12:53:03 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (259558ms till timeout)
2022-03-28 12:53:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1246133ms till timeout)
2022-03-28 12:53:03 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1279123ms till timeout)
2022-03-28 12:53:04 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (53254ms till timeout)
2022-03-28 12:53:04 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:444] KafkaUser: my-user-1621717414-1679128198 is in desired state: Ready
2022-03-28 12:53:04 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1309831ms till timeout)
2022-03-28 12:53:04 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:04 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update Deployment http-bridge-tls-st-kafka-clients in namespace http-bridge-tls-st
2022-03-28 12:53:04 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:04 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:04 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:53:04 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 14
2022-03-28 12:53:04 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (258439ms till timeout)
2022-03-28 12:53:04 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1245013ms till timeout)
2022-03-28 12:53:04 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients
2022-03-28 12:53:05 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1278004ms till timeout)
2022-03-28 12:53:05 [ForkJoinPool-1-worker-9] INFO  [DeploymentUtils:161] Wait for Deployment: http-bridge-tls-st-kafka-clients will be ready
2022-03-28 12:53:05 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Wait for Deployment: http-bridge-tls-st-kafka-clients will be ready
2022-03-28 12:53:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Wait for Deployment: http-bridge-tls-st-kafka-clients will be ready not ready, will try again in 1000 ms (479891ms till timeout)
2022-03-28 12:53:05 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (52058ms till timeout)
2022-03-28 12:53:05 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1308723ms till timeout)
2022-03-28 12:53:05 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:05 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:05 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:05 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:53:05 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 13
2022-03-28 12:53:05 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (257329ms till timeout)
2022-03-28 12:53:05 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1243902ms till timeout)
2022-03-28 12:53:06 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1276893ms till timeout)
2022-03-28 12:53:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Wait for Deployment: http-bridge-tls-st-kafka-clients will be ready not ready, will try again in 1000 ms (478782ms till timeout)
2022-03-28 12:53:06 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc describe nodes
2022-03-28 12:53:06 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:06 [ForkJoinPool-1-worker-15] DEBUG [AbstractST:674] ============================================================================
2022-03-28 12:53:06 [ForkJoinPool-1-worker-15] DEBUG [AbstractST:675] [operators.ReconciliationST - After Each] - Clean up after test
2022-03-28 12:53:06 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:344] ############################################################################
2022-03-28 12:53:06 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:348] Delete all resources for testPauseReconciliationInKafkaRebalanceAndTopic
2022-03-28 12:53:06 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:241] Delete of KafkaTopic my-topic-712754859-1910580676 in namespace namespace-1
2022-03-28 12:53:06 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (50945ms till timeout)
2022-03-28 12:53:06 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-712754859-1910580676
2022-03-28 12:53:06 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:241] Delete of Kafka my-cluster-30bb976d in namespace namespace-1
2022-03-28 12:53:06 [ForkJoinPool-1-worker-15] INFO  [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-1, for cruise control Kafka cluster my-cluster-30bb976d
2022-03-28 12:53:06 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1307523ms till timeout)
2022-03-28 12:53:06 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:07 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:07 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:07 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:53:07 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 12
2022-03-28 12:53:07 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (256130ms till timeout)
2022-03-28 12:53:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1242703ms till timeout)
2022-03-28 12:53:07 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1275696ms till timeout)
2022-03-28 12:53:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Wait for Deployment: http-bridge-tls-st-kafka-clients will be ready not ready, will try again in 1000 ms (477587ms till timeout)
2022-03-28 12:53:07 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (49749ms till timeout)
2022-03-28 12:53:07 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1306328ms till timeout)
2022-03-28 12:53:08 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:08 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-30bb976d
2022-03-28 12:53:08 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:08 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:08 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:53:08 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 11
2022-03-28 12:53:08 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (254936ms till timeout)
2022-03-28 12:53:08 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1241504ms till timeout)
2022-03-28 12:53:08 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:369] ############################################################################
2022-03-28 12:53:08 [ForkJoinPool-1-worker-15] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-1 for test case:testPauseReconciliationInKafkaRebalanceAndTopic
2022-03-28 12:53:08 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Namespace namespace-1 removal
2022-03-28 12:53:08 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-1 -o yaml
2022-03-28 12:53:08 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1274479ms till timeout)
2022-03-28 12:53:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Wait for Deployment: http-bridge-tls-st-kafka-clients will be ready not ready, will try again in 1000 ms (476470ms till timeout)
2022-03-28 12:53:08 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (48634ms till timeout)
2022-03-28 12:53:09 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1305220ms till timeout)
2022-03-28 12:53:09 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:09 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:09 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:09 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:53:09 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 10
2022-03-28 12:53:09 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (253818ms till timeout)
2022-03-28 12:53:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1240391ms till timeout)
2022-03-28 12:53:09 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1273368ms till timeout)
2022-03-28 12:53:09 [ForkJoinPool-1-worker-9] INFO  [DeploymentUtils:168] Deployment: http-bridge-tls-st-kafka-clients is ready
2022-03-28 12:53:09 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update KafkaBridge http-bridge-tls-cluster-name in namespace http-bridge-tls-st
2022-03-28 12:53:10 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (47401ms till timeout)
2022-03-28 12:53:10 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaBridge:http-bridge-tls-cluster-name
2022-03-28 12:53:10 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1304104ms till timeout)
2022-03-28 12:53:10 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:433] Wait for KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready
2022-03-28 12:53:10 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready
2022-03-28 12:53:10 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (479891ms till timeout)
2022-03-28 12:53:10 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:10 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:10 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:53:10 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 9
2022-03-28 12:53:10 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (252601ms till timeout)
2022-03-28 12:53:10 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1239175ms till timeout)
2022-03-28 12:53:10 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1272258ms till timeout)
2022-03-28 12:53:11 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (46287ms till timeout)
2022-03-28 12:53:11 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1302993ms till timeout)
2022-03-28 12:53:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (478783ms till timeout)
2022-03-28 12:53:11 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:11 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:11 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:11 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:53:11 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 8
2022-03-28 12:53:11 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (251490ms till timeout)
2022-03-28 12:53:11 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1238064ms till timeout)
2022-03-28 12:53:12 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1271022ms till timeout)
2022-03-28 12:53:12 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (45172ms till timeout)
2022-03-28 12:53:12 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1301880ms till timeout)
2022-03-28 12:53:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (477673ms till timeout)
2022-03-28 12:53:12 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:12 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:12 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:12 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:53:12 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 7
2022-03-28 12:53:12 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (250378ms till timeout)
2022-03-28 12:53:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1236951ms till timeout)
2022-03-28 12:53:13 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1269908ms till timeout)
2022-03-28 12:53:13 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (44057ms till timeout)
2022-03-28 12:53:13 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1300769ms till timeout)
2022-03-28 12:53:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (476561ms till timeout)
2022-03-28 12:53:13 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:13 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:13 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:13 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:53:13 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 6
2022-03-28 12:53:13 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (249267ms till timeout)
2022-03-28 12:53:13 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-1 -o yaml
2022-03-28 12:53:13 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:13 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (474503ms till timeout)
2022-03-28 12:53:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1235834ms till timeout)
2022-03-28 12:53:14 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1268787ms till timeout)
2022-03-28 12:53:14 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (42943ms till timeout)
2022-03-28 12:53:14 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1299660ms till timeout)
2022-03-28 12:53:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (475452ms till timeout)
2022-03-28 12:53:14 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:14 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-1 -o yaml
2022-03-28 12:53:14 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:14 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:14 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:53:14 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 5
2022-03-28 12:53:14 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (248152ms till timeout)
2022-03-28 12:53:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1234723ms till timeout)
2022-03-28 12:53:15 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1267676ms till timeout)
2022-03-28 12:53:15 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-1 -o yaml
2022-03-28 12:53:15 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:15 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (473007ms till timeout)
2022-03-28 12:53:15 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (41828ms till timeout)
2022-03-28 12:53:15 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1298549ms till timeout)
2022-03-28 12:53:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (474343ms till timeout)
2022-03-28 12:53:15 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:16 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:16 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:16 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:53:16 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 4
2022-03-28 12:53:16 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (247041ms till timeout)
2022-03-28 12:53:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1233615ms till timeout)
2022-03-28 12:53:16 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-1 -o yaml
2022-03-28 12:53:16 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1266566ms till timeout)
2022-03-28 12:53:16 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (40714ms till timeout)
2022-03-28 12:53:16 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1297439ms till timeout)
2022-03-28 12:53:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (473233ms till timeout)
2022-03-28 12:53:17 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:17 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:17 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:17 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:53:17 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 3
2022-03-28 12:53:17 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (245929ms till timeout)
2022-03-28 12:53:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1232503ms till timeout)
2022-03-28 12:53:17 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1265455ms till timeout)
2022-03-28 12:53:17 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (39601ms till timeout)
2022-03-28 12:53:17 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1296330ms till timeout)
2022-03-28 12:53:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (472123ms till timeout)
2022-03-28 12:53:18 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:18 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:18 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:18 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:53:18 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 2
2022-03-28 12:53:18 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (244818ms till timeout)
2022-03-28 12:53:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1231389ms till timeout)
2022-03-28 12:53:18 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1264341ms till timeout)
2022-03-28 12:53:18 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (38488ms till timeout)
2022-03-28 12:53:19 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1295221ms till timeout)
2022-03-28 12:53:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (471015ms till timeout)
2022-03-28 12:53:19 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:19 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:19 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:19 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:53:19 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 1
2022-03-28 12:53:19 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (243707ms till timeout)
2022-03-28 12:53:19 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1230280ms till timeout)
2022-03-28 12:53:19 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1263230ms till timeout)
2022-03-28 12:53:20 [ForkJoinPool-1-worker-11] INFO  [PodUtils:189] Message org.apache.kafka.common.errors.ThrottlingQuotaExceededException: The throttling quota has been exceeded. found in create-admin-my-cluster-0690c7b2-kafka-clients-m8d7g log
2022-03-28 12:53:20 [ForkJoinPool-1-worker-11] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment create-admin-my-cluster-0690c7b2-kafka-clients deletion
2022-03-28 12:53:20 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for ReplicaSet create-admin-my-cluster-0690c7b2-kafka-clients to be deleted
2022-03-28 12:53:20 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1294061ms till timeout)
2022-03-28 12:53:20 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] ReplicaSet create-admin-my-cluster-0690c7b2-kafka-clients to be deleted not ready, will try again in 5000 ms (179790ms till timeout)
2022-03-28 12:53:20 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (469852ms till timeout)
2022-03-28 12:53:20 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:20 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa}
2022-03-28 12:53:20 [ForkJoinPool-1-worker-3] DEBUG [RollingUpdateUtils:50] At least my-cluster-31feb41b-kafka-0 hasn't rolled
2022-03-28 12:53:20 [ForkJoinPool-1-worker-3] INFO  [RollingUpdateUtils:143] {my-cluster-31feb41b-kafka-0=e948c027-1c91-4c31-aa59-5af8fe68aa04, my-cluster-31feb41b-kafka-1=dd653b46-a2ac-441b-8edd-29d4bfb49dc8, my-cluster-31feb41b-kafka-2=09dd42e8-73da-4c53-a98a-2b036b2962fa} pods didn't roll. Remaining seconds for stability: 0
2022-03-28 12:53:20 [ForkJoinPool-1-worker-3] INFO  [CruiseControlConfigurationST:174] Verifying new configuration in the Kafka CR
2022-03-28 12:53:20 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1229137ms till timeout)
2022-03-28 12:53:20 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Verify that kafka contains cruise control topics with related configuration.
2022-03-28 12:53:20 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Verify that kafka contains cruise control topics with related configuration.
2022-03-28 12:53:21 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1262009ms till timeout)
2022-03-28 12:53:21 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:674] ============================================================================
2022-03-28 12:53:21 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:675] [cruisecontrol.CruiseControlConfigurationST - After Each] - Clean up after test
2022-03-28 12:53:21 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:344] ############################################################################
2022-03-28 12:53:21 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:348] Delete all resources for testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods
2022-03-28 12:53:21 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:241] Delete of Kafka my-cluster-31feb41b in namespace namespace-4
2022-03-28 12:53:21 [ForkJoinPool-1-worker-3] INFO  [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-4, for cruise control Kafka cluster my-cluster-31feb41b
2022-03-28 12:53:21 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1292859ms till timeout)
2022-03-28 12:53:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (468652ms till timeout)
2022-03-28 12:53:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1227940ms till timeout)
2022-03-28 12:53:21 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-1 -o yaml
2022-03-28 12:53:21 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (466479ms till timeout)
2022-03-28 12:53:22 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1260822ms till timeout)
2022-03-28 12:53:22 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-31feb41b
2022-03-28 12:53:22 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1291668ms till timeout)
2022-03-28 12:53:22 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:369] ############################################################################
2022-03-28 12:53:22 [ForkJoinPool-1-worker-3] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-4 for test case:testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods
2022-03-28 12:53:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (467446ms till timeout)
2022-03-28 12:53:22 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-1 -o yaml
2022-03-28 12:53:22 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Namespace namespace-4 removal
2022-03-28 12:53:22 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 12:53:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1226729ms till timeout)
2022-03-28 12:53:23 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1259713ms till timeout)
2022-03-28 12:53:23 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1290557ms till timeout)
2022-03-28 12:53:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (466336ms till timeout)
2022-03-28 12:53:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1225619ms till timeout)
2022-03-28 12:53:24 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1258603ms till timeout)
2022-03-28 12:53:24 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1289447ms till timeout)
2022-03-28 12:53:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (465227ms till timeout)
2022-03-28 12:53:25 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1224509ms till timeout)
2022-03-28 12:53:25 [ForkJoinPool-1-worker-11] DEBUG [JobUtils:40] Job create-admin-my-cluster-0690c7b2-kafka-clients was deleted
2022-03-28 12:53:25 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1257454ms till timeout)
2022-03-28 12:53:25 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-0
2022-03-28 12:53:25 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1288338ms till timeout)
2022-03-28 12:53:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (464116ms till timeout)
2022-03-28 12:53:26 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-0
2022-03-28 12:53:26 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:26 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-1
2022-03-28 12:53:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1223397ms till timeout)
2022-03-28 12:53:26 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1256343ms till timeout)
2022-03-28 12:53:27 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1287228ms till timeout)
2022-03-28 12:53:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (463006ms till timeout)
2022-03-28 12:53:27 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1222288ms till timeout)
2022-03-28 12:53:27 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1255233ms till timeout)
2022-03-28 12:53:28 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1286117ms till timeout)
2022-03-28 12:53:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (461897ms till timeout)
2022-03-28 12:53:28 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-1 -o yaml
2022-03-28 12:53:28 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:28 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (459993ms till timeout)
2022-03-28 12:53:28 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 12:53:28 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:28 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (474519ms till timeout)
2022-03-28 12:53:28 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1221179ms till timeout)
2022-03-28 12:53:28 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1254124ms till timeout)
2022-03-28 12:53:29 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1285007ms till timeout)
2022-03-28 12:53:29 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-1 -o yaml
2022-03-28 12:53:29 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 12:53:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (460785ms till timeout)
2022-03-28 12:53:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1220068ms till timeout)
2022-03-28 12:53:30 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1253014ms till timeout)
2022-03-28 12:53:30 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 12:53:30 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:30 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (472596ms till timeout)
2022-03-28 12:53:30 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-1 -o yaml
2022-03-28 12:53:30 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:30 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (458047ms till timeout)
2022-03-28 12:53:30 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1283899ms till timeout)
2022-03-28 12:53:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] KafkaBridge: http-bridge-tls-cluster-name will have desired state: Ready not ready, will try again in 1000 ms (459675ms till timeout)
2022-03-28 12:53:30 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1218959ms till timeout)
2022-03-28 12:53:31 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1251904ms till timeout)
2022-03-28 12:53:31 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 12:53:31 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-1 -o yaml
2022-03-28 12:53:31 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1282787ms till timeout)
2022-03-28 12:53:31 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:444] KafkaBridge: http-bridge-tls-cluster-name is in desired state: Ready
2022-03-28 12:53:31 [ForkJoinPool-1-worker-9] INFO  [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-03-28 12:53:31 [ForkJoinPool-1-worker-9] INFO  [TestSeparator:23] ############################################################################
2022-03-28 12:53:31 [ForkJoinPool-1-worker-9] INFO  [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeTlsST.testSendSimpleMessageTls-STARTED
2022-03-28 12:53:31 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:658] ============================================================================
2022-03-28 12:53:31 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:659] [bridge.HttpBridgeTlsST - Before Each] - Setup test case environment
2022-03-28 12:53:31 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:77] [bridge.HttpBridgeTlsST] - Adding parallel test: testSendSimpleMessageTls
2022-03-28 12:53:31 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:81] [bridge.HttpBridgeTlsST] - Parallel test count: 7
2022-03-28 12:53:31 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:205] [testSendSimpleMessageTls] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:53:31 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:210] testSendSimpleMessageTls is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:53:31 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 12:53:31 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:31 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (471131ms till timeout)
2022-03-28 12:53:31 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-1 -o yaml
2022-03-28 12:53:31 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:31 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (456587ms till timeout)
2022-03-28 12:53:31 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1217850ms till timeout)
2022-03-28 12:53:32 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-1
2022-03-28 12:53:32 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:32 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-10
2022-03-28 12:53:32 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1250794ms till timeout)
2022-03-28 12:53:32 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-10
2022-03-28 12:53:32 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:32 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-106
2022-03-28 12:53:32 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1281676ms till timeout)
2022-03-28 12:53:32 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 12:53:32 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-1 -o yaml
2022-03-28 12:53:33 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Kafka: my-cluster-020ceaf8 will have desired state: Ready not ready, will try again in 1000 ms (1216739ms till timeout)
2022-03-28 12:53:33 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-106
2022-03-28 12:53:33 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:33 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-107
2022-03-28 12:53:33 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-1 -o yaml
2022-03-28 12:53:33 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:33 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (455141ms till timeout)
2022-03-28 12:53:33 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 12:53:33 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:33 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (469678ms till timeout)
2022-03-28 12:53:33 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1249685ms till timeout)
2022-03-28 12:53:33 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-107
2022-03-28 12:53:33 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:33 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-109
2022-03-28 12:53:33 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1280567ms till timeout)
2022-03-28 12:53:34 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:444] Kafka: my-cluster-020ceaf8 is in desired state: Ready
2022-03-28 12:53:34 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-109
2022-03-28 12:53:34 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:34 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-11
2022-03-28 12:53:34 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-1 -o yaml
2022-03-28 12:53:34 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 12:53:34 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1248522ms till timeout)
2022-03-28 12:53:34 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-5 exec my-cluster-020ceaf8-cruise-control-7654f699c4-62w29 -- /bin/bash -c cat /tmp/cruisecontrol.properties
2022-03-28 12:53:34 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-1 -o yaml
2022-03-28 12:53:34 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:34 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (453670ms till timeout)
2022-03-28 12:53:34 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 12:53:34 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:34 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (468204ms till timeout)
2022-03-28 12:53:34 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-11
2022-03-28 12:53:34 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:34 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-111
2022-03-28 12:53:34 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1279457ms till timeout)
2022-03-28 12:53:35 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-111
2022-03-28 12:53:35 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:35 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-12
2022-03-28 12:53:35 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1247414ms till timeout)
2022-03-28 12:53:35 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Command: oc --namespace namespace-5 exec my-cluster-020ceaf8-cruise-control-7654f699c4-62w29 -- /bin/bash -c cat /tmp/cruisecontrol.properties
2022-03-28 12:53:35 [ForkJoinPool-1-worker-7] INFO  [Exec:417] Return code: 0
2022-03-28 12:53:35 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:674] ============================================================================
2022-03-28 12:53:35 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:675] [cruisecontrol.CruiseControlConfigurationST - After Each] - Clean up after test
2022-03-28 12:53:35 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:344] ############################################################################
2022-03-28 12:53:35 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:348] Delete all resources for testConfigurationFileIsCreated
2022-03-28 12:53:35 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of Kafka my-cluster-020ceaf8 in namespace namespace-5
2022-03-28 12:53:35 [ForkJoinPool-1-worker-7] INFO  [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-5, for cruise control Kafka cluster my-cluster-020ceaf8
2022-03-28 12:53:35 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-1 -o yaml
2022-03-28 12:53:35 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 12:53:35 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-12
2022-03-28 12:53:35 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:35 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-120
2022-03-28 12:53:35 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1278323ms till timeout)
2022-03-28 12:53:36 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-1 -o yaml
2022-03-28 12:53:36 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:36 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (452185ms till timeout)
2022-03-28 12:53:36 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 12:53:36 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:36 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (466712ms till timeout)
2022-03-28 12:53:36 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-120
2022-03-28 12:53:36 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:36 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-121
2022-03-28 12:53:36 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-020ceaf8
2022-03-28 12:53:36 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:230] testSendSimpleMessageTls test now can proceed its execution
2022-03-28 12:53:36 [ForkJoinPool-1-worker-9] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 12:53:36 [ForkJoinPool-1-worker-9] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testScramUserWithQuotas=my-cluster-90c82d82, testReceiveSimpleMessageTlsScramSha=my-cluster-455915b9, testConfigurationFileIsCreated=my-cluster-020ceaf8, testSendSimpleMessageTls=my-cluster-cfce2f5b, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5, testUpdateUser=my-cluster-e16ccf16, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d, testTlsExternalUserWithQuotas=my-cluster-3e2e11de, testUserWithNameMoreThan64Chars=my-cluster-ed39a269, testTlsUserWithQuotas=my-cluster-3b191485, testDeployAndUnDeployCruiseControl=my-cluster-534609d0, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2, testCapacityFile=my-cluster-39ee3f62, testConfigurationPerformanceOptions=my-cluster-bc0ed00a, testTlsExternalUser=my-cluster-965e9d1e, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-31feb41b, testSendSimpleMessageTlsScramSha=my-cluster-27773859, testUserTemplate=my-cluster-157df94e}
2022-03-28 12:53:36 [ForkJoinPool-1-worker-9] TRACE [AbstractST:607] USERS_NAME_MAP: {testScramUserWithQuotas=my-user-1857980152-2145707698, testReceiveSimpleMessageTlsScramSha=my-user-1329598231-1183867861, testConfigurationFileIsCreated=my-user-1609977058-1196012647, testSendSimpleMessageTls=my-user-111586425-1868899021, testCreatingUsersWithSecretPrefix=my-user-1928899390-1444689439, testUpdateUser=my-user-1694542041-806176635, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-467839358-1504804463, testTlsExternalUserWithQuotas=my-user-1786174319-1274634874, testUserWithNameMoreThan64Chars=my-user-1048816470-1103470031, testTlsUserWithQuotas=my-user-420001054-1296284129, testDeployAndUnDeployCruiseControl=my-user-475346616-1293712526, testThrottlingQuotasCreateTopic=my-user-896662438-260898887, testCapacityFile=my-user-597365340-661017603, testConfigurationPerformanceOptions=my-user-722474509-712960391, testTlsExternalUser=my-user-1295334081-1602801897, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2100837352-1014792902, testSendSimpleMessageTlsScramSha=my-user-507582460-1003875464, testUserTemplate=my-user-1253881534-190261188}
2022-03-28 12:53:36 [ForkJoinPool-1-worker-9] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testScramUserWithQuotas=my-topic-315112689-1043284277, testReceiveSimpleMessageTlsScramSha=my-topic-170116874-1830928683, testConfigurationFileIsCreated=my-topic-155202447-1921897899, testSendSimpleMessageTls=my-topic-100192180-936335618, testCreatingUsersWithSecretPrefix=my-topic-2033928996-1209892355, testUpdateUser=my-topic-1239518257-2056139809, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-712754859-1910580676, testTlsExternalUserWithQuotas=my-topic-593679216-1309035072, testUserWithNameMoreThan64Chars=my-topic-1603054043-1686316417, testTlsUserWithQuotas=my-topic-1752299353-1315959033, testDeployAndUnDeployCruiseControl=my-topic-1815491150-540070233, testThrottlingQuotasCreateTopic=my-topic-1532506723-1472611013, testCapacityFile=my-topic-1390944843-775067696, testConfigurationPerformanceOptions=my-topic-1891296149-770116276, testTlsExternalUser=my-topic-476529121-306489993, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1084162749-1576061083, testSendSimpleMessageTlsScramSha=my-topic-62182003-1963040205, testUserTemplate=my-topic-515111334-943904347}
2022-03-28 12:53:36 [ForkJoinPool-1-worker-9] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testScramUserWithQuotas=my-cluster-90c82d82-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-455915b9-kafka-clients, testConfigurationFileIsCreated=my-cluster-020ceaf8-kafka-clients, testSendSimpleMessageTls=my-cluster-cfce2f5b-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5-kafka-clients, testUpdateUser=my-cluster-e16ccf16-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-3e2e11de-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-ed39a269-kafka-clients, testTlsUserWithQuotas=my-cluster-3b191485-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-534609d0-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2-kafka-clients, testCapacityFile=my-cluster-39ee3f62-kafka-clients, testConfigurationPerformanceOptions=my-cluster-bc0ed00a-kafka-clients, testTlsExternalUser=my-cluster-965e9d1e-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-31feb41b-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-27773859-kafka-clients, testUserTemplate=my-cluster-157df94e-kafka-clients}
2022-03-28 12:53:36 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update KafkaTopic my-topic-1486414252-1474955860 in namespace http-bridge-tls-st
2022-03-28 12:53:36 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1246254ms till timeout)
2022-03-28 12:53:36 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-020ceaf8 not ready, will try again in 10000 ms (839771ms till timeout)
2022-03-28 12:53:36 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1486414252-1474955860
2022-03-28 12:53:37 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-1486414252-1474955860 will have desired state: Ready
2022-03-28 12:53:37 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-1486414252-1474955860 will have desired state: Ready
2022-03-28 12:53:37 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1277159ms till timeout)
2022-03-28 12:53:37 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-121
2022-03-28 12:53:37 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:37 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-122
2022-03-28 12:53:37 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:444] KafkaTopic: my-topic-1486414252-1474955860 is in desired state: Ready
2022-03-28 12:53:37 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update Job producer-1516930780 in namespace http-bridge-tls-st
2022-03-28 12:53:37 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-1 -o yaml
2022-03-28 12:53:37 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 12:53:37 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:producer-1516930780
2022-03-28 12:53:37 [ForkJoinPool-1-worker-9] INFO  [JobUtils:81] Waiting for job: producer-1516930780 will be in active state
2022-03-28 12:53:37 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 12:53:37 [ForkJoinPool-1-worker-9] INFO  [ClientUtils:76] Waiting for producer/consumer:producer-1516930780 to finished
2022-03-28 12:53:37 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 12:53:37 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job producer-1516930780 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:53:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:53:37 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-122
2022-03-28 12:53:37 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:37 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-123
2022-03-28 12:53:37 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-1 -o yaml
2022-03-28 12:53:37 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:37 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-1 removal not ready, will try again in 1000 ms (450692ms till timeout)
2022-03-28 12:53:37 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 12:53:37 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:37 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (465217ms till timeout)
2022-03-28 12:53:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (219782ms till timeout)
2022-03-28 12:53:37 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1245084ms till timeout)
2022-03-28 12:53:38 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1276051ms till timeout)
2022-03-28 12:53:38 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-123
2022-03-28 12:53:38 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:38 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-124
2022-03-28 12:53:38 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-1 -o yaml
2022-03-28 12:53:38 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 12:53:38 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-124
2022-03-28 12:53:38 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:38 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-125
2022-03-28 12:53:38 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job producer-1516930780 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:53:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:53:39 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1243972ms till timeout)
2022-03-28 12:53:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (218456ms till timeout)
2022-03-28 12:53:39 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-7 get Namespace namespace-1 -o yaml
2022-03-28 12:53:39 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 1
2022-03-28 12:53:39 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 12:53:39 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-1" not found
2022-03-28 12:53:39 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] ======STDERR END======
2022-03-28 12:53:39 [ForkJoinPool-1-worker-15] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@aec142ef=[namespace-5], io.strimzi.test.logs.CollectorElement@f851b6c3=[namespace-7], io.strimzi.test.logs.CollectorElement@3881d5f2=[namespace-4], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[infra-namespace, reconciliation-st], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-6], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 12:53:39 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:267] testPauseReconciliationInKafkaRebalanceAndTopic - Notifies waiting test cases:[testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testTlsExternalUserWithQuotas, testUserTemplate, testUpdateUser, testTlsExternalUser, testReceiveSimpleMessageTlsScramSha, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testSendSimpleMessageTlsScramSha, testUserWithNameMoreThan64Chars, testTlsUserWithQuotas, testConfigurationFileIsCreated, testScramUserWithQuotas, testDeployAndUnDeployCruiseControl, testConfigurationPerformanceOptions] to and randomly select one to start execution
2022-03-28 12:53:39 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:93] [operators.ReconciliationST] - Removing parallel test: testPauseReconciliationInKafkaRebalanceAndTopic
2022-03-28 12:53:39 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:97] [operators.ReconciliationST] - Parallel test count: 6
2022-03-28 12:53:39 [ForkJoinPool-1-worker-15] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.ReconciliationST.testPauseReconciliationInKafkaRebalanceAndTopic-FINISHED
2022-03-28 12:53:39 [ForkJoinPool-1-worker-15] INFO  [TestSeparator:30] ############################################################################
2022-03-28 12:53:39 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 12:53:39 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:39 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (463765ms till timeout)
2022-03-28 12:53:39 [ForkJoinPool-1-worker-15] INFO  [TestSeparator:23] ############################################################################
2022-03-28 12:53:39 [ForkJoinPool-1-worker-15] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.ReconciliationST.testPauseReconciliationInKafkaAndKafkaConnectWithConnector-STARTED
2022-03-28 12:53:39 [ForkJoinPool-1-worker-15] DEBUG [AbstractST:658] ============================================================================
2022-03-28 12:53:39 [ForkJoinPool-1-worker-15] DEBUG [AbstractST:659] [operators.ReconciliationST - Before Each] - Setup test case environment
2022-03-28 12:53:39 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:77] [operators.ReconciliationST] - Adding parallel test: testPauseReconciliationInKafkaAndKafkaConnectWithConnector
2022-03-28 12:53:39 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:81] [operators.ReconciliationST] - Parallel test count: 7
2022-03-28 12:53:39 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:205] [testPauseReconciliationInKafkaAndKafkaConnectWithConnector] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:53:39 [ForkJoinPool-1-worker-15] TRACE [SuiteThreadController:210] testPauseReconciliationInKafkaAndKafkaConnectWithConnector is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:53:39 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-125
2022-03-28 12:53:39 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:39 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-126
2022-03-28 12:53:39 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1274942ms till timeout)
2022-03-28 12:53:39 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-126
2022-03-28 12:53:39 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:39 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-127
2022-03-28 12:53:40 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1242862ms till timeout)
2022-03-28 12:53:40 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 12:53:40 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job producer-1516930780 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:53:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:53:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (217238ms till timeout)
2022-03-28 12:53:40 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-127
2022-03-28 12:53:40 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:40 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-128
2022-03-28 12:53:40 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1273824ms till timeout)
2022-03-28 12:53:40 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 12:53:40 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:40 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (462342ms till timeout)
2022-03-28 12:53:40 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-128
2022-03-28 12:53:40 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:40 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-129
2022-03-28 12:53:41 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1241751ms till timeout)
2022-03-28 12:53:41 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-129
2022-03-28 12:53:41 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:41 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-13
2022-03-28 12:53:41 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job producer-1516930780 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:53:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:53:41 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1272716ms till timeout)
2022-03-28 12:53:41 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 12:53:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (215915ms till timeout)
2022-03-28 12:53:41 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-13
2022-03-28 12:53:41 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:41 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-133
2022-03-28 12:53:42 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 12:53:42 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:42 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (460923ms till timeout)
2022-03-28 12:53:42 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1240642ms till timeout)
2022-03-28 12:53:42 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-133
2022-03-28 12:53:42 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:42 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-134
2022-03-28 12:53:42 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1271605ms till timeout)
2022-03-28 12:53:42 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job producer-1516930780 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:53:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:53:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (214694ms till timeout)
2022-03-28 12:53:43 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-134
2022-03-28 12:53:43 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:43 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-14
2022-03-28 12:53:43 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 12:53:43 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1239530ms till timeout)
2022-03-28 12:53:43 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 12:53:43 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:43 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (459470ms till timeout)
2022-03-28 12:53:43 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-14
2022-03-28 12:53:43 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:43 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-140
2022-03-28 12:53:43 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1270496ms till timeout)
2022-03-28 12:53:44 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job producer-1516930780 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:53:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:53:44 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-140
2022-03-28 12:53:44 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:44 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-141
2022-03-28 12:53:44 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:230] testPauseReconciliationInKafkaAndKafkaConnectWithConnector test now can proceed its execution
2022-03-28 12:53:44 [ForkJoinPool-1-worker-15] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 12:53:44 [ForkJoinPool-1-worker-15] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testScramUserWithQuotas=my-cluster-90c82d82, testReceiveSimpleMessageTlsScramSha=my-cluster-455915b9, testConfigurationFileIsCreated=my-cluster-020ceaf8, testSendSimpleMessageTls=my-cluster-cfce2f5b, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5, testUpdateUser=my-cluster-e16ccf16, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d, testTlsExternalUserWithQuotas=my-cluster-3e2e11de, testUserWithNameMoreThan64Chars=my-cluster-ed39a269, testTlsUserWithQuotas=my-cluster-3b191485, testDeployAndUnDeployCruiseControl=my-cluster-534609d0, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2, testCapacityFile=my-cluster-39ee3f62, testConfigurationPerformanceOptions=my-cluster-bc0ed00a, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-0b062573, testTlsExternalUser=my-cluster-965e9d1e, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-31feb41b, testSendSimpleMessageTlsScramSha=my-cluster-27773859, testUserTemplate=my-cluster-157df94e}
2022-03-28 12:53:44 [ForkJoinPool-1-worker-15] TRACE [AbstractST:607] USERS_NAME_MAP: {testScramUserWithQuotas=my-user-1857980152-2145707698, testReceiveSimpleMessageTlsScramSha=my-user-1329598231-1183867861, testConfigurationFileIsCreated=my-user-1609977058-1196012647, testSendSimpleMessageTls=my-user-111586425-1868899021, testCreatingUsersWithSecretPrefix=my-user-1928899390-1444689439, testUpdateUser=my-user-1694542041-806176635, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-467839358-1504804463, testTlsExternalUserWithQuotas=my-user-1786174319-1274634874, testUserWithNameMoreThan64Chars=my-user-1048816470-1103470031, testTlsUserWithQuotas=my-user-420001054-1296284129, testDeployAndUnDeployCruiseControl=my-user-475346616-1293712526, testThrottlingQuotasCreateTopic=my-user-896662438-260898887, testCapacityFile=my-user-597365340-661017603, testConfigurationPerformanceOptions=my-user-722474509-712960391, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-1153908156-1369197188, testTlsExternalUser=my-user-1295334081-1602801897, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2100837352-1014792902, testSendSimpleMessageTlsScramSha=my-user-507582460-1003875464, testUserTemplate=my-user-1253881534-190261188}
2022-03-28 12:53:44 [ForkJoinPool-1-worker-15] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testScramUserWithQuotas=my-topic-315112689-1043284277, testReceiveSimpleMessageTlsScramSha=my-topic-170116874-1830928683, testConfigurationFileIsCreated=my-topic-155202447-1921897899, testSendSimpleMessageTls=my-topic-100192180-936335618, testCreatingUsersWithSecretPrefix=my-topic-2033928996-1209892355, testUpdateUser=my-topic-1239518257-2056139809, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-712754859-1910580676, testTlsExternalUserWithQuotas=my-topic-593679216-1309035072, testUserWithNameMoreThan64Chars=my-topic-1603054043-1686316417, testTlsUserWithQuotas=my-topic-1752299353-1315959033, testDeployAndUnDeployCruiseControl=my-topic-1815491150-540070233, testThrottlingQuotasCreateTopic=my-topic-1532506723-1472611013, testCapacityFile=my-topic-1390944843-775067696, testConfigurationPerformanceOptions=my-topic-1891296149-770116276, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-613737897-291522718, testTlsExternalUser=my-topic-476529121-306489993, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1084162749-1576061083, testSendSimpleMessageTlsScramSha=my-topic-62182003-1963040205, testUserTemplate=my-topic-515111334-943904347}
2022-03-28 12:53:44 [ForkJoinPool-1-worker-15] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testScramUserWithQuotas=my-cluster-90c82d82-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-455915b9-kafka-clients, testConfigurationFileIsCreated=my-cluster-020ceaf8-kafka-clients, testSendSimpleMessageTls=my-cluster-cfce2f5b-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5-kafka-clients, testUpdateUser=my-cluster-e16ccf16-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-3e2e11de-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-ed39a269-kafka-clients, testTlsUserWithQuotas=my-cluster-3b191485-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-534609d0-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2-kafka-clients, testCapacityFile=my-cluster-39ee3f62-kafka-clients, testConfigurationPerformanceOptions=my-cluster-bc0ed00a-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-0b062573-kafka-clients, testTlsExternalUser=my-cluster-965e9d1e-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-31feb41b-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-27773859-kafka-clients, testUserTemplate=my-cluster-157df94e-kafka-clients}
2022-03-28 12:53:44 [ForkJoinPool-1-worker-15] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-8 for test case:testPauseReconciliationInKafkaAndKafkaConnectWithConnector
2022-03-28 12:53:44 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment producer-1516930780 deletion
2022-03-28 12:53:44 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for ReplicaSet producer-1516930780 to be deleted
2022-03-28 12:53:44 [ForkJoinPool-1-worker-15] INFO  [KubeClusterResource:156] Creating Namespace: namespace-8
2022-03-28 12:53:44 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:40] Job producer-1516930780 was deleted
2022-03-28 12:53:44 [ForkJoinPool-1-worker-9] INFO  [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-03-28 12:53:44 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update Job consumer-296284049 in namespace http-bridge-tls-st
2022-03-28 12:53:44 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 12:53:44 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Namespace namespace-8
2022-03-28 12:53:44 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-8 -o json
2022-03-28 12:53:44 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1238344ms till timeout)
2022-03-28 12:53:44 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:consumer-296284049
2022-03-28 12:53:44 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-141
2022-03-28 12:53:44 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:44 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-145
2022-03-28 12:53:44 [ForkJoinPool-1-worker-9] INFO  [JobUtils:81] Waiting for job: consumer-296284049 will be in active state
2022-03-28 12:53:44 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 12:53:44 [ForkJoinPool-1-worker-9] INFO  [ClientUtils:76] Waiting for producer/consumer:consumer-296284049 to finished
2022-03-28 12:53:44 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 12:53:44 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 12:53:44 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:44 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (458016ms till timeout)
2022-03-28 12:53:45 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1269300ms till timeout)
2022-03-28 12:53:45 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-8 -o json
2022-03-28 12:53:45 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:45 [ForkJoinPool-1-worker-15] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c28,c27",
            "openshift.io/sa.scc.supplemental-groups": "1000810000/10000",
            "openshift.io/sa.scc.uid-range": "1000810000/10000"
        },
        "creationTimestamp": "2022-03-28T12:53:40Z",
        "labels": {
            "kubernetes.io/metadata.name": "namespace-8"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T12:53:40Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T12:53:40Z"
            }
        ],
        "name": "namespace-8",
        "resourceVersion": "52776",
        "uid": "53772769-0d19-427a-a32f-2b7c43283d91"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 12:53:45 [ForkJoinPool-1-worker-15] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@aec142ef=[namespace-5], io.strimzi.test.logs.CollectorElement@24b97ba9=[namespace-8], io.strimzi.test.logs.CollectorElement@f851b6c3=[namespace-7], io.strimzi.test.logs.CollectorElement@3881d5f2=[namespace-4], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[infra-namespace, reconciliation-st], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-6], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 12:53:45 [ForkJoinPool-1-worker-15] INFO  [KubeClusterResource:82] Client use Namespace: namespace-8
2022-03-28 12:53:45 [ForkJoinPool-1-worker-15] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-8, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 12:53:45 [ForkJoinPool-1-worker-15] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-8
2022-03-28 12:53:45 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-0b062573 in namespace namespace-8
2022-03-28 12:53:45 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:164] Using Namespace: namespace-8
2022-03-28 12:53:45 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job consumer-296284049 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:53:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:53:45 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-0b062573
2022-03-28 12:53:45 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-145
2022-03-28 12:53:45 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:45 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-15
2022-03-28 12:53:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (219572ms till timeout)
2022-03-28 12:53:45 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-0b062573 will have desired state: Ready
2022-03-28 12:53:45 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-0b062573 will have desired state: Ready
2022-03-28 12:53:45 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (839892ms till timeout)
2022-03-28 12:53:45 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-15
2022-03-28 12:53:45 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:45 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-156
2022-03-28 12:53:45 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1237234ms till timeout)
2022-03-28 12:53:45 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 12:53:46 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1268189ms till timeout)
2022-03-28 12:53:46 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-156
2022-03-28 12:53:46 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:46 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-16
2022-03-28 12:53:46 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 12:53:46 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:46 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (456562ms till timeout)
2022-03-28 12:53:46 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job consumer-296284049 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:53:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:53:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (218354ms till timeout)
2022-03-28 12:53:46 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (838781ms till timeout)
2022-03-28 12:53:46 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-16
2022-03-28 12:53:46 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:46 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-167
2022-03-28 12:53:46 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1236123ms till timeout)
2022-03-28 12:53:47 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:369] ############################################################################
2022-03-28 12:53:47 [ForkJoinPool-1-worker-7] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-5 for test case:testConfigurationFileIsCreated
2022-03-28 12:53:47 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Namespace namespace-5 removal
2022-03-28 12:53:47 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 12:53:47 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1267080ms till timeout)
2022-03-28 12:53:47 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-167
2022-03-28 12:53:47 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:47 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-168
2022-03-28 12:53:47 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 12:53:47 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 12:53:47 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (479537ms till timeout)
2022-03-28 12:53:47 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job consumer-296284049 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:53:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:53:47 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (837672ms till timeout)
2022-03-28 12:53:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (217031ms till timeout)
2022-03-28 12:53:47 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 12:53:47 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:47 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (455103ms till timeout)
2022-03-28 12:53:47 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-168
2022-03-28 12:53:47 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:47 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-17
2022-03-28 12:53:48 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1235015ms till timeout)
2022-03-28 12:53:48 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1265969ms till timeout)
2022-03-28 12:53:48 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-17
2022-03-28 12:53:48 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:48 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-175
2022-03-28 12:53:48 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 12:53:48 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (836560ms till timeout)
2022-03-28 12:53:48 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 12:53:48 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job consumer-296284049 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:53:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:53:49 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 12:53:49 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:49 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (478067ms till timeout)
2022-03-28 12:53:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (215808ms till timeout)
2022-03-28 12:53:49 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-175
2022-03-28 12:53:49 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:49 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-18
2022-03-28 12:53:49 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1233826ms till timeout)
2022-03-28 12:53:49 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 12:53:49 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:49 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (453630ms till timeout)
2022-03-28 12:53:49 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1264860ms till timeout)
2022-03-28 12:53:49 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-18
2022-03-28 12:53:49 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:49 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-183
2022-03-28 12:53:49 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (835448ms till timeout)
2022-03-28 12:53:50 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 12:53:50 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job consumer-296284049 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:53:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:53:50 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-183
2022-03-28 12:53:50 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:50 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-186
2022-03-28 12:53:50 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1232713ms till timeout)
2022-03-28 12:53:50 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 12:53:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (214483ms till timeout)
2022-03-28 12:53:50 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 12:53:50 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:50 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (476594ms till timeout)
2022-03-28 12:53:50 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1263752ms till timeout)
2022-03-28 12:53:50 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-186
2022-03-28 12:53:50 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:50 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-187
2022-03-28 12:53:50 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 12:53:50 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:50 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-4 removal not ready, will try again in 1000 ms (452177ms till timeout)
2022-03-28 12:53:51 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (834338ms till timeout)
2022-03-28 12:53:51 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-187
2022-03-28 12:53:51 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:51 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-19
2022-03-28 12:53:51 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1231602ms till timeout)
2022-03-28 12:53:51 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job consumer-296284049 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:53:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:53:51 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 12:53:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (213261ms till timeout)
2022-03-28 12:53:51 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1262561ms till timeout)
2022-03-28 12:53:51 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 12:53:51 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-19
2022-03-28 12:53:51 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:51 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-198
2022-03-28 12:53:52 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 12:53:52 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:52 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (475128ms till timeout)
2022-03-28 12:53:52 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (833228ms till timeout)
2022-03-28 12:53:52 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-7 get Namespace namespace-4 -o yaml
2022-03-28 12:53:52 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 1
2022-03-28 12:53:52 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 12:53:52 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-4" not found
2022-03-28 12:53:52 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] ======STDERR END======
2022-03-28 12:53:52 [ForkJoinPool-1-worker-3] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@aec142ef=[namespace-5], io.strimzi.test.logs.CollectorElement@24b97ba9=[namespace-8], io.strimzi.test.logs.CollectorElement@f851b6c3=[namespace-7], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[infra-namespace, reconciliation-st], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-6], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 12:53:52 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:267] testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods - Notifies waiting test cases:[testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testTlsExternalUserWithQuotas, testUserTemplate, testUpdateUser, testTlsExternalUser, testReceiveSimpleMessageTlsScramSha, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testSendSimpleMessageTlsScramSha, testUserWithNameMoreThan64Chars, testTlsUserWithQuotas, testConfigurationFileIsCreated, testScramUserWithQuotas, testDeployAndUnDeployCruiseControl, testConfigurationPerformanceOptions] to and randomly select one to start execution
2022-03-28 12:53:52 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:93] [cruisecontrol.CruiseControlConfigurationST] - Removing parallel test: testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods
2022-03-28 12:53:52 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:97] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 6
2022-03-28 12:53:52 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods-FINISHED
2022-03-28 12:53:52 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:30] ############################################################################
2022-03-28 12:53:52 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:23] ############################################################################
2022-03-28 12:53:52 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:24] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationReflection-STARTED
2022-03-28 12:53:52 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:658] ============================================================================
2022-03-28 12:53:52 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:659] [cruisecontrol.CruiseControlConfigurationST - Before Each] - Setup test case environment
2022-03-28 12:53:52 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:77] [cruisecontrol.CruiseControlConfigurationST] - Adding parallel test: testConfigurationReflection
2022-03-28 12:53:52 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:81] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 7
2022-03-28 12:53:52 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:205] [testConfigurationReflection] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:53:52 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testConfigurationReflection is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:53:52 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-198
2022-03-28 12:53:52 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:52 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-199
2022-03-28 12:53:52 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1230492ms till timeout)
2022-03-28 12:53:52 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job consumer-296284049 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:53:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:53:52 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1261452ms till timeout)
2022-03-28 12:53:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (211938ms till timeout)
2022-03-28 12:53:52 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-199
2022-03-28 12:53:52 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:52 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-2
2022-03-28 12:53:53 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 12:53:53 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (832115ms till timeout)
2022-03-28 12:53:53 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 12:53:53 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (473682ms till timeout)
2022-03-28 12:53:53 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-2
2022-03-28 12:53:53 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:53 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-20
2022-03-28 12:53:53 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1229383ms till timeout)
2022-03-28 12:53:53 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1260339ms till timeout)
2022-03-28 12:53:54 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-20
2022-03-28 12:53:54 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:54 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-200
2022-03-28 12:53:54 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job consumer-296284049 in namespace http-bridge-tls-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:53:40Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:53:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (210713ms till timeout)
2022-03-28 12:53:54 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (831005ms till timeout)
2022-03-28 12:53:54 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 12:53:54 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-200
2022-03-28 12:53:54 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:54 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-201
2022-03-28 12:53:54 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1228266ms till timeout)
2022-03-28 12:53:54 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 12:53:54 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (472230ms till timeout)
2022-03-28 12:53:55 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1259230ms till timeout)
2022-03-28 12:53:55 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-201
2022-03-28 12:53:55 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:55 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-202
2022-03-28 12:53:55 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job consumer-296284049 in namespace http-bridge-tls-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T12:53:50Z, conditions=[JobCondition(lastProbeTime=2022-03-28T12:53:50Z, lastTransitionTime=2022-03-28T12:53:50Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T12:53:40Z, succeeded=1, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:53:55 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment consumer-296284049 deletion
2022-03-28 12:53:55 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for ReplicaSet consumer-296284049 to be deleted
2022-03-28 12:53:55 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (829804ms till timeout)
2022-03-28 12:53:55 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:40] Job consumer-296284049 was deleted
2022-03-28 12:53:55 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:674] ============================================================================
2022-03-28 12:53:55 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:675] [bridge.HttpBridgeTlsST - After Each] - Clean up after test
2022-03-28 12:53:55 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:344] ############################################################################
2022-03-28 12:53:55 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:348] Delete all resources for testSendSimpleMessageTls
2022-03-28 12:53:55 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:241] Delete of Job producer-1516930780 in namespace http-bridge-tls-st
2022-03-28 12:53:55 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-202
2022-03-28 12:53:55 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:55 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-203
2022-03-28 12:53:55 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:producer-1516930780
2022-03-28 12:53:55 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 12:53:55 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1227069ms till timeout)
2022-03-28 12:53:56 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:241] Delete of Job consumer-296284049 in namespace http-bridge-tls-st
2022-03-28 12:53:56 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:consumer-296284049
2022-03-28 12:53:56 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-203
2022-03-28 12:53:56 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:56 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-204
2022-03-28 12:53:56 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1258031ms till timeout)
2022-03-28 12:53:56 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 12:53:56 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (470811ms till timeout)
2022-03-28 12:53:56 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:241] Delete of KafkaTopic my-topic-1486414252-1474955860 in namespace http-bridge-tls-st
2022-03-28 12:53:56 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1486414252-1474955860
2022-03-28 12:53:56 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:369] ############################################################################
2022-03-28 12:53:56 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:267] testSendSimpleMessageTls - Notifies waiting test cases:[testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testTlsExternalUserWithQuotas, testUserTemplate, testUpdateUser, testTlsExternalUser, testReceiveSimpleMessageTlsScramSha, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testSendSimpleMessageTlsScramSha, testUserWithNameMoreThan64Chars, testTlsUserWithQuotas, testConfigurationFileIsCreated, testScramUserWithQuotas, testDeployAndUnDeployCruiseControl, testConfigurationPerformanceOptions, testConfigurationReflection] to and randomly select one to start execution
2022-03-28 12:53:56 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:93] [bridge.HttpBridgeTlsST] - Removing parallel test: testSendSimpleMessageTls
2022-03-28 12:53:56 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:97] [bridge.HttpBridgeTlsST] - Parallel test count: 6
2022-03-28 12:53:56 [ForkJoinPool-1-worker-9] INFO  [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeTlsST.testSendSimpleMessageTls-FINISHED
2022-03-28 12:53:56 [ForkJoinPool-1-worker-9] INFO  [TestSeparator:30] ############################################################################
2022-03-28 12:53:56 [ForkJoinPool-1-worker-9] INFO  [TestSeparator:23] ############################################################################
2022-03-28 12:53:56 [ForkJoinPool-1-worker-9] INFO  [TestSeparator:24] io.strimzi.systemtest.bridge.HttpBridgeTlsST.testReceiveSimpleMessageTls-STARTED
2022-03-28 12:53:56 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:658] ============================================================================
2022-03-28 12:53:56 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:659] [bridge.HttpBridgeTlsST - Before Each] - Setup test case environment
2022-03-28 12:53:56 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:77] [bridge.HttpBridgeTlsST] - Adding parallel test: testReceiveSimpleMessageTls
2022-03-28 12:53:56 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:81] [bridge.HttpBridgeTlsST] - Parallel test count: 7
2022-03-28 12:53:56 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:205] [testReceiveSimpleMessageTls] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:53:56 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:210] testReceiveSimpleMessageTls is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:53:56 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (828696ms till timeout)
2022-03-28 12:53:56 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-204
2022-03-28 12:53:56 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:56 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-205
2022-03-28 12:53:57 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1225959ms till timeout)
2022-03-28 12:53:57 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 12:53:57 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:230] testConfigurationReflection test now can proceed its execution
2022-03-28 12:53:57 [ForkJoinPool-1-worker-3] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 12:53:57 [ForkJoinPool-1-worker-3] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testScramUserWithQuotas=my-cluster-90c82d82, testReceiveSimpleMessageTlsScramSha=my-cluster-455915b9, testConfigurationFileIsCreated=my-cluster-020ceaf8, testSendSimpleMessageTls=my-cluster-cfce2f5b, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5, testUpdateUser=my-cluster-e16ccf16, testConfigurationReflection=my-cluster-aa0ecd2f, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d, testTlsExternalUserWithQuotas=my-cluster-3e2e11de, testUserWithNameMoreThan64Chars=my-cluster-ed39a269, testTlsUserWithQuotas=my-cluster-3b191485, testDeployAndUnDeployCruiseControl=my-cluster-534609d0, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2, testCapacityFile=my-cluster-39ee3f62, testConfigurationPerformanceOptions=my-cluster-bc0ed00a, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-0b062573, testTlsExternalUser=my-cluster-965e9d1e, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-31feb41b, testSendSimpleMessageTlsScramSha=my-cluster-27773859, testUserTemplate=my-cluster-157df94e}
2022-03-28 12:53:57 [ForkJoinPool-1-worker-3] TRACE [AbstractST:607] USERS_NAME_MAP: {testScramUserWithQuotas=my-user-1857980152-2145707698, testReceiveSimpleMessageTlsScramSha=my-user-1329598231-1183867861, testConfigurationFileIsCreated=my-user-1609977058-1196012647, testSendSimpleMessageTls=my-user-111586425-1868899021, testCreatingUsersWithSecretPrefix=my-user-1928899390-1444689439, testUpdateUser=my-user-1694542041-806176635, testConfigurationReflection=my-user-1580949953-976509997, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-467839358-1504804463, testTlsExternalUserWithQuotas=my-user-1786174319-1274634874, testUserWithNameMoreThan64Chars=my-user-1048816470-1103470031, testTlsUserWithQuotas=my-user-420001054-1296284129, testDeployAndUnDeployCruiseControl=my-user-475346616-1293712526, testThrottlingQuotasCreateTopic=my-user-896662438-260898887, testCapacityFile=my-user-597365340-661017603, testConfigurationPerformanceOptions=my-user-722474509-712960391, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-1153908156-1369197188, testTlsExternalUser=my-user-1295334081-1602801897, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2100837352-1014792902, testSendSimpleMessageTlsScramSha=my-user-507582460-1003875464, testUserTemplate=my-user-1253881534-190261188}
2022-03-28 12:53:57 [ForkJoinPool-1-worker-3] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testScramUserWithQuotas=my-topic-315112689-1043284277, testReceiveSimpleMessageTlsScramSha=my-topic-170116874-1830928683, testConfigurationFileIsCreated=my-topic-155202447-1921897899, testSendSimpleMessageTls=my-topic-100192180-936335618, testCreatingUsersWithSecretPrefix=my-topic-2033928996-1209892355, testUpdateUser=my-topic-1239518257-2056139809, testConfigurationReflection=my-topic-1437230713-1827996725, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-712754859-1910580676, testTlsExternalUserWithQuotas=my-topic-593679216-1309035072, testUserWithNameMoreThan64Chars=my-topic-1603054043-1686316417, testTlsUserWithQuotas=my-topic-1752299353-1315959033, testDeployAndUnDeployCruiseControl=my-topic-1815491150-540070233, testThrottlingQuotasCreateTopic=my-topic-1532506723-1472611013, testCapacityFile=my-topic-1390944843-775067696, testConfigurationPerformanceOptions=my-topic-1891296149-770116276, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-613737897-291522718, testTlsExternalUser=my-topic-476529121-306489993, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1084162749-1576061083, testSendSimpleMessageTlsScramSha=my-topic-62182003-1963040205, testUserTemplate=my-topic-515111334-943904347}
2022-03-28 12:53:57 [ForkJoinPool-1-worker-3] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testScramUserWithQuotas=my-cluster-90c82d82-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-455915b9-kafka-clients, testConfigurationFileIsCreated=my-cluster-020ceaf8-kafka-clients, testSendSimpleMessageTls=my-cluster-cfce2f5b-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5-kafka-clients, testUpdateUser=my-cluster-e16ccf16-kafka-clients, testConfigurationReflection=my-cluster-aa0ecd2f-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-3e2e11de-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-ed39a269-kafka-clients, testTlsUserWithQuotas=my-cluster-3b191485-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-534609d0-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2-kafka-clients, testCapacityFile=my-cluster-39ee3f62-kafka-clients, testConfigurationPerformanceOptions=my-cluster-bc0ed00a-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-0b062573-kafka-clients, testTlsExternalUser=my-cluster-965e9d1e-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-31feb41b-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-27773859-kafka-clients, testUserTemplate=my-cluster-157df94e-kafka-clients}
2022-03-28 12:53:57 [ForkJoinPool-1-worker-3] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-9 for test case:testConfigurationReflection
2022-03-28 12:53:57 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1256920ms till timeout)
2022-03-28 12:53:57 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-205
2022-03-28 12:53:57 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:57 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-206
2022-03-28 12:53:57 [ForkJoinPool-1-worker-3] INFO  [KubeClusterResource:156] Creating Namespace: namespace-9
2022-03-28 12:53:57 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Namespace namespace-9
2022-03-28 12:53:57 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-9 -o json
2022-03-28 12:53:57 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 12:53:57 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:57 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (469355ms till timeout)
2022-03-28 12:53:57 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (827585ms till timeout)
2022-03-28 12:53:58 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-206
2022-03-28 12:53:58 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:58 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-207
2022-03-28 12:53:58 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-9 -o json
2022-03-28 12:53:58 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:58 [ForkJoinPool-1-worker-3] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c29,c4",
            "openshift.io/sa.scc.supplemental-groups": "1000820000/10000",
            "openshift.io/sa.scc.uid-range": "1000820000/10000"
        },
        "creationTimestamp": "2022-03-28T12:53:53Z",
        "labels": {
            "kubernetes.io/metadata.name": "namespace-9"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T12:53:53Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T12:53:53Z"
            }
        ],
        "name": "namespace-9",
        "resourceVersion": "53147",
        "uid": "6b083b14-b9ee-48ed-a402-31320078a504"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 12:53:58 [ForkJoinPool-1-worker-3] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[namespace-9], io.strimzi.test.logs.CollectorElement@aec142ef=[namespace-5], io.strimzi.test.logs.CollectorElement@24b97ba9=[namespace-8], io.strimzi.test.logs.CollectorElement@f851b6c3=[namespace-7], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[infra-namespace, reconciliation-st], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-6], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 12:53:58 [ForkJoinPool-1-worker-3] INFO  [KubeClusterResource:82] Client use Namespace: namespace-9
2022-03-28 12:53:58 [ForkJoinPool-1-worker-3] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-9, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 12:53:58 [ForkJoinPool-1-worker-3] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-9
2022-03-28 12:53:58 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-aa0ecd2f in namespace namespace-9
2022-03-28 12:53:58 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:164] Using Namespace: namespace-9
2022-03-28 12:53:58 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1224849ms till timeout)
2022-03-28 12:53:58 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-aa0ecd2f
2022-03-28 12:53:58 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-aa0ecd2f will have desired state: Ready
2022-03-28 12:53:58 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-aa0ecd2f will have desired state: Ready
2022-03-28 12:53:58 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1255794ms till timeout)
2022-03-28 12:53:58 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-207
2022-03-28 12:53:58 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:58 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-208
2022-03-28 12:53:58 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1319780ms till timeout)
2022-03-28 12:53:58 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 12:53:58 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (826476ms till timeout)
2022-03-28 12:53:59 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-208
2022-03-28 12:53:59 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:59 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-209
2022-03-28 12:53:59 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 12:53:59 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:59 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (467893ms till timeout)
2022-03-28 12:53:59 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1223719ms till timeout)
2022-03-28 12:53:59 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1254683ms till timeout)
2022-03-28 12:53:59 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-209
2022-03-28 12:53:59 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:53:59 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-21
2022-03-28 12:53:59 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1318667ms till timeout)
2022-03-28 12:54:00 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (825360ms till timeout)
2022-03-28 12:54:00 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-21
2022-03-28 12:54:00 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:00 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-210
2022-03-28 12:54:00 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 12:54:00 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1222607ms till timeout)
2022-03-28 12:54:00 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 12:54:00 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:00 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (466410ms till timeout)
2022-03-28 12:54:00 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1253572ms till timeout)
2022-03-28 12:54:00 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-210
2022-03-28 12:54:00 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:00 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-211
2022-03-28 12:54:00 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1317551ms till timeout)
2022-03-28 12:54:01 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (824251ms till timeout)
2022-03-28 12:54:01 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-211
2022-03-28 12:54:01 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:01 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-212
2022-03-28 12:54:01 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1221498ms till timeout)
2022-03-28 12:54:01 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:210] testReceiveSimpleMessageTls is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:54:01 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 12:54:01 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1252462ms till timeout)
2022-03-28 12:54:01 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-212
2022-03-28 12:54:01 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:01 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-213
2022-03-28 12:54:01 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1316442ms till timeout)
2022-03-28 12:54:02 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 12:54:02 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (464958ms till timeout)
2022-03-28 12:54:02 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (823141ms till timeout)
2022-03-28 12:54:02 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-213
2022-03-28 12:54:02 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:02 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-214
2022-03-28 12:54:02 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1220390ms till timeout)
2022-03-28 12:54:02 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1251353ms till timeout)
2022-03-28 12:54:02 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-214
2022-03-28 12:54:02 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:02 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-215
2022-03-28 12:54:03 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1315332ms till timeout)
2022-03-28 12:54:03 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 12:54:03 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (822017ms till timeout)
2022-03-28 12:54:03 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-215
2022-03-28 12:54:03 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:03 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-216
2022-03-28 12:54:03 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 12:54:03 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (463541ms till timeout)
2022-03-28 12:54:03 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1219230ms till timeout)
2022-03-28 12:54:04 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-216
2022-03-28 12:54:04 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:04 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-217
2022-03-28 12:54:04 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1250244ms till timeout)
2022-03-28 12:54:04 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1314223ms till timeout)
2022-03-28 12:54:04 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (820908ms till timeout)
2022-03-28 12:54:04 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 12:54:04 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-217
2022-03-28 12:54:04 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:04 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-218
2022-03-28 12:54:04 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1218119ms till timeout)
2022-03-28 12:54:05 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 12:54:05 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:05 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (462093ms till timeout)
2022-03-28 12:54:05 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-218
2022-03-28 12:54:05 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:05 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-219
2022-03-28 12:54:05 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1249136ms till timeout)
2022-03-28 12:54:05 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1313114ms till timeout)
2022-03-28 12:54:05 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (819798ms till timeout)
2022-03-28 12:54:05 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-219
2022-03-28 12:54:05 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:05 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-22
2022-03-28 12:54:06 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1217007ms till timeout)
2022-03-28 12:54:06 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 12:54:06 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-22
2022-03-28 12:54:06 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:06 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-221
2022-03-28 12:54:06 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1248027ms till timeout)
2022-03-28 12:54:06 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1312005ms till timeout)
2022-03-28 12:54:06 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 12:54:06 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:06 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (460638ms till timeout)
2022-03-28 12:54:06 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:210] testReceiveSimpleMessageTls is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:54:06 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (818687ms till timeout)
2022-03-28 12:54:06 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-221
2022-03-28 12:54:06 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:06 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-225
2022-03-28 12:54:07 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1215896ms till timeout)
2022-03-28 12:54:07 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-225
2022-03-28 12:54:07 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:07 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-229
2022-03-28 12:54:07 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1246917ms till timeout)
2022-03-28 12:54:07 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 12:54:07 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1310896ms till timeout)
2022-03-28 12:54:07 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (817576ms till timeout)
2022-03-28 12:54:07 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-229
2022-03-28 12:54:07 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:07 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-23
2022-03-28 12:54:07 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 12:54:07 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (459196ms till timeout)
2022-03-28 12:54:08 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1214785ms till timeout)
2022-03-28 12:54:08 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-23
2022-03-28 12:54:08 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:08 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-235
2022-03-28 12:54:08 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1245807ms till timeout)
2022-03-28 12:54:08 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1309786ms till timeout)
2022-03-28 12:54:08 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-235
2022-03-28 12:54:08 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:08 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-238
2022-03-28 12:54:08 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 12:54:08 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (816467ms till timeout)
2022-03-28 12:54:09 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1213675ms till timeout)
2022-03-28 12:54:09 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 12:54:09 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (457748ms till timeout)
2022-03-28 12:54:09 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-238
2022-03-28 12:54:09 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:09 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-24
2022-03-28 12:54:09 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1244693ms till timeout)
2022-03-28 12:54:09 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1308677ms till timeout)
2022-03-28 12:54:10 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-24
2022-03-28 12:54:10 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:10 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-240
2022-03-28 12:54:10 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (815358ms till timeout)
2022-03-28 12:54:10 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 12:54:10 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1212564ms till timeout)
2022-03-28 12:54:10 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-240
2022-03-28 12:54:10 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:10 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-241
2022-03-28 12:54:10 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1243584ms till timeout)
2022-03-28 12:54:10 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1307569ms till timeout)
2022-03-28 12:54:10 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 12:54:10 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:10 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (456287ms till timeout)
2022-03-28 12:54:11 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-241
2022-03-28 12:54:11 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:11 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-242
2022-03-28 12:54:11 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (814247ms till timeout)
2022-03-28 12:54:11 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1211454ms till timeout)
2022-03-28 12:54:11 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-242
2022-03-28 12:54:11 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:11 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-243
2022-03-28 12:54:11 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:210] testReceiveSimpleMessageTls is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:54:11 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1242476ms till timeout)
2022-03-28 12:54:11 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 12:54:11 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1306456ms till timeout)
2022-03-28 12:54:12 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-243
2022-03-28 12:54:12 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:12 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-244
2022-03-28 12:54:12 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 12:54:12 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (454838ms till timeout)
2022-03-28 12:54:12 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (813137ms till timeout)
2022-03-28 12:54:12 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-244
2022-03-28 12:54:12 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:12 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-245
2022-03-28 12:54:12 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1210343ms till timeout)
2022-03-28 12:54:12 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1241366ms till timeout)
2022-03-28 12:54:13 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1305346ms till timeout)
2022-03-28 12:54:13 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-245
2022-03-28 12:54:13 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:13 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-246
2022-03-28 12:54:13 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 12:54:13 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (812027ms till timeout)
2022-03-28 12:54:13 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 12:54:13 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (453383ms till timeout)
2022-03-28 12:54:13 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-246
2022-03-28 12:54:13 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:13 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-247
2022-03-28 12:54:13 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1209232ms till timeout)
2022-03-28 12:54:14 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1240257ms till timeout)
2022-03-28 12:54:14 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1304237ms till timeout)
2022-03-28 12:54:14 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-247
2022-03-28 12:54:14 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:14 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-248
2022-03-28 12:54:14 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (810913ms till timeout)
2022-03-28 12:54:14 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 12:54:14 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-248
2022-03-28 12:54:14 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:14 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-249
2022-03-28 12:54:14 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Kafka: my-cluster-534609d0 will have desired state: Ready not ready, will try again in 1000 ms (1208122ms till timeout)
2022-03-28 12:54:15 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1239149ms till timeout)
2022-03-28 12:54:15 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 12:54:15 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (451907ms till timeout)
2022-03-28 12:54:15 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1303125ms till timeout)
2022-03-28 12:54:15 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-249
2022-03-28 12:54:15 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:15 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-25
2022-03-28 12:54:15 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (809802ms till timeout)
2022-03-28 12:54:15 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-25
2022-03-28 12:54:15 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:15 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-253
2022-03-28 12:54:16 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:444] Kafka: my-cluster-534609d0 is in desired state: Ready
2022-03-28 12:54:16 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 12:54:16 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1238039ms till timeout)
2022-03-28 12:54:16 [ForkJoinPool-1-worker-1] INFO  [CruiseControlConfigurationST:111] Removing Cruise Control to the classic Kafka.
2022-03-28 12:54:16 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1301918ms till timeout)
2022-03-28 12:54:16 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-253
2022-03-28 12:54:16 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:16 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-254
2022-03-28 12:54:16 [ForkJoinPool-1-worker-1] INFO  [RollingUpdateUtils:73] Waiting for component with name: my-cluster-534609d0-kafka rolling update
2022-03-28 12:54:16 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:74] Waiting for rolling update of component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})
2022-03-28 12:54:16 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for component with name my-cluster-534609d0-kafka rolling update
2022-03-28 12:54:16 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-534609d0-kafka-0=e07a77c9-92a3-4cd0-a1b6-a8aa20d99d2c, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:54:16 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:210] testReceiveSimpleMessageTls is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:54:16 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-534609d0-kafka-0=e07a77c9-92a3-4cd0-a1b6-a8aa20d99d2c, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:54:16 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-534609d0-kafka-0=e07a77c9-92a3-4cd0-a1b6-a8aa20d99d2c, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:54:16 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-534609d0-kafka-0 hasn't rolled
2022-03-28 12:54:16 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-534609d0-kafka rolling update not ready, will try again in 5000 ms (1799890ms till timeout)
2022-03-28 12:54:16 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 12:54:16 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (450402ms till timeout)
2022-03-28 12:54:16 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (808619ms till timeout)
2022-03-28 12:54:17 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-254
2022-03-28 12:54:17 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:17 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-257
2022-03-28 12:54:17 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1236929ms till timeout)
2022-03-28 12:54:17 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1300808ms till timeout)
2022-03-28 12:54:17 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-257
2022-03-28 12:54:17 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:17 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-26
2022-03-28 12:54:17 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 12:54:17 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (807510ms till timeout)
2022-03-28 12:54:18 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 12:54:18 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (448924ms till timeout)
2022-03-28 12:54:18 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-26
2022-03-28 12:54:18 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:18 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-261
2022-03-28 12:54:18 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1235819ms till timeout)
2022-03-28 12:54:18 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1299696ms till timeout)
2022-03-28 12:54:18 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-261
2022-03-28 12:54:18 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:18 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-263
2022-03-28 12:54:19 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (806399ms till timeout)
2022-03-28 12:54:19 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 12:54:19 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-263
2022-03-28 12:54:19 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:19 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-266
2022-03-28 12:54:19 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1234709ms till timeout)
2022-03-28 12:54:19 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 12:54:19 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:19 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (447464ms till timeout)
2022-03-28 12:54:19 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1298586ms till timeout)
2022-03-28 12:54:19 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-266
2022-03-28 12:54:19 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:19 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-269
2022-03-28 12:54:20 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (805288ms till timeout)
2022-03-28 12:54:20 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-269
2022-03-28 12:54:20 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:20 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-27
2022-03-28 12:54:20 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 12:54:20 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1233597ms till timeout)
2022-03-28 12:54:20 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1297478ms till timeout)
2022-03-28 12:54:21 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-27
2022-03-28 12:54:21 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:21 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-275
2022-03-28 12:54:21 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 12:54:21 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Namespace namespace-5 removal not ready, will try again in 1000 ms (446016ms till timeout)
2022-03-28 12:54:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (804179ms till timeout)
2022-03-28 12:54:21 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:210] testReceiveSimpleMessageTls is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:54:21 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-275
2022-03-28 12:54:21 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:21 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-28
2022-03-28 12:54:21 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-534609d0-kafka-0=e07a77c9-92a3-4cd0-a1b6-a8aa20d99d2c, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:54:21 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-534609d0-kafka-0=e07a77c9-92a3-4cd0-a1b6-a8aa20d99d2c, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:54:21 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-534609d0-kafka-0=e07a77c9-92a3-4cd0-a1b6-a8aa20d99d2c, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:54:21 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-534609d0-kafka-0 hasn't rolled
2022-03-28 12:54:21 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-534609d0-kafka rolling update not ready, will try again in 5000 ms (1794671ms till timeout)
2022-03-28 12:54:21 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1232376ms till timeout)
2022-03-28 12:54:22 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1296367ms till timeout)
2022-03-28 12:54:22 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 12:54:22 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-28
2022-03-28 12:54:22 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:22 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-284
2022-03-28 12:54:22 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (803068ms till timeout)
2022-03-28 12:54:22 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-8 get Namespace namespace-5 -o yaml
2022-03-28 12:54:22 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 1
2022-03-28 12:54:22 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 12:54:22 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-5" not found
2022-03-28 12:54:22 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] ======STDERR END======
2022-03-28 12:54:22 [ForkJoinPool-1-worker-7] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[namespace-9], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[namespace-8], io.strimzi.test.logs.CollectorElement@f851b6c3=[namespace-7], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[http-bridge-tls-st], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[infra-namespace, reconciliation-st], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-6], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 12:54:22 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:267] testConfigurationFileIsCreated - Notifies waiting test cases:[testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testTlsExternalUserWithQuotas, testUserTemplate, testUpdateUser, testTlsExternalUser, testReceiveSimpleMessageTlsScramSha, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testSendSimpleMessageTlsScramSha, testUserWithNameMoreThan64Chars, testTlsUserWithQuotas, testConfigurationFileIsCreated, testScramUserWithQuotas, testDeployAndUnDeployCruiseControl, testConfigurationPerformanceOptions, testReceiveSimpleMessageTls] to and randomly select one to start execution
2022-03-28 12:54:22 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:93] [cruisecontrol.CruiseControlConfigurationST] - Removing parallel test: testConfigurationFileIsCreated
2022-03-28 12:54:22 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:97] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 6
2022-03-28 12:54:22 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationFileIsCreated-FINISHED
2022-03-28 12:54:22 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:30] ############################################################################
2022-03-28 12:54:22 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:23] ############################################################################
2022-03-28 12:54:22 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testThrottlingQuotasCreateAlterPartitions-STARTED
2022-03-28 12:54:22 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:658] ============================================================================
2022-03-28 12:54:22 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:659] [operators.topic.ThrottlingQuotaST - Before Each] - Setup test case environment
2022-03-28 12:54:22 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:77] [operators.topic.ThrottlingQuotaST] - Adding parallel test: testThrottlingQuotasCreateAlterPartitions
2022-03-28 12:54:22 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:81] [operators.topic.ThrottlingQuotaST] - Parallel test count: 7
2022-03-28 12:54:22 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:205] [testThrottlingQuotasCreateAlterPartitions] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:54:22 [ForkJoinPool-1-worker-7] TRACE [SuiteThreadController:210] testThrottlingQuotasCreateAlterPartitions is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:54:22 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-284
2022-03-28 12:54:22 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:22 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-287
2022-03-28 12:54:23 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1231267ms till timeout)
2022-03-28 12:54:23 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1295254ms till timeout)
2022-03-28 12:54:23 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-287
2022-03-28 12:54:23 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:23 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-29
2022-03-28 12:54:23 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (801950ms till timeout)
2022-03-28 12:54:23 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-29
2022-03-28 12:54:23 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:23 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-290
2022-03-28 12:54:24 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1230157ms till timeout)
2022-03-28 12:54:24 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1294145ms till timeout)
2022-03-28 12:54:24 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-290
2022-03-28 12:54:24 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:24 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-295
2022-03-28 12:54:24 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (800841ms till timeout)
2022-03-28 12:54:24 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-295
2022-03-28 12:54:24 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:24 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-296
2022-03-28 12:54:25 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1229047ms till timeout)
2022-03-28 12:54:25 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1293035ms till timeout)
2022-03-28 12:54:25 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-296
2022-03-28 12:54:25 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:25 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-297
2022-03-28 12:54:25 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (799732ms till timeout)
2022-03-28 12:54:26 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-297
2022-03-28 12:54:26 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:26 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-3
2022-03-28 12:54:26 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1227919ms till timeout)
2022-03-28 12:54:26 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1291906ms till timeout)
2022-03-28 12:54:26 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-3
2022-03-28 12:54:26 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:26 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-30
2022-03-28 12:54:26 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:230] testReceiveSimpleMessageTls test now can proceed its execution
2022-03-28 12:54:26 [ForkJoinPool-1-worker-9] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 12:54:26 [ForkJoinPool-1-worker-9] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testScramUserWithQuotas=my-cluster-90c82d82, testReceiveSimpleMessageTlsScramSha=my-cluster-455915b9, testConfigurationFileIsCreated=my-cluster-020ceaf8, testSendSimpleMessageTls=my-cluster-cfce2f5b, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5, testUpdateUser=my-cluster-e16ccf16, testConfigurationReflection=my-cluster-aa0ecd2f, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d, testTlsExternalUserWithQuotas=my-cluster-3e2e11de, testUserWithNameMoreThan64Chars=my-cluster-ed39a269, testTlsUserWithQuotas=my-cluster-3b191485, testDeployAndUnDeployCruiseControl=my-cluster-534609d0, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2, testCapacityFile=my-cluster-39ee3f62, testConfigurationPerformanceOptions=my-cluster-bc0ed00a, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-0b062573, testReceiveSimpleMessageTls=my-cluster-362cd511, testTlsExternalUser=my-cluster-965e9d1e, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-31feb41b, testSendSimpleMessageTlsScramSha=my-cluster-27773859, testUserTemplate=my-cluster-157df94e}
2022-03-28 12:54:26 [ForkJoinPool-1-worker-9] TRACE [AbstractST:607] USERS_NAME_MAP: {testScramUserWithQuotas=my-user-1857980152-2145707698, testReceiveSimpleMessageTlsScramSha=my-user-1329598231-1183867861, testConfigurationFileIsCreated=my-user-1609977058-1196012647, testSendSimpleMessageTls=my-user-111586425-1868899021, testCreatingUsersWithSecretPrefix=my-user-1928899390-1444689439, testUpdateUser=my-user-1694542041-806176635, testConfigurationReflection=my-user-1580949953-976509997, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-467839358-1504804463, testTlsExternalUserWithQuotas=my-user-1786174319-1274634874, testUserWithNameMoreThan64Chars=my-user-1048816470-1103470031, testTlsUserWithQuotas=my-user-420001054-1296284129, testDeployAndUnDeployCruiseControl=my-user-475346616-1293712526, testThrottlingQuotasCreateTopic=my-user-896662438-260898887, testCapacityFile=my-user-597365340-661017603, testConfigurationPerformanceOptions=my-user-722474509-712960391, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-1153908156-1369197188, testReceiveSimpleMessageTls=my-user-2012197250-2103354173, testTlsExternalUser=my-user-1295334081-1602801897, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2100837352-1014792902, testSendSimpleMessageTlsScramSha=my-user-507582460-1003875464, testUserTemplate=my-user-1253881534-190261188}
2022-03-28 12:54:26 [ForkJoinPool-1-worker-9] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testScramUserWithQuotas=my-topic-315112689-1043284277, testReceiveSimpleMessageTlsScramSha=my-topic-170116874-1830928683, testConfigurationFileIsCreated=my-topic-155202447-1921897899, testSendSimpleMessageTls=my-topic-100192180-936335618, testCreatingUsersWithSecretPrefix=my-topic-2033928996-1209892355, testUpdateUser=my-topic-1239518257-2056139809, testConfigurationReflection=my-topic-1437230713-1827996725, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-712754859-1910580676, testTlsExternalUserWithQuotas=my-topic-593679216-1309035072, testUserWithNameMoreThan64Chars=my-topic-1603054043-1686316417, testTlsUserWithQuotas=my-topic-1752299353-1315959033, testDeployAndUnDeployCruiseControl=my-topic-1815491150-540070233, testThrottlingQuotasCreateTopic=my-topic-1532506723-1472611013, testCapacityFile=my-topic-1390944843-775067696, testConfigurationPerformanceOptions=my-topic-1891296149-770116276, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-613737897-291522718, testReceiveSimpleMessageTls=my-topic-1264858218-642520562, testTlsExternalUser=my-topic-476529121-306489993, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1084162749-1576061083, testSendSimpleMessageTlsScramSha=my-topic-62182003-1963040205, testUserTemplate=my-topic-515111334-943904347}
2022-03-28 12:54:26 [ForkJoinPool-1-worker-9] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testScramUserWithQuotas=my-cluster-90c82d82-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-455915b9-kafka-clients, testConfigurationFileIsCreated=my-cluster-020ceaf8-kafka-clients, testSendSimpleMessageTls=my-cluster-cfce2f5b-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5-kafka-clients, testUpdateUser=my-cluster-e16ccf16-kafka-clients, testConfigurationReflection=my-cluster-aa0ecd2f-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-3e2e11de-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-ed39a269-kafka-clients, testTlsUserWithQuotas=my-cluster-3b191485-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-534609d0-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2-kafka-clients, testCapacityFile=my-cluster-39ee3f62-kafka-clients, testConfigurationPerformanceOptions=my-cluster-bc0ed00a-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-0b062573-kafka-clients, testReceiveSimpleMessageTls=my-cluster-362cd511-kafka-clients, testTlsExternalUser=my-cluster-965e9d1e-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-31feb41b-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-27773859-kafka-clients, testUserTemplate=my-cluster-157df94e-kafka-clients}
2022-03-28 12:54:26 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update KafkaTopic my-topic-821913653-652910798 in namespace http-bridge-tls-st
2022-03-28 12:54:26 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-821913653-652910798
2022-03-28 12:54:26 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (798606ms till timeout)
2022-03-28 12:54:26 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-534609d0-kafka-0=e07a77c9-92a3-4cd0-a1b6-a8aa20d99d2c, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:54:26 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-821913653-652910798 will have desired state: Ready
2022-03-28 12:54:26 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-821913653-652910798 will have desired state: Ready
2022-03-28 12:54:27 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-30
2022-03-28 12:54:27 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:27 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-300
2022-03-28 12:54:27 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:444] KafkaTopic: my-topic-821913653-652910798 is in desired state: Ready
2022-03-28 12:54:27 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:54:27 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:54:27 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-534609d0-kafka-1 hasn't rolled
2022-03-28 12:54:27 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-534609d0-kafka rolling update not ready, will try again in 5000 ms (1789438ms till timeout)
2022-03-28 12:54:27 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update Job consumer-125971958 in namespace http-bridge-tls-st
2022-03-28 12:54:27 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:consumer-125971958
2022-03-28 12:54:27 [ForkJoinPool-1-worker-9] INFO  [JobUtils:81] Waiting for job: consumer-125971958 will be in active state
2022-03-28 12:54:27 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 12:54:27 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1226775ms till timeout)
2022-03-28 12:54:27 [ForkJoinPool-1-worker-7] TRACE [SuiteThreadController:210] testThrottlingQuotasCreateAlterPartitions is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:54:27 [ForkJoinPool-1-worker-9] INFO  [KafkaClients:87] Consumer group were not specified going to create the random one.
2022-03-28 12:54:27 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update Job producer-1299923547 in namespace http-bridge-tls-st
2022-03-28 12:54:27 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1290759ms till timeout)
2022-03-28 12:54:27 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-300
2022-03-28 12:54:27 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:27 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-301
2022-03-28 12:54:27 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:producer-1299923547
2022-03-28 12:54:27 [ForkJoinPool-1-worker-9] INFO  [JobUtils:81] Waiting for job: producer-1299923547 will be in active state
2022-03-28 12:54:27 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 12:54:27 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (797460ms till timeout)
2022-03-28 12:54:28 [ForkJoinPool-1-worker-9] INFO  [ClientUtils:61] Waiting till producer producer-1299923547 and consumer consumer-125971958 finish
2022-03-28 12:54:28 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for clients finished
2022-03-28 12:54:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (219892ms till timeout)
2022-03-28 12:54:28 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-301
2022-03-28 12:54:28 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:28 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-302
2022-03-28 12:54:28 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1225666ms till timeout)
2022-03-28 12:54:28 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1289651ms till timeout)
2022-03-28 12:54:28 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-302
2022-03-28 12:54:28 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:28 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-303
2022-03-28 12:54:29 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (796350ms till timeout)
2022-03-28 12:54:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (218783ms till timeout)
2022-03-28 12:54:29 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-303
2022-03-28 12:54:29 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:29 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-304
2022-03-28 12:54:29 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1224556ms till timeout)
2022-03-28 12:54:29 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1288541ms till timeout)
2022-03-28 12:54:29 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-304
2022-03-28 12:54:29 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:29 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-305
2022-03-28 12:54:30 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (795241ms till timeout)
2022-03-28 12:54:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (217674ms till timeout)
2022-03-28 12:54:30 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-305
2022-03-28 12:54:30 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:30 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-306
2022-03-28 12:54:30 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1223447ms till timeout)
2022-03-28 12:54:30 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1287431ms till timeout)
2022-03-28 12:54:31 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-306
2022-03-28 12:54:31 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:31 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-307
2022-03-28 12:54:31 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (794130ms till timeout)
2022-03-28 12:54:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (216566ms till timeout)
2022-03-28 12:54:31 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-307
2022-03-28 12:54:31 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:31 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-308
2022-03-28 12:54:31 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1222337ms till timeout)
2022-03-28 12:54:32 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-308
2022-03-28 12:54:32 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:32 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-309
2022-03-28 12:54:32 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1286322ms till timeout)
2022-03-28 12:54:32 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-534609d0-kafka-0=e07a77c9-92a3-4cd0-a1b6-a8aa20d99d2c, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:54:32 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:54:32 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:54:32 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-534609d0-kafka-1 hasn't rolled
2022-03-28 12:54:32 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-534609d0-kafka rolling update not ready, will try again in 5000 ms (1784222ms till timeout)
2022-03-28 12:54:32 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (793021ms till timeout)
2022-03-28 12:54:32 [ForkJoinPool-1-worker-7] TRACE [SuiteThreadController:210] testThrottlingQuotasCreateAlterPartitions is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:54:32 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-309
2022-03-28 12:54:32 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:32 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-31
2022-03-28 12:54:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (215456ms till timeout)
2022-03-28 12:54:33 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1221228ms till timeout)
2022-03-28 12:54:33 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-31
2022-03-28 12:54:33 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:33 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-310
2022-03-28 12:54:33 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1285213ms till timeout)
2022-03-28 12:54:33 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (791910ms till timeout)
2022-03-28 12:54:33 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-310
2022-03-28 12:54:33 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:33 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-311
2022-03-28 12:54:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (214347ms till timeout)
2022-03-28 12:54:34 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-311
2022-03-28 12:54:34 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:34 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-312
2022-03-28 12:54:34 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1220118ms till timeout)
2022-03-28 12:54:34 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1284104ms till timeout)
2022-03-28 12:54:34 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (790799ms till timeout)
2022-03-28 12:54:34 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-312
2022-03-28 12:54:34 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:34 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-313
2022-03-28 12:54:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (213238ms till timeout)
2022-03-28 12:54:35 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-313
2022-03-28 12:54:35 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:35 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-314
2022-03-28 12:54:35 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1219008ms till timeout)
2022-03-28 12:54:35 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1282995ms till timeout)
2022-03-28 12:54:35 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (789690ms till timeout)
2022-03-28 12:54:35 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-314
2022-03-28 12:54:35 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:35 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-315
2022-03-28 12:54:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (212130ms till timeout)
2022-03-28 12:54:36 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-315
2022-03-28 12:54:36 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:36 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-316
2022-03-28 12:54:36 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1217898ms till timeout)
2022-03-28 12:54:36 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1281885ms till timeout)
2022-03-28 12:54:36 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-316
2022-03-28 12:54:36 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:36 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-317
2022-03-28 12:54:36 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (788582ms till timeout)
2022-03-28 12:54:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (211021ms till timeout)
2022-03-28 12:54:37 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-317
2022-03-28 12:54:37 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:37 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-318
2022-03-28 12:54:37 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-534609d0-kafka-0=e07a77c9-92a3-4cd0-a1b6-a8aa20d99d2c, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:54:37 [ForkJoinPool-1-worker-7] TRACE [SuiteThreadController:210] testThrottlingQuotasCreateAlterPartitions is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:54:37 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:54:37 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:54:37 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-534609d0-kafka-1 hasn't rolled
2022-03-28 12:54:37 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-534609d0-kafka rolling update not ready, will try again in 5000 ms (1779004ms till timeout)
2022-03-28 12:54:37 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1216710ms till timeout)
2022-03-28 12:54:37 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1280701ms till timeout)
2022-03-28 12:54:37 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-318
2022-03-28 12:54:37 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:37 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-319
2022-03-28 12:54:37 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (787472ms till timeout)
2022-03-28 12:54:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] clients finished not ready, will try again in 1000 ms (209798ms till timeout)
2022-03-28 12:54:38 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-319
2022-03-28 12:54:38 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:38 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-32
2022-03-28 12:54:38 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1215600ms till timeout)
2022-03-28 12:54:38 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1279589ms till timeout)
2022-03-28 12:54:38 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-32
2022-03-28 12:54:38 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:38 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-320
2022-03-28 12:54:39 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (786362ms till timeout)
2022-03-28 12:54:39 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-320
2022-03-28 12:54:39 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:39 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-321
2022-03-28 12:54:39 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment producer-1299923547 deletion
2022-03-28 12:54:39 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for ReplicaSet producer-1299923547 to be deleted
2022-03-28 12:54:39 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:40] Job producer-1299923547 was deleted
2022-03-28 12:54:39 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1214475ms till timeout)
2022-03-28 12:54:39 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1278462ms till timeout)
2022-03-28 12:54:39 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment consumer-125971958 deletion
2022-03-28 12:54:39 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for ReplicaSet consumer-125971958 to be deleted
2022-03-28 12:54:39 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-321
2022-03-28 12:54:39 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:39 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-322
2022-03-28 12:54:40 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:40] Job consumer-125971958 was deleted
2022-03-28 12:54:40 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:674] ============================================================================
2022-03-28 12:54:40 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:675] [bridge.HttpBridgeTlsST - After Each] - Clean up after test
2022-03-28 12:54:40 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:344] ############################################################################
2022-03-28 12:54:40 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:348] Delete all resources for testReceiveSimpleMessageTls
2022-03-28 12:54:40 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:241] Delete of Job consumer-125971958 in namespace http-bridge-tls-st
2022-03-28 12:54:40 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:consumer-125971958
2022-03-28 12:54:40 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (785168ms till timeout)
2022-03-28 12:54:40 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:241] Delete of Job producer-1299923547 in namespace http-bridge-tls-st
2022-03-28 12:54:40 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-322
2022-03-28 12:54:40 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:40 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-323
2022-03-28 12:54:40 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:producer-1299923547
2022-03-28 12:54:40 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:241] Delete of KafkaTopic my-topic-821913653-652910798 in namespace http-bridge-tls-st
2022-03-28 12:54:40 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-821913653-652910798
2022-03-28 12:54:40 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:369] ############################################################################
2022-03-28 12:54:40 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:267] testReceiveSimpleMessageTls - Notifies waiting test cases:[testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testTlsExternalUserWithQuotas, testUserTemplate, testUpdateUser, testTlsExternalUser, testReceiveSimpleMessageTlsScramSha, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testSendSimpleMessageTlsScramSha, testUserWithNameMoreThan64Chars, testTlsUserWithQuotas, testConfigurationFileIsCreated, testScramUserWithQuotas, testDeployAndUnDeployCruiseControl, testConfigurationPerformanceOptions, testThrottlingQuotasCreateAlterPartitions] to and randomly select one to start execution
2022-03-28 12:54:40 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:93] [bridge.HttpBridgeTlsST] - Removing parallel test: testReceiveSimpleMessageTls
2022-03-28 12:54:40 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:97] [bridge.HttpBridgeTlsST] - Parallel test count: 6
2022-03-28 12:54:40 [ForkJoinPool-1-worker-9] INFO  [TestSeparator:29] io.strimzi.systemtest.bridge.HttpBridgeTlsST.testReceiveSimpleMessageTls-FINISHED
2022-03-28 12:54:40 [ForkJoinPool-1-worker-9] INFO  [TestSeparator:30] ############################################################################
2022-03-28 12:54:40 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:689] ============================================================================
2022-03-28 12:54:40 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:690] [bridge.HttpBridgeTlsST - After All] - Clean up after test suite
2022-03-28 12:54:40 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:344] ############################################################################
2022-03-28 12:54:40 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:348] Delete all resources for HttpBridgeTlsST
2022-03-28 12:54:40 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:241] Delete of Deployment http-bridge-tls-st-kafka-clients in namespace http-bridge-tls-st
2022-03-28 12:54:40 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients
2022-03-28 12:54:41 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-323
2022-03-28 12:54:41 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:41 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-324
2022-03-28 12:54:41 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1213280ms till timeout)
2022-03-28 12:54:41 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1277163ms till timeout)
2022-03-28 12:54:41 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (783979ms till timeout)
2022-03-28 12:54:41 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-324
2022-03-28 12:54:41 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:41 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-325
2022-03-28 12:54:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients not ready, will try again in 10000 ms (479362ms till timeout)
2022-03-28 12:54:42 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-325
2022-03-28 12:54:42 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:42 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-326
2022-03-28 12:54:42 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1212171ms till timeout)
2022-03-28 12:54:42 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1276052ms till timeout)
2022-03-28 12:54:42 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (782868ms till timeout)
2022-03-28 12:54:42 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-326
2022-03-28 12:54:42 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:42 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-327
2022-03-28 12:54:42 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:230] testThrottlingQuotasCreateAlterPartitions test now can proceed its execution
2022-03-28 12:54:42 [ForkJoinPool-1-worker-7] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 12:54:42 [ForkJoinPool-1-worker-7] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testScramUserWithQuotas=my-cluster-90c82d82, testReceiveSimpleMessageTlsScramSha=my-cluster-455915b9, testConfigurationFileIsCreated=my-cluster-020ceaf8, testThrottlingQuotasCreateAlterPartitions=my-cluster-d4dad0de, testSendSimpleMessageTls=my-cluster-cfce2f5b, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5, testUpdateUser=my-cluster-e16ccf16, testConfigurationReflection=my-cluster-aa0ecd2f, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d, testTlsExternalUserWithQuotas=my-cluster-3e2e11de, testUserWithNameMoreThan64Chars=my-cluster-ed39a269, testTlsUserWithQuotas=my-cluster-3b191485, testDeployAndUnDeployCruiseControl=my-cluster-534609d0, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2, testCapacityFile=my-cluster-39ee3f62, testConfigurationPerformanceOptions=my-cluster-bc0ed00a, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-0b062573, testReceiveSimpleMessageTls=my-cluster-362cd511, testTlsExternalUser=my-cluster-965e9d1e, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-31feb41b, testSendSimpleMessageTlsScramSha=my-cluster-27773859, testUserTemplate=my-cluster-157df94e}
2022-03-28 12:54:42 [ForkJoinPool-1-worker-7] TRACE [AbstractST:607] USERS_NAME_MAP: {testScramUserWithQuotas=my-user-1857980152-2145707698, testReceiveSimpleMessageTlsScramSha=my-user-1329598231-1183867861, testConfigurationFileIsCreated=my-user-1609977058-1196012647, testThrottlingQuotasCreateAlterPartitions=my-user-1809876063-64793399, testSendSimpleMessageTls=my-user-111586425-1868899021, testCreatingUsersWithSecretPrefix=my-user-1928899390-1444689439, testUpdateUser=my-user-1694542041-806176635, testConfigurationReflection=my-user-1580949953-976509997, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-467839358-1504804463, testTlsExternalUserWithQuotas=my-user-1786174319-1274634874, testUserWithNameMoreThan64Chars=my-user-1048816470-1103470031, testTlsUserWithQuotas=my-user-420001054-1296284129, testDeployAndUnDeployCruiseControl=my-user-475346616-1293712526, testThrottlingQuotasCreateTopic=my-user-896662438-260898887, testCapacityFile=my-user-597365340-661017603, testConfigurationPerformanceOptions=my-user-722474509-712960391, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-1153908156-1369197188, testReceiveSimpleMessageTls=my-user-2012197250-2103354173, testTlsExternalUser=my-user-1295334081-1602801897, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2100837352-1014792902, testSendSimpleMessageTlsScramSha=my-user-507582460-1003875464, testUserTemplate=my-user-1253881534-190261188}
2022-03-28 12:54:42 [ForkJoinPool-1-worker-7] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testScramUserWithQuotas=my-topic-315112689-1043284277, testReceiveSimpleMessageTlsScramSha=my-topic-170116874-1830928683, testConfigurationFileIsCreated=my-topic-155202447-1921897899, testThrottlingQuotasCreateAlterPartitions=my-topic-1581297458-635252970, testSendSimpleMessageTls=my-topic-100192180-936335618, testCreatingUsersWithSecretPrefix=my-topic-2033928996-1209892355, testUpdateUser=my-topic-1239518257-2056139809, testConfigurationReflection=my-topic-1437230713-1827996725, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-712754859-1910580676, testTlsExternalUserWithQuotas=my-topic-593679216-1309035072, testUserWithNameMoreThan64Chars=my-topic-1603054043-1686316417, testTlsUserWithQuotas=my-topic-1752299353-1315959033, testDeployAndUnDeployCruiseControl=my-topic-1815491150-540070233, testThrottlingQuotasCreateTopic=my-topic-1532506723-1472611013, testCapacityFile=my-topic-1390944843-775067696, testConfigurationPerformanceOptions=my-topic-1891296149-770116276, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-613737897-291522718, testReceiveSimpleMessageTls=my-topic-1264858218-642520562, testTlsExternalUser=my-topic-476529121-306489993, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1084162749-1576061083, testSendSimpleMessageTlsScramSha=my-topic-62182003-1963040205, testUserTemplate=my-topic-515111334-943904347}
2022-03-28 12:54:42 [ForkJoinPool-1-worker-7] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testScramUserWithQuotas=my-cluster-90c82d82-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-455915b9-kafka-clients, testConfigurationFileIsCreated=my-cluster-020ceaf8-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-d4dad0de-kafka-clients, testSendSimpleMessageTls=my-cluster-cfce2f5b-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5-kafka-clients, testUpdateUser=my-cluster-e16ccf16-kafka-clients, testConfigurationReflection=my-cluster-aa0ecd2f-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-3e2e11de-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-ed39a269-kafka-clients, testTlsUserWithQuotas=my-cluster-3b191485-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-534609d0-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2-kafka-clients, testCapacityFile=my-cluster-39ee3f62-kafka-clients, testConfigurationPerformanceOptions=my-cluster-bc0ed00a-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-0b062573-kafka-clients, testReceiveSimpleMessageTls=my-cluster-362cd511-kafka-clients, testTlsExternalUser=my-cluster-965e9d1e-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-31feb41b-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-27773859-kafka-clients, testUserTemplate=my-cluster-157df94e-kafka-clients}
2022-03-28 12:54:42 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update KafkaUser my-user-1809876063-64793399 in namespace throttling-quota-st
2022-03-28 12:54:42 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-534609d0-kafka-0=e07a77c9-92a3-4cd0-a1b6-a8aa20d99d2c, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:54:42 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-1809876063-64793399
2022-03-28 12:54:42 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:54:42 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:54:42 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-534609d0-kafka-1 hasn't rolled
2022-03-28 12:54:42 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-534609d0-kafka rolling update not ready, will try again in 5000 ms (1773682ms till timeout)
2022-03-28 12:54:42 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-1809876063-64793399 will have desired state: Ready
2022-03-28 12:54:42 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-1809876063-64793399 will have desired state: Ready
2022-03-28 12:54:43 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:444] KafkaUser: my-user-1809876063-64793399 is in desired state: Ready
2022-03-28 12:54:43 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-327
2022-03-28 12:54:43 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:43 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-328
2022-03-28 12:54:43 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update Job create-admin-my-cluster-d4dad0de-kafka-clients in namespace throttling-quota-st
2022-03-28 12:54:43 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1211063ms till timeout)
2022-03-28 12:54:43 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:create-admin-my-cluster-d4dad0de-kafka-clients
2022-03-28 12:54:43 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1274943ms till timeout)
2022-03-28 12:54:43 [ForkJoinPool-1-worker-7] INFO  [JobUtils:81] Waiting for job: create-admin-my-cluster-d4dad0de-kafka-clients will be in active state
2022-03-28 12:54:43 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 12:54:43 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-328
2022-03-28 12:54:43 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:43 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-329
2022-03-28 12:54:43 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (781757ms till timeout)
2022-03-28 12:54:43 [ForkJoinPool-1-worker-7] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 12:54:43 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 12:54:44 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (299885ms till timeout)
2022-03-28 12:54:44 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-329
2022-03-28 12:54:44 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:44 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-33
2022-03-28 12:54:44 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1209953ms till timeout)
2022-03-28 12:54:44 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1273835ms till timeout)
2022-03-28 12:54:44 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-33
2022-03-28 12:54:44 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:44 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-330
2022-03-28 12:54:44 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (780647ms till timeout)
2022-03-28 12:54:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (298769ms till timeout)
2022-03-28 12:54:45 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-330
2022-03-28 12:54:45 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:45 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-331
2022-03-28 12:54:45 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Kafka: my-cluster-bc0ed00a will have desired state: Ready not ready, will try again in 1000 ms (1208843ms till timeout)
2022-03-28 12:54:45 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1272725ms till timeout)
2022-03-28 12:54:45 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-331
2022-03-28 12:54:45 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:45 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-332
2022-03-28 12:54:45 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (779539ms till timeout)
2022-03-28 12:54:46 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (297658ms till timeout)
2022-03-28 12:54:46 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-332
2022-03-28 12:54:46 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:46 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-333
2022-03-28 12:54:46 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:444] Kafka: my-cluster-bc0ed00a is in desired state: Ready
2022-03-28 12:54:46 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1271571ms till timeout)
2022-03-28 12:54:46 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-333
2022-03-28 12:54:46 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:46 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-334
2022-03-28 12:54:47 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (778430ms till timeout)
2022-03-28 12:54:47 [ForkJoinPool-1-worker-13] INFO  [CruiseControlConfigurationST:271] Changing cruise control performance tuning options
2022-03-28 12:54:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (296451ms till timeout)
2022-03-28 12:54:47 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-334
2022-03-28 12:54:47 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:47 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-335
2022-03-28 12:54:47 [ForkJoinPool-1-worker-13] INFO  [CruiseControlConfigurationST:277] Verifying that CC pod is rolling, after changing options
2022-03-28 12:54:47 [ForkJoinPool-1-worker-13] INFO  [DeploymentUtils:136] Waiting for Deployment my-cluster-bc0ed00a-cruise-control rolling update
2022-03-28 12:54:47 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Deployment my-cluster-bc0ed00a-cruise-control rolling update in namespace:namespace-7
2022-03-28 12:54:47 [ForkJoinPool-1-worker-13] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-bc0ed00a-cruise-control-694bdd464b-9m774=a29e3e1d-9f9f-4686-bb39-2d532690dc15}
2022-03-28 12:54:47 [ForkJoinPool-1-worker-13] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-bc0ed00a-cruise-control-694bdd464b-9m774=a29e3e1d-9f9f-4686-bb39-2d532690dc15}
2022-03-28 12:54:47 [ForkJoinPool-1-worker-13] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-bc0ed00a-cruise-control-694bdd464b-9m774=a29e3e1d-9f9f-4686-bb39-2d532690dc15}
2022-03-28 12:54:47 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Deployment my-cluster-bc0ed00a-cruise-control rolling update in namespace:namespace-7 not ready, will try again in 5000 ms (599779ms till timeout)
2022-03-28 12:54:47 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-534609d0-kafka-0=e07a77c9-92a3-4cd0-a1b6-a8aa20d99d2c, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:54:47 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1270461ms till timeout)
2022-03-28 12:54:48 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-335
2022-03-28 12:54:48 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:48 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-336
2022-03-28 12:54:48 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:54:48 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:54:48 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-534609d0-kafka-1 hasn't rolled
2022-03-28 12:54:48 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-534609d0-kafka rolling update not ready, will try again in 5000 ms (1768546ms till timeout)
2022-03-28 12:54:48 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (777278ms till timeout)
2022-03-28 12:54:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (295340ms till timeout)
2022-03-28 12:54:48 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-336
2022-03-28 12:54:48 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:48 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-337
2022-03-28 12:54:49 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1269350ms till timeout)
2022-03-28 12:54:49 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-337
2022-03-28 12:54:49 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:49 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-338
2022-03-28 12:54:49 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (776169ms till timeout)
2022-03-28 12:54:49 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (294228ms till timeout)
2022-03-28 12:54:49 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-338
2022-03-28 12:54:49 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:49 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-339
2022-03-28 12:54:50 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1268237ms till timeout)
2022-03-28 12:54:50 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-339
2022-03-28 12:54:50 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:50 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-34
2022-03-28 12:54:50 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (775051ms till timeout)
2022-03-28 12:54:50 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (293109ms till timeout)
2022-03-28 12:54:50 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-34
2022-03-28 12:54:50 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:50 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-345
2022-03-28 12:54:51 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1267128ms till timeout)
2022-03-28 12:54:51 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-345
2022-03-28 12:54:51 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:51 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-346
2022-03-28 12:54:51 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (773940ms till timeout)
2022-03-28 12:54:51 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-346
2022-03-28 12:54:51 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:51 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-35
2022-03-28 12:54:52 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (291896ms till timeout)
2022-03-28 12:54:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients not ready, will try again in 10000 ms (468818ms till timeout)
2022-03-28 12:54:52 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1266014ms till timeout)
2022-03-28 12:54:52 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-35
2022-03-28 12:54:52 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:52 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-351
2022-03-28 12:54:52 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (772831ms till timeout)
2022-03-28 12:54:52 [ForkJoinPool-1-worker-13] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-bc0ed00a-cruise-control-694bdd464b-9m774=a29e3e1d-9f9f-4686-bb39-2d532690dc15}
2022-03-28 12:54:53 [ForkJoinPool-1-worker-13] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-bc0ed00a-cruise-control-694bdd464b-9m774=a29e3e1d-9f9f-4686-bb39-2d532690dc15, my-cluster-bc0ed00a-cruise-control-6df659bbb4-sc5wb=29b11795-6341-44de-aa10-5e355ce335cf}
2022-03-28 12:54:53 [ForkJoinPool-1-worker-13] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-bc0ed00a-cruise-control-694bdd464b-9m774=a29e3e1d-9f9f-4686-bb39-2d532690dc15}
2022-03-28 12:54:53 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Deployment my-cluster-bc0ed00a-cruise-control rolling update in namespace:namespace-7 not ready, will try again in 5000 ms (594559ms till timeout)
2022-03-28 12:54:53 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-351
2022-03-28 12:54:53 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:53 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-353
2022-03-28 12:54:53 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-534609d0-kafka-0=e07a77c9-92a3-4cd0-a1b6-a8aa20d99d2c, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:54:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (290783ms till timeout)
2022-03-28 12:54:53 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:54:53 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:54:53 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-534609d0-kafka-1 hasn't rolled
2022-03-28 12:54:53 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-534609d0-kafka rolling update not ready, will try again in 5000 ms (1763378ms till timeout)
2022-03-28 12:54:53 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1264905ms till timeout)
2022-03-28 12:54:53 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-353
2022-03-28 12:54:53 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:53 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-359
2022-03-28 12:54:53 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (771721ms till timeout)
2022-03-28 12:54:54 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-359
2022-03-28 12:54:54 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:54 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-36
2022-03-28 12:54:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (289670ms till timeout)
2022-03-28 12:54:54 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1263794ms till timeout)
2022-03-28 12:54:54 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-36
2022-03-28 12:54:54 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:54 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-360
2022-03-28 12:54:54 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (770611ms till timeout)
2022-03-28 12:54:55 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-360
2022-03-28 12:54:55 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:55 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-361
2022-03-28 12:54:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (288557ms till timeout)
2022-03-28 12:54:55 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1262685ms till timeout)
2022-03-28 12:54:55 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-361
2022-03-28 12:54:55 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:55 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-362
2022-03-28 12:54:55 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (769501ms till timeout)
2022-03-28 12:54:56 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-362
2022-03-28 12:54:56 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:56 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-363
2022-03-28 12:54:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (287342ms till timeout)
2022-03-28 12:54:56 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-363
2022-03-28 12:54:56 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:56 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-364
2022-03-28 12:54:56 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1261574ms till timeout)
2022-03-28 12:54:57 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (768392ms till timeout)
2022-03-28 12:54:57 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-364
2022-03-28 12:54:57 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:57 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-366
2022-03-28 12:54:57 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (286128ms till timeout)
2022-03-28 12:54:57 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-366
2022-03-28 12:54:57 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:57 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-367
2022-03-28 12:54:57 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1260466ms till timeout)
2022-03-28 12:54:58 [ForkJoinPool-1-worker-13] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-bc0ed00a-cruise-control-694bdd464b-9m774=a29e3e1d-9f9f-4686-bb39-2d532690dc15}
2022-03-28 12:54:58 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (767214ms till timeout)
2022-03-28 12:54:58 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-534609d0-kafka-0=e07a77c9-92a3-4cd0-a1b6-a8aa20d99d2c, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:54:58 [ForkJoinPool-1-worker-13] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-bc0ed00a-cruise-control-694bdd464b-9m774=a29e3e1d-9f9f-4686-bb39-2d532690dc15, my-cluster-bc0ed00a-cruise-control-6df659bbb4-sc5wb=29b11795-6341-44de-aa10-5e355ce335cf}
2022-03-28 12:54:58 [ForkJoinPool-1-worker-13] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-bc0ed00a-cruise-control-694bdd464b-9m774=a29e3e1d-9f9f-4686-bb39-2d532690dc15}
2022-03-28 12:54:58 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Deployment my-cluster-bc0ed00a-cruise-control rolling update in namespace:namespace-7 not ready, will try again in 5000 ms (589228ms till timeout)
2022-03-28 12:54:58 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-367
2022-03-28 12:54:58 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:58 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-368
2022-03-28 12:54:58 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:54:58 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:54:58 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-534609d0-kafka-1 hasn't rolled
2022-03-28 12:54:58 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-534609d0-kafka rolling update not ready, will try again in 5000 ms (1758157ms till timeout)
2022-03-28 12:54:58 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (285004ms till timeout)
2022-03-28 12:54:58 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-368
2022-03-28 12:54:58 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:58 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-369
2022-03-28 12:54:59 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1259354ms till timeout)
2022-03-28 12:54:59 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (766105ms till timeout)
2022-03-28 12:54:59 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-369
2022-03-28 12:54:59 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:54:59 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-37
2022-03-28 12:55:00 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-37
2022-03-28 12:55:00 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:00 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-370
2022-03-28 12:55:00 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (283789ms till timeout)
2022-03-28 12:55:00 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1258245ms till timeout)
2022-03-28 12:55:00 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (764996ms till timeout)
2022-03-28 12:55:00 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-370
2022-03-28 12:55:00 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:00 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-38
2022-03-28 12:55:01 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-38
2022-03-28 12:55:01 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:01 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-380
2022-03-28 12:55:01 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (282572ms till timeout)
2022-03-28 12:55:01 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1257074ms till timeout)
2022-03-28 12:55:01 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (763886ms till timeout)
2022-03-28 12:55:01 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-380
2022-03-28 12:55:01 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:01 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-383
2022-03-28 12:55:02 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-383
2022-03-28 12:55:02 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:02 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-389
2022-03-28 12:55:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (281458ms till timeout)
2022-03-28 12:55:02 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1255860ms till timeout)
2022-03-28 12:55:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:http-bridge-tls-st-kafka-clients not ready, will try again in 10000 ms (458275ms till timeout)
2022-03-28 12:55:02 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-389
2022-03-28 12:55:02 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:02 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-39
2022-03-28 12:55:02 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (762680ms till timeout)
2022-03-28 12:55:03 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-39
2022-03-28 12:55:03 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:03 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-396
2022-03-28 12:55:03 [ForkJoinPool-1-worker-13] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-bc0ed00a-cruise-control-694bdd464b-9m774=a29e3e1d-9f9f-4686-bb39-2d532690dc15}
2022-03-28 12:55:03 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-534609d0-kafka-0=e07a77c9-92a3-4cd0-a1b6-a8aa20d99d2c, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:55:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (280340ms till timeout)
2022-03-28 12:55:03 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1254749ms till timeout)
2022-03-28 12:55:03 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:55:03 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:55:03 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-534609d0-kafka-1 hasn't rolled
2022-03-28 12:55:03 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-534609d0-kafka rolling update not ready, will try again in 5000 ms (1752939ms till timeout)
2022-03-28 12:55:03 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-396
2022-03-28 12:55:03 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:03 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-4
2022-03-28 12:55:03 [ForkJoinPool-1-worker-13] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-bc0ed00a-cruise-control-694bdd464b-9m774=a29e3e1d-9f9f-4686-bb39-2d532690dc15, my-cluster-bc0ed00a-cruise-control-6df659bbb4-sc5wb=29b11795-6341-44de-aa10-5e355ce335cf}
2022-03-28 12:55:03 [ForkJoinPool-1-worker-13] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-bc0ed00a-cruise-control-694bdd464b-9m774=a29e3e1d-9f9f-4686-bb39-2d532690dc15}
2022-03-28 12:55:03 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Deployment my-cluster-bc0ed00a-cruise-control rolling update in namespace:namespace-7 not ready, will try again in 5000 ms (583799ms till timeout)
2022-03-28 12:55:03 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (761566ms till timeout)
2022-03-28 12:55:04 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-4
2022-03-28 12:55:04 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:04 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-403
2022-03-28 12:55:04 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (279227ms till timeout)
2022-03-28 12:55:04 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1253626ms till timeout)
2022-03-28 12:55:04 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-403
2022-03-28 12:55:04 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:04 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-404
2022-03-28 12:55:04 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (760456ms till timeout)
2022-03-28 12:55:05 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-404
2022-03-28 12:55:05 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:05 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-407
2022-03-28 12:55:05 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (278012ms till timeout)
2022-03-28 12:55:05 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1252515ms till timeout)
2022-03-28 12:55:05 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-407
2022-03-28 12:55:05 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:05 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-410
2022-03-28 12:55:06 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (759347ms till timeout)
2022-03-28 12:55:06 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-410
2022-03-28 12:55:06 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:06 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-411
2022-03-28 12:55:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (276792ms till timeout)
2022-03-28 12:55:07 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1251293ms till timeout)
2022-03-28 12:55:07 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (758223ms till timeout)
2022-03-28 12:55:07 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-411
2022-03-28 12:55:07 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:07 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-414
2022-03-28 12:55:07 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-414
2022-03-28 12:55:07 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:07 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-418
2022-03-28 12:55:08 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (275576ms till timeout)
2022-03-28 12:55:08 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1250081ms till timeout)
2022-03-28 12:55:08 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (757112ms till timeout)
2022-03-28 12:55:08 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-418
2022-03-28 12:55:08 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:08 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-42
2022-03-28 12:55:08 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-534609d0-kafka-0=e07a77c9-92a3-4cd0-a1b6-a8aa20d99d2c, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:55:08 [ForkJoinPool-1-worker-13] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-bc0ed00a-cruise-control-694bdd464b-9m774=a29e3e1d-9f9f-4686-bb39-2d532690dc15}
2022-03-28 12:55:08 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:55:08 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:55:08 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-534609d0-kafka-1 hasn't rolled
2022-03-28 12:55:08 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-534609d0-kafka rolling update not ready, will try again in 5000 ms (1747722ms till timeout)
2022-03-28 12:55:08 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-42
2022-03-28 12:55:08 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:08 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-420
2022-03-28 12:55:09 [ForkJoinPool-1-worker-13] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-bc0ed00a-cruise-control-694bdd464b-9m774=a29e3e1d-9f9f-4686-bb39-2d532690dc15, my-cluster-bc0ed00a-cruise-control-6df659bbb4-sc5wb=29b11795-6341-44de-aa10-5e355ce335cf}
2022-03-28 12:55:09 [ForkJoinPool-1-worker-13] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-bc0ed00a-cruise-control-694bdd464b-9m774=a29e3e1d-9f9f-4686-bb39-2d532690dc15}
2022-03-28 12:55:09 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Deployment my-cluster-bc0ed00a-cruise-control rolling update in namespace:namespace-7 not ready, will try again in 5000 ms (578470ms till timeout)
2022-03-28 12:55:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (274460ms till timeout)
2022-03-28 12:55:09 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-420
2022-03-28 12:55:09 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:09 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-421
2022-03-28 12:55:09 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (755891ms till timeout)
2022-03-28 12:55:09 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1248859ms till timeout)
2022-03-28 12:55:10 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-421
2022-03-28 12:55:10 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:10 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-422
2022-03-28 12:55:10 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (273245ms till timeout)
2022-03-28 12:55:10 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1247747ms till timeout)
2022-03-28 12:55:10 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (754779ms till timeout)
2022-03-28 12:55:10 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-422
2022-03-28 12:55:10 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:10 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-423
2022-03-28 12:55:11 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-423
2022-03-28 12:55:11 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:11 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-424
2022-03-28 12:55:11 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-424
2022-03-28 12:55:11 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:11 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-425
2022-03-28 12:55:11 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (272031ms till timeout)
2022-03-28 12:55:11 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1246534ms till timeout)
2022-03-28 12:55:11 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (753566ms till timeout)
2022-03-28 12:55:12 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-425
2022-03-28 12:55:12 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:12 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-426
2022-03-28 12:55:12 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:241] Delete of KafkaBridge http-bridge-tls-cluster-name in namespace http-bridge-tls-st
2022-03-28 12:55:12 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-426
2022-03-28 12:55:12 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:12 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-427
2022-03-28 12:55:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (270918ms till timeout)
2022-03-28 12:55:13 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1245318ms till timeout)
2022-03-28 12:55:13 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (752350ms till timeout)
2022-03-28 12:55:13 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaBridge:http-bridge-tls-cluster-name
2022-03-28 12:55:13 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:241] Delete of KafkaUser my-user-1621717414-1679128198 in namespace http-bridge-tls-st
2022-03-28 12:55:13 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1621717414-1679128198
2022-03-28 12:55:13 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:241] Delete of Kafka http-bridge-tls-cluster-name in namespace http-bridge-tls-st
2022-03-28 12:55:13 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-427
2022-03-28 12:55:13 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:13 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-428
2022-03-28 12:55:13 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:http-bridge-tls-cluster-name
2022-03-28 12:55:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:http-bridge-tls-cluster-name not ready, will try again in 10000 ms (839889ms till timeout)
2022-03-28 12:55:13 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-534609d0-kafka-0=e07a77c9-92a3-4cd0-a1b6-a8aa20d99d2c, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:55:13 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-428
2022-03-28 12:55:13 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:13 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-429
2022-03-28 12:55:14 [ForkJoinPool-1-worker-13] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-bc0ed00a-cruise-control-694bdd464b-9m774=a29e3e1d-9f9f-4686-bb39-2d532690dc15}
2022-03-28 12:55:14 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:55:14 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:55:14 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-534609d0-kafka-1 hasn't rolled
2022-03-28 12:55:14 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-534609d0-kafka rolling update not ready, will try again in 5000 ms (1742506ms till timeout)
2022-03-28 12:55:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (269793ms till timeout)
2022-03-28 12:55:14 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1244203ms till timeout)
2022-03-28 12:55:14 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (751235ms till timeout)
2022-03-28 12:55:14 [ForkJoinPool-1-worker-13] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-bc0ed00a-cruise-control-694bdd464b-9m774=a29e3e1d-9f9f-4686-bb39-2d532690dc15, my-cluster-bc0ed00a-cruise-control-6df659bbb4-sc5wb=29b11795-6341-44de-aa10-5e355ce335cf}
2022-03-28 12:55:14 [ForkJoinPool-1-worker-13] DEBUG [DeploymentUtils:122] Some pods still need to roll: {my-cluster-bc0ed00a-cruise-control-694bdd464b-9m774=a29e3e1d-9f9f-4686-bb39-2d532690dc15}
2022-03-28 12:55:14 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Deployment my-cluster-bc0ed00a-cruise-control rolling update in namespace:namespace-7 not ready, will try again in 5000 ms (573145ms till timeout)
2022-03-28 12:55:14 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-429
2022-03-28 12:55:14 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:14 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-430
2022-03-28 12:55:15 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-430
2022-03-28 12:55:15 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:15 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-431
2022-03-28 12:55:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (268681ms till timeout)
2022-03-28 12:55:15 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (750113ms till timeout)
2022-03-28 12:55:15 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1243081ms till timeout)
2022-03-28 12:55:15 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-431
2022-03-28 12:55:15 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:15 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-432
2022-03-28 12:55:16 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-432
2022-03-28 12:55:16 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:16 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-433
2022-03-28 12:55:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (267465ms till timeout)
2022-03-28 12:55:16 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1241965ms till timeout)
2022-03-28 12:55:16 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (748997ms till timeout)
2022-03-28 12:55:16 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-433
2022-03-28 12:55:16 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:16 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-434
2022-03-28 12:55:17 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-434
2022-03-28 12:55:17 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:17 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-435
2022-03-28 12:55:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (266251ms till timeout)
2022-03-28 12:55:17 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1240751ms till timeout)
2022-03-28 12:55:17 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (747783ms till timeout)
2022-03-28 12:55:17 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-435
2022-03-28 12:55:17 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:17 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-436
2022-03-28 12:55:18 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-436
2022-03-28 12:55:18 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:18 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-437
2022-03-28 12:55:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (265036ms till timeout)
2022-03-28 12:55:18 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (746571ms till timeout)
2022-03-28 12:55:18 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1239538ms till timeout)
2022-03-28 12:55:18 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-437
2022-03-28 12:55:18 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:18 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-438
2022-03-28 12:55:19 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-534609d0-kafka-0=e07a77c9-92a3-4cd0-a1b6-a8aa20d99d2c, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:55:19 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:55:19 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:55:19 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-534609d0-kafka-1 hasn't rolled
2022-03-28 12:55:19 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-534609d0-kafka rolling update not ready, will try again in 5000 ms (1737289ms till timeout)
2022-03-28 12:55:19 [ForkJoinPool-1-worker-13] DEBUG [DeploymentUtils:113] Existing snapshot: {my-cluster-bc0ed00a-cruise-control-694bdd464b-9m774=a29e3e1d-9f9f-4686-bb39-2d532690dc15}
2022-03-28 12:55:19 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-438
2022-03-28 12:55:19 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:19 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-439
2022-03-28 12:55:19 [ForkJoinPool-1-worker-13] DEBUG [DeploymentUtils:115] Current  snapshot: {my-cluster-bc0ed00a-cruise-control-6df659bbb4-sc5wb=29b11795-6341-44de-aa10-5e355ce335cf}
2022-03-28 12:55:19 [ForkJoinPool-1-worker-13] DEBUG [DeploymentUtils:119] All pods seem to have rolled
2022-03-28 12:55:19 [ForkJoinPool-1-worker-13] INFO  [DeploymentUtils:161] Wait for Deployment: my-cluster-bc0ed00a-cruise-control will be ready
2022-03-28 12:55:19 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Wait for Deployment: my-cluster-bc0ed00a-cruise-control will be ready
2022-03-28 12:55:19 [ForkJoinPool-1-worker-13] INFO  [DeploymentUtils:168] Deployment: my-cluster-bc0ed00a-cruise-control is ready
2022-03-28 12:55:19 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-bc0ed00a, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-bc0ed00a-cruise-control}, additionalProperties={})to be ready
2022-03-28 12:55:19 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-bc0ed00a-cruise-control-6df659bbb4-sc5wb not ready: cruise-control)
2022-03-28 12:55:19 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-bc0ed00a-cruise-control-6df659bbb4-sc5wb not ready: tls-sidecar)
2022-03-28 12:55:19 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:106] Pods my-cluster-bc0ed00a-cruise-control-6df659bbb4-sc5wb are ready
2022-03-28 12:55:19 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-bc0ed00a, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-bc0ed00a-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599890ms till timeout)
2022-03-28 12:55:20 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-439
2022-03-28 12:55:20 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:20 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-440
2022-03-28 12:55:20 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1238331ms till timeout)
2022-03-28 12:55:20 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (745361ms till timeout)
2022-03-28 12:55:20 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (263819ms till timeout)
2022-03-28 12:55:20 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-440
2022-03-28 12:55:20 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:20 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-441
2022-03-28 12:55:21 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-bc0ed00a-cruise-control-6df659bbb4-sc5wb not ready: cruise-control)
2022-03-28 12:55:21 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-bc0ed00a-cruise-control-6df659bbb4-sc5wb not ready: tls-sidecar)
2022-03-28 12:55:21 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:106] Pods my-cluster-bc0ed00a-cruise-control-6df659bbb4-sc5wb are ready
2022-03-28 12:55:21 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-bc0ed00a, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-bc0ed00a-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598675ms till timeout)
2022-03-28 12:55:21 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1237219ms till timeout)
2022-03-28 12:55:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (744250ms till timeout)
2022-03-28 12:55:21 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-441
2022-03-28 12:55:21 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:21 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-442
2022-03-28 12:55:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (262609ms till timeout)
2022-03-28 12:55:21 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-442
2022-03-28 12:55:21 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:21 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-443
2022-03-28 12:55:22 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-443
2022-03-28 12:55:22 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:22 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-444
2022-03-28 12:55:22 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-bc0ed00a-cruise-control-6df659bbb4-sc5wb not ready: cruise-control)
2022-03-28 12:55:22 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-bc0ed00a-cruise-control-6df659bbb4-sc5wb not ready: tls-sidecar)
2022-03-28 12:55:22 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:106] Pods my-cluster-bc0ed00a-cruise-control-6df659bbb4-sc5wb are ready
2022-03-28 12:55:22 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-bc0ed00a, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-bc0ed00a-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597459ms till timeout)
2022-03-28 12:55:22 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (743033ms till timeout)
2022-03-28 12:55:22 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1236000ms till timeout)
2022-03-28 12:55:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (261394ms till timeout)
2022-03-28 12:55:22 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-444
2022-03-28 12:55:22 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:22 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-445
2022-03-28 12:55:23 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-445
2022-03-28 12:55:23 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:23 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-446
2022-03-28 12:55:23 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-bc0ed00a-cruise-control-6df659bbb4-sc5wb not ready: cruise-control)
2022-03-28 12:55:23 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-bc0ed00a-cruise-control-6df659bbb4-sc5wb not ready: tls-sidecar)
2022-03-28 12:55:23 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:106] Pods my-cluster-bc0ed00a-cruise-control-6df659bbb4-sc5wb are ready
2022-03-28 12:55:23 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-bc0ed00a, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-bc0ed00a-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596242ms till timeout)
2022-03-28 12:55:23 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1234785ms till timeout)
2022-03-28 12:55:23 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (741814ms till timeout)
2022-03-28 12:55:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (260177ms till timeout)
2022-03-28 12:55:23 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:369] ############################################################################
2022-03-28 12:55:23 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Namespace http-bridge-tls-st removal
2022-03-28 12:55:23 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace http-bridge-tls-st -o yaml
2022-03-28 12:55:24 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-534609d0-kafka-0=e07a77c9-92a3-4cd0-a1b6-a8aa20d99d2c, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:55:24 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:55:24 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:55:24 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-534609d0-kafka-2 hasn't rolled
2022-03-28 12:55:24 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-534609d0-kafka rolling update not ready, will try again in 5000 ms (1732064ms till timeout)
2022-03-28 12:55:24 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-bc0ed00a-cruise-control-6df659bbb4-sc5wb not ready: cruise-control)
2022-03-28 12:55:24 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-bc0ed00a-cruise-control-6df659bbb4-sc5wb not ready: tls-sidecar)
2022-03-28 12:55:24 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:106] Pods my-cluster-bc0ed00a-cruise-control-6df659bbb4-sc5wb are ready
2022-03-28 12:55:24 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-bc0ed00a, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-bc0ed00a-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595131ms till timeout)
2022-03-28 12:55:24 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (740601ms till timeout)
2022-03-28 12:55:24 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1233568ms till timeout)
2022-03-28 12:55:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (259059ms till timeout)
2022-03-28 12:55:25 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-bc0ed00a-cruise-control-6df659bbb4-sc5wb not ready: cruise-control)
2022-03-28 12:55:25 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-bc0ed00a-cruise-control-6df659bbb4-sc5wb not ready: tls-sidecar)
2022-03-28 12:55:25 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:106] Pods my-cluster-bc0ed00a-cruise-control-6df659bbb4-sc5wb are ready
2022-03-28 12:55:25 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-bc0ed00a, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-bc0ed00a-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593916ms till timeout)
2022-03-28 12:55:25 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:444] Kafka: my-cluster-0b062573 is in desired state: Ready
2022-03-28 12:55:25 [ForkJoinPool-1-worker-15] INFO  [ReconciliationST:80] Adding pause annotation into Kafka resource and also scaling replicas to 4, new pod should not appear
2022-03-28 12:55:26 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1232351ms till timeout)
2022-03-28 12:55:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (257838ms till timeout)
2022-03-28 12:55:26 [ForkJoinPool-1-worker-15] INFO  [ReconciliationST:86] Kafka should contain status with ReconciliationPaused
2022-03-28 12:55:26 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-0b062573 will have desired state: ReconciliationPaused
2022-03-28 12:55:26 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-0b062573 will have desired state: ReconciliationPaused
2022-03-28 12:55:26 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: ReconciliationPaused not ready, will try again in 1000 ms (839888ms till timeout)
2022-03-28 12:55:27 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-bc0ed00a-cruise-control-6df659bbb4-sc5wb not ready: cruise-control)
2022-03-28 12:55:27 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-bc0ed00a-cruise-control-6df659bbb4-sc5wb not ready: tls-sidecar)
2022-03-28 12:55:27 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:106] Pods my-cluster-bc0ed00a-cruise-control-6df659bbb4-sc5wb are ready
2022-03-28 12:55:27 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-bc0ed00a, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-bc0ed00a-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592806ms till timeout)
2022-03-28 12:55:27 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1231237ms till timeout)
2022-03-28 12:55:27 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (256618ms till timeout)
2022-03-28 12:55:27 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Kafka: my-cluster-0b062573 will have desired state: ReconciliationPaused not ready, will try again in 1000 ms (838778ms till timeout)
2022-03-28 12:55:28 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-bc0ed00a-cruise-control-6df659bbb4-sc5wb not ready: cruise-control)
2022-03-28 12:55:28 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-bc0ed00a-cruise-control-6df659bbb4-sc5wb not ready: tls-sidecar)
2022-03-28 12:55:28 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:106] Pods my-cluster-bc0ed00a-cruise-control-6df659bbb4-sc5wb are ready
2022-03-28 12:55:28 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-bc0ed00a, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-bc0ed00a-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591590ms till timeout)
2022-03-28 12:55:28 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1230126ms till timeout)
2022-03-28 12:55:28 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (255501ms till timeout)
2022-03-28 12:55:28 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:444] Kafka: my-cluster-0b062573 is in desired state: ReconciliationPaused
2022-03-28 12:55:28 [ForkJoinPool-1-worker-15] INFO  [PodUtils:209] Wait until Pod my-cluster-0b062573-kafka will have stable 3 replicas
2022-03-28 12:55:28 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for  Podmy-cluster-0b062573-kafka will have 3 replicas
2022-03-28 12:55:29 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 19 polls
2022-03-28 12:55:29 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-0b062573-kafka will have 3 replicas not ready, will try again in 1000 ms (179679ms till timeout)
2022-03-28 12:55:29 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-446
2022-03-28 12:55:29 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:29 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-447
2022-03-28 12:55:29 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-bc0ed00a-cruise-control-6df659bbb4-sc5wb not ready: cruise-control)
2022-03-28 12:55:29 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-bc0ed00a-cruise-control-6df659bbb4-sc5wb not ready: tls-sidecar)
2022-03-28 12:55:29 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:106] Pods my-cluster-bc0ed00a-cruise-control-6df659bbb4-sc5wb are ready
2022-03-28 12:55:29 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-bc0ed00a, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-bc0ed00a-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590479ms till timeout)
2022-03-28 12:55:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace http-bridge-tls-st -o yaml
2022-03-28 12:55:29 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (474474ms till timeout)
2022-03-28 12:55:29 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1228918ms till timeout)
2022-03-28 12:55:29 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-534609d0-kafka-0=e07a77c9-92a3-4cd0-a1b6-a8aa20d99d2c, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:55:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (254296ms till timeout)
2022-03-28 12:55:29 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:55:29 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:55:29 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-534609d0-kafka-2 hasn't rolled
2022-03-28 12:55:29 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-534609d0-kafka rolling update not ready, will try again in 5000 ms (1726893ms till timeout)
2022-03-28 12:55:30 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 18 polls
2022-03-28 12:55:30 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-0b062573-kafka will have 3 replicas not ready, will try again in 1000 ms (178462ms till timeout)
2022-03-28 12:55:30 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace http-bridge-tls-st -o yaml
2022-03-28 12:55:30 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-bc0ed00a-cruise-control-6df659bbb4-sc5wb not ready: cruise-control)
2022-03-28 12:55:30 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-bc0ed00a-cruise-control-6df659bbb4-sc5wb not ready: tls-sidecar)
2022-03-28 12:55:30 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:106] Pods my-cluster-bc0ed00a-cruise-control-6df659bbb4-sc5wb are ready
2022-03-28 12:55:30 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-bc0ed00a, strimzi.io/kind=Kafka, strimzi.io/name=my-cluster-bc0ed00a-cruise-control}, additionalProperties={})to be ready not ready, will try again in 1000 ms (589369ms till timeout)
2022-03-28 12:55:30 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1227807ms till timeout)
2022-03-28 12:55:30 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (253170ms till timeout)
2022-03-28 12:55:31 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-bc0ed00a-cruise-control-6df659bbb4-sc5wb not ready: cruise-control)
2022-03-28 12:55:31 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-bc0ed00a-cruise-control-6df659bbb4-sc5wb not ready: tls-sidecar)
2022-03-28 12:55:31 [ForkJoinPool-1-worker-13] DEBUG [PodUtils:106] Pods my-cluster-bc0ed00a-cruise-control-6df659bbb4-sc5wb are ready
2022-03-28 12:55:31 [ForkJoinPool-1-worker-13] INFO  [DeploymentUtils:141] Deployment my-cluster-bc0ed00a-cruise-control rolling update finished
2022-03-28 12:55:31 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 17 polls
2022-03-28 12:55:31 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-0b062573-kafka will have 3 replicas not ready, will try again in 1000 ms (177139ms till timeout)
2022-03-28 12:55:31 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1226663ms till timeout)
2022-03-28 12:55:31 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (252041ms till timeout)
2022-03-28 12:55:31 [ForkJoinPool-1-worker-13] INFO  [CruiseControlConfigurationST:280] Verifying that Kafka pods did not roll
2022-03-28 12:55:31 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Waiting for stability of rolling update will be not triggered
2022-03-28 12:55:31 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:32 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:32 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:32 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:55:32 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 50
2022-03-28 12:55:32 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (299889ms till timeout)
2022-03-28 12:55:33 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1225392ms till timeout)
2022-03-28 12:55:33 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 16 polls
2022-03-28 12:55:33 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-0b062573-kafka will have 3 replicas not ready, will try again in 1000 ms (175820ms till timeout)
2022-03-28 12:55:33 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (250877ms till timeout)
2022-03-28 12:55:33 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:33 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:33 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:33 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:55:33 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 49
2022-03-28 12:55:33 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (298777ms till timeout)
2022-03-28 12:55:34 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1224280ms till timeout)
2022-03-28 12:55:34 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:34 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (249353ms till timeout)
2022-03-28 12:55:34 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 15 polls
2022-03-28 12:55:34 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-0b062573-kafka will have 3 replicas not ready, will try again in 1000 ms (174289ms till timeout)
2022-03-28 12:55:34 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:34 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:34 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:55:34 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 48
2022-03-28 12:55:34 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (297411ms till timeout)
2022-03-28 12:55:34 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-534609d0-kafka-0=e07a77c9-92a3-4cd0-a1b6-a8aa20d99d2c, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:55:34 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:55:34 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:55:34 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-534609d0-kafka-2 hasn't rolled
2022-03-28 12:55:34 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-534609d0-kafka rolling update not ready, will try again in 5000 ms (1721781ms till timeout)
2022-03-28 12:55:35 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1223170ms till timeout)
2022-03-28 12:55:35 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-447
2022-03-28 12:55:35 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:35 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-448
2022-03-28 12:55:35 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:35 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (248236ms till timeout)
2022-03-28 12:55:35 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 14 polls
2022-03-28 12:55:35 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-0b062573-kafka will have 3 replicas not ready, will try again in 1000 ms (172960ms till timeout)
2022-03-28 12:55:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace http-bridge-tls-st -o yaml
2022-03-28 12:55:35 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Namespace http-bridge-tls-st removal not ready, will try again in 1000 ms (467980ms till timeout)
2022-03-28 12:55:35 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:35 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:35 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:55:35 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 47
2022-03-28 12:55:35 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (295980ms till timeout)
2022-03-28 12:55:36 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-448
2022-03-28 12:55:36 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:36 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-449
2022-03-28 12:55:36 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1222060ms till timeout)
2022-03-28 12:55:36 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-449
2022-03-28 12:55:36 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:36 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-450
2022-03-28 12:55:36 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (247124ms till timeout)
2022-03-28 12:55:36 [ForkJoinPool-1-worker-9] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace http-bridge-tls-st -o yaml
2022-03-28 12:55:36 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:37 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 13 polls
2022-03-28 12:55:37 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-0b062573-kafka will have 3 replicas not ready, will try again in 1000 ms (171743ms till timeout)
2022-03-28 12:55:37 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:37 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:37 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:55:37 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 46
2022-03-28 12:55:37 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (294763ms till timeout)
2022-03-28 12:55:37 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1220949ms till timeout)
2022-03-28 12:55:37 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (246013ms till timeout)
2022-03-28 12:55:38 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:38 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 12 polls
2022-03-28 12:55:38 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-0b062573-kafka will have 3 replicas not ready, will try again in 1000 ms (170526ms till timeout)
2022-03-28 12:55:38 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:38 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:38 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:55:38 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 45
2022-03-28 12:55:38 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (293546ms till timeout)
2022-03-28 12:55:38 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1219839ms till timeout)
2022-03-28 12:55:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (244899ms till timeout)
2022-03-28 12:55:39 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:39 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 11 polls
2022-03-28 12:55:39 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-0b062573-kafka will have 3 replicas not ready, will try again in 1000 ms (169310ms till timeout)
2022-03-28 12:55:39 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:39 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:39 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:55:39 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 44
2022-03-28 12:55:39 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (292330ms till timeout)
2022-03-28 12:55:39 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1218731ms till timeout)
2022-03-28 12:55:39 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-534609d0-kafka-0=e07a77c9-92a3-4cd0-a1b6-a8aa20d99d2c, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:55:39 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:55:39 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:55:39 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-534609d0-kafka-2 hasn't rolled
2022-03-28 12:55:39 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-534609d0-kafka rolling update not ready, will try again in 5000 ms (1716668ms till timeout)
2022-03-28 12:55:40 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (243787ms till timeout)
2022-03-28 12:55:40 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:40 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 10 polls
2022-03-28 12:55:40 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-0b062573-kafka will have 3 replicas not ready, will try again in 1000 ms (168094ms till timeout)
2022-03-28 12:55:40 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:40 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:40 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:55:40 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 43
2022-03-28 12:55:40 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (291212ms till timeout)
2022-03-28 12:55:40 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-aa0ecd2f will have desired state: Ready not ready, will try again in 1000 ms (1217557ms till timeout)
2022-03-28 12:55:41 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (242674ms till timeout)
2022-03-28 12:55:41 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:42 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:444] Kafka: my-cluster-aa0ecd2f is in desired state: Ready
2022-03-28 12:55:42 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:42 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 9 polls
2022-03-28 12:55:42 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:42 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-0b062573-kafka will have 3 replicas not ready, will try again in 1000 ms (166770ms till timeout)
2022-03-28 12:55:42 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:55:42 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 42
2022-03-28 12:55:42 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (289893ms till timeout)
2022-03-28 12:55:42 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 exec my-cluster-aa0ecd2f-cruise-control-7485568b8f-44k2j -- /bin/bash -c cat /tmp/cruisecontrol.properties
2022-03-28 12:55:42 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-450
2022-03-28 12:55:42 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:42 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-451
2022-03-28 12:55:42 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (241564ms till timeout)
2022-03-28 12:55:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-9 get Namespace http-bridge-tls-st -o yaml
2022-03-28 12:55:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Return code: 1
2022-03-28 12:55:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 12:55:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] Error from server (NotFound): namespaces "http-bridge-tls-st" not found
2022-03-28 12:55:42 [ForkJoinPool-1-worker-9] DEBUG [Exec:419] ======STDERR END======
2022-03-28 12:55:42 [ForkJoinPool-1-worker-9] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[namespace-9], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[namespace-8], io.strimzi.test.logs.CollectorElement@f851b6c3=[namespace-7], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[infra-namespace, reconciliation-st], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-6], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 12:55:42 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:254] HttpBridgeTlsST - Notifies waiting test suites:[UserST, TopicST, ReconciliationST, CruiseControlConfigurationST, HttpBridgeScramShaST, ThrottlingQuotaST] to and randomly select one to start execution
2022-03-28 12:55:42 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:85] [bridge.HttpBridgeTlsST] - Removing parallel suite: HttpBridgeTlsST
2022-03-28 12:55:42 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:89] [bridge.HttpBridgeTlsST] - Parallel suites count: 4
[ERROR] Tests run: 5, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 592.606 s <<< FAILURE! - in io.strimzi.systemtest.bridge.HttpBridgeTlsST
[ERROR] io.strimzi.systemtest.operators.ReconciliationST.testPauseReconciliationInKafkaRebalanceAndTopic(ExtensionContext)  Time elapsed: 410.567 s  <<< ERROR!
io.strimzi.test.WaitException: Timeout after 180000 ms waiting for KafkaTopic's spec will be stable
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:170)
	at io.strimzi.test.TestUtils.waitFor(TestUtils.java:121)
	at io.strimzi.systemtest.utils.kafkaUtils.KafkaTopicUtils.waitForKafkaTopicSpecStability(KafkaTopicUtils.java:197)
	at io.strimzi.systemtest.operators.ReconciliationST.testPauseReconciliationInKafkaRebalanceAndTopic(ReconciliationST.java:154)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:725)
	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:149)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:140)
	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:84)
	at org.junit.jupiter.engine.execution.ExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(ExecutableInvoker.java:115)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.lambda$invoke$0(ExecutableInvoker.java:105)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:104)
	at org.junit.jupiter.engine.execution.ExecutableInvoker.invoke(ExecutableInvoker.java:98)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:214)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:210)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:135)
	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:66)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinTask.doJoin(ForkJoinTask.java:396)
	at java.base/java.util.concurrent.ForkJoinTask.join(ForkJoinTask.java:721)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.joinConcurrentTasksInReverseOrderToEnableWorkStealing(ForkJoinPoolHierarchicalTestExecutorService.java:162)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:136)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
	at java.base/java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
	at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
	at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
	at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
	at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
	at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)

2022-03-28 12:55:42 [ForkJoinPool-1-worker-9] INFO  [TestSeparator:23] ############################################################################
2022-03-28 12:55:42 [ForkJoinPool-1-worker-9] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testKafkaAdminTopicOperations-STARTED
2022-03-28 12:55:42 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:658] ============================================================================
2022-03-28 12:55:42 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:659] [operators.topic.ThrottlingQuotaST - Before Each] - Setup test case environment
2022-03-28 12:55:42 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:77] [operators.topic.ThrottlingQuotaST] - Adding parallel test: testKafkaAdminTopicOperations
2022-03-28 12:55:42 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:81] [operators.topic.ThrottlingQuotaST] - Parallel test count: 7
2022-03-28 12:55:42 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:205] [testKafkaAdminTopicOperations] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:55:42 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:210] testKafkaAdminTopicOperations is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:55:42 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-451
2022-03-28 12:55:42 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:42 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-452
2022-03-28 12:55:43 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:43 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 8 polls
2022-03-28 12:55:43 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-0b062573-kafka will have 3 replicas not ready, will try again in 1000 ms (165554ms till timeout)
2022-03-28 12:55:43 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:43 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:43 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:55:43 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 41
2022-03-28 12:55:43 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (288672ms till timeout)
2022-03-28 12:55:43 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (240453ms till timeout)
2022-03-28 12:55:43 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-9 exec my-cluster-aa0ecd2f-cruise-control-7485568b8f-44k2j -- /bin/bash -c cat /tmp/cruisecontrol.properties
2022-03-28 12:55:43 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 12:55:43 [ForkJoinPool-1-worker-3] INFO  [CruiseControlConfigurationST:221] Verifying that all configuration in the cruise control container matching the cruise control file /tmp/cruisecontrol.properties properties
2022-03-28 12:55:43 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:674] ============================================================================
2022-03-28 12:55:43 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:675] [cruisecontrol.CruiseControlConfigurationST - After Each] - Clean up after test
2022-03-28 12:55:43 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:344] ############################################################################
2022-03-28 12:55:43 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:348] Delete all resources for testConfigurationReflection
2022-03-28 12:55:43 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:241] Delete of Kafka my-cluster-aa0ecd2f in namespace namespace-9
2022-03-28 12:55:43 [ForkJoinPool-1-worker-3] INFO  [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-9, for cruise control Kafka cluster my-cluster-aa0ecd2f
2022-03-28 12:55:44 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-452
2022-03-28 12:55:44 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:44 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-453
2022-03-28 12:55:44 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:44 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-aa0ecd2f
2022-03-28 12:55:44 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 7 polls
2022-03-28 12:55:44 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-0b062573-kafka will have 3 replicas not ready, will try again in 1000 ms (164366ms till timeout)
2022-03-28 12:55:44 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:44 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:44 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:55:44 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 40
2022-03-28 12:55:44 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (287480ms till timeout)
2022-03-28 12:55:44 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-aa0ecd2f not ready, will try again in 10000 ms (839791ms till timeout)
2022-03-28 12:55:44 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (239313ms till timeout)
2022-03-28 12:55:44 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-453
2022-03-28 12:55:44 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:44 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-454
2022-03-28 12:55:44 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-534609d0-kafka-0=e07a77c9-92a3-4cd0-a1b6-a8aa20d99d2c, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:55:45 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:55:45 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:55:45 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-534609d0-kafka-2 hasn't rolled
2022-03-28 12:55:45 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-534609d0-kafka rolling update not ready, will try again in 5000 ms (1711550ms till timeout)
2022-03-28 12:55:45 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:45 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-454
2022-03-28 12:55:45 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:45 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-455
2022-03-28 12:55:45 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 6 polls
2022-03-28 12:55:45 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-0b062573-kafka will have 3 replicas not ready, will try again in 1000 ms (163153ms till timeout)
2022-03-28 12:55:45 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:45 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:45 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:55:45 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 39
2022-03-28 12:55:45 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (286271ms till timeout)
2022-03-28 12:55:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (238105ms till timeout)
2022-03-28 12:55:46 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:47 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 5 polls
2022-03-28 12:55:47 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-0b062573-kafka will have 3 replicas not ready, will try again in 1000 ms (161828ms till timeout)
2022-03-28 12:55:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (236789ms till timeout)
2022-03-28 12:55:47 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:47 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:47 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:55:47 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 38
2022-03-28 12:55:47 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (284848ms till timeout)
2022-03-28 12:55:47 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:210] testKafkaAdminTopicOperations is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:55:48 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:48 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 4 polls
2022-03-28 12:55:48 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-0b062573-kafka will have 3 replicas not ready, will try again in 1000 ms (160507ms till timeout)
2022-03-28 12:55:48 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:48 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:48 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:55:48 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 37
2022-03-28 12:55:48 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (283630ms till timeout)
2022-03-28 12:55:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (235468ms till timeout)
2022-03-28 12:55:49 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:49 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 3 polls
2022-03-28 12:55:49 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:49 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-0b062573-kafka will have 3 replicas not ready, will try again in 1000 ms (159184ms till timeout)
2022-03-28 12:55:49 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:49 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:55:49 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 36
2022-03-28 12:55:49 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (282307ms till timeout)
2022-03-28 12:55:49 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (234145ms till timeout)
2022-03-28 12:55:50 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-534609d0-kafka-0=e07a77c9-92a3-4cd0-a1b6-a8aa20d99d2c, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:55:50 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:55:50 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:55:50 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-534609d0-kafka-2 hasn't rolled
2022-03-28 12:55:50 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-534609d0-kafka rolling update not ready, will try again in 5000 ms (1706432ms till timeout)
2022-03-28 12:55:50 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:50 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 2 polls
2022-03-28 12:55:50 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-0b062573-kafka will have 3 replicas not ready, will try again in 1000 ms (157966ms till timeout)
2022-03-28 12:55:50 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (232927ms till timeout)
2022-03-28 12:55:50 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:50 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:50 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:55:50 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 35
2022-03-28 12:55:50 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (280986ms till timeout)
2022-03-28 12:55:51 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-455
2022-03-28 12:55:51 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:51 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-456
2022-03-28 12:55:51 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:52 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (231707ms till timeout)
2022-03-28 12:55:52 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 1 polls
2022-03-28 12:55:52 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-0b062573-kafka will have 3 replicas not ready, will try again in 1000 ms (156643ms till timeout)
2022-03-28 12:55:52 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:52 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:52 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:55:52 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 34
2022-03-28 12:55:52 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (279766ms till timeout)
2022-03-28 12:55:52 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:210] testKafkaAdminTopicOperations is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:55:53 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (230596ms till timeout)
2022-03-28 12:55:53 [ForkJoinPool-1-worker-15] INFO  [PodUtils:217] Pod replicas are stable for 20 polls intervals
2022-03-28 12:55:53 [ForkJoinPool-1-worker-15] INFO  [PodUtils:228] Pod my-cluster-0b062573-kafka has 3 replicas
2022-03-28 12:55:53 [ForkJoinPool-1-worker-15] INFO  [ReconciliationST:90] Setting annotation to "false", Kafka should be scaled to 4
2022-03-28 12:55:53 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:53 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:53 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:55:53 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 33
2022-03-28 12:55:53 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (278338ms till timeout)
2022-03-28 12:55:53 [ForkJoinPool-1-worker-15] INFO  [RollingUpdateUtils:127] Waiting for 4 Pod(s) of my-cluster-0b062573-kafka to be ready
2022-03-28 12:55:53 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready
2022-03-28 12:55:53 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:55:53 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2399888ms till timeout)
2022-03-28 12:55:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (229483ms till timeout)
2022-03-28 12:55:54 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:54 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:369] ############################################################################
2022-03-28 12:55:54 [ForkJoinPool-1-worker-3] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-9 for test case:testConfigurationReflection
2022-03-28 12:55:54 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:54 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:54 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:55:54 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 32
2022-03-28 12:55:54 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (277156ms till timeout)
2022-03-28 12:55:54 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Namespace namespace-9 removal
2022-03-28 12:55:54 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 12:55:55 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:55:55 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2398730ms till timeout)
2022-03-28 12:55:55 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-534609d0-kafka-0=e07a77c9-92a3-4cd0-a1b6-a8aa20d99d2c, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:55:55 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:55:55 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:55:55 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-534609d0-kafka-2 hasn't rolled
2022-03-28 12:55:55 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-534609d0-kafka rolling update not ready, will try again in 5000 ms (1701321ms till timeout)
2022-03-28 12:55:55 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 12:55:55 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:55 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (479443ms till timeout)
2022-03-28 12:55:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (228373ms till timeout)
2022-03-28 12:55:55 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:55 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:55 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:55 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:55:55 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 31
2022-03-28 12:55:55 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (276043ms till timeout)
2022-03-28 12:55:56 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:55:56 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2397617ms till timeout)
2022-03-28 12:55:56 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 12:55:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (227261ms till timeout)
2022-03-28 12:55:56 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-456
2022-03-28 12:55:56 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:56 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-457
2022-03-28 12:55:56 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:57 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:57 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:57 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:55:57 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 30
2022-03-28 12:55:57 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (274826ms till timeout)
2022-03-28 12:55:57 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:55:57 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2396506ms till timeout)
2022-03-28 12:55:57 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-457
2022-03-28 12:55:57 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:57 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-458
2022-03-28 12:55:57 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:210] testKafkaAdminTopicOperations is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:55:57 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (226148ms till timeout)
2022-03-28 12:55:58 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-458
2022-03-28 12:55:58 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:58 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-459
2022-03-28 12:55:58 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:58 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:58 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:58 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:55:58 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 29
2022-03-28 12:55:58 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (273606ms till timeout)
2022-03-28 12:55:58 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:55:58 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2395290ms till timeout)
2022-03-28 12:55:58 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-459
2022-03-28 12:55:58 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:58 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-462
2022-03-28 12:55:58 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (225036ms till timeout)
2022-03-28 12:55:59 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:59 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-462
2022-03-28 12:55:59 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:55:59 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-466
2022-03-28 12:55:59 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:59 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:55:59 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:55:59 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 28
2022-03-28 12:55:59 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (272392ms till timeout)
2022-03-28 12:55:59 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:55:59 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2394077ms till timeout)
2022-03-28 12:55:59 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (223925ms till timeout)
2022-03-28 12:56:00 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-466
2022-03-28 12:56:00 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:56:00 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-473
2022-03-28 12:56:00 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-534609d0-kafka-0=e07a77c9-92a3-4cd0-a1b6-a8aa20d99d2c, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:56:00 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:56:00 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:56:00 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-534609d0-kafka-2 hasn't rolled
2022-03-28 12:56:00 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-534609d0-kafka rolling update not ready, will try again in 5000 ms (1696206ms till timeout)
2022-03-28 12:56:00 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:00 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:00 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:00 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:56:00 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 27
2022-03-28 12:56:00 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (271279ms till timeout)
2022-03-28 12:56:00 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:56:00 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2392960ms till timeout)
2022-03-28 12:56:01 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-473
2022-03-28 12:56:01 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:56:01 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-475
2022-03-28 12:56:01 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (222815ms till timeout)
2022-03-28 12:56:01 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:01 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-475
2022-03-28 12:56:01 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:56:01 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-477
2022-03-28 12:56:01 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:01 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:01 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:56:01 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 26
2022-03-28 12:56:01 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (270063ms till timeout)
2022-03-28 12:56:01 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 12:56:01 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:56:01 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (472949ms till timeout)
2022-03-28 12:56:02 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:56:02 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2391747ms till timeout)
2022-03-28 12:56:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (221699ms till timeout)
2022-03-28 12:56:02 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-477
2022-03-28 12:56:02 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:56:02 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-480
2022-03-28 12:56:02 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:210] testKafkaAdminTopicOperations is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:56:02 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:02 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-480
2022-03-28 12:56:02 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:56:02 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-481
2022-03-28 12:56:02 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 12:56:03 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:03 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:03 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:56:03 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 25
2022-03-28 12:56:03 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (268845ms till timeout)
2022-03-28 12:56:03 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:56:03 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2390529ms till timeout)
2022-03-28 12:56:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (220587ms till timeout)
2022-03-28 12:56:03 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 12:56:03 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:56:03 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (471412ms till timeout)
2022-03-28 12:56:03 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-481
2022-03-28 12:56:03 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:56:03 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-482
2022-03-28 12:56:04 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:04 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-482
2022-03-28 12:56:04 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:56:04 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-483
2022-03-28 12:56:04 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:04 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:04 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:56:04 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 24
2022-03-28 12:56:04 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (267627ms till timeout)
2022-03-28 12:56:04 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:56:04 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2389312ms till timeout)
2022-03-28 12:56:04 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (219456ms till timeout)
2022-03-28 12:56:04 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 12:56:05 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:05 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-534609d0-kafka-0=e07a77c9-92a3-4cd0-a1b6-a8aa20d99d2c, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:56:05 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:05 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:05 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:56:05 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 23
2022-03-28 12:56:05 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (266408ms till timeout)
2022-03-28 12:56:05 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:56:05 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2388093ms till timeout)
2022-03-28 12:56:05 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (218143ms till timeout)
2022-03-28 12:56:05 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:56:05 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:56:05 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-534609d0-kafka-2 hasn't rolled
2022-03-28 12:56:05 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-534609d0-kafka rolling update not ready, will try again in 5000 ms (1690846ms till timeout)
2022-03-28 12:56:06 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:06 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:06 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:06 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:56:06 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 22
2022-03-28 12:56:06 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (265189ms till timeout)
2022-03-28 12:56:06 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:56:06 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2386973ms till timeout)
2022-03-28 12:56:06 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (217019ms till timeout)
2022-03-28 12:56:07 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:210] testKafkaAdminTopicOperations is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:56:07 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:08 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:08 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:08 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:56:08 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 21
2022-03-28 12:56:08 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (263938ms till timeout)
2022-03-28 12:56:08 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (215776ms till timeout)
2022-03-28 12:56:08 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:56:08 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2385622ms till timeout)
2022-03-28 12:56:09 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:09 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:09 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:09 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:56:09 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 20
2022-03-28 12:56:09 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (262719ms till timeout)
2022-03-28 12:56:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (214558ms till timeout)
2022-03-28 12:56:09 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:56:09 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2384404ms till timeout)
2022-03-28 12:56:09 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-483
2022-03-28 12:56:09 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:56:09 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-484
2022-03-28 12:56:10 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 12:56:10 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:56:10 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (464871ms till timeout)
2022-03-28 12:56:10 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:10 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:10 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:10 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:56:10 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 19
2022-03-28 12:56:10 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (261503ms till timeout)
2022-03-28 12:56:10 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (213341ms till timeout)
2022-03-28 12:56:10 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:56:10 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2383188ms till timeout)
2022-03-28 12:56:10 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-534609d0-kafka-0=e07a77c9-92a3-4cd0-a1b6-a8aa20d99d2c, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:56:10 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:56:10 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:56:10 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-534609d0-kafka-2 hasn't rolled
2022-03-28 12:56:10 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-534609d0-kafka rolling update not ready, will try again in 5000 ms (1685732ms till timeout)
2022-03-28 12:56:11 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 12:56:11 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:11 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:11 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:11 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:56:11 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 18
2022-03-28 12:56:11 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (260287ms till timeout)
2022-03-28 12:56:11 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (212124ms till timeout)
2022-03-28 12:56:11 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:56:11 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2381972ms till timeout)
2022-03-28 12:56:12 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:210] testKafkaAdminTopicOperations is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:56:12 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:12 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:12 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:12 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:56:12 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 17
2022-03-28 12:56:12 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (259069ms till timeout)
2022-03-28 12:56:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (210905ms till timeout)
2022-03-28 12:56:12 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:56:12 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2380753ms till timeout)
2022-03-28 12:56:13 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:14 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:14 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:14 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:56:14 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 16
2022-03-28 12:56:14 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (257852ms till timeout)
2022-03-28 12:56:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (209689ms till timeout)
2022-03-28 12:56:14 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:56:14 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2379535ms till timeout)
2022-03-28 12:56:15 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:15 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:15 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:15 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:56:15 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 15
2022-03-28 12:56:15 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (256634ms till timeout)
2022-03-28 12:56:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (208471ms till timeout)
2022-03-28 12:56:15 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:56:15 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2378318ms till timeout)
2022-03-28 12:56:15 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-484
2022-03-28 12:56:15 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:56:15 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-485
2022-03-28 12:56:15 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-534609d0-kafka-0=e07a77c9-92a3-4cd0-a1b6-a8aa20d99d2c, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:56:15 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:56:15 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:56:15 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-534609d0-kafka-2 hasn't rolled
2022-03-28 12:56:15 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-534609d0-kafka rolling update not ready, will try again in 5000 ms (1680619ms till timeout)
2022-03-28 12:56:16 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-485
2022-03-28 12:56:16 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:56:16 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-486
2022-03-28 12:56:16 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:16 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:16 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:16 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:56:16 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 14
2022-03-28 12:56:16 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (255522ms till timeout)
2022-03-28 12:56:16 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 12:56:16 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:56:16 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (458397ms till timeout)
2022-03-28 12:56:16 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:56:16 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2377202ms till timeout)
2022-03-28 12:56:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (207353ms till timeout)
2022-03-28 12:56:17 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:17 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:210] testKafkaAdminTopicOperations is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:56:17 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 12:56:17 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:17 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:17 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:56:17 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 13
2022-03-28 12:56:17 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (254301ms till timeout)
2022-03-28 12:56:17 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:56:17 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2375985ms till timeout)
2022-03-28 12:56:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (206127ms till timeout)
2022-03-28 12:56:18 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 12:56:18 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:56:18 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (456789ms till timeout)
2022-03-28 12:56:18 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:18 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:18 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:18 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:56:18 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 12
2022-03-28 12:56:18 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (253084ms till timeout)
2022-03-28 12:56:18 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:56:18 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2374767ms till timeout)
2022-03-28 12:56:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (204915ms till timeout)
2022-03-28 12:56:19 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 12:56:19 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 12:56:19 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:56:19 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (455242ms till timeout)
2022-03-28 12:56:19 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:20 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:20 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:20 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:56:20 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 11
2022-03-28 12:56:20 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (251868ms till timeout)
2022-03-28 12:56:20 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:56:20 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2373553ms till timeout)
2022-03-28 12:56:20 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (203702ms till timeout)
2022-03-28 12:56:20 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 12:56:20 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-534609d0-kafka-0=e07a77c9-92a3-4cd0-a1b6-a8aa20d99d2c, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:56:21 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:21 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:56:21 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:56:21 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-534609d0-kafka-2 hasn't rolled
2022-03-28 12:56:21 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-534609d0-kafka rolling update not ready, will try again in 5000 ms (1675403ms till timeout)
2022-03-28 12:56:21 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:21 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:21 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:56:21 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 10
2022-03-28 12:56:21 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (250754ms till timeout)
2022-03-28 12:56:21 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:56:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2372438ms till timeout)
2022-03-28 12:56:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (202590ms till timeout)
2022-03-28 12:56:21 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 12:56:21 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:56:21 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (453316ms till timeout)
2022-03-28 12:56:21 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-486
2022-03-28 12:56:21 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:56:21 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-487
2022-03-28 12:56:22 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:22 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-487
2022-03-28 12:56:22 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:56:22 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-488
2022-03-28 12:56:22 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:22 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:22 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:56:22 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 9
2022-03-28 12:56:22 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (249540ms till timeout)
2022-03-28 12:56:22 [ForkJoinPool-1-worker-9] TRACE [SuiteThreadController:210] testKafkaAdminTopicOperations is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:56:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (201377ms till timeout)
2022-03-28 12:56:22 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:56:22 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2371224ms till timeout)
2022-03-28 12:56:22 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 12:56:23 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 12:56:23 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:56:23 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace namespace-9 removal not ready, will try again in 1000 ms (451794ms till timeout)
2022-03-28 12:56:23 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:23 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:23 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:23 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:56:23 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 8
2022-03-28 12:56:23 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (248321ms till timeout)
2022-03-28 12:56:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (200159ms till timeout)
2022-03-28 12:56:23 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:56:23 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2370006ms till timeout)
2022-03-28 12:56:24 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 12:56:24 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-9 get Namespace namespace-9 -o yaml
2022-03-28 12:56:24 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 1
2022-03-28 12:56:24 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 12:56:24 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-9" not found
2022-03-28 12:56:24 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] ======STDERR END======
2022-03-28 12:56:24 [ForkJoinPool-1-worker-3] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[namespace-8], io.strimzi.test.logs.CollectorElement@f851b6c3=[namespace-7], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[infra-namespace, reconciliation-st], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-6], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 12:56:24 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:267] testConfigurationReflection - Notifies waiting test cases:[testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testTlsExternalUserWithQuotas, testUserTemplate, testUpdateUser, testTlsExternalUser, testReceiveSimpleMessageTlsScramSha, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testSendSimpleMessageTlsScramSha, testUserWithNameMoreThan64Chars, testTlsUserWithQuotas, testConfigurationFileIsCreated, testScramUserWithQuotas, testDeployAndUnDeployCruiseControl, testConfigurationPerformanceOptions, testKafkaAdminTopicOperations] to and randomly select one to start execution
2022-03-28 12:56:24 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:93] [cruisecontrol.CruiseControlConfigurationST] - Removing parallel test: testConfigurationReflection
2022-03-28 12:56:24 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:97] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 6
2022-03-28 12:56:24 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationReflection-FINISHED
2022-03-28 12:56:24 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:30] ############################################################################
2022-03-28 12:56:24 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:23] ############################################################################
2022-03-28 12:56:24 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.topic.TopicST.testTopicModificationOfReplicationFactor-STARTED
2022-03-28 12:56:24 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:658] ============================================================================
2022-03-28 12:56:24 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:659] [operators.topic.TopicST - Before Each] - Setup test case environment
2022-03-28 12:56:24 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:77] [operators.topic.TopicST] - Adding parallel test: testTopicModificationOfReplicationFactor
2022-03-28 12:56:24 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:81] [operators.topic.TopicST] - Parallel test count: 7
2022-03-28 12:56:24 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:205] [testTopicModificationOfReplicationFactor] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:56:24 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTopicModificationOfReplicationFactor is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:56:24 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:24 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:24 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:24 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:56:24 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 7
2022-03-28 12:56:24 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (247106ms till timeout)
2022-03-28 12:56:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (198943ms till timeout)
2022-03-28 12:56:24 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:56:24 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2368790ms till timeout)
2022-03-28 12:56:25 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:26 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:26 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:26 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:56:26 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 6
2022-03-28 12:56:26 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (245891ms till timeout)
2022-03-28 12:56:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (197729ms till timeout)
2022-03-28 12:56:26 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:56:26 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2367575ms till timeout)
2022-03-28 12:56:26 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-534609d0-kafka-0=e07a77c9-92a3-4cd0-a1b6-a8aa20d99d2c, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:56:26 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:56:26 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:56:26 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-534609d0-kafka-2 hasn't rolled
2022-03-28 12:56:26 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-534609d0-kafka rolling update not ready, will try again in 5000 ms (1670260ms till timeout)
2022-03-28 12:56:27 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:27 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:27 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:27 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:56:27 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 5
2022-03-28 12:56:27 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (244675ms till timeout)
2022-03-28 12:56:27 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (196513ms till timeout)
2022-03-28 12:56:27 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:56:27 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2366359ms till timeout)
2022-03-28 12:56:27 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:230] testKafkaAdminTopicOperations test now can proceed its execution
2022-03-28 12:56:27 [ForkJoinPool-1-worker-9] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 12:56:27 [ForkJoinPool-1-worker-9] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testScramUserWithQuotas=my-cluster-90c82d82, testKafkaAdminTopicOperations=my-cluster-6acb723c, testReceiveSimpleMessageTlsScramSha=my-cluster-455915b9, testConfigurationFileIsCreated=my-cluster-020ceaf8, testThrottlingQuotasCreateAlterPartitions=my-cluster-d4dad0de, testSendSimpleMessageTls=my-cluster-cfce2f5b, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5, testUpdateUser=my-cluster-e16ccf16, testConfigurationReflection=my-cluster-aa0ecd2f, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d, testTlsExternalUserWithQuotas=my-cluster-3e2e11de, testUserWithNameMoreThan64Chars=my-cluster-ed39a269, testTlsUserWithQuotas=my-cluster-3b191485, testDeployAndUnDeployCruiseControl=my-cluster-534609d0, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2, testCapacityFile=my-cluster-39ee3f62, testConfigurationPerformanceOptions=my-cluster-bc0ed00a, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-0b062573, testReceiveSimpleMessageTls=my-cluster-362cd511, testTlsExternalUser=my-cluster-965e9d1e, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-31feb41b, testSendSimpleMessageTlsScramSha=my-cluster-27773859, testUserTemplate=my-cluster-157df94e}
2022-03-28 12:56:27 [ForkJoinPool-1-worker-9] TRACE [AbstractST:607] USERS_NAME_MAP: {testScramUserWithQuotas=my-user-1857980152-2145707698, testKafkaAdminTopicOperations=my-user-2014252818-589270412, testReceiveSimpleMessageTlsScramSha=my-user-1329598231-1183867861, testConfigurationFileIsCreated=my-user-1609977058-1196012647, testThrottlingQuotasCreateAlterPartitions=my-user-1809876063-64793399, testSendSimpleMessageTls=my-user-111586425-1868899021, testCreatingUsersWithSecretPrefix=my-user-1928899390-1444689439, testUpdateUser=my-user-1694542041-806176635, testConfigurationReflection=my-user-1580949953-976509997, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-467839358-1504804463, testTlsExternalUserWithQuotas=my-user-1786174319-1274634874, testUserWithNameMoreThan64Chars=my-user-1048816470-1103470031, testTlsUserWithQuotas=my-user-420001054-1296284129, testDeployAndUnDeployCruiseControl=my-user-475346616-1293712526, testThrottlingQuotasCreateTopic=my-user-896662438-260898887, testCapacityFile=my-user-597365340-661017603, testConfigurationPerformanceOptions=my-user-722474509-712960391, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-1153908156-1369197188, testReceiveSimpleMessageTls=my-user-2012197250-2103354173, testTlsExternalUser=my-user-1295334081-1602801897, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2100837352-1014792902, testSendSimpleMessageTlsScramSha=my-user-507582460-1003875464, testUserTemplate=my-user-1253881534-190261188}
2022-03-28 12:56:27 [ForkJoinPool-1-worker-9] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testScramUserWithQuotas=my-topic-315112689-1043284277, testKafkaAdminTopicOperations=my-topic-1915664839-460245273, testReceiveSimpleMessageTlsScramSha=my-topic-170116874-1830928683, testConfigurationFileIsCreated=my-topic-155202447-1921897899, testThrottlingQuotasCreateAlterPartitions=my-topic-1581297458-635252970, testSendSimpleMessageTls=my-topic-100192180-936335618, testCreatingUsersWithSecretPrefix=my-topic-2033928996-1209892355, testUpdateUser=my-topic-1239518257-2056139809, testConfigurationReflection=my-topic-1437230713-1827996725, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-712754859-1910580676, testTlsExternalUserWithQuotas=my-topic-593679216-1309035072, testUserWithNameMoreThan64Chars=my-topic-1603054043-1686316417, testTlsUserWithQuotas=my-topic-1752299353-1315959033, testDeployAndUnDeployCruiseControl=my-topic-1815491150-540070233, testThrottlingQuotasCreateTopic=my-topic-1532506723-1472611013, testCapacityFile=my-topic-1390944843-775067696, testConfigurationPerformanceOptions=my-topic-1891296149-770116276, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-613737897-291522718, testReceiveSimpleMessageTls=my-topic-1264858218-642520562, testTlsExternalUser=my-topic-476529121-306489993, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1084162749-1576061083, testSendSimpleMessageTlsScramSha=my-topic-62182003-1963040205, testUserTemplate=my-topic-515111334-943904347}
2022-03-28 12:56:27 [ForkJoinPool-1-worker-9] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testScramUserWithQuotas=my-cluster-90c82d82-kafka-clients, testKafkaAdminTopicOperations=my-cluster-6acb723c-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-455915b9-kafka-clients, testConfigurationFileIsCreated=my-cluster-020ceaf8-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-d4dad0de-kafka-clients, testSendSimpleMessageTls=my-cluster-cfce2f5b-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5-kafka-clients, testUpdateUser=my-cluster-e16ccf16-kafka-clients, testConfigurationReflection=my-cluster-aa0ecd2f-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-3e2e11de-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-ed39a269-kafka-clients, testTlsUserWithQuotas=my-cluster-3b191485-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-534609d0-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2-kafka-clients, testCapacityFile=my-cluster-39ee3f62-kafka-clients, testConfigurationPerformanceOptions=my-cluster-bc0ed00a-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-0b062573-kafka-clients, testReceiveSimpleMessageTls=my-cluster-362cd511-kafka-clients, testTlsExternalUser=my-cluster-965e9d1e-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-31feb41b-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-27773859-kafka-clients, testUserTemplate=my-cluster-157df94e-kafka-clients}
2022-03-28 12:56:27 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update KafkaUser my-user-2014252818-589270412 in namespace throttling-quota-st
2022-03-28 12:56:27 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-2014252818-589270412
2022-03-28 12:56:27 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-2014252818-589270412 will have desired state: Ready
2022-03-28 12:56:27 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-2014252818-589270412 will have desired state: Ready
2022-03-28 12:56:27 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:444] KafkaUser: my-user-2014252818-589270412 is in desired state: Ready
2022-03-28 12:56:27 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update Job create-admin-my-cluster-6acb723c-kafka-clients in namespace throttling-quota-st
2022-03-28 12:56:28 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:create-admin-my-cluster-6acb723c-kafka-clients
2022-03-28 12:56:28 [ForkJoinPool-1-worker-9] INFO  [JobUtils:81] Waiting for job: create-admin-my-cluster-6acb723c-kafka-clients will be in active state
2022-03-28 12:56:28 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 12:56:28 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:28 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:28 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:28 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:56:28 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 4
2022-03-28 12:56:28 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (243551ms till timeout)
2022-03-28 12:56:28 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-488
2022-03-28 12:56:28 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:56:28 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-489
2022-03-28 12:56:28 [ForkJoinPool-1-worker-9] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 12:56:28 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 12:56:28 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:56:28 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2365230ms till timeout)
2022-03-28 12:56:28 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (195380ms till timeout)
2022-03-28 12:56:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (119887ms till timeout)
2022-03-28 12:56:29 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:29 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:29 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:29 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:56:29 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 3
2022-03-28 12:56:29 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (242334ms till timeout)
2022-03-28 12:56:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (194269ms till timeout)
2022-03-28 12:56:29 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:56:29 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2364116ms till timeout)
2022-03-28 12:56:29 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTopicModificationOfReplicationFactor is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:56:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (118778ms till timeout)
2022-03-28 12:56:30 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:30 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:30 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:30 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:56:30 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 2
2022-03-28 12:56:30 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (241117ms till timeout)
2022-03-28 12:56:30 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (192954ms till timeout)
2022-03-28 12:56:30 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:56:30 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2362801ms till timeout)
2022-03-28 12:56:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (117565ms till timeout)
2022-03-28 12:56:31 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-534609d0-kafka-0=e07a77c9-92a3-4cd0-a1b6-a8aa20d99d2c, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:56:31 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:56:31 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:56:31 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-534609d0-kafka-2 hasn't rolled
2022-03-28 12:56:31 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-534609d0-kafka rolling update not ready, will try again in 5000 ms (1665148ms till timeout)
2022-03-28 12:56:31 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:31 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:31 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:31 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:56:31 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 1
2022-03-28 12:56:31 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Waiting for stability of rolling update will be not triggered not ready, will try again in 1000 ms (240005ms till timeout)
2022-03-28 12:56:32 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:56:32 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2361685ms till timeout)
2022-03-28 12:56:32 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (191836ms till timeout)
2022-03-28 12:56:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (116347ms till timeout)
2022-03-28 12:56:32 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:33 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:33 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970, my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c}
2022-03-28 12:56:33 [ForkJoinPool-1-worker-13] DEBUG [RollingUpdateUtils:50] At least my-cluster-bc0ed00a-kafka-1 hasn't rolled
2022-03-28 12:56:33 [ForkJoinPool-1-worker-13] INFO  [RollingUpdateUtils:143] {my-cluster-bc0ed00a-kafka-1=63ad21fd-fd44-44d0-a220-f8034085b282, my-cluster-bc0ed00a-kafka-2=08042b5b-f9cf-4394-920f-c5b95a354f7c, my-cluster-bc0ed00a-kafka-0=11b9ff5d-1682-42c9-ad1a-ae119e730970} pods didn't roll. Remaining seconds for stability: 0
2022-03-28 12:56:33 [ForkJoinPool-1-worker-13] INFO  [CruiseControlConfigurationST:283] Verifying new configuration in the Kafka CR
2022-03-28 12:56:33 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:56:33 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2360574ms till timeout)
2022-03-28 12:56:33 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (190628ms till timeout)
2022-03-28 12:56:33 [ForkJoinPool-1-worker-13] INFO  [CruiseControlConfigurationST:300] Verifying Cruise control performance options are set in Kafka CR
2022-03-28 12:56:33 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:674] ============================================================================
2022-03-28 12:56:33 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:675] [cruisecontrol.CruiseControlConfigurationST - After Each] - Clean up after test
2022-03-28 12:56:33 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:344] ############################################################################
2022-03-28 12:56:33 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:348] Delete all resources for testConfigurationPerformanceOptions
2022-03-28 12:56:33 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of Kafka my-cluster-bc0ed00a in namespace namespace-7
2022-03-28 12:56:33 [ForkJoinPool-1-worker-13] INFO  [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-7, for cruise control Kafka cluster my-cluster-bc0ed00a
2022-03-28 12:56:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (115128ms till timeout)
2022-03-28 12:56:34 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-489
2022-03-28 12:56:34 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:56:34 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-497
2022-03-28 12:56:34 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:56:34 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2359447ms till timeout)
2022-03-28 12:56:34 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-bc0ed00a
2022-03-28 12:56:34 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (189481ms till timeout)
2022-03-28 12:56:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (113950ms till timeout)
2022-03-28 12:56:34 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTopicModificationOfReplicationFactor is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:56:34 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-497
2022-03-28 12:56:34 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:56:34 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-5
2022-03-28 12:56:34 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-bc0ed00a not ready, will try again in 10000 ms (839752ms till timeout)
2022-03-28 12:56:35 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-5
2022-03-28 12:56:35 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:56:35 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-6
2022-03-28 12:56:35 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:56:35 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2358332ms till timeout)
2022-03-28 12:56:35 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (188349ms till timeout)
2022-03-28 12:56:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (112828ms till timeout)
2022-03-28 12:56:35 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-6
2022-03-28 12:56:35 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:56:35 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-62
2022-03-28 12:56:36 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-62
2022-03-28 12:56:36 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:56:36 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-63
2022-03-28 12:56:36 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-534609d0-kafka-0=e07a77c9-92a3-4cd0-a1b6-a8aa20d99d2c, my-cluster-534609d0-kafka-1=b248c525-658e-41fa-935a-67237dc06282, my-cluster-534609d0-kafka-2=e4acff6c-8882-400b-9f28-20fe36466fa0}
2022-03-28 12:56:36 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:56:36 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2357119ms till timeout)
2022-03-28 12:56:36 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=2fe26899-cd27-4864-948c-40c8f3bbb961}
2022-03-28 12:56:36 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=2fe26899-cd27-4864-948c-40c8f3bbb961}
2022-03-28 12:56:36 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:55] All pods seem to have rolled
2022-03-28 12:56:36 [ForkJoinPool-1-worker-1] INFO  [RollingUpdateUtils:86] Component with name: my-cluster-534609d0-kafka has been successfully rolled
2022-03-28 12:56:36 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:87] Component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={}) successfully rolled
2022-03-28 12:56:36 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (187163ms till timeout)
2022-03-28 12:56:36 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-63
2022-03-28 12:56:36 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:56:36 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-68
2022-03-28 12:56:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (111674ms till timeout)
2022-03-28 12:56:36 [ForkJoinPool-1-worker-1] INFO  [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-534609d0-kafka to be ready
2022-03-28 12:56:36 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})to be ready
2022-03-28 12:56:37 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-0 not ready: kafka)
2022-03-28 12:56:37 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-1 not ready: kafka)
2022-03-28 12:56:37 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-534609d0-kafka-2)
2022-03-28 12:56:37 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1799889ms till timeout)
2022-03-28 12:56:37 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-68
2022-03-28 12:56:37 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:56:37 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-69
2022-03-28 12:56:37 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:56:37 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2355905ms till timeout)
2022-03-28 12:56:37 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (186040ms till timeout)
2022-03-28 12:56:37 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-69
2022-03-28 12:56:37 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:56:37 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-7
2022-03-28 12:56:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (110560ms till timeout)
2022-03-28 12:56:38 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-0 not ready: kafka)
2022-03-28 12:56:38 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-1 not ready: kafka)
2022-03-28 12:56:38 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-534609d0-kafka-2)
2022-03-28 12:56:38 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1798779ms till timeout)
2022-03-28 12:56:38 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-7
2022-03-28 12:56:38 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:56:38 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-70
2022-03-28 12:56:39 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-70
2022-03-28 12:56:39 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:56:39 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-8
2022-03-28 12:56:39 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:56:39 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2354687ms till timeout)
2022-03-28 12:56:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (184833ms till timeout)
2022-03-28 12:56:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (109348ms till timeout)
2022-03-28 12:56:39 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-0 not ready: kafka)
2022-03-28 12:56:39 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-1 not ready: kafka)
2022-03-28 12:56:39 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-534609d0-kafka-2)
2022-03-28 12:56:39 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1797666ms till timeout)
2022-03-28 12:56:39 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-8
2022-03-28 12:56:39 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:56:39 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-80
2022-03-28 12:56:39 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testTopicModificationOfReplicationFactor is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:56:40 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-80
2022-03-28 12:56:40 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:56:40 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-83
2022-03-28 12:56:40 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:56:40 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2353473ms till timeout)
2022-03-28 12:56:40 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (183618ms till timeout)
2022-03-28 12:56:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (108139ms till timeout)
2022-03-28 12:56:40 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-0 not ready: kafka)
2022-03-28 12:56:40 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-1 not ready: kafka)
2022-03-28 12:56:40 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-534609d0-kafka-2)
2022-03-28 12:56:40 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1796556ms till timeout)
2022-03-28 12:56:40 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-83
2022-03-28 12:56:40 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:56:40 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-9
2022-03-28 12:56:41 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-9
2022-03-28 12:56:41 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:56:41 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-91
2022-03-28 12:56:41 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:56:41 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2352257ms till timeout)
2022-03-28 12:56:41 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (182308ms till timeout)
2022-03-28 12:56:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (106924ms till timeout)
2022-03-28 12:56:41 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-0 not ready: kafka)
2022-03-28 12:56:41 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-1 not ready: kafka)
2022-03-28 12:56:41 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-534609d0-kafka-2)
2022-03-28 12:56:41 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1795261ms till timeout)
2022-03-28 12:56:41 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-91
2022-03-28 12:56:41 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:56:41 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-96
2022-03-28 12:56:42 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-96
2022-03-28 12:56:42 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 12:56:42 [ForkJoinPool-1-worker-11] DEBUG [AbstractST:674] ============================================================================
2022-03-28 12:56:42 [ForkJoinPool-1-worker-11] DEBUG [AbstractST:675] [operators.topic.ThrottlingQuotaST - After Each] - Clean up after test
2022-03-28 12:56:42 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:344] ############################################################################
2022-03-28 12:56:42 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:348] Delete all resources for testThrottlingQuotasCreateTopic
2022-03-28 12:56:42 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:241] Delete of Job create-admin-my-cluster-0690c7b2-kafka-clients in namespace throttling-quota-st
2022-03-28 12:56:42 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:create-admin-my-cluster-0690c7b2-kafka-clients
2022-03-28 12:56:42 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:241] Delete of KafkaUser my-user-896662438-260898887 in namespace throttling-quota-st
2022-03-28 12:56:42 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:56:42 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2351008ms till timeout)
2022-03-28 12:56:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (105771ms till timeout)
2022-03-28 12:56:42 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-896662438-260898887
2022-03-28 12:56:42 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (181124ms till timeout)
2022-03-28 12:56:42 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-0 not ready: kafka)
2022-03-28 12:56:42 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-1 not ready: kafka)
2022-03-28 12:56:42 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-534609d0-kafka-2)
2022-03-28 12:56:42 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1794108ms till timeout)
2022-03-28 12:56:42 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:369] ############################################################################
2022-03-28 12:56:42 [ForkJoinPool-1-worker-11] DEBUG [SuiteThreadController:267] testThrottlingQuotasCreateTopic - Notifies waiting test cases:[testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testTlsExternalUserWithQuotas, testUserTemplate, testUpdateUser, testTlsExternalUser, testReceiveSimpleMessageTlsScramSha, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testSendSimpleMessageTlsScramSha, testUserWithNameMoreThan64Chars, testTlsUserWithQuotas, testConfigurationFileIsCreated, testScramUserWithQuotas, testDeployAndUnDeployCruiseControl, testConfigurationPerformanceOptions, testTopicModificationOfReplicationFactor] to and randomly select one to start execution
2022-03-28 12:56:42 [ForkJoinPool-1-worker-11] DEBUG [SuiteThreadController:93] [operators.topic.ThrottlingQuotaST] - Removing parallel test: testThrottlingQuotasCreateTopic
2022-03-28 12:56:42 [ForkJoinPool-1-worker-11] DEBUG [SuiteThreadController:97] [operators.topic.ThrottlingQuotaST] - Parallel test count: 6
2022-03-28 12:56:42 [ForkJoinPool-1-worker-11] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testThrottlingQuotasCreateTopic-FINISHED
2022-03-28 12:56:42 [ForkJoinPool-1-worker-11] INFO  [TestSeparator:30] ############################################################################
2022-03-28 12:56:42 [ForkJoinPool-1-worker-11] INFO  [TestSeparator:23] ############################################################################
2022-03-28 12:56:42 [ForkJoinPool-1-worker-11] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testThrottlingQuotasDeleteTopic-STARTED
2022-03-28 12:56:42 [ForkJoinPool-1-worker-11] DEBUG [AbstractST:658] ============================================================================
2022-03-28 12:56:42 [ForkJoinPool-1-worker-11] DEBUG [AbstractST:659] [operators.topic.ThrottlingQuotaST - Before Each] - Setup test case environment
2022-03-28 12:56:42 [ForkJoinPool-1-worker-11] DEBUG [SuiteThreadController:77] [operators.topic.ThrottlingQuotaST] - Adding parallel test: testThrottlingQuotasDeleteTopic
2022-03-28 12:56:42 [ForkJoinPool-1-worker-11] DEBUG [SuiteThreadController:81] [operators.topic.ThrottlingQuotaST] - Parallel test count: 7
2022-03-28 12:56:42 [ForkJoinPool-1-worker-11] DEBUG [SuiteThreadController:205] [testThrottlingQuotasDeleteTopic] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:56:42 [ForkJoinPool-1-worker-11] TRACE [SuiteThreadController:210] testThrottlingQuotasDeleteTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:56:43 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:56:43 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2349893ms till timeout)
2022-03-28 12:56:43 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-0 not ready: kafka)
2022-03-28 12:56:43 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-1 not ready: kafka)
2022-03-28 12:56:43 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-534609d0-kafka-2)
2022-03-28 12:56:43 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1792996ms till timeout)
2022-03-28 12:56:43 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (179927ms till timeout)
2022-03-28 12:56:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (104453ms till timeout)
2022-03-28 12:56:44 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:230] testTopicModificationOfReplicationFactor test now can proceed its execution
2022-03-28 12:56:44 [ForkJoinPool-1-worker-3] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 12:56:44 [ForkJoinPool-1-worker-3] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testScramUserWithQuotas=my-cluster-90c82d82, testTopicModificationOfReplicationFactor=my-cluster-421b59dd, testKafkaAdminTopicOperations=my-cluster-6acb723c, testReceiveSimpleMessageTlsScramSha=my-cluster-455915b9, testConfigurationFileIsCreated=my-cluster-020ceaf8, testThrottlingQuotasCreateAlterPartitions=my-cluster-d4dad0de, testSendSimpleMessageTls=my-cluster-cfce2f5b, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5, testUpdateUser=my-cluster-e16ccf16, testConfigurationReflection=my-cluster-aa0ecd2f, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d, testTlsExternalUserWithQuotas=my-cluster-3e2e11de, testUserWithNameMoreThan64Chars=my-cluster-ed39a269, testTlsUserWithQuotas=my-cluster-3b191485, testDeployAndUnDeployCruiseControl=my-cluster-534609d0, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2, testCapacityFile=my-cluster-39ee3f62, testConfigurationPerformanceOptions=my-cluster-bc0ed00a, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-0b062573, testReceiveSimpleMessageTls=my-cluster-362cd511, testTlsExternalUser=my-cluster-965e9d1e, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-31feb41b, testSendSimpleMessageTlsScramSha=my-cluster-27773859, testUserTemplate=my-cluster-157df94e}
2022-03-28 12:56:44 [ForkJoinPool-1-worker-3] TRACE [AbstractST:607] USERS_NAME_MAP: {testScramUserWithQuotas=my-user-1857980152-2145707698, testTopicModificationOfReplicationFactor=my-user-380015619-613911473, testKafkaAdminTopicOperations=my-user-2014252818-589270412, testReceiveSimpleMessageTlsScramSha=my-user-1329598231-1183867861, testConfigurationFileIsCreated=my-user-1609977058-1196012647, testThrottlingQuotasCreateAlterPartitions=my-user-1809876063-64793399, testSendSimpleMessageTls=my-user-111586425-1868899021, testCreatingUsersWithSecretPrefix=my-user-1928899390-1444689439, testUpdateUser=my-user-1694542041-806176635, testConfigurationReflection=my-user-1580949953-976509997, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-467839358-1504804463, testTlsExternalUserWithQuotas=my-user-1786174319-1274634874, testUserWithNameMoreThan64Chars=my-user-1048816470-1103470031, testTlsUserWithQuotas=my-user-420001054-1296284129, testDeployAndUnDeployCruiseControl=my-user-475346616-1293712526, testThrottlingQuotasCreateTopic=my-user-896662438-260898887, testCapacityFile=my-user-597365340-661017603, testConfigurationPerformanceOptions=my-user-722474509-712960391, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-1153908156-1369197188, testReceiveSimpleMessageTls=my-user-2012197250-2103354173, testTlsExternalUser=my-user-1295334081-1602801897, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2100837352-1014792902, testSendSimpleMessageTlsScramSha=my-user-507582460-1003875464, testUserTemplate=my-user-1253881534-190261188}
2022-03-28 12:56:44 [ForkJoinPool-1-worker-3] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testScramUserWithQuotas=my-topic-315112689-1043284277, testTopicModificationOfReplicationFactor=my-topic-1016572292-592627742, testKafkaAdminTopicOperations=my-topic-1915664839-460245273, testReceiveSimpleMessageTlsScramSha=my-topic-170116874-1830928683, testConfigurationFileIsCreated=my-topic-155202447-1921897899, testThrottlingQuotasCreateAlterPartitions=my-topic-1581297458-635252970, testSendSimpleMessageTls=my-topic-100192180-936335618, testCreatingUsersWithSecretPrefix=my-topic-2033928996-1209892355, testUpdateUser=my-topic-1239518257-2056139809, testConfigurationReflection=my-topic-1437230713-1827996725, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-712754859-1910580676, testTlsExternalUserWithQuotas=my-topic-593679216-1309035072, testUserWithNameMoreThan64Chars=my-topic-1603054043-1686316417, testTlsUserWithQuotas=my-topic-1752299353-1315959033, testDeployAndUnDeployCruiseControl=my-topic-1815491150-540070233, testThrottlingQuotasCreateTopic=my-topic-1532506723-1472611013, testCapacityFile=my-topic-1390944843-775067696, testConfigurationPerformanceOptions=my-topic-1891296149-770116276, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-613737897-291522718, testReceiveSimpleMessageTls=my-topic-1264858218-642520562, testTlsExternalUser=my-topic-476529121-306489993, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1084162749-1576061083, testSendSimpleMessageTlsScramSha=my-topic-62182003-1963040205, testUserTemplate=my-topic-515111334-943904347}
2022-03-28 12:56:44 [ForkJoinPool-1-worker-3] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testScramUserWithQuotas=my-cluster-90c82d82-kafka-clients, testTopicModificationOfReplicationFactor=my-cluster-421b59dd-kafka-clients, testKafkaAdminTopicOperations=my-cluster-6acb723c-kafka-clients, testReceiveSimpleMessageTlsScramSha=my-cluster-455915b9-kafka-clients, testConfigurationFileIsCreated=my-cluster-020ceaf8-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-d4dad0de-kafka-clients, testSendSimpleMessageTls=my-cluster-cfce2f5b-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5-kafka-clients, testUpdateUser=my-cluster-e16ccf16-kafka-clients, testConfigurationReflection=my-cluster-aa0ecd2f-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-3e2e11de-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-ed39a269-kafka-clients, testTlsUserWithQuotas=my-cluster-3b191485-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-534609d0-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2-kafka-clients, testCapacityFile=my-cluster-39ee3f62-kafka-clients, testConfigurationPerformanceOptions=my-cluster-bc0ed00a-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-0b062573-kafka-clients, testReceiveSimpleMessageTls=my-cluster-362cd511-kafka-clients, testTlsExternalUser=my-cluster-965e9d1e-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-31feb41b-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-27773859-kafka-clients, testUserTemplate=my-cluster-157df94e-kafka-clients}
2022-03-28 12:56:44 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:155] Create/Update KafkaTopic my-topic-1016572292-592627742 in namespace topic-st
2022-03-28 12:56:44 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1016572292-592627742
2022-03-28 12:56:44 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:369] ############################################################################
2022-03-28 12:56:44 [ForkJoinPool-1-worker-13] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-7 for test case:testConfigurationPerformanceOptions
2022-03-28 12:56:45 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-1016572292-592627742 will have desired state: Ready
2022-03-28 12:56:45 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-1016572292-592627742 will have desired state: Ready
2022-03-28 12:56:45 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:56:45 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2348737ms till timeout)
2022-03-28 12:56:45 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-0 not ready: kafka)
2022-03-28 12:56:45 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-1 not ready: kafka)
2022-03-28 12:56:45 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-534609d0-kafka-2)
2022-03-28 12:56:45 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1791875ms till timeout)
2022-03-28 12:56:45 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Namespace namespace-7 removal
2022-03-28 12:56:45 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-7 -o yaml
2022-03-28 12:56:45 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:444] KafkaTopic: my-topic-1016572292-592627742 is in desired state: Ready
2022-03-28 12:56:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (103296ms till timeout)
2022-03-28 12:56:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (178662ms till timeout)
2022-03-28 12:56:45 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-1016572292-592627742 will have desired state: NotReady
2022-03-28 12:56:45 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-1016572292-592627742 will have desired state: NotReady
2022-03-28 12:56:45 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] KafkaTopic: my-topic-1016572292-592627742 will have desired state: NotReady not ready, will try again in 1000 ms (179891ms till timeout)
2022-03-28 12:56:45 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-7 -o yaml
2022-03-28 12:56:45 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 12:56:45 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (479490ms till timeout)
2022-03-28 12:56:46 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:56:46 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2347626ms till timeout)
2022-03-28 12:56:46 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-0 not ready: kafka)
2022-03-28 12:56:46 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-1 not ready: kafka)
2022-03-28 12:56:46 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-534609d0-kafka-2)
2022-03-28 12:56:46 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1790728ms till timeout)
2022-03-28 12:56:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (102178ms till timeout)
2022-03-28 12:56:46 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (177453ms till timeout)
2022-03-28 12:56:46 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-7 -o yaml
2022-03-28 12:56:46 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:444] KafkaTopic: my-topic-1016572292-592627742 is in desired state: NotReady
2022-03-28 12:56:46 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace topic-st delete kafkatopic my-topic-1016572292-592627742
2022-03-28 12:56:47 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-7 -o yaml
2022-03-28 12:56:47 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 12:56:47 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (478023ms till timeout)
2022-03-28 12:56:47 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:56:47 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2346514ms till timeout)
2022-03-28 12:56:47 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-0 not ready: kafka)
2022-03-28 12:56:47 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-1 not ready: kafka)
2022-03-28 12:56:47 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-534609d0-kafka-2)
2022-03-28 12:56:47 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1789614ms till timeout)
2022-03-28 12:56:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (101061ms till timeout)
2022-03-28 12:56:47 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace topic-st delete kafkatopic my-topic-1016572292-592627742
2022-03-28 12:56:47 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 12:56:47 [ForkJoinPool-1-worker-3] INFO  [KafkaTopicUtils:104] Waiting for KafkaTopic my-topic-1016572292-592627742 deletion
2022-03-28 12:56:47 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for KafkaTopic deletion my-topic-1016572292-592627742
2022-03-28 12:56:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (176337ms till timeout)
2022-03-28 12:56:47 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:674] ============================================================================
2022-03-28 12:56:47 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:675] [operators.topic.TopicST - After Each] - Clean up after test
2022-03-28 12:56:47 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:344] ############################################################################
2022-03-28 12:56:47 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:348] Delete all resources for testTopicModificationOfReplicationFactor
2022-03-28 12:56:47 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:241] Delete of KafkaTopic my-topic-1016572292-592627742 in namespace topic-st
2022-03-28 12:56:47 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1016572292-592627742
2022-03-28 12:56:47 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:369] ############################################################################
2022-03-28 12:56:47 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:267] testTopicModificationOfReplicationFactor - Notifies waiting test cases:[testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testTlsExternalUserWithQuotas, testUserTemplate, testUpdateUser, testTlsExternalUser, testReceiveSimpleMessageTlsScramSha, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testSendSimpleMessageTlsScramSha, testUserWithNameMoreThan64Chars, testTlsUserWithQuotas, testConfigurationFileIsCreated, testScramUserWithQuotas, testDeployAndUnDeployCruiseControl, testConfigurationPerformanceOptions, testThrottlingQuotasDeleteTopic] to and randomly select one to start execution
2022-03-28 12:56:47 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:93] [operators.topic.TopicST] - Removing parallel test: testTopicModificationOfReplicationFactor
2022-03-28 12:56:47 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:97] [operators.topic.TopicST] - Parallel test count: 6
2022-03-28 12:56:47 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.topic.TopicST.testTopicModificationOfReplicationFactor-FINISHED
2022-03-28 12:56:47 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:30] ############################################################################
2022-03-28 12:56:47 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:23] ############################################################################
2022-03-28 12:56:47 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.topic.TopicST.testSendingMessagesToNonExistingTopic-STARTED
2022-03-28 12:56:47 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:658] ============================================================================
2022-03-28 12:56:47 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:659] [operators.topic.TopicST - Before Each] - Setup test case environment
2022-03-28 12:56:47 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:77] [operators.topic.TopicST] - Adding parallel test: testSendingMessagesToNonExistingTopic
2022-03-28 12:56:47 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:81] [operators.topic.TopicST] - Parallel test count: 7
2022-03-28 12:56:47 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:205] [testSendingMessagesToNonExistingTopic] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:56:47 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testSendingMessagesToNonExistingTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:56:47 [ForkJoinPool-1-worker-11] DEBUG [SuiteThreadController:230] testThrottlingQuotasDeleteTopic test now can proceed its execution
2022-03-28 12:56:47 [ForkJoinPool-1-worker-11] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 12:56:47 [ForkJoinPool-1-worker-11] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-455915b9, testThrottlingQuotasCreateAlterPartitions=my-cluster-d4dad0de, testSendSimpleMessageTls=my-cluster-cfce2f5b, testConfigurationReflection=my-cluster-aa0ecd2f, testUserWithNameMoreThan64Chars=my-cluster-ed39a269, testDeployAndUnDeployCruiseControl=my-cluster-534609d0, testCapacityFile=my-cluster-39ee3f62, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-0b062573, testTlsExternalUser=my-cluster-965e9d1e, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-31feb41b, testSendSimpleMessageTlsScramSha=my-cluster-27773859, testScramUserWithQuotas=my-cluster-90c82d82, testTopicModificationOfReplicationFactor=my-cluster-421b59dd, testKafkaAdminTopicOperations=my-cluster-6acb723c, testConfigurationFileIsCreated=my-cluster-020ceaf8, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5, testUpdateUser=my-cluster-e16ccf16, testThrottlingQuotasDeleteTopic=my-cluster-b6bd312e, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d, testTlsExternalUserWithQuotas=my-cluster-3e2e11de, testTlsUserWithQuotas=my-cluster-3b191485, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2, testConfigurationPerformanceOptions=my-cluster-bc0ed00a, testReceiveSimpleMessageTls=my-cluster-362cd511, testUserTemplate=my-cluster-157df94e}
2022-03-28 12:56:47 [ForkJoinPool-1-worker-11] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-1329598231-1183867861, testThrottlingQuotasCreateAlterPartitions=my-user-1809876063-64793399, testSendSimpleMessageTls=my-user-111586425-1868899021, testConfigurationReflection=my-user-1580949953-976509997, testUserWithNameMoreThan64Chars=my-user-1048816470-1103470031, testDeployAndUnDeployCruiseControl=my-user-475346616-1293712526, testCapacityFile=my-user-597365340-661017603, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-1153908156-1369197188, testTlsExternalUser=my-user-1295334081-1602801897, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2100837352-1014792902, testSendSimpleMessageTlsScramSha=my-user-507582460-1003875464, testScramUserWithQuotas=my-user-1857980152-2145707698, testTopicModificationOfReplicationFactor=my-user-380015619-613911473, testKafkaAdminTopicOperations=my-user-2014252818-589270412, testConfigurationFileIsCreated=my-user-1609977058-1196012647, testCreatingUsersWithSecretPrefix=my-user-1928899390-1444689439, testUpdateUser=my-user-1694542041-806176635, testThrottlingQuotasDeleteTopic=my-user-284582998-729631500, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-467839358-1504804463, testTlsExternalUserWithQuotas=my-user-1786174319-1274634874, testTlsUserWithQuotas=my-user-420001054-1296284129, testThrottlingQuotasCreateTopic=my-user-896662438-260898887, testConfigurationPerformanceOptions=my-user-722474509-712960391, testReceiveSimpleMessageTls=my-user-2012197250-2103354173, testUserTemplate=my-user-1253881534-190261188}
2022-03-28 12:56:47 [ForkJoinPool-1-worker-11] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-170116874-1830928683, testThrottlingQuotasCreateAlterPartitions=my-topic-1581297458-635252970, testSendSimpleMessageTls=my-topic-100192180-936335618, testConfigurationReflection=my-topic-1437230713-1827996725, testUserWithNameMoreThan64Chars=my-topic-1603054043-1686316417, testDeployAndUnDeployCruiseControl=my-topic-1815491150-540070233, testCapacityFile=my-topic-1390944843-775067696, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-613737897-291522718, testTlsExternalUser=my-topic-476529121-306489993, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1084162749-1576061083, testSendSimpleMessageTlsScramSha=my-topic-62182003-1963040205, testScramUserWithQuotas=my-topic-315112689-1043284277, testTopicModificationOfReplicationFactor=my-topic-1016572292-592627742, testKafkaAdminTopicOperations=my-topic-1915664839-460245273, testConfigurationFileIsCreated=my-topic-155202447-1921897899, testCreatingUsersWithSecretPrefix=my-topic-2033928996-1209892355, testUpdateUser=my-topic-1239518257-2056139809, testThrottlingQuotasDeleteTopic=my-topic-1036812834-1980126190, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-712754859-1910580676, testTlsExternalUserWithQuotas=my-topic-593679216-1309035072, testTlsUserWithQuotas=my-topic-1752299353-1315959033, testThrottlingQuotasCreateTopic=my-topic-1532506723-1472611013, testConfigurationPerformanceOptions=my-topic-1891296149-770116276, testReceiveSimpleMessageTls=my-topic-1264858218-642520562, testUserTemplate=my-topic-515111334-943904347}
2022-03-28 12:56:47 [ForkJoinPool-1-worker-11] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-455915b9-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-d4dad0de-kafka-clients, testSendSimpleMessageTls=my-cluster-cfce2f5b-kafka-clients, testConfigurationReflection=my-cluster-aa0ecd2f-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-ed39a269-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-534609d0-kafka-clients, testCapacityFile=my-cluster-39ee3f62-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-0b062573-kafka-clients, testTlsExternalUser=my-cluster-965e9d1e-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-31feb41b-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-27773859-kafka-clients, testScramUserWithQuotas=my-cluster-90c82d82-kafka-clients, testTopicModificationOfReplicationFactor=my-cluster-421b59dd-kafka-clients, testKafkaAdminTopicOperations=my-cluster-6acb723c-kafka-clients, testConfigurationFileIsCreated=my-cluster-020ceaf8-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5-kafka-clients, testUpdateUser=my-cluster-e16ccf16-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-b6bd312e-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-3e2e11de-kafka-clients, testTlsUserWithQuotas=my-cluster-3b191485-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2-kafka-clients, testConfigurationPerformanceOptions=my-cluster-bc0ed00a-kafka-clients, testReceiveSimpleMessageTls=my-cluster-362cd511-kafka-clients, testUserTemplate=my-cluster-157df94e-kafka-clients}
2022-03-28 12:56:47 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update KafkaUser my-user-284582998-729631500 in namespace throttling-quota-st
2022-03-28 12:56:48 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-7 -o yaml
2022-03-28 12:56:48 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaUser:my-user-284582998-729631500
2022-03-28 12:56:48 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:433] Wait for KafkaUser: my-user-284582998-729631500 will have desired state: Ready
2022-03-28 12:56:48 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for KafkaUser: my-user-284582998-729631500 will have desired state: Ready
2022-03-28 12:56:48 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:444] KafkaUser: my-user-284582998-729631500 is in desired state: Ready
2022-03-28 12:56:48 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:56:48 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2345338ms till timeout)
2022-03-28 12:56:48 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-7 -o yaml
2022-03-28 12:56:48 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 12:56:48 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (476566ms till timeout)
2022-03-28 12:56:48 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-0 not ready: kafka)
2022-03-28 12:56:48 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-1 not ready: kafka)
2022-03-28 12:56:48 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-534609d0-kafka-2)
2022-03-28 12:56:48 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1788438ms till timeout)
2022-03-28 12:56:48 [ForkJoinPool-1-worker-11] INFO  [ThrottlingQuotaST:112] Executing 1/5 iteration.
2022-03-28 12:56:48 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st
2022-03-28 12:56:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (99889ms till timeout)
2022-03-28 12:56:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (175160ms till timeout)
2022-03-28 12:56:48 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:create-admin-my-cluster-b6bd312e-kafka-clients
2022-03-28 12:56:48 [ForkJoinPool-1-worker-11] INFO  [JobUtils:81] Waiting for job: create-admin-my-cluster-b6bd312e-kafka-clients will be in active state
2022-03-28 12:56:48 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 12:56:48 [ForkJoinPool-1-worker-11] INFO  [ClientUtils:76] Waiting for producer/consumer:create-admin-my-cluster-b6bd312e-kafka-clients to finished
2022-03-28 12:56:48 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 12:56:49 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:56:49 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (219783ms till timeout)
2022-03-28 12:56:49 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-7 -o yaml
2022-03-28 12:56:49 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:56:49 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2344225ms till timeout)
2022-03-28 12:56:49 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-0 not ready: kafka)
2022-03-28 12:56:49 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-1 not ready: kafka)
2022-03-28 12:56:49 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-534609d0-kafka-2)
2022-03-28 12:56:49 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1787326ms till timeout)
2022-03-28 12:56:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (98774ms till timeout)
2022-03-28 12:56:49 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (174047ms till timeout)
2022-03-28 12:56:49 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-7 -o yaml
2022-03-28 12:56:49 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 12:56:49 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (475097ms till timeout)
2022-03-28 12:56:50 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:56:50 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (218562ms till timeout)
2022-03-28 12:56:50 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:56:50 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2343114ms till timeout)
2022-03-28 12:56:50 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-0 not ready: kafka)
2022-03-28 12:56:50 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-1 not ready: kafka)
2022-03-28 12:56:50 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-534609d0-kafka-2)
2022-03-28 12:56:50 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1786213ms till timeout)
2022-03-28 12:56:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (97663ms till timeout)
2022-03-28 12:56:50 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (172933ms till timeout)
2022-03-28 12:56:50 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-7 -o yaml
2022-03-28 12:56:51 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:56:51 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (217344ms till timeout)
2022-03-28 12:56:51 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:56:51 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2342003ms till timeout)
2022-03-28 12:56:51 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-0 not ready: kafka)
2022-03-28 12:56:51 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-1 not ready: kafka)
2022-03-28 12:56:51 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-534609d0-kafka-2)
2022-03-28 12:56:51 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1785099ms till timeout)
2022-03-28 12:56:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (96549ms till timeout)
2022-03-28 12:56:52 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (171819ms till timeout)
2022-03-28 12:56:52 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-7 -o yaml
2022-03-28 12:56:52 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 12:56:52 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (472974ms till timeout)
2022-03-28 12:56:52 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:56:52 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (216124ms till timeout)
2022-03-28 12:56:52 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testSendingMessagesToNonExistingTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:56:53 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:56:53 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2340701ms till timeout)
2022-03-28 12:56:53 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-7 -o yaml
2022-03-28 12:56:53 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-0 not ready: kafka)
2022-03-28 12:56:53 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-1 not ready: kafka)
2022-03-28 12:56:53 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-2 not ready: kafka)
2022-03-28 12:56:53 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:106] Pods my-cluster-534609d0-kafka-0, my-cluster-534609d0-kafka-1, my-cluster-534609d0-kafka-2 are ready
2022-03-28 12:56:53 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1783808ms till timeout)
2022-03-28 12:56:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (95360ms till timeout)
2022-03-28 12:56:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (170640ms till timeout)
2022-03-28 12:56:53 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-7 -o yaml
2022-03-28 12:56:53 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 12:56:53 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (471491ms till timeout)
2022-03-28 12:56:53 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:56:54 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (214906ms till timeout)
2022-03-28 12:56:54 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:56:54 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2339485ms till timeout)
2022-03-28 12:56:54 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-0 not ready: kafka)
2022-03-28 12:56:54 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-1 not ready: kafka)
2022-03-28 12:56:54 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-2 not ready: kafka)
2022-03-28 12:56:54 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:106] Pods my-cluster-534609d0-kafka-0, my-cluster-534609d0-kafka-1, my-cluster-534609d0-kafka-2 are ready
2022-03-28 12:56:54 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1782591ms till timeout)
2022-03-28 12:56:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (169527ms till timeout)
2022-03-28 12:56:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (94144ms till timeout)
2022-03-28 12:56:54 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-7 -o yaml
2022-03-28 12:56:55 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-7 -o yaml
2022-03-28 12:56:55 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 12:56:55 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (469955ms till timeout)
2022-03-28 12:56:55 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:56:55 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (213688ms till timeout)
2022-03-28 12:56:55 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:56:55 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2338266ms till timeout)
2022-03-28 12:56:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (92932ms till timeout)
2022-03-28 12:56:55 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-0 not ready: kafka)
2022-03-28 12:56:55 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-1 not ready: kafka)
2022-03-28 12:56:55 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-2 not ready: kafka)
2022-03-28 12:56:55 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:106] Pods my-cluster-534609d0-kafka-0, my-cluster-534609d0-kafka-1, my-cluster-534609d0-kafka-2 are ready
2022-03-28 12:56:55 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1781373ms till timeout)
2022-03-28 12:56:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (168312ms till timeout)
2022-03-28 12:56:56 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-7 -o yaml
2022-03-28 12:56:56 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:56:56 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (212468ms till timeout)
2022-03-28 12:56:56 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:56:56 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2337046ms till timeout)
2022-03-28 12:56:56 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-0 not ready: kafka)
2022-03-28 12:56:56 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-1 not ready: kafka)
2022-03-28 12:56:56 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-2 not ready: kafka)
2022-03-28 12:56:56 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:106] Pods my-cluster-534609d0-kafka-0, my-cluster-534609d0-kafka-1, my-cluster-534609d0-kafka-2 are ready
2022-03-28 12:56:56 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1780250ms till timeout)
2022-03-28 12:56:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (167191ms till timeout)
2022-03-28 12:56:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (91708ms till timeout)
2022-03-28 12:56:57 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:56:57 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (211248ms till timeout)
2022-03-28 12:56:57 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testSendingMessagesToNonExistingTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:56:57 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:56:57 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2335825ms till timeout)
2022-03-28 12:56:57 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-0 not ready: kafka)
2022-03-28 12:56:57 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-1 not ready: kafka)
2022-03-28 12:56:57 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-2 not ready: kafka)
2022-03-28 12:56:57 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:106] Pods my-cluster-534609d0-kafka-0, my-cluster-534609d0-kafka-1, my-cluster-534609d0-kafka-2 are ready
2022-03-28 12:56:57 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1779029ms till timeout)
2022-03-28 12:56:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (90486ms till timeout)
2022-03-28 12:56:58 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (165869ms till timeout)
2022-03-28 12:56:58 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:56:58 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (210029ms till timeout)
2022-03-28 12:56:59 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:56:59 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2334675ms till timeout)
2022-03-28 12:56:59 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-0 not ready: kafka)
2022-03-28 12:56:59 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-1 not ready: kafka)
2022-03-28 12:56:59 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-2 not ready: kafka)
2022-03-28 12:56:59 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:106] Pods my-cluster-534609d0-kafka-0, my-cluster-534609d0-kafka-1, my-cluster-534609d0-kafka-2 are ready
2022-03-28 12:56:59 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1777815ms till timeout)
2022-03-28 12:56:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (89372ms till timeout)
2022-03-28 12:56:59 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (164655ms till timeout)
2022-03-28 12:57:00 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:57:00 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (208810ms till timeout)
2022-03-28 12:57:00 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:57:00 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2333493ms till timeout)
2022-03-28 12:57:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (88159ms till timeout)
2022-03-28 12:57:00 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-0 not ready: kafka)
2022-03-28 12:57:00 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-1 not ready: kafka)
2022-03-28 12:57:00 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-2 not ready: kafka)
2022-03-28 12:57:00 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:106] Pods my-cluster-534609d0-kafka-0, my-cluster-534609d0-kafka-1, my-cluster-534609d0-kafka-2 are ready
2022-03-28 12:57:00 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1776599ms till timeout)
2022-03-28 12:57:00 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (163535ms till timeout)
2022-03-28 12:57:01 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:57:01 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:57:01 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2332275ms till timeout)
2022-03-28 12:57:01 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (207483ms till timeout)
2022-03-28 12:57:01 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-0 not ready: kafka)
2022-03-28 12:57:01 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-1 not ready: kafka)
2022-03-28 12:57:01 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-2 not ready: kafka)
2022-03-28 12:57:01 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:106] Pods my-cluster-534609d0-kafka-0, my-cluster-534609d0-kafka-1, my-cluster-534609d0-kafka-2 are ready
2022-03-28 12:57:01 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1775481ms till timeout)
2022-03-28 12:57:01 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (162322ms till timeout)
2022-03-28 12:57:01 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (86939ms till timeout)
2022-03-28 12:57:01 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-7 -o yaml
2022-03-28 12:57:01 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 12:57:01 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (463455ms till timeout)
2022-03-28 12:57:02 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-7 -o yaml
2022-03-28 12:57:02 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:57:02 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:57:02 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2331053ms till timeout)
2022-03-28 12:57:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (85719ms till timeout)
2022-03-28 12:57:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (161102ms till timeout)
2022-03-28 12:57:02 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-0 not ready: kafka)
2022-03-28 12:57:02 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-1 not ready: kafka)
2022-03-28 12:57:02 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-2 not ready: kafka)
2022-03-28 12:57:02 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:106] Pods my-cluster-534609d0-kafka-0, my-cluster-534609d0-kafka-1, my-cluster-534609d0-kafka-2 are ready
2022-03-28 12:57:02 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1774159ms till timeout)
2022-03-28 12:57:02 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (206154ms till timeout)
2022-03-28 12:57:02 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testSendingMessagesToNonExistingTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:57:03 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-7 -o yaml
2022-03-28 12:57:03 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 12:57:03 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (461936ms till timeout)
2022-03-28 12:57:03 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:57:03 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2329836ms till timeout)
2022-03-28 12:57:04 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:57:04 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-0 not ready: kafka)
2022-03-28 12:57:04 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-1 not ready: kafka)
2022-03-28 12:57:04 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-2 not ready: kafka)
2022-03-28 12:57:04 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:106] Pods my-cluster-534609d0-kafka-0, my-cluster-534609d0-kafka-1, my-cluster-534609d0-kafka-2 are ready
2022-03-28 12:57:04 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1772942ms till timeout)
2022-03-28 12:57:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (84499ms till timeout)
2022-03-28 12:57:04 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (159882ms till timeout)
2022-03-28 12:57:04 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (204833ms till timeout)
2022-03-28 12:57:04 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-7 -o yaml
2022-03-28 12:57:04 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-7 -o yaml
2022-03-28 12:57:04 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 12:57:04 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (460426ms till timeout)
2022-03-28 12:57:05 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:57:05 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2328621ms till timeout)
2022-03-28 12:57:05 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-0 not ready: kafka)
2022-03-28 12:57:05 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-1 not ready: kafka)
2022-03-28 12:57:05 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-2 not ready: kafka)
2022-03-28 12:57:05 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:106] Pods my-cluster-534609d0-kafka-0, my-cluster-534609d0-kafka-1, my-cluster-534609d0-kafka-2 are ready
2022-03-28 12:57:05 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (158768ms till timeout)
2022-03-28 12:57:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (83283ms till timeout)
2022-03-28 12:57:05 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:57:05 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-534609d0 will have desired state: Ready
2022-03-28 12:57:05 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-534609d0 will have desired state: Ready
2022-03-28 12:57:05 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (203510ms till timeout)
2022-03-28 12:57:05 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:444] Kafka: my-cluster-534609d0 is in desired state: Ready
2022-03-28 12:57:05 [ForkJoinPool-1-worker-1] INFO  [RollingUpdateUtils:132] Kafka: my-cluster-534609d0 is ready
2022-03-28 12:57:05 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-7 -o yaml
2022-03-28 12:57:05 [ForkJoinPool-1-worker-1] INFO  [CruiseControlConfigurationST:117] Verifying that in Cruise Control is not present in the Kafka cluster
2022-03-28 12:57:05 [ForkJoinPool-1-worker-1] INFO  [CruiseControlConfigurationST:120] Verifying that my-cluster-534609d0-cruise-control- pod is not present
2022-03-28 12:57:05 [ForkJoinPool-1-worker-1] INFO  [PodUtils:209] Wait until Pod my-cluster-534609d0-cruise-control- will have stable 0 replicas
2022-03-28 12:57:05 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for  Podmy-cluster-534609d0-cruise-control- will have 0 replicas
2022-03-28 12:57:05 [ForkJoinPool-1-worker-1] INFO  [PodUtils:225] Pod replicas gonna be stable in 19 polls
2022-03-28 12:57:05 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176]  Podmy-cluster-534609d0-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (179888ms till timeout)
2022-03-28 12:57:06 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:57:06 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2327509ms till timeout)
2022-03-28 12:57:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (82170ms till timeout)
2022-03-28 12:57:06 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (157553ms till timeout)
2022-03-28 12:57:06 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:57:06 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (202292ms till timeout)
2022-03-28 12:57:07 [ForkJoinPool-1-worker-1] INFO  [PodUtils:225] Pod replicas gonna be stable in 18 polls
2022-03-28 12:57:07 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176]  Podmy-cluster-534609d0-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (178671ms till timeout)
2022-03-28 12:57:07 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:57:07 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2326399ms till timeout)
2022-03-28 12:57:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (81058ms till timeout)
2022-03-28 12:57:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (156439ms till timeout)
2022-03-28 12:57:07 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:57:07 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (201075ms till timeout)
2022-03-28 12:57:07 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testSendingMessagesToNonExistingTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:57:08 [ForkJoinPool-1-worker-1] INFO  [PodUtils:225] Pod replicas gonna be stable in 17 polls
2022-03-28 12:57:08 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176]  Podmy-cluster-534609d0-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (177352ms till timeout)
2022-03-28 12:57:08 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:57:08 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2325287ms till timeout)
2022-03-28 12:57:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (79946ms till timeout)
2022-03-28 12:57:08 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (155220ms till timeout)
2022-03-28 12:57:08 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:57:09 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (199858ms till timeout)
2022-03-28 12:57:09 [ForkJoinPool-1-worker-1] INFO  [PodUtils:225] Pod replicas gonna be stable in 16 polls
2022-03-28 12:57:09 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176]  Podmy-cluster-534609d0-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (176138ms till timeout)
2022-03-28 12:57:09 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:57:09 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2323998ms till timeout)
2022-03-28 12:57:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (78761ms till timeout)
2022-03-28 12:57:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (154040ms till timeout)
2022-03-28 12:57:10 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:57:10 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (198641ms till timeout)
2022-03-28 12:57:10 [ForkJoinPool-1-worker-1] INFO  [PodUtils:225] Pod replicas gonna be stable in 15 polls
2022-03-28 12:57:10 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176]  Podmy-cluster-534609d0-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (174921ms till timeout)
2022-03-28 12:57:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (77551ms till timeout)
2022-03-28 12:57:10 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:57:10 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2322781ms till timeout)
2022-03-28 12:57:10 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (152928ms till timeout)
2022-03-28 12:57:11 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-7 -o yaml
2022-03-28 12:57:11 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 12:57:11 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (453939ms till timeout)
2022-03-28 12:57:11 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:57:11 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (197422ms till timeout)
2022-03-28 12:57:12 [ForkJoinPool-1-worker-1] INFO  [PodUtils:225] Pod replicas gonna be stable in 14 polls
2022-03-28 12:57:12 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176]  Podmy-cluster-534609d0-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (173705ms till timeout)
2022-03-28 12:57:12 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-7 -o yaml
2022-03-28 12:57:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (76336ms till timeout)
2022-03-28 12:57:12 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:57:12 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2321566ms till timeout)
2022-03-28 12:57:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (151713ms till timeout)
2022-03-28 12:57:12 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:57:12 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-7 -o yaml
2022-03-28 12:57:12 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 12:57:12 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (452433ms till timeout)
2022-03-28 12:57:12 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (196205ms till timeout)
2022-03-28 12:57:12 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testSendingMessagesToNonExistingTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:57:13 [ForkJoinPool-1-worker-1] INFO  [PodUtils:225] Pod replicas gonna be stable in 13 polls
2022-03-28 12:57:13 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176]  Podmy-cluster-534609d0-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (172489ms till timeout)
2022-03-28 12:57:13 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:57:13 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2320448ms till timeout)
2022-03-28 12:57:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (150503ms till timeout)
2022-03-28 12:57:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (75120ms till timeout)
2022-03-28 12:57:13 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-7 -o yaml
2022-03-28 12:57:13 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:57:13 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (194988ms till timeout)
2022-03-28 12:57:14 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-7 -o yaml
2022-03-28 12:57:14 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 12:57:14 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (450949ms till timeout)
2022-03-28 12:57:14 [ForkJoinPool-1-worker-1] INFO  [PodUtils:225] Pod replicas gonna be stable in 12 polls
2022-03-28 12:57:14 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176]  Podmy-cluster-534609d0-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (171272ms till timeout)
2022-03-28 12:57:14 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:57:14 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2319231ms till timeout)
2022-03-28 12:57:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (149285ms till timeout)
2022-03-28 12:57:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (73902ms till timeout)
2022-03-28 12:57:15 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:57:15 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-7 -o yaml
2022-03-28 12:57:15 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (193770ms till timeout)
2022-03-28 12:57:15 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-7 -o yaml
2022-03-28 12:57:15 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 12:57:15 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (449489ms till timeout)
2022-03-28 12:57:15 [ForkJoinPool-1-worker-1] INFO  [PodUtils:225] Pod replicas gonna be stable in 11 polls
2022-03-28 12:57:15 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176]  Podmy-cluster-534609d0-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (170056ms till timeout)
2022-03-28 12:57:15 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:57:15 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2318013ms till timeout)
2022-03-28 12:57:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (72687ms till timeout)
2022-03-28 12:57:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (148070ms till timeout)
2022-03-28 12:57:16 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:57:16 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (192551ms till timeout)
2022-03-28 12:57:16 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-7 -o yaml
2022-03-28 12:57:16 [ForkJoinPool-1-worker-1] INFO  [PodUtils:225] Pod replicas gonna be stable in 10 polls
2022-03-28 12:57:16 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176]  Podmy-cluster-534609d0-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (168837ms till timeout)
2022-03-28 12:57:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (71565ms till timeout)
2022-03-28 12:57:16 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:57:16 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2316792ms till timeout)
2022-03-28 12:57:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (146851ms till timeout)
2022-03-28 12:57:17 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-7 -o yaml
2022-03-28 12:57:17 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 12:57:17 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (448014ms till timeout)
2022-03-28 12:57:17 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:57:17 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (191332ms till timeout)
2022-03-28 12:57:17 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testSendingMessagesToNonExistingTopic is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:57:18 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-7 -o yaml
2022-03-28 12:57:18 [ForkJoinPool-1-worker-1] INFO  [PodUtils:225] Pod replicas gonna be stable in 9 polls
2022-03-28 12:57:18 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176]  Podmy-cluster-534609d0-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (167623ms till timeout)
2022-03-28 12:57:18 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:57:18 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2315583ms till timeout)
2022-03-28 12:57:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (70350ms till timeout)
2022-03-28 12:57:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (145637ms till timeout)
2022-03-28 12:57:18 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-7 -o yaml
2022-03-28 12:57:18 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 12:57:18 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (446570ms till timeout)
2022-03-28 12:57:18 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:57:18 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (190114ms till timeout)
2022-03-28 12:57:19 [ForkJoinPool-1-worker-1] INFO  [PodUtils:225] Pod replicas gonna be stable in 8 polls
2022-03-28 12:57:19 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176]  Podmy-cluster-534609d0-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (166407ms till timeout)
2022-03-28 12:57:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (69134ms till timeout)
2022-03-28 12:57:19 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:57:19 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2314365ms till timeout)
2022-03-28 12:57:19 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (144420ms till timeout)
2022-03-28 12:57:19 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-7 -o yaml
2022-03-28 12:57:19 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:57:19 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-7 -o yaml
2022-03-28 12:57:19 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 12:57:19 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Namespace namespace-7 removal not ready, will try again in 1000 ms (445104ms till timeout)
2022-03-28 12:57:20 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (188896ms till timeout)
2022-03-28 12:57:20 [ForkJoinPool-1-worker-1] INFO  [PodUtils:225] Pod replicas gonna be stable in 7 polls
2022-03-28 12:57:20 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176]  Podmy-cluster-534609d0-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (165184ms till timeout)
2022-03-28 12:57:20 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:57:20 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2313144ms till timeout)
2022-03-28 12:57:20 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (143198ms till timeout)
2022-03-28 12:57:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (67815ms till timeout)
2022-03-28 12:57:20 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-7 -o yaml
2022-03-28 12:57:21 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:57:21 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (187678ms till timeout)
2022-03-28 12:57:21 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-9 get Namespace namespace-7 -o yaml
2022-03-28 12:57:21 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 1
2022-03-28 12:57:21 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 12:57:21 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-7" not found
2022-03-28 12:57:21 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] ======STDERR END======
2022-03-28 12:57:21 [ForkJoinPool-1-worker-13] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[namespace-8], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[infra-namespace, reconciliation-st], io.strimzi.test.logs.CollectorElement@f4405b3b=[namespace-6], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 12:57:21 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:267] testConfigurationPerformanceOptions - Notifies waiting test cases:[testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testTlsExternalUserWithQuotas, testUserTemplate, testUpdateUser, testTlsExternalUser, testReceiveSimpleMessageTlsScramSha, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testSendSimpleMessageTlsScramSha, testUserWithNameMoreThan64Chars, testTlsUserWithQuotas, testConfigurationFileIsCreated, testScramUserWithQuotas, testDeployAndUnDeployCruiseControl, testConfigurationPerformanceOptions, testSendingMessagesToNonExistingTopic] to and randomly select one to start execution
2022-03-28 12:57:21 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:93] [cruisecontrol.CruiseControlConfigurationST] - Removing parallel test: testConfigurationPerformanceOptions
2022-03-28 12:57:21 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:97] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 6
2022-03-28 12:57:21 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testConfigurationPerformanceOptions-FINISHED
2022-03-28 12:57:21 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:30] ############################################################################
2022-03-28 12:57:21 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:23] ############################################################################
2022-03-28 12:57:21 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.topic.TopicST.testMoreReplicasThanAvailableBrokers-STARTED
2022-03-28 12:57:21 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:658] ============================================================================
2022-03-28 12:57:21 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:659] [operators.topic.TopicST - Before Each] - Setup test case environment
2022-03-28 12:57:21 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:77] [operators.topic.TopicST] - Adding parallel test: testMoreReplicasThanAvailableBrokers
2022-03-28 12:57:21 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:81] [operators.topic.TopicST] - Parallel test count: 7
2022-03-28 12:57:21 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:205] [testMoreReplicasThanAvailableBrokers] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:57:21 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testMoreReplicasThanAvailableBrokers is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:57:21 [ForkJoinPool-1-worker-1] INFO  [PodUtils:225] Pod replicas gonna be stable in 6 polls
2022-03-28 12:57:21 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176]  Podmy-cluster-534609d0-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (163969ms till timeout)
2022-03-28 12:57:21 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:87] Expected pods LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={}) are not ready
2022-03-28 12:57:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2311929ms till timeout)
2022-03-28 12:57:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (66698ms till timeout)
2022-03-28 12:57:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (141983ms till timeout)
2022-03-28 12:57:22 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:57:22 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (186455ms till timeout)
2022-03-28 12:57:22 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:230] testSendingMessagesToNonExistingTopic test now can proceed its execution
2022-03-28 12:57:22 [ForkJoinPool-1-worker-3] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 12:57:22 [ForkJoinPool-1-worker-3] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-455915b9, testThrottlingQuotasCreateAlterPartitions=my-cluster-d4dad0de, testSendSimpleMessageTls=my-cluster-cfce2f5b, testConfigurationReflection=my-cluster-aa0ecd2f, testUserWithNameMoreThan64Chars=my-cluster-ed39a269, testDeployAndUnDeployCruiseControl=my-cluster-534609d0, testCapacityFile=my-cluster-39ee3f62, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-0b062573, testTlsExternalUser=my-cluster-965e9d1e, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-31feb41b, testSendSimpleMessageTlsScramSha=my-cluster-27773859, testScramUserWithQuotas=my-cluster-90c82d82, testTopicModificationOfReplicationFactor=my-cluster-421b59dd, testKafkaAdminTopicOperations=my-cluster-6acb723c, testConfigurationFileIsCreated=my-cluster-020ceaf8, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5, testUpdateUser=my-cluster-e16ccf16, testThrottlingQuotasDeleteTopic=my-cluster-b6bd312e, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d, testTlsExternalUserWithQuotas=my-cluster-3e2e11de, testTlsUserWithQuotas=my-cluster-3b191485, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2, testSendingMessagesToNonExistingTopic=my-cluster-64488b2e, testConfigurationPerformanceOptions=my-cluster-bc0ed00a, testReceiveSimpleMessageTls=my-cluster-362cd511, testUserTemplate=my-cluster-157df94e}
2022-03-28 12:57:22 [ForkJoinPool-1-worker-3] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-1329598231-1183867861, testThrottlingQuotasCreateAlterPartitions=my-user-1809876063-64793399, testSendSimpleMessageTls=my-user-111586425-1868899021, testConfigurationReflection=my-user-1580949953-976509997, testUserWithNameMoreThan64Chars=my-user-1048816470-1103470031, testDeployAndUnDeployCruiseControl=my-user-475346616-1293712526, testCapacityFile=my-user-597365340-661017603, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-1153908156-1369197188, testTlsExternalUser=my-user-1295334081-1602801897, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2100837352-1014792902, testSendSimpleMessageTlsScramSha=my-user-507582460-1003875464, testScramUserWithQuotas=my-user-1857980152-2145707698, testTopicModificationOfReplicationFactor=my-user-380015619-613911473, testKafkaAdminTopicOperations=my-user-2014252818-589270412, testConfigurationFileIsCreated=my-user-1609977058-1196012647, testCreatingUsersWithSecretPrefix=my-user-1928899390-1444689439, testUpdateUser=my-user-1694542041-806176635, testThrottlingQuotasDeleteTopic=my-user-284582998-729631500, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-467839358-1504804463, testTlsExternalUserWithQuotas=my-user-1786174319-1274634874, testTlsUserWithQuotas=my-user-420001054-1296284129, testThrottlingQuotasCreateTopic=my-user-896662438-260898887, testSendingMessagesToNonExistingTopic=my-user-248322508-1660982370, testConfigurationPerformanceOptions=my-user-722474509-712960391, testReceiveSimpleMessageTls=my-user-2012197250-2103354173, testUserTemplate=my-user-1253881534-190261188}
2022-03-28 12:57:22 [ForkJoinPool-1-worker-3] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-170116874-1830928683, testThrottlingQuotasCreateAlterPartitions=my-topic-1581297458-635252970, testSendSimpleMessageTls=my-topic-100192180-936335618, testConfigurationReflection=my-topic-1437230713-1827996725, testUserWithNameMoreThan64Chars=my-topic-1603054043-1686316417, testDeployAndUnDeployCruiseControl=my-topic-1815491150-540070233, testCapacityFile=my-topic-1390944843-775067696, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-613737897-291522718, testTlsExternalUser=my-topic-476529121-306489993, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1084162749-1576061083, testSendSimpleMessageTlsScramSha=my-topic-62182003-1963040205, testScramUserWithQuotas=my-topic-315112689-1043284277, testTopicModificationOfReplicationFactor=my-topic-1016572292-592627742, testKafkaAdminTopicOperations=my-topic-1915664839-460245273, testConfigurationFileIsCreated=my-topic-155202447-1921897899, testCreatingUsersWithSecretPrefix=my-topic-2033928996-1209892355, testUpdateUser=my-topic-1239518257-2056139809, testThrottlingQuotasDeleteTopic=my-topic-1036812834-1980126190, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-712754859-1910580676, testTlsExternalUserWithQuotas=my-topic-593679216-1309035072, testTlsUserWithQuotas=my-topic-1752299353-1315959033, testThrottlingQuotasCreateTopic=my-topic-1532506723-1472611013, testSendingMessagesToNonExistingTopic=my-topic-1513508632-1348735863, testConfigurationPerformanceOptions=my-topic-1891296149-770116276, testReceiveSimpleMessageTls=my-topic-1264858218-642520562, testUserTemplate=my-topic-515111334-943904347}
2022-03-28 12:57:22 [ForkJoinPool-1-worker-3] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-455915b9-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-d4dad0de-kafka-clients, testSendSimpleMessageTls=my-cluster-cfce2f5b-kafka-clients, testConfigurationReflection=my-cluster-aa0ecd2f-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-ed39a269-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-534609d0-kafka-clients, testCapacityFile=my-cluster-39ee3f62-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-0b062573-kafka-clients, testTlsExternalUser=my-cluster-965e9d1e-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-31feb41b-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-27773859-kafka-clients, testScramUserWithQuotas=my-cluster-90c82d82-kafka-clients, testTopicModificationOfReplicationFactor=my-cluster-421b59dd-kafka-clients, testKafkaAdminTopicOperations=my-cluster-6acb723c-kafka-clients, testConfigurationFileIsCreated=my-cluster-020ceaf8-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5-kafka-clients, testUpdateUser=my-cluster-e16ccf16-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-b6bd312e-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-3e2e11de-kafka-clients, testTlsUserWithQuotas=my-cluster-3b191485-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2-kafka-clients, testSendingMessagesToNonExistingTopic=my-cluster-64488b2e-kafka-clients, testConfigurationPerformanceOptions=my-cluster-bc0ed00a-kafka-clients, testReceiveSimpleMessageTls=my-cluster-362cd511-kafka-clients, testUserTemplate=my-cluster-157df94e-kafka-clients}
2022-03-28 12:57:22 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:155] Create/Update Deployment topic-cluster-name-kafka-clients in namespace topic-st
2022-03-28 12:57:23 [ForkJoinPool-1-worker-1] INFO  [PodUtils:225] Pod replicas gonna be stable in 5 polls
2022-03-28 12:57:23 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176]  Podmy-cluster-534609d0-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (162753ms till timeout)
2022-03-28 12:57:23 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-0 not ready: kafka)
2022-03-28 12:57:23 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-1 not ready: kafka)
2022-03-28 12:57:23 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-2 not ready: kafka)
2022-03-28 12:57:23 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-0b062573-kafka-3)
2022-03-28 12:57:23 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2310712ms till timeout)
2022-03-28 12:57:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (140767ms till timeout)
2022-03-28 12:57:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (65384ms till timeout)
2022-03-28 12:57:23 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:topic-cluster-name-kafka-clients
2022-03-28 12:57:23 [ForkJoinPool-1-worker-3] INFO  [DeploymentUtils:161] Wait for Deployment: topic-cluster-name-kafka-clients will be ready
2022-03-28 12:57:23 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Wait for Deployment: topic-cluster-name-kafka-clients will be ready
2022-03-28 12:57:23 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: topic-cluster-name-kafka-clients will be ready not ready, will try again in 1000 ms (479892ms till timeout)
2022-03-28 12:57:23 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:57:23 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (185235ms till timeout)
2022-03-28 12:57:24 [ForkJoinPool-1-worker-1] INFO  [PodUtils:225] Pod replicas gonna be stable in 4 polls
2022-03-28 12:57:24 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176]  Podmy-cluster-534609d0-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (161640ms till timeout)
2022-03-28 12:57:24 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-0 not ready: kafka)
2022-03-28 12:57:24 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-1 not ready: kafka)
2022-03-28 12:57:24 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-2 not ready: kafka)
2022-03-28 12:57:24 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-0b062573-kafka-3)
2022-03-28 12:57:24 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2309496ms till timeout)
2022-03-28 12:57:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (139648ms till timeout)
2022-03-28 12:57:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (64262ms till timeout)
2022-03-28 12:57:24 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: topic-cluster-name-kafka-clients will be ready not ready, will try again in 1000 ms (478784ms till timeout)
2022-03-28 12:57:24 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:57:24 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (184018ms till timeout)
2022-03-28 12:57:25 [ForkJoinPool-1-worker-1] INFO  [PodUtils:225] Pod replicas gonna be stable in 3 polls
2022-03-28 12:57:25 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176]  Podmy-cluster-534609d0-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (160424ms till timeout)
2022-03-28 12:57:25 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (138536ms till timeout)
2022-03-28 12:57:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (63049ms till timeout)
2022-03-28 12:57:25 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-0 not ready: kafka)
2022-03-28 12:57:25 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-1 not ready: kafka)
2022-03-28 12:57:25 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-2 not ready: kafka)
2022-03-28 12:57:25 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-0b062573-kafka-3)
2022-03-28 12:57:25 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2308278ms till timeout)
2022-03-28 12:57:25 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Wait for Deployment: topic-cluster-name-kafka-clients will be ready not ready, will try again in 1000 ms (477675ms till timeout)
2022-03-28 12:57:26 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:57:26 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (182800ms till timeout)
2022-03-28 12:57:26 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testMoreReplicasThanAvailableBrokers is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:57:26 [ForkJoinPool-1-worker-1] INFO  [PodUtils:225] Pod replicas gonna be stable in 2 polls
2022-03-28 12:57:26 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176]  Podmy-cluster-534609d0-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (159208ms till timeout)
2022-03-28 12:57:26 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-0 not ready: kafka)
2022-03-28 12:57:26 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-1 not ready: kafka)
2022-03-28 12:57:26 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-2 not ready: kafka)
2022-03-28 12:57:26 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-0b062573-kafka-3)
2022-03-28 12:57:26 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2307167ms till timeout)
2022-03-28 12:57:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (61934ms till timeout)
2022-03-28 12:57:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (137315ms till timeout)
2022-03-28 12:57:26 [ForkJoinPool-1-worker-3] INFO  [DeploymentUtils:168] Deployment: topic-cluster-name-kafka-clients is ready
2022-03-28 12:57:26 [ForkJoinPool-1-worker-3] INFO  [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-03-28 12:57:26 [ForkJoinPool-1-worker-3] INFO  [TopicST:320] Checking if my-topic-1513508632-1348735863 is on topic list
2022-03-28 12:57:26 [ForkJoinPool-1-worker-3] INFO  [TopicST:456] Checking topic my-topic-1513508632-1348735863 in Kafka
2022-03-28 12:57:26 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 12:57:27 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:57:27 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (181576ms till timeout)
2022-03-28 12:57:27 [ForkJoinPool-1-worker-1] INFO  [PodUtils:225] Pod replicas gonna be stable in 1 polls
2022-03-28 12:57:27 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176]  Podmy-cluster-534609d0-cruise-control- will have 0 replicas not ready, will try again in 1000 ms (157993ms till timeout)
2022-03-28 12:57:27 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-0 not ready: kafka)
2022-03-28 12:57:27 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-1 not ready: kafka)
2022-03-28 12:57:27 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-2 not ready: kafka)
2022-03-28 12:57:27 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-0b062573-kafka-3)
2022-03-28 12:57:27 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2305944ms till timeout)
2022-03-28 12:57:27 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (136069ms till timeout)
2022-03-28 12:57:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (60623ms till timeout)
2022-03-28 12:57:28 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:57:28 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (180355ms till timeout)
2022-03-28 12:57:28 [ForkJoinPool-1-worker-1] INFO  [PodUtils:217] Pod replicas are stable for 20 polls intervals
2022-03-28 12:57:28 [ForkJoinPool-1-worker-1] INFO  [PodUtils:228] Pod my-cluster-534609d0-cruise-control- has 0 replicas
2022-03-28 12:57:28 [ForkJoinPool-1-worker-1] INFO  [CruiseControlConfigurationST:123] Verifying that in Kafka config map there is no configuration to cruise control metric reporter
2022-03-28 12:57:29 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-0 not ready: kafka)
2022-03-28 12:57:29 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-1 not ready: kafka)
2022-03-28 12:57:29 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-2 not ready: kafka)
2022-03-28 12:57:29 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-0b062573-kafka-3)
2022-03-28 12:57:29 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2304737ms till timeout)
2022-03-28 12:57:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (59505ms till timeout)
2022-03-28 12:57:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (134887ms till timeout)
2022-03-28 12:57:29 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties
2022-03-28 12:57:29 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (120000ms till timeout)
2022-03-28 12:57:29 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:57:29 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (179137ms till timeout)
2022-03-28 12:57:30 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (119000ms till timeout)
2022-03-28 12:57:30 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-0 not ready: kafka)
2022-03-28 12:57:30 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-1 not ready: kafka)
2022-03-28 12:57:30 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-2 not ready: kafka)
2022-03-28 12:57:30 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-0b062573-kafka-3)
2022-03-28 12:57:30 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2303625ms till timeout)
2022-03-28 12:57:30 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (133664ms till timeout)
2022-03-28 12:57:30 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (58281ms till timeout)
2022-03-28 12:57:30 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Command: oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 12:57:30 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 12:57:30 [ForkJoinPool-1-worker-3] INFO  [TopicST:323] Topic with name my-topic-1513508632-1348735863 is not created yet
2022-03-28 12:57:30 [ForkJoinPool-1-worker-3] INFO  [TopicST:325] Trying to send messages to non-existing topic my-topic-1513508632-1348735863
2022-03-28 12:57:30 [ForkJoinPool-1-worker-3] DEBUG [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@51b6f6f4, which are set.
2022-03-28 12:57:30 [ForkJoinPool-1-worker-3] INFO  [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@1dd1054b, messages=[], arguments=[--topic, my-topic-1513508632-1348735863, --max-messages, 100, --bootstrap-server, topic-cluster-name-kafka-bootstrap.topic-st.svc:9092], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='topic-cluster-name-kafka-clients-7c47b4459f-4xdmp', podNamespace='topic-st', bootstrapServer='topic-cluster-name-kafka-bootstrap.topic-st.svc:9092', topicName='my-topic-1513508632-1348735863', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@51b6f6f4}
2022-03-28 12:57:30 [ForkJoinPool-1-worker-3] INFO  [InternalKafkaClient:94] Producing 100 messages to topic-cluster-name-kafka-bootstrap.topic-st.svc:9092:my-topic-1513508632-1348735863 from pod topic-cluster-name-kafka-clients-7c47b4459f-4xdmp
2022-03-28 12:57:30 [ForkJoinPool-1-worker-3] INFO  [VerifiableClient:192] Client command: oc exec topic-cluster-name-kafka-clients-7c47b4459f-4xdmp -n topic-st -- /opt/kafka/producer.sh --topic my-topic-1513508632-1348735863 --max-messages 100 --bootstrap-server topic-cluster-name-kafka-bootstrap.topic-st.svc:9092
2022-03-28 12:57:30 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc exec topic-cluster-name-kafka-clients-7c47b4459f-4xdmp -n topic-st -- /opt/kafka/producer.sh --topic my-topic-1513508632-1348735863 --max-messages 100 --bootstrap-server topic-cluster-name-kafka-bootstrap.topic-st.svc:9092
2022-03-28 12:57:30 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:57:31 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (177918ms till timeout)
2022-03-28 12:57:31 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (117999ms till timeout)
2022-03-28 12:57:31 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-0 not ready: kafka)
2022-03-28 12:57:31 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-1 not ready: kafka)
2022-03-28 12:57:31 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-2 not ready: kafka)
2022-03-28 12:57:31 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-0b062573-kafka-3)
2022-03-28 12:57:31 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2302512ms till timeout)
2022-03-28 12:57:31 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (132553ms till timeout)
2022-03-28 12:57:31 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testMoreReplicasThanAvailableBrokers is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:57:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (57036ms till timeout)
2022-03-28 12:57:32 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (116999ms till timeout)
2022-03-28 12:57:32 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:57:32 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (176697ms till timeout)
2022-03-28 12:57:32 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-0 not ready: kafka)
2022-03-28 12:57:32 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-1 not ready: kafka)
2022-03-28 12:57:32 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-2 not ready: kafka)
2022-03-28 12:57:32 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-0b062573-kafka-3)
2022-03-28 12:57:32 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2301273ms till timeout)
2022-03-28 12:57:32 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (131418ms till timeout)
2022-03-28 12:57:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (55925ms till timeout)
2022-03-28 12:57:33 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (115998ms till timeout)
2022-03-28 12:57:33 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:57:33 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (175478ms till timeout)
2022-03-28 12:57:33 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-0 not ready: kafka)
2022-03-28 12:57:33 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-1 not ready: kafka)
2022-03-28 12:57:33 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-2 not ready: kafka)
2022-03-28 12:57:33 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-0b062573-kafka-3)
2022-03-28 12:57:33 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2300054ms till timeout)
2022-03-28 12:57:33 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (130104ms till timeout)
2022-03-28 12:57:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (54714ms till timeout)
2022-03-28 12:57:34 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (114998ms till timeout)
2022-03-28 12:57:34 [ForkJoinPool-1-worker-3] INFO  [InternalKafkaClient:97] Producer finished correctly: true
2022-03-28 12:57:34 [ForkJoinPool-1-worker-3] INFO  [InternalKafkaClient:101] Producer produced 100 messages
2022-03-28 12:57:34 [ForkJoinPool-1-worker-3] DEBUG [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@477670c, which are set.
2022-03-28 12:57:34 [ForkJoinPool-1-worker-3] INFO  [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@1f0ad07c, messages=[], arguments=[--topic, my-topic-1513508632-1348735863, --max-messages, 100, --group-id, my-consumer-group-422859519, --group-instance-id, instance1109545001, --bootstrap-server, topic-cluster-name-kafka-bootstrap.topic-st.svc:9092], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='topic-cluster-name-kafka-clients-7c47b4459f-4xdmp', podNamespace='topic-st', bootstrapServer='topic-cluster-name-kafka-bootstrap.topic-st.svc:9092', topicName='my-topic-1513508632-1348735863', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-422859519', consumerInstanceId='instance1109545001', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@477670c}
2022-03-28 12:57:34 [ForkJoinPool-1-worker-3] INFO  [InternalKafkaClient:157] Consuming 100 messages from topic-cluster-name-kafka-bootstrap.topic-st.svc:9092#my-topic-1513508632-1348735863 from pod topic-cluster-name-kafka-clients-7c47b4459f-4xdmp
2022-03-28 12:57:34 [ForkJoinPool-1-worker-3] INFO  [VerifiableClient:192] Client command: oc exec topic-cluster-name-kafka-clients-7c47b4459f-4xdmp -n topic-st -- /opt/kafka/consumer.sh --topic my-topic-1513508632-1348735863 --max-messages 100 --group-id my-consumer-group-422859519 --group-instance-id instance1109545001 --bootstrap-server topic-cluster-name-kafka-bootstrap.topic-st.svc:9092
2022-03-28 12:57:34 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc exec topic-cluster-name-kafka-clients-7c47b4459f-4xdmp -n topic-st -- /opt/kafka/consumer.sh --topic my-topic-1513508632-1348735863 --max-messages 100 --group-id my-consumer-group-422859519 --group-instance-id instance1109545001 --bootstrap-server topic-cluster-name-kafka-bootstrap.topic-st.svc:9092
2022-03-28 12:57:34 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:57:34 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (174260ms till timeout)
2022-03-28 12:57:34 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-0 not ready: kafka)
2022-03-28 12:57:34 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-1 not ready: kafka)
2022-03-28 12:57:34 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-2 not ready: kafka)
2022-03-28 12:57:34 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-0b062573-kafka-3)
2022-03-28 12:57:34 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2298838ms till timeout)
2022-03-28 12:57:34 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (128984ms till timeout)
2022-03-28 12:57:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (53601ms till timeout)
2022-03-28 12:57:35 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (113997ms till timeout)
2022-03-28 12:57:35 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:57:35 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (173036ms till timeout)
2022-03-28 12:57:36 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (112997ms till timeout)
2022-03-28 12:57:36 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (52384ms till timeout)
2022-03-28 12:57:36 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-0 not ready: kafka)
2022-03-28 12:57:36 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-1 not ready: kafka)
2022-03-28 12:57:36 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-2 not ready: kafka)
2022-03-28 12:57:36 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-0b062573-kafka-3)
2022-03-28 12:57:36 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2297613ms till timeout)
2022-03-28 12:57:36 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (127663ms till timeout)
2022-03-28 12:57:36 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testMoreReplicasThanAvailableBrokers is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:57:37 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:57:37 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (111996ms till timeout)
2022-03-28 12:57:37 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (171817ms till timeout)
2022-03-28 12:57:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (51164ms till timeout)
2022-03-28 12:57:37 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-0 not ready: kafka)
2022-03-28 12:57:37 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-1 not ready: kafka)
2022-03-28 12:57:37 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-2 not ready: kafka)
2022-03-28 12:57:37 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-0b062573-kafka-3)
2022-03-28 12:57:37 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2296393ms till timeout)
2022-03-28 12:57:37 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (126444ms till timeout)
2022-03-28 12:57:38 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (110995ms till timeout)
2022-03-28 12:57:38 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:57:38 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (170598ms till timeout)
2022-03-28 12:57:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (49946ms till timeout)
2022-03-28 12:57:38 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-0 not ready: kafka)
2022-03-28 12:57:38 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-1 not ready: kafka)
2022-03-28 12:57:38 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-2 not ready: kafka)
2022-03-28 12:57:38 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-0b062573-kafka-3)
2022-03-28 12:57:38 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2295175ms till timeout)
2022-03-28 12:57:38 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (125225ms till timeout)
2022-03-28 12:57:39 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (109995ms till timeout)
2022-03-28 12:57:39 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:57:39 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (169366ms till timeout)
2022-03-28 12:57:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (48712ms till timeout)
2022-03-28 12:57:39 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-0 not ready: kafka)
2022-03-28 12:57:39 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-1 not ready: kafka)
2022-03-28 12:57:39 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-2 not ready: kafka)
2022-03-28 12:57:39 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-0b062573-kafka-3)
2022-03-28 12:57:39 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2293941ms till timeout)
2022-03-28 12:57:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (123991ms till timeout)
2022-03-28 12:57:40 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (108994ms till timeout)
2022-03-28 12:57:40 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:57:40 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (168149ms till timeout)
2022-03-28 12:57:40 [ForkJoinPool-1-worker-3] INFO  [InternalKafkaClient:160] Consumer finished correctly: true
2022-03-28 12:57:40 [ForkJoinPool-1-worker-3] INFO  [InternalKafkaClient:163] Consumer consumed 100 messages
2022-03-28 12:57:40 [ForkJoinPool-1-worker-3] INFO  [TopicST:341] Checking if my-topic-1513508632-1348735863 is on topic list
2022-03-28 12:57:40 [ForkJoinPool-1-worker-3] INFO  [TopicST:456] Checking topic my-topic-1513508632-1348735863 in Kafka
2022-03-28 12:57:40 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 12:57:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (47595ms till timeout)
2022-03-28 12:57:41 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-0 not ready: kafka)
2022-03-28 12:57:41 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-1 not ready: kafka)
2022-03-28 12:57:41 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-2 not ready: kafka)
2022-03-28 12:57:41 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-0b062573-kafka-3)
2022-03-28 12:57:41 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2292718ms till timeout)
2022-03-28 12:57:41 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (107993ms till timeout)
2022-03-28 12:57:41 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (122769ms till timeout)
2022-03-28 12:57:41 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testMoreReplicasThanAvailableBrokers is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:57:41 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:57:42 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (166930ms till timeout)
2022-03-28 12:57:42 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (106993ms till timeout)
2022-03-28 12:57:42 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (46379ms till timeout)
2022-03-28 12:57:42 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-0 not ready: kafka)
2022-03-28 12:57:42 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-1 not ready: kafka)
2022-03-28 12:57:42 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-2 not ready: kafka)
2022-03-28 12:57:42 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-0b062573-kafka-3)
2022-03-28 12:57:42 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2291501ms till timeout)
2022-03-28 12:57:42 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (121551ms till timeout)
2022-03-28 12:57:43 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (105992ms till timeout)
2022-03-28 12:57:43 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:57:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (45266ms till timeout)
2022-03-28 12:57:43 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (165601ms till timeout)
2022-03-28 12:57:43 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-0 not ready: kafka)
2022-03-28 12:57:43 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-1 not ready: kafka)
2022-03-28 12:57:43 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-2 not ready: kafka)
2022-03-28 12:57:43 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-0b062573-kafka-3)
2022-03-28 12:57:43 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2290288ms till timeout)
2022-03-28 12:57:43 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (120433ms till timeout)
2022-03-28 12:57:44 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (104992ms till timeout)
2022-03-28 12:57:44 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Command: oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 12:57:44 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 12:57:44 [ForkJoinPool-1-worker-3] INFO  [KafkaTopicUtils:78] Waiting for KafkaTopic my-topic-1513508632-1348735863 creation 
2022-03-28 12:57:44 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for KafkaTopic creation my-topic-1513508632-1348735863
2022-03-28 12:57:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (44152ms till timeout)
2022-03-28 12:57:44 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:57:44 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-0 not ready: kafka)
2022-03-28 12:57:44 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-1 not ready: kafka)
2022-03-28 12:57:44 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-2 not ready: kafka)
2022-03-28 12:57:44 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-0b062573-kafka-3)
2022-03-28 12:57:44 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2289166ms till timeout)
2022-03-28 12:57:44 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (119217ms till timeout)
2022-03-28 12:57:44 [ForkJoinPool-1-worker-3] INFO  [TopicST:353] Topic successfully created
2022-03-28 12:57:44 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (164269ms till timeout)
2022-03-28 12:57:44 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:674] ============================================================================
2022-03-28 12:57:44 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:675] [operators.topic.TopicST - After Each] - Clean up after test
2022-03-28 12:57:44 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:344] ############################################################################
2022-03-28 12:57:44 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:348] Delete all resources for testSendingMessagesToNonExistingTopic
2022-03-28 12:57:44 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:241] Delete of Deployment topic-cluster-name-kafka-clients in namespace topic-st
2022-03-28 12:57:44 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:topic-cluster-name-kafka-clients
2022-03-28 12:57:45 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (103991ms till timeout)
2022-03-28 12:57:45 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:topic-cluster-name-kafka-clients not ready, will try again in 10000 ms (479669ms till timeout)
2022-03-28 12:57:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (43041ms till timeout)
2022-03-28 12:57:45 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-0 not ready: kafka)
2022-03-28 12:57:45 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-1 not ready: kafka)
2022-03-28 12:57:45 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-2 not ready: kafka)
2022-03-28 12:57:45 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-0b062573-kafka-3)
2022-03-28 12:57:45 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2288052ms till timeout)
2022-03-28 12:57:45 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:57:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (118096ms till timeout)
2022-03-28 12:57:45 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (163047ms till timeout)
2022-03-28 12:57:46 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (102990ms till timeout)
2022-03-28 12:57:46 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testMoreReplicasThanAvailableBrokers is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:57:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (41926ms till timeout)
2022-03-28 12:57:46 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-0 not ready: kafka)
2022-03-28 12:57:46 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-1 not ready: kafka)
2022-03-28 12:57:46 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-2 not ready: kafka)
2022-03-28 12:57:46 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-0b062573-kafka-3)
2022-03-28 12:57:46 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2286935ms till timeout)
2022-03-28 12:57:46 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (116972ms till timeout)
2022-03-28 12:57:47 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:57:47 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (101990ms till timeout)
2022-03-28 12:57:47 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (161807ms till timeout)
2022-03-28 12:57:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (40813ms till timeout)
2022-03-28 12:57:47 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-0 not ready: kafka)
2022-03-28 12:57:47 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-1 not ready: kafka)
2022-03-28 12:57:47 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-2 not ready: kafka)
2022-03-28 12:57:47 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-0b062573-kafka-3)
2022-03-28 12:57:47 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2285816ms till timeout)
2022-03-28 12:57:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (115856ms till timeout)
2022-03-28 12:57:48 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (100989ms till timeout)
2022-03-28 12:57:48 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:57:48 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (160588ms till timeout)
2022-03-28 12:57:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (39701ms till timeout)
2022-03-28 12:57:49 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-0 not ready: kafka)
2022-03-28 12:57:49 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-1 not ready: kafka)
2022-03-28 12:57:49 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-2 not ready: kafka)
2022-03-28 12:57:49 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-0b062573-kafka-3)
2022-03-28 12:57:49 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2284701ms till timeout)
2022-03-28 12:57:49 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (99989ms till timeout)
2022-03-28 12:57:49 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (114745ms till timeout)
2022-03-28 12:57:49 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:57:49 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (159369ms till timeout)
2022-03-28 12:57:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (38487ms till timeout)
2022-03-28 12:57:50 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (98988ms till timeout)
2022-03-28 12:57:50 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-0 not ready: kafka)
2022-03-28 12:57:50 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-1 not ready: kafka)
2022-03-28 12:57:50 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-2 not ready: kafka)
2022-03-28 12:57:50 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-0b062573-kafka-3)
2022-03-28 12:57:50 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2283485ms till timeout)
2022-03-28 12:57:50 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (113632ms till timeout)
2022-03-28 12:57:50 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:57:50 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (158150ms till timeout)
2022-03-28 12:57:51 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (97987ms till timeout)
2022-03-28 12:57:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (37272ms till timeout)
2022-03-28 12:57:51 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testMoreReplicasThanAvailableBrokers is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:57:51 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-0 not ready: kafka)
2022-03-28 12:57:51 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-1 not ready: kafka)
2022-03-28 12:57:51 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-2 not ready: kafka)
2022-03-28 12:57:51 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-0b062573-kafka-3)
2022-03-28 12:57:51 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2282270ms till timeout)
2022-03-28 12:57:51 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (112415ms till timeout)
2022-03-28 12:57:51 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:57:52 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (156925ms till timeout)
2022-03-28 12:57:52 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (96987ms till timeout)
2022-03-28 12:57:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (36055ms till timeout)
2022-03-28 12:57:52 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-0 not ready: kafka)
2022-03-28 12:57:52 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-1 not ready: kafka)
2022-03-28 12:57:52 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-2 not ready: kafka)
2022-03-28 12:57:52 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-0b062573-kafka-3)
2022-03-28 12:57:52 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2281052ms till timeout)
2022-03-28 12:57:52 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (111199ms till timeout)
2022-03-28 12:57:53 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (95986ms till timeout)
2022-03-28 12:57:53 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:57:53 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (155694ms till timeout)
2022-03-28 12:57:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (34839ms till timeout)
2022-03-28 12:57:53 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-0 not ready: kafka)
2022-03-28 12:57:53 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-1 not ready: kafka)
2022-03-28 12:57:53 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-2 not ready: kafka)
2022-03-28 12:57:53 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-0b062573-kafka-3)
2022-03-28 12:57:53 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2279835ms till timeout)
2022-03-28 12:57:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (109982ms till timeout)
2022-03-28 12:57:54 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (94985ms till timeout)
2022-03-28 12:57:54 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:57:54 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (154473ms till timeout)
2022-03-28 12:57:54 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (33726ms till timeout)
2022-03-28 12:57:55 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-0 not ready: kafka)
2022-03-28 12:57:55 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-1 not ready: kafka)
2022-03-28 12:57:55 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-2 not ready: kafka)
2022-03-28 12:57:55 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-0b062573-kafka-3)
2022-03-28 12:57:55 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2278720ms till timeout)
2022-03-28 12:57:55 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (93985ms till timeout)
2022-03-28 12:57:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (108762ms till timeout)
2022-03-28 12:57:55 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:topic-cluster-name-kafka-clients not ready, will try again in 10000 ms (469337ms till timeout)
2022-03-28 12:57:55 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:57:55 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (153255ms till timeout)
2022-03-28 12:57:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (32613ms till timeout)
2022-03-28 12:57:56 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (92984ms till timeout)
2022-03-28 12:57:56 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-0 not ready: kafka)
2022-03-28 12:57:56 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-1 not ready: kafka)
2022-03-28 12:57:56 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-2 not ready: kafka)
2022-03-28 12:57:56 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-0b062573-kafka-3)
2022-03-28 12:57:56 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2277606ms till timeout)
2022-03-28 12:57:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (107548ms till timeout)
2022-03-28 12:57:56 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testMoreReplicasThanAvailableBrokers is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:57:56 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:57:56 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (152035ms till timeout)
2022-03-28 12:57:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (31486ms till timeout)
2022-03-28 12:57:57 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (91984ms till timeout)
2022-03-28 12:57:57 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-0 not ready: kafka)
2022-03-28 12:57:57 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-1 not ready: kafka)
2022-03-28 12:57:57 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-2 not ready: kafka)
2022-03-28 12:57:57 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-0b062573-kafka-3)
2022-03-28 12:57:57 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2276493ms till timeout)
2022-03-28 12:57:57 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (106430ms till timeout)
2022-03-28 12:57:58 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:57:58 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (90983ms till timeout)
2022-03-28 12:57:58 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (150816ms till timeout)
2022-03-28 12:57:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (30266ms till timeout)
2022-03-28 12:57:58 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-0 not ready: kafka)
2022-03-28 12:57:58 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-1 not ready: kafka)
2022-03-28 12:57:58 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-2 not ready: kafka)
2022-03-28 12:57:58 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-0b062573-kafka-3)
2022-03-28 12:57:58 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2275380ms till timeout)
2022-03-28 12:57:58 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (105316ms till timeout)
2022-03-28 12:57:59 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (89983ms till timeout)
2022-03-28 12:57:59 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:57:59 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (149596ms till timeout)
2022-03-28 12:57:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (29045ms till timeout)
2022-03-28 12:57:59 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-0 not ready: kafka)
2022-03-28 12:57:59 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-1 not ready: kafka)
2022-03-28 12:57:59 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-2 not ready: kafka)
2022-03-28 12:57:59 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-3 not ready: kafka)
2022-03-28 12:57:59 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-0b062573-kafka-0, my-cluster-0b062573-kafka-1, my-cluster-0b062573-kafka-2, my-cluster-0b062573-kafka-3 are ready
2022-03-28 12:57:59 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2274167ms till timeout)
2022-03-28 12:57:59 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (104204ms till timeout)
2022-03-28 12:58:00 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (88982ms till timeout)
2022-03-28 12:58:00 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:58:00 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (148380ms till timeout)
2022-03-28 12:58:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (27727ms till timeout)
2022-03-28 12:58:00 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-0 not ready: kafka)
2022-03-28 12:58:00 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-1 not ready: kafka)
2022-03-28 12:58:00 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-2 not ready: kafka)
2022-03-28 12:58:00 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-3 not ready: kafka)
2022-03-28 12:58:00 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-0b062573-kafka-0, my-cluster-0b062573-kafka-1, my-cluster-0b062573-kafka-2, my-cluster-0b062573-kafka-3 are ready
2022-03-28 12:58:00 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2272853ms till timeout)
2022-03-28 12:58:00 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (102999ms till timeout)
2022-03-28 12:58:01 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (87981ms till timeout)
2022-03-28 12:58:01 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testMoreReplicasThanAvailableBrokers is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:58:01 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:58:01 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (147160ms till timeout)
2022-03-28 12:58:02 [ForkJoinPool-1-worker-9] INFO  [PodUtils:189] Message All topics created found in create-admin-my-cluster-6acb723c-kafka-clients-2krrp log
2022-03-28 12:58:02 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-0 not ready: kafka)
2022-03-28 12:58:02 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-1 not ready: kafka)
2022-03-28 12:58:02 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-2 not ready: kafka)
2022-03-28 12:58:02 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-3 not ready: kafka)
2022-03-28 12:58:02 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-0b062573-kafka-0, my-cluster-0b062573-kafka-1, my-cluster-0b062573-kafka-2, my-cluster-0b062573-kafka-3 are ready
2022-03-28 12:58:02 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2271632ms till timeout)
2022-03-28 12:58:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (101780ms till timeout)
2022-03-28 12:58:02 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment create-admin-my-cluster-6acb723c-kafka-clients deletion
2022-03-28 12:58:02 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for ReplicaSet create-admin-my-cluster-6acb723c-kafka-clients to be deleted
2022-03-28 12:58:02 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (86981ms till timeout)
2022-03-28 12:58:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] ReplicaSet create-admin-my-cluster-6acb723c-kafka-clients to be deleted not ready, will try again in 5000 ms (179890ms till timeout)
2022-03-28 12:58:02 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:58:03 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (145938ms till timeout)
2022-03-28 12:58:03 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (85980ms till timeout)
2022-03-28 12:58:03 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-0 not ready: kafka)
2022-03-28 12:58:03 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-1 not ready: kafka)
2022-03-28 12:58:03 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-2 not ready: kafka)
2022-03-28 12:58:03 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-3 not ready: kafka)
2022-03-28 12:58:03 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-0b062573-kafka-0, my-cluster-0b062573-kafka-1, my-cluster-0b062573-kafka-2, my-cluster-0b062573-kafka-3 are ready
2022-03-28 12:58:03 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2270418ms till timeout)
2022-03-28 12:58:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (100564ms till timeout)
2022-03-28 12:58:04 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:58:04 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (84980ms till timeout)
2022-03-28 12:58:04 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (144719ms till timeout)
2022-03-28 12:58:04 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-0 not ready: kafka)
2022-03-28 12:58:04 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-1 not ready: kafka)
2022-03-28 12:58:04 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-2 not ready: kafka)
2022-03-28 12:58:04 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-3 not ready: kafka)
2022-03-28 12:58:04 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-0b062573-kafka-0, my-cluster-0b062573-kafka-1, my-cluster-0b062573-kafka-2, my-cluster-0b062573-kafka-3 are ready
2022-03-28 12:58:04 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2269201ms till timeout)
2022-03-28 12:58:04 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (99251ms till timeout)
2022-03-28 12:58:05 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (83979ms till timeout)
2022-03-28 12:58:05 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:58:05 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (143499ms till timeout)
2022-03-28 12:58:05 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-0 not ready: kafka)
2022-03-28 12:58:05 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-1 not ready: kafka)
2022-03-28 12:58:05 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-2 not ready: kafka)
2022-03-28 12:58:05 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-3 not ready: kafka)
2022-03-28 12:58:05 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-0b062573-kafka-0, my-cluster-0b062573-kafka-1, my-cluster-0b062573-kafka-2, my-cluster-0b062573-kafka-3 are ready
2022-03-28 12:58:05 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2267960ms till timeout)
2022-03-28 12:58:05 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (98111ms till timeout)
2022-03-28 12:58:05 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:topic-cluster-name-kafka-clients not ready, will try again in 10000 ms (458805ms till timeout)
2022-03-28 12:58:06 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (82978ms till timeout)
2022-03-28 12:58:06 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testMoreReplicasThanAvailableBrokers is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:58:06 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:58:06 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (142281ms till timeout)
2022-03-28 12:58:07 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-0 not ready: kafka)
2022-03-28 12:58:07 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-1 not ready: kafka)
2022-03-28 12:58:07 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-2 not ready: kafka)
2022-03-28 12:58:07 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-3 not ready: kafka)
2022-03-28 12:58:07 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-0b062573-kafka-0, my-cluster-0b062573-kafka-1, my-cluster-0b062573-kafka-2, my-cluster-0b062573-kafka-3 are ready
2022-03-28 12:58:07 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2266746ms till timeout)
2022-03-28 12:58:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (96890ms till timeout)
2022-03-28 12:58:07 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (81978ms till timeout)
2022-03-28 12:58:07 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:40] Job create-admin-my-cluster-6acb723c-kafka-clients was deleted
2022-03-28 12:58:07 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update Job list-admin-my-cluster-6acb723c-kafka-clients in namespace throttling-quota-st
2022-03-28 12:58:07 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:list-admin-my-cluster-6acb723c-kafka-clients
2022-03-28 12:58:07 [ForkJoinPool-1-worker-9] INFO  [JobUtils:81] Waiting for job: list-admin-my-cluster-6acb723c-kafka-clients will be in active state
2022-03-28 12:58:07 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 12:58:07 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:58:07 [ForkJoinPool-1-worker-9] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 12:58:07 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 12:58:08 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (140850ms till timeout)
2022-03-28 12:58:08 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (80977ms till timeout)
2022-03-28 12:58:08 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (95580ms till timeout)
2022-03-28 12:58:08 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (119678ms till timeout)
2022-03-28 12:58:08 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-0 not ready: kafka)
2022-03-28 12:58:08 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-1 not ready: kafka)
2022-03-28 12:58:08 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-2 not ready: kafka)
2022-03-28 12:58:08 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-3 not ready: kafka)
2022-03-28 12:58:08 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-0b062573-kafka-0, my-cluster-0b062573-kafka-1, my-cluster-0b062573-kafka-2, my-cluster-0b062573-kafka-3 are ready
2022-03-28 12:58:08 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2265427ms till timeout)
2022-03-28 12:58:09 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (79977ms till timeout)
2022-03-28 12:58:09 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:58:09 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (139630ms till timeout)
2022-03-28 12:58:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (94361ms till timeout)
2022-03-28 12:58:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (118458ms till timeout)
2022-03-28 12:58:09 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-0 not ready: kafka)
2022-03-28 12:58:09 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-1 not ready: kafka)
2022-03-28 12:58:09 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-2 not ready: kafka)
2022-03-28 12:58:09 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-3 not ready: kafka)
2022-03-28 12:58:09 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-0b062573-kafka-0, my-cluster-0b062573-kafka-1, my-cluster-0b062573-kafka-2, my-cluster-0b062573-kafka-3 are ready
2022-03-28 12:58:09 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2264207ms till timeout)
2022-03-28 12:58:10 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (78976ms till timeout)
2022-03-28 12:58:10 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:58:10 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (138412ms till timeout)
2022-03-28 12:58:10 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-0 not ready: kafka)
2022-03-28 12:58:10 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-1 not ready: kafka)
2022-03-28 12:58:10 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-2 not ready: kafka)
2022-03-28 12:58:10 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-3 not ready: kafka)
2022-03-28 12:58:10 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-0b062573-kafka-0, my-cluster-0b062573-kafka-1, my-cluster-0b062573-kafka-2, my-cluster-0b062573-kafka-3 are ready
2022-03-28 12:58:10 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-0b062573-kafka, strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (2262990ms till timeout)
2022-03-28 12:58:10 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (93040ms till timeout)
2022-03-28 12:58:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (117137ms till timeout)
2022-03-28 12:58:11 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (77976ms till timeout)
2022-03-28 12:58:11 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testMoreReplicasThanAvailableBrokers is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:58:11 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:58:11 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (137156ms till timeout)
2022-03-28 12:58:12 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-0 not ready: kafka)
2022-03-28 12:58:12 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-1 not ready: kafka)
2022-03-28 12:58:12 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-2 not ready: kafka)
2022-03-28 12:58:12 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-kafka-3 not ready: kafka)
2022-03-28 12:58:12 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-0b062573-kafka-0, my-cluster-0b062573-kafka-1, my-cluster-0b062573-kafka-2, my-cluster-0b062573-kafka-3 are ready
2022-03-28 12:58:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (91785ms till timeout)
2022-03-28 12:58:12 [ForkJoinPool-1-worker-9] INFO  [PodUtils:189] Message quota-topic-test-simple-99 found in list-admin-my-cluster-6acb723c-kafka-clients-b2wj5 log
2022-03-28 12:58:12 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-0b062573 will have desired state: Ready
2022-03-28 12:58:12 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-0b062573 will have desired state: Ready
2022-03-28 12:58:12 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (76975ms till timeout)
2022-03-28 12:58:12 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment list-admin-my-cluster-6acb723c-kafka-clients deletion
2022-03-28 12:58:12 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for ReplicaSet list-admin-my-cluster-6acb723c-kafka-clients to be deleted
2022-03-28 12:58:12 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:444] Kafka: my-cluster-0b062573 is in desired state: Ready
2022-03-28 12:58:12 [ForkJoinPool-1-worker-15] INFO  [RollingUpdateUtils:132] Kafka: my-cluster-0b062573 is ready
2022-03-28 12:58:12 [ForkJoinPool-1-worker-15] INFO  [ReconciliationST:94] Deploying KafkaConnect with pause annotation from the start, no pods should appear
2022-03-28 12:58:12 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:155] Create/Update Deployment my-cluster-0b062573-kafka-clients in namespace namespace-9
2022-03-28 12:58:12 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:164] Using Namespace: namespace-8
2022-03-28 12:58:12 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:40] Job list-admin-my-cluster-6acb723c-kafka-clients was deleted
2022-03-28 12:58:12 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update Job delete-admin-my-cluster-6acb723c-kafka-clients in namespace throttling-quota-st
2022-03-28 12:58:12 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-0b062573-kafka-clients
2022-03-28 12:58:12 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:delete-admin-my-cluster-6acb723c-kafka-clients
2022-03-28 12:58:12 [ForkJoinPool-1-worker-9] INFO  [JobUtils:81] Waiting for job: delete-admin-my-cluster-6acb723c-kafka-clients will be in active state
2022-03-28 12:58:12 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 12:58:12 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:58:12 [ForkJoinPool-1-worker-15] INFO  [DeploymentUtils:161] Wait for Deployment: my-cluster-0b062573-kafka-clients will be ready
2022-03-28 12:58:12 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Wait for Deployment: my-cluster-0b062573-kafka-clients will be ready
2022-03-28 12:58:13 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (75974ms till timeout)
2022-03-28 12:58:13 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-kafka-clients will be ready not ready, will try again in 1000 ms (479787ms till timeout)
2022-03-28 12:58:13 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (135766ms till timeout)
2022-03-28 12:58:13 [ForkJoinPool-1-worker-9] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 12:58:13 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 12:58:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (90600ms till timeout)
2022-03-28 12:58:13 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (119885ms till timeout)
2022-03-28 12:58:14 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (74974ms till timeout)
2022-03-28 12:58:14 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:58:14 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-kafka-clients will be ready not ready, will try again in 1000 ms (478675ms till timeout)
2022-03-28 12:58:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (89487ms till timeout)
2022-03-28 12:58:14 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (134437ms till timeout)
2022-03-28 12:58:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (118670ms till timeout)
2022-03-28 12:58:15 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (73973ms till timeout)
2022-03-28 12:58:15 [ForkJoinPool-1-worker-15] INFO  [DeploymentUtils:168] Deployment: my-cluster-0b062573-kafka-clients is ready
2022-03-28 12:58:15 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:155] Create/Update Deployment my-cluster-0b062573-scraper in namespace namespace-8
2022-03-28 12:58:15 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:164] Using Namespace: namespace-8
2022-03-28 12:58:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (88271ms till timeout)
2022-03-28 12:58:15 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:58:15 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-0b062573-scraper
2022-03-28 12:58:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (117558ms till timeout)
2022-03-28 12:58:15 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (133109ms till timeout)
2022-03-28 12:58:15 [ForkJoinPool-1-worker-15] INFO  [DeploymentUtils:161] Wait for Deployment: my-cluster-0b062573-scraper will be ready
2022-03-28 12:58:15 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Wait for Deployment: my-cluster-0b062573-scraper will be ready
2022-03-28 12:58:15 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-scraper will be ready not ready, will try again in 1000 ms (479891ms till timeout)
2022-03-28 12:58:16 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (72973ms till timeout)
2022-03-28 12:58:16 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:topic-cluster-name-kafka-clients not ready, will try again in 10000 ms (448473ms till timeout)
2022-03-28 12:58:16 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testMoreReplicasThanAvailableBrokers is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:58:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (87157ms till timeout)
2022-03-28 12:58:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (116443ms till timeout)
2022-03-28 12:58:16 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:58:17 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-scraper will be ready not ready, will try again in 1000 ms (478781ms till timeout)
2022-03-28 12:58:17 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (131883ms till timeout)
2022-03-28 12:58:17 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (71972ms till timeout)
2022-03-28 12:58:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (85942ms till timeout)
2022-03-28 12:58:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (115333ms till timeout)
2022-03-28 12:58:18 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (70972ms till timeout)
2022-03-28 12:58:18 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-scraper will be ready not ready, will try again in 1000 ms (477667ms till timeout)
2022-03-28 12:58:18 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:58:18 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (130395ms till timeout)
2022-03-28 12:58:19 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (84827ms till timeout)
2022-03-28 12:58:19 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (69971ms till timeout)
2022-03-28 12:58:19 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (114119ms till timeout)
2022-03-28 12:58:19 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-scraper will be ready not ready, will try again in 1000 ms (476556ms till timeout)
2022-03-28 12:58:19 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:58:19 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129177ms till timeout)
2022-03-28 12:58:20 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (68970ms till timeout)
2022-03-28 12:58:20 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (83612ms till timeout)
2022-03-28 12:58:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (112971ms till timeout)
2022-03-28 12:58:20 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-scraper will be ready not ready, will try again in 1000 ms (475445ms till timeout)
2022-03-28 12:58:20 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:58:20 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (127959ms till timeout)
2022-03-28 12:58:21 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (67970ms till timeout)
2022-03-28 12:58:21 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testMoreReplicasThanAvailableBrokers is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:58:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (82397ms till timeout)
2022-03-28 12:58:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (111784ms till timeout)
2022-03-28 12:58:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-scraper will be ready not ready, will try again in 1000 ms (474241ms till timeout)
2022-03-28 12:58:22 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:58:22 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (66969ms till timeout)
2022-03-28 12:58:22 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (126739ms till timeout)
2022-03-28 12:58:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (81181ms till timeout)
2022-03-28 12:58:22 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-scraper will be ready not ready, will try again in 1000 ms (473131ms till timeout)
2022-03-28 12:58:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (110477ms till timeout)
2022-03-28 12:58:23 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (65969ms till timeout)
2022-03-28 12:58:23 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T12:58:17Z, conditions=[JobCondition(lastProbeTime=2022-03-28T12:58:17Z, lastTransitionTime=2022-03-28T12:58:17Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T12:56:44Z, succeeded=1, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:58:23 [ForkJoinPool-1-worker-11] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 12:58:23 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 12:58:23 [ForkJoinPool-1-worker-11] INFO  [PodUtils:189] Message All topics created found in create-admin-my-cluster-b6bd312e-kafka-clients-h52fx log
2022-03-28 12:58:23 [ForkJoinPool-1-worker-11] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment create-admin-my-cluster-b6bd312e-kafka-clients deletion
2022-03-28 12:58:23 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for ReplicaSet create-admin-my-cluster-b6bd312e-kafka-clients to be deleted
2022-03-28 12:58:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (80021ms till timeout)
2022-03-28 12:58:23 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-scraper will be ready not ready, will try again in 1000 ms (471962ms till timeout)
2022-03-28 12:58:23 [ForkJoinPool-1-worker-11] DEBUG [JobUtils:40] Job create-admin-my-cluster-b6bd312e-kafka-clients was deleted
2022-03-28 12:58:23 [ForkJoinPool-1-worker-11] INFO  [ThrottlingQuotaST:112] Executing 2/5 iteration.
2022-03-28 12:58:23 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st
2022-03-28 12:58:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (109312ms till timeout)
2022-03-28 12:58:24 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:create-admin-my-cluster-b6bd312e-kafka-clients
2022-03-28 12:58:24 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (64968ms till timeout)
2022-03-28 12:58:24 [ForkJoinPool-1-worker-11] INFO  [JobUtils:81] Waiting for job: create-admin-my-cluster-b6bd312e-kafka-clients will be in active state
2022-03-28 12:58:24 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 12:58:24 [ForkJoinPool-1-worker-11] INFO  [ClientUtils:76] Waiting for producer/consumer:create-admin-my-cluster-b6bd312e-kafka-clients to finished
2022-03-28 12:58:24 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 12:58:24 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:58:24 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (219783ms till timeout)
2022-03-28 12:58:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (78910ms till timeout)
2022-03-28 12:58:25 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-scraper will be ready not ready, will try again in 1000 ms (470757ms till timeout)
2022-03-28 12:58:25 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (63968ms till timeout)
2022-03-28 12:58:25 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (108092ms till timeout)
2022-03-28 12:58:25 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:58:25 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (218559ms till timeout)
2022-03-28 12:58:26 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (62967ms till timeout)
2022-03-28 12:58:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (77696ms till timeout)
2022-03-28 12:58:26 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-scraper will be ready not ready, will try again in 1000 ms (469645ms till timeout)
2022-03-28 12:58:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (106882ms till timeout)
2022-03-28 12:58:26 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testMoreReplicasThanAvailableBrokers is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:58:26 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:369] ############################################################################
2022-03-28 12:58:26 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:267] testSendingMessagesToNonExistingTopic - Notifies waiting test cases:[testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testTlsExternalUserWithQuotas, testUserTemplate, testUpdateUser, testTlsExternalUser, testReceiveSimpleMessageTlsScramSha, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testSendSimpleMessageTlsScramSha, testUserWithNameMoreThan64Chars, testTlsUserWithQuotas, testConfigurationFileIsCreated, testScramUserWithQuotas, testDeployAndUnDeployCruiseControl, testConfigurationPerformanceOptions, testMoreReplicasThanAvailableBrokers] to and randomly select one to start execution
2022-03-28 12:58:26 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:93] [operators.topic.TopicST] - Removing parallel test: testSendingMessagesToNonExistingTopic
2022-03-28 12:58:26 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:97] [operators.topic.TopicST] - Parallel test count: 6
2022-03-28 12:58:26 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.topic.TopicST.testSendingMessagesToNonExistingTopic-FINISHED
2022-03-28 12:58:26 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:30] ############################################################################
2022-03-28 12:58:26 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:23] ############################################################################
2022-03-28 12:58:26 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.topic.TopicST.testCreateTopicViaKafka-STARTED
2022-03-28 12:58:26 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:658] ============================================================================
2022-03-28 12:58:26 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:659] [operators.topic.TopicST - Before Each] - Setup test case environment
2022-03-28 12:58:26 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:77] [operators.topic.TopicST] - Adding parallel test: testCreateTopicViaKafka
2022-03-28 12:58:26 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:81] [operators.topic.TopicST] - Parallel test count: 7
2022-03-28 12:58:26 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:205] [testCreateTopicViaKafka] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:58:26 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testCreateTopicViaKafka is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:58:26 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:58:26 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (217338ms till timeout)
2022-03-28 12:58:27 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (61967ms till timeout)
2022-03-28 12:58:27 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (76481ms till timeout)
2022-03-28 12:58:27 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-scraper will be ready not ready, will try again in 1000 ms (468429ms till timeout)
2022-03-28 12:58:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (105768ms till timeout)
2022-03-28 12:58:28 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:58:28 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (60966ms till timeout)
2022-03-28 12:58:28 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (216117ms till timeout)
2022-03-28 12:58:28 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (75265ms till timeout)
2022-03-28 12:58:28 [ForkJoinPool-1-worker-15] INFO  [DeploymentUtils:168] Deployment: my-cluster-0b062573-scraper is ready
2022-03-28 12:58:28 [ForkJoinPool-1-worker-15] INFO  [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-0b062573-scraper to be ready
2022-03-28 12:58:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (104553ms till timeout)
2022-03-28 12:58:28 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-0b062573-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready
2022-03-28 12:58:28 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-scraper-7b96c89f96-lhqc2 not ready: my-cluster-0b062573-scraper)
2022-03-28 12:58:28 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-0b062573-scraper-7b96c89f96-lhqc2 are ready
2022-03-28 12:58:28 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-0b062573-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599890ms till timeout)
2022-03-28 12:58:29 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (59965ms till timeout)
2022-03-28 12:58:29 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:58:29 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (214899ms till timeout)
2022-03-28 12:58:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (74050ms till timeout)
2022-03-28 12:58:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (103438ms till timeout)
2022-03-28 12:58:30 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-scraper-7b96c89f96-lhqc2 not ready: my-cluster-0b062573-scraper)
2022-03-28 12:58:30 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-0b062573-scraper-7b96c89f96-lhqc2 are ready
2022-03-28 12:58:30 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-0b062573-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598777ms till timeout)
2022-03-28 12:58:30 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (58965ms till timeout)
2022-03-28 12:58:30 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:58:30 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (213683ms till timeout)
2022-03-28 12:58:31 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (72835ms till timeout)
2022-03-28 12:58:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (102193ms till timeout)
2022-03-28 12:58:31 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (57964ms till timeout)
2022-03-28 12:58:31 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-scraper-7b96c89f96-lhqc2 not ready: my-cluster-0b062573-scraper)
2022-03-28 12:58:31 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-0b062573-scraper-7b96c89f96-lhqc2 are ready
2022-03-28 12:58:31 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-0b062573-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597667ms till timeout)
2022-03-28 12:58:31 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:230] testMoreReplicasThanAvailableBrokers test now can proceed its execution
2022-03-28 12:58:31 [ForkJoinPool-1-worker-13] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 12:58:31 [ForkJoinPool-1-worker-13] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-455915b9, testThrottlingQuotasCreateAlterPartitions=my-cluster-d4dad0de, testSendSimpleMessageTls=my-cluster-cfce2f5b, testConfigurationReflection=my-cluster-aa0ecd2f, testUserWithNameMoreThan64Chars=my-cluster-ed39a269, testDeployAndUnDeployCruiseControl=my-cluster-534609d0, testCapacityFile=my-cluster-39ee3f62, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-0b062573, testTlsExternalUser=my-cluster-965e9d1e, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-31feb41b, testSendSimpleMessageTlsScramSha=my-cluster-27773859, testScramUserWithQuotas=my-cluster-90c82d82, testTopicModificationOfReplicationFactor=my-cluster-421b59dd, testKafkaAdminTopicOperations=my-cluster-6acb723c, testConfigurationFileIsCreated=my-cluster-020ceaf8, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5, testUpdateUser=my-cluster-e16ccf16, testThrottlingQuotasDeleteTopic=my-cluster-b6bd312e, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d, testTlsExternalUserWithQuotas=my-cluster-3e2e11de, testTlsUserWithQuotas=my-cluster-3b191485, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2, testSendingMessagesToNonExistingTopic=my-cluster-64488b2e, testConfigurationPerformanceOptions=my-cluster-bc0ed00a, testReceiveSimpleMessageTls=my-cluster-362cd511, testUserTemplate=my-cluster-157df94e, testMoreReplicasThanAvailableBrokers=my-cluster-1f5cbe38}
2022-03-28 12:58:31 [ForkJoinPool-1-worker-13] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-1329598231-1183867861, testThrottlingQuotasCreateAlterPartitions=my-user-1809876063-64793399, testSendSimpleMessageTls=my-user-111586425-1868899021, testConfigurationReflection=my-user-1580949953-976509997, testUserWithNameMoreThan64Chars=my-user-1048816470-1103470031, testDeployAndUnDeployCruiseControl=my-user-475346616-1293712526, testCapacityFile=my-user-597365340-661017603, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-1153908156-1369197188, testTlsExternalUser=my-user-1295334081-1602801897, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2100837352-1014792902, testSendSimpleMessageTlsScramSha=my-user-507582460-1003875464, testScramUserWithQuotas=my-user-1857980152-2145707698, testTopicModificationOfReplicationFactor=my-user-380015619-613911473, testKafkaAdminTopicOperations=my-user-2014252818-589270412, testConfigurationFileIsCreated=my-user-1609977058-1196012647, testCreatingUsersWithSecretPrefix=my-user-1928899390-1444689439, testUpdateUser=my-user-1694542041-806176635, testThrottlingQuotasDeleteTopic=my-user-284582998-729631500, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-467839358-1504804463, testTlsExternalUserWithQuotas=my-user-1786174319-1274634874, testTlsUserWithQuotas=my-user-420001054-1296284129, testThrottlingQuotasCreateTopic=my-user-896662438-260898887, testSendingMessagesToNonExistingTopic=my-user-248322508-1660982370, testConfigurationPerformanceOptions=my-user-722474509-712960391, testReceiveSimpleMessageTls=my-user-2012197250-2103354173, testUserTemplate=my-user-1253881534-190261188, testMoreReplicasThanAvailableBrokers=my-user-1381751556-1357722017}
2022-03-28 12:58:31 [ForkJoinPool-1-worker-13] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-170116874-1830928683, testThrottlingQuotasCreateAlterPartitions=my-topic-1581297458-635252970, testSendSimpleMessageTls=my-topic-100192180-936335618, testConfigurationReflection=my-topic-1437230713-1827996725, testUserWithNameMoreThan64Chars=my-topic-1603054043-1686316417, testDeployAndUnDeployCruiseControl=my-topic-1815491150-540070233, testCapacityFile=my-topic-1390944843-775067696, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-613737897-291522718, testTlsExternalUser=my-topic-476529121-306489993, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1084162749-1576061083, testSendSimpleMessageTlsScramSha=my-topic-62182003-1963040205, testScramUserWithQuotas=my-topic-315112689-1043284277, testTopicModificationOfReplicationFactor=my-topic-1016572292-592627742, testKafkaAdminTopicOperations=my-topic-1915664839-460245273, testConfigurationFileIsCreated=my-topic-155202447-1921897899, testCreatingUsersWithSecretPrefix=my-topic-2033928996-1209892355, testUpdateUser=my-topic-1239518257-2056139809, testThrottlingQuotasDeleteTopic=my-topic-1036812834-1980126190, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-712754859-1910580676, testTlsExternalUserWithQuotas=my-topic-593679216-1309035072, testTlsUserWithQuotas=my-topic-1752299353-1315959033, testThrottlingQuotasCreateTopic=my-topic-1532506723-1472611013, testSendingMessagesToNonExistingTopic=my-topic-1513508632-1348735863, testConfigurationPerformanceOptions=my-topic-1891296149-770116276, testReceiveSimpleMessageTls=my-topic-1264858218-642520562, testUserTemplate=my-topic-515111334-943904347, testMoreReplicasThanAvailableBrokers=my-topic-2107805499-2107307630}
2022-03-28 12:58:31 [ForkJoinPool-1-worker-13] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-455915b9-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-d4dad0de-kafka-clients, testSendSimpleMessageTls=my-cluster-cfce2f5b-kafka-clients, testConfigurationReflection=my-cluster-aa0ecd2f-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-ed39a269-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-534609d0-kafka-clients, testCapacityFile=my-cluster-39ee3f62-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-0b062573-kafka-clients, testTlsExternalUser=my-cluster-965e9d1e-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-31feb41b-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-27773859-kafka-clients, testScramUserWithQuotas=my-cluster-90c82d82-kafka-clients, testTopicModificationOfReplicationFactor=my-cluster-421b59dd-kafka-clients, testKafkaAdminTopicOperations=my-cluster-6acb723c-kafka-clients, testConfigurationFileIsCreated=my-cluster-020ceaf8-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5-kafka-clients, testUpdateUser=my-cluster-e16ccf16-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-b6bd312e-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-3e2e11de-kafka-clients, testTlsUserWithQuotas=my-cluster-3b191485-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2-kafka-clients, testSendingMessagesToNonExistingTopic=my-cluster-64488b2e-kafka-clients, testConfigurationPerformanceOptions=my-cluster-bc0ed00a-kafka-clients, testReceiveSimpleMessageTls=my-cluster-362cd511-kafka-clients, testUserTemplate=my-cluster-157df94e-kafka-clients, testMoreReplicasThanAvailableBrokers=my-cluster-1f5cbe38-kafka-clients}
2022-03-28 12:58:31 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update KafkaTopic my-topic-2107805499-2107307630 in namespace topic-st
2022-03-28 12:58:31 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testCreateTopicViaKafka is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:58:31 [ForkJoinPool-1-worker-13] INFO  [TopicST:461] Checking in KafkaTopic CR that topic my-topic-2107805499-2107307630 exists
2022-03-28 12:58:31 [ForkJoinPool-1-worker-13] INFO  [TopicST:456] Checking topic my-topic-2107805499-2107307630 in Kafka
2022-03-28 12:58:31 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 12:58:31 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:58:31 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (212463ms till timeout)
2022-03-28 12:58:32 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (56963ms till timeout)
2022-03-28 12:58:32 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (71620ms till timeout)
2022-03-28 12:58:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (100977ms till timeout)
2022-03-28 12:58:32 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-scraper-7b96c89f96-lhqc2 not ready: my-cluster-0b062573-scraper)
2022-03-28 12:58:32 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-0b062573-scraper-7b96c89f96-lhqc2 are ready
2022-03-28 12:58:32 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-0b062573-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596461ms till timeout)
2022-03-28 12:58:32 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:58:33 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (211245ms till timeout)
2022-03-28 12:58:33 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (55963ms till timeout)
2022-03-28 12:58:33 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (70396ms till timeout)
2022-03-28 12:58:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (99754ms till timeout)
2022-03-28 12:58:33 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-scraper-7b96c89f96-lhqc2 not ready: my-cluster-0b062573-scraper)
2022-03-28 12:58:33 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-0b062573-scraper-7b96c89f96-lhqc2 are ready
2022-03-28 12:58:33 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-0b062573-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595245ms till timeout)
2022-03-28 12:58:34 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (54962ms till timeout)
2022-03-28 12:58:34 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:58:34 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (210026ms till timeout)
2022-03-28 12:58:34 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (69180ms till timeout)
2022-03-28 12:58:34 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-scraper-7b96c89f96-lhqc2 not ready: my-cluster-0b062573-scraper)
2022-03-28 12:58:34 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-0b062573-scraper-7b96c89f96-lhqc2 are ready
2022-03-28 12:58:34 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-0b062573-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594129ms till timeout)
2022-03-28 12:58:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (98472ms till timeout)
2022-03-28 12:58:35 [ForkJoinPool-1-worker-13] INFO  [Exec:417] Command: oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 12:58:35 [ForkJoinPool-1-worker-13] INFO  [Exec:417] Return code: 0
2022-03-28 12:58:35 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-2107805499-2107307630 will have desired state: NotReady
2022-03-28 12:58:35 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-2107805499-2107307630 will have desired state: NotReady
2022-03-28 12:58:35 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:444] KafkaTopic: my-topic-2107805499-2107307630 is in desired state: NotReady
2022-03-28 12:58:35 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (53962ms till timeout)
2022-03-28 12:58:35 [ForkJoinPool-1-worker-13] INFO  [TopicST:90] Delete topic my-topic-2107805499-2107307630
2022-03-28 12:58:35 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace topic-st delete kafkatopic my-topic-2107805499-2107307630
2022-03-28 12:58:35 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:58:35 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (208810ms till timeout)
2022-03-28 12:58:35 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace topic-st delete kafkatopic my-topic-2107805499-2107307630
2022-03-28 12:58:35 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 12:58:35 [ForkJoinPool-1-worker-13] INFO  [KafkaTopicUtils:104] Waiting for KafkaTopic my-topic-2107805499-2107307630 deletion
2022-03-28 12:58:35 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaTopic deletion my-topic-2107805499-2107307630
2022-03-28 12:58:35 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (68067ms till timeout)
2022-03-28 12:58:35 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-scraper-7b96c89f96-lhqc2 not ready: my-cluster-0b062573-scraper)
2022-03-28 12:58:35 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-0b062573-scraper-7b96c89f96-lhqc2 are ready
2022-03-28 12:58:35 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-0b062573-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592914ms till timeout)
2022-03-28 12:58:35 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update KafkaTopic topic-example-new in namespace topic-st
2022-03-28 12:58:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (97319ms till timeout)
2022-03-28 12:58:36 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:topic-example-new
2022-03-28 12:58:36 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (52961ms till timeout)
2022-03-28 12:58:36 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:433] Wait for KafkaTopic: topic-example-new will have desired state: Ready
2022-03-28 12:58:36 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaTopic: topic-example-new will have desired state: Ready
2022-03-28 12:58:36 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:444] KafkaTopic: topic-example-new is in desired state: Ready
2022-03-28 12:58:36 [ForkJoinPool-1-worker-13] INFO  [TopicST:456] Checking topic topic-example-new in Kafka
2022-03-28 12:58:36 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 12:58:36 [ForkJoinPool-1-worker-3] TRACE [SuiteThreadController:210] testCreateTopicViaKafka is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:58:36 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:58:36 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (207592ms till timeout)
2022-03-28 12:58:36 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (66953ms till timeout)
2022-03-28 12:58:37 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-scraper-7b96c89f96-lhqc2 not ready: my-cluster-0b062573-scraper)
2022-03-28 12:58:37 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-0b062573-scraper-7b96c89f96-lhqc2 are ready
2022-03-28 12:58:37 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-0b062573-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591798ms till timeout)
2022-03-28 12:58:37 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (51961ms till timeout)
2022-03-28 12:58:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (96134ms till timeout)
2022-03-28 12:58:37 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:58:37 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (206375ms till timeout)
2022-03-28 12:58:38 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (50960ms till timeout)
2022-03-28 12:58:38 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (65739ms till timeout)
2022-03-28 12:58:38 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-scraper-7b96c89f96-lhqc2 not ready: my-cluster-0b062573-scraper)
2022-03-28 12:58:38 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-0b062573-scraper-7b96c89f96-lhqc2 are ready
2022-03-28 12:58:38 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-0b062573-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590686ms till timeout)
2022-03-28 12:58:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (95022ms till timeout)
2022-03-28 12:58:39 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:58:39 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (49959ms till timeout)
2022-03-28 12:58:39 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (205154ms till timeout)
2022-03-28 12:58:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (64524ms till timeout)
2022-03-28 12:58:39 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-scraper-7b96c89f96-lhqc2 not ready: my-cluster-0b062573-scraper)
2022-03-28 12:58:39 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-0b062573-scraper-7b96c89f96-lhqc2 are ready
2022-03-28 12:58:39 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={app=my-cluster-0b062573-scraper, deployment-type=Scraper, user-test-app=scraper}, additionalProperties={})to be ready not ready, will try again in 1000 ms (589473ms till timeout)
2022-03-28 12:58:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (93813ms till timeout)
2022-03-28 12:58:39 [ForkJoinPool-1-worker-13] INFO  [Exec:417] Command: oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 12:58:39 [ForkJoinPool-1-worker-13] INFO  [Exec:417] Return code: 0
2022-03-28 12:58:39 [ForkJoinPool-1-worker-13] INFO  [TopicST:461] Checking in KafkaTopic CR that topic topic-example-new exists
2022-03-28 12:58:39 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:674] ============================================================================
2022-03-28 12:58:39 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:675] [operators.topic.TopicST - After Each] - Clean up after test
2022-03-28 12:58:39 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:344] ############################################################################
2022-03-28 12:58:39 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:348] Delete all resources for testMoreReplicasThanAvailableBrokers
2022-03-28 12:58:39 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of KafkaTopic topic-example-new in namespace topic-st
2022-03-28 12:58:39 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:topic-example-new
2022-03-28 12:58:39 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of KafkaTopic my-topic-2107805499-2107307630 in namespace topic-st
2022-03-28 12:58:39 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-2107805499-2107307630
2022-03-28 12:58:40 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:369] ############################################################################
2022-03-28 12:58:40 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:267] testMoreReplicasThanAvailableBrokers - Notifies waiting test cases:[testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testTlsExternalUserWithQuotas, testUserTemplate, testUpdateUser, testTlsExternalUser, testReceiveSimpleMessageTlsScramSha, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testSendSimpleMessageTlsScramSha, testUserWithNameMoreThan64Chars, testTlsUserWithQuotas, testConfigurationFileIsCreated, testScramUserWithQuotas, testDeployAndUnDeployCruiseControl, testConfigurationPerformanceOptions, testCreateTopicViaKafka] to and randomly select one to start execution
2022-03-28 12:58:40 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:93] [operators.topic.TopicST] - Removing parallel test: testMoreReplicasThanAvailableBrokers
2022-03-28 12:58:40 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:97] [operators.topic.TopicST] - Parallel test count: 6
2022-03-28 12:58:40 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.topic.TopicST.testMoreReplicasThanAvailableBrokers-FINISHED
2022-03-28 12:58:40 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:30] ############################################################################
2022-03-28 12:58:40 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:23] ############################################################################
2022-03-28 12:58:40 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.topic.TopicST.testCreateTopicAfterUnsupportedOperation-STARTED
2022-03-28 12:58:40 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:658] ============================================================================
2022-03-28 12:58:40 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:659] [operators.topic.TopicST - Before Each] - Setup test case environment
2022-03-28 12:58:40 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:77] [operators.topic.TopicST] - Adding parallel test: testCreateTopicAfterUnsupportedOperation
2022-03-28 12:58:40 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:81] [operators.topic.TopicST] - Parallel test count: 7
2022-03-28 12:58:40 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:205] [testCreateTopicAfterUnsupportedOperation] moved to the WaitZone, because current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:58:40 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testCreateTopicAfterUnsupportedOperation is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:58:40 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (48959ms till timeout)
2022-03-28 12:58:40 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:58:40 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (203938ms till timeout)
2022-03-28 12:58:40 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (63410ms till timeout)
2022-03-28 12:58:40 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-scraper-7b96c89f96-lhqc2 not ready: my-cluster-0b062573-scraper)
2022-03-28 12:58:40 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-0b062573-scraper-7b96c89f96-lhqc2 are ready
2022-03-28 12:58:40 [ForkJoinPool-1-worker-15] INFO  [DeploymentUtils:197] Deployment my-cluster-0b062573-scraper is ready
2022-03-28 12:58:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (92700ms till timeout)
2022-03-28 12:58:40 [ForkJoinPool-1-worker-15] INFO  [NetworkPolicyResource:187] Apply NetworkPolicy access to my-cluster-0b062573-connect from pods with LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={})
2022-03-28 12:58:40 [ForkJoinPool-1-worker-15] DEBUG [NetworkPolicyResource:227] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=my-cluster-0b062573-allow, namespace=namespace-8, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[NetworkPolicyIngressRule(from=[NetworkPolicyPeer(ipBlock=null, namespaceSelector=null, podSelector=LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}), additionalProperties={})], ports=[NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8083, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=9404, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=8080, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={}), NetworkPolicyPort(endPort=null, port=IntOrString(IntVal=9999, Kind=0, StrVal=null, additionalProperties={}), protocol=TCP, additionalProperties={})], additionalProperties={})], podSelector=LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-0b062573-connect}, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 12:58:40 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:155] Create/Update NetworkPolicy my-cluster-0b062573-allow in namespace namespace-8
2022-03-28 12:58:40 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:164] Using Namespace: namespace-8
2022-03-28 12:58:40 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource NetworkPolicy:my-cluster-0b062573-allow
2022-03-28 12:58:40 [ForkJoinPool-1-worker-15] INFO  [NetworkPolicyResource:229] Network policy for LabelSelector LabelSelector(matchExpressions=[], matchLabels={user-test-app=scraper}, additionalProperties={}) successfully created
2022-03-28 12:58:40 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:155] Create/Update KafkaConnect my-cluster-0b062573 in namespace namespace-9
2022-03-28 12:58:40 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:164] Using Namespace: namespace-8
2022-03-28 12:58:40 [ForkJoinPool-1-worker-15] WARN  [VersionUsageUtils:60] The client is using resource type 'kafkaconnects' with unstable version 'v1beta2'
2022-03-28 12:58:41 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (47958ms till timeout)
2022-03-28 12:58:41 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:433] Wait for KafkaConnect: my-cluster-0b062573 will have desired state: ReconciliationPaused
2022-03-28 12:58:41 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for KafkaConnect: my-cluster-0b062573 will have desired state: ReconciliationPaused
2022-03-28 12:58:41 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:444] KafkaConnect: my-cluster-0b062573 is in desired state: ReconciliationPaused
2022-03-28 12:58:41 [ForkJoinPool-1-worker-15] INFO  [PodUtils:209] Wait until Pod my-cluster-0b062573-connect will have stable 0 replicas
2022-03-28 12:58:41 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for  Podmy-cluster-0b062573-connect will have 0 replicas
2022-03-28 12:58:41 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:230] testCreateTopicViaKafka test now can proceed its execution
2022-03-28 12:58:41 [ForkJoinPool-1-worker-3] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 12:58:41 [ForkJoinPool-1-worker-3] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-455915b9, testThrottlingQuotasCreateAlterPartitions=my-cluster-d4dad0de, testSendSimpleMessageTls=my-cluster-cfce2f5b, testConfigurationReflection=my-cluster-aa0ecd2f, testUserWithNameMoreThan64Chars=my-cluster-ed39a269, testDeployAndUnDeployCruiseControl=my-cluster-534609d0, testCapacityFile=my-cluster-39ee3f62, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-0b062573, testTlsExternalUser=my-cluster-965e9d1e, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-31feb41b, testSendSimpleMessageTlsScramSha=my-cluster-27773859, testScramUserWithQuotas=my-cluster-90c82d82, testTopicModificationOfReplicationFactor=my-cluster-421b59dd, testKafkaAdminTopicOperations=my-cluster-6acb723c, testConfigurationFileIsCreated=my-cluster-020ceaf8, testCreateTopicViaKafka=my-cluster-43bd601f, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5, testUpdateUser=my-cluster-e16ccf16, testThrottlingQuotasDeleteTopic=my-cluster-b6bd312e, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d, testTlsExternalUserWithQuotas=my-cluster-3e2e11de, testTlsUserWithQuotas=my-cluster-3b191485, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2, testSendingMessagesToNonExistingTopic=my-cluster-64488b2e, testConfigurationPerformanceOptions=my-cluster-bc0ed00a, testReceiveSimpleMessageTls=my-cluster-362cd511, testUserTemplate=my-cluster-157df94e, testMoreReplicasThanAvailableBrokers=my-cluster-1f5cbe38}
2022-03-28 12:58:41 [ForkJoinPool-1-worker-3] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-1329598231-1183867861, testThrottlingQuotasCreateAlterPartitions=my-user-1809876063-64793399, testSendSimpleMessageTls=my-user-111586425-1868899021, testConfigurationReflection=my-user-1580949953-976509997, testUserWithNameMoreThan64Chars=my-user-1048816470-1103470031, testDeployAndUnDeployCruiseControl=my-user-475346616-1293712526, testCapacityFile=my-user-597365340-661017603, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-1153908156-1369197188, testTlsExternalUser=my-user-1295334081-1602801897, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2100837352-1014792902, testSendSimpleMessageTlsScramSha=my-user-507582460-1003875464, testScramUserWithQuotas=my-user-1857980152-2145707698, testTopicModificationOfReplicationFactor=my-user-380015619-613911473, testKafkaAdminTopicOperations=my-user-2014252818-589270412, testConfigurationFileIsCreated=my-user-1609977058-1196012647, testCreateTopicViaKafka=my-user-1379798639-1428271857, testCreatingUsersWithSecretPrefix=my-user-1928899390-1444689439, testUpdateUser=my-user-1694542041-806176635, testThrottlingQuotasDeleteTopic=my-user-284582998-729631500, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-467839358-1504804463, testTlsExternalUserWithQuotas=my-user-1786174319-1274634874, testTlsUserWithQuotas=my-user-420001054-1296284129, testThrottlingQuotasCreateTopic=my-user-896662438-260898887, testSendingMessagesToNonExistingTopic=my-user-248322508-1660982370, testConfigurationPerformanceOptions=my-user-722474509-712960391, testReceiveSimpleMessageTls=my-user-2012197250-2103354173, testUserTemplate=my-user-1253881534-190261188, testMoreReplicasThanAvailableBrokers=my-user-1381751556-1357722017}
2022-03-28 12:58:41 [ForkJoinPool-1-worker-3] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-170116874-1830928683, testThrottlingQuotasCreateAlterPartitions=my-topic-1581297458-635252970, testSendSimpleMessageTls=my-topic-100192180-936335618, testConfigurationReflection=my-topic-1437230713-1827996725, testUserWithNameMoreThan64Chars=my-topic-1603054043-1686316417, testDeployAndUnDeployCruiseControl=my-topic-1815491150-540070233, testCapacityFile=my-topic-1390944843-775067696, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-613737897-291522718, testTlsExternalUser=my-topic-476529121-306489993, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1084162749-1576061083, testSendSimpleMessageTlsScramSha=my-topic-62182003-1963040205, testScramUserWithQuotas=my-topic-315112689-1043284277, testTopicModificationOfReplicationFactor=my-topic-1016572292-592627742, testKafkaAdminTopicOperations=my-topic-1915664839-460245273, testConfigurationFileIsCreated=my-topic-155202447-1921897899, testCreateTopicViaKafka=my-topic-664105056-981624555, testCreatingUsersWithSecretPrefix=my-topic-2033928996-1209892355, testUpdateUser=my-topic-1239518257-2056139809, testThrottlingQuotasDeleteTopic=my-topic-1036812834-1980126190, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-712754859-1910580676, testTlsExternalUserWithQuotas=my-topic-593679216-1309035072, testTlsUserWithQuotas=my-topic-1752299353-1315959033, testThrottlingQuotasCreateTopic=my-topic-1532506723-1472611013, testSendingMessagesToNonExistingTopic=my-topic-1513508632-1348735863, testConfigurationPerformanceOptions=my-topic-1891296149-770116276, testReceiveSimpleMessageTls=my-topic-1264858218-642520562, testUserTemplate=my-topic-515111334-943904347, testMoreReplicasThanAvailableBrokers=my-topic-2107805499-2107307630}
2022-03-28 12:58:41 [ForkJoinPool-1-worker-3] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-455915b9-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-d4dad0de-kafka-clients, testSendSimpleMessageTls=my-cluster-cfce2f5b-kafka-clients, testConfigurationReflection=my-cluster-aa0ecd2f-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-ed39a269-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-534609d0-kafka-clients, testCapacityFile=my-cluster-39ee3f62-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-0b062573-kafka-clients, testTlsExternalUser=my-cluster-965e9d1e-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-31feb41b-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-27773859-kafka-clients, testScramUserWithQuotas=my-cluster-90c82d82-kafka-clients, testTopicModificationOfReplicationFactor=my-cluster-421b59dd-kafka-clients, testKafkaAdminTopicOperations=my-cluster-6acb723c-kafka-clients, testConfigurationFileIsCreated=my-cluster-020ceaf8-kafka-clients, testCreateTopicViaKafka=my-cluster-43bd601f-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5-kafka-clients, testUpdateUser=my-cluster-e16ccf16-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-b6bd312e-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-3e2e11de-kafka-clients, testTlsUserWithQuotas=my-cluster-3b191485-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2-kafka-clients, testSendingMessagesToNonExistingTopic=my-cluster-64488b2e-kafka-clients, testConfigurationPerformanceOptions=my-cluster-bc0ed00a-kafka-clients, testReceiveSimpleMessageTls=my-cluster-362cd511-kafka-clients, testUserTemplate=my-cluster-157df94e-kafka-clients, testMoreReplicasThanAvailableBrokers=my-cluster-1f5cbe38-kafka-clients}
2022-03-28 12:58:41 [ForkJoinPool-1-worker-3] DEBUG [TopicST:113] Creating topic my-topic-664105056-981624555 with 3 replicas and 3 partitions
2022-03-28 12:58:41 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --bootstrap-server localhost:9092 --create  --topic my-topic-664105056-981624555 --replication-factor 3 --partitions 3
2022-03-28 12:58:41 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:58:41 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 19 polls
2022-03-28 12:58:41 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-0b062573-connect will have 0 replicas not ready, will try again in 1000 ms (179681ms till timeout)
2022-03-28 12:58:41 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (62273ms till timeout)
2022-03-28 12:58:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (91566ms till timeout)
2022-03-28 12:58:41 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (202477ms till timeout)
2022-03-28 12:58:42 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (46958ms till timeout)
2022-03-28 12:58:43 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (60855ms till timeout)
2022-03-28 12:58:43 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 18 polls
2022-03-28 12:58:43 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-0b062573-connect will have 0 replicas not ready, will try again in 1000 ms (178256ms till timeout)
2022-03-28 12:58:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (90151ms till timeout)
2022-03-28 12:58:43 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:58:43 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (45957ms till timeout)
2022-03-28 12:58:43 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (201060ms till timeout)
2022-03-28 12:58:44 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (44955ms till timeout)
2022-03-28 12:58:44 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (59639ms till timeout)
2022-03-28 12:58:44 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 17 polls
2022-03-28 12:58:44 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-0b062573-connect will have 0 replicas not ready, will try again in 1000 ms (176826ms till timeout)
2022-03-28 12:58:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (88721ms till timeout)
2022-03-28 12:58:44 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:58:44 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (199628ms till timeout)
2022-03-28 12:58:45 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testCreateTopicAfterUnsupportedOperation is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:58:45 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Command: oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --bootstrap-server localhost:9092 --create  --topic my-topic-664105056-981624555 --replication-factor 3 --partitions 3
2022-03-28 12:58:45 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 12:58:45 [ForkJoinPool-1-worker-3] INFO  [KafkaTopicUtils:78] Waiting for KafkaTopic my-topic-664105056-981624555 creation 
2022-03-28 12:58:45 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for KafkaTopic creation my-topic-664105056-981624555
2022-03-28 12:58:45 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (43954ms till timeout)
2022-03-28 12:58:45 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] KafkaTopic creation my-topic-664105056-981624555 not ready, will try again in 1000 ms (179891ms till timeout)
2022-03-28 12:58:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (58528ms till timeout)
2022-03-28 12:58:45 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (87504ms till timeout)
2022-03-28 12:58:45 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 16 polls
2022-03-28 12:58:45 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-0b062573-connect will have 0 replicas not ready, will try again in 1000 ms (175506ms till timeout)
2022-03-28 12:58:45 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:58:45 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (198412ms till timeout)
2022-03-28 12:58:46 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (42954ms till timeout)
2022-03-28 12:58:46 [ForkJoinPool-1-worker-3] INFO  [TopicST:482] Checking in KafkaTopic CR that topic my-topic-664105056-981624555 was created with expected settings
2022-03-28 12:58:46 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 12:58:46 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (57356ms till timeout)
2022-03-28 12:58:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (86288ms till timeout)
2022-03-28 12:58:47 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (41953ms till timeout)
2022-03-28 12:58:47 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:58:47 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 15 polls
2022-03-28 12:58:47 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-0b062573-connect will have 0 replicas not ready, will try again in 1000 ms (174077ms till timeout)
2022-03-28 12:58:47 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (196984ms till timeout)
2022-03-28 12:58:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (56245ms till timeout)
2022-03-28 12:58:48 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (40952ms till timeout)
2022-03-28 12:58:48 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (85072ms till timeout)
2022-03-28 12:58:48 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 14 polls
2022-03-28 12:58:48 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-0b062573-connect will have 0 replicas not ready, will try again in 1000 ms (172755ms till timeout)
2022-03-28 12:58:48 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:58:48 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (195662ms till timeout)
2022-03-28 12:58:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (55132ms till timeout)
2022-03-28 12:58:49 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (39952ms till timeout)
2022-03-28 12:58:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (83959ms till timeout)
2022-03-28 12:58:49 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Command: oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 12:58:49 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 12:58:49 [ForkJoinPool-1-worker-3] INFO  [TopicST:121] Editing topic via Kafka, settings to partitions 5
2022-03-28 12:58:49 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --bootstrap-server localhost:9092 --alter --topic my-topic-664105056-981624555 --partitions 5
2022-03-28 12:58:49 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:58:49 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 13 polls
2022-03-28 12:58:49 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-0b062573-connect will have 0 replicas not ready, will try again in 1000 ms (171430ms till timeout)
2022-03-28 12:58:49 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (54021ms till timeout)
2022-03-28 12:58:49 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (194331ms till timeout)
2022-03-28 12:58:50 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testCreateTopicAfterUnsupportedOperation is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:58:50 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (38951ms till timeout)
2022-03-28 12:58:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (82828ms till timeout)
2022-03-28 12:58:51 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (37950ms till timeout)
2022-03-28 12:58:51 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (52710ms till timeout)
2022-03-28 12:58:51 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 12 polls
2022-03-28 12:58:51 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-0b062573-connect will have 0 replicas not ready, will try again in 1000 ms (170110ms till timeout)
2022-03-28 12:58:51 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:58:51 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (193014ms till timeout)
2022-03-28 12:58:51 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (81715ms till timeout)
2022-03-28 12:58:52 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (36950ms till timeout)
2022-03-28 12:58:52 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (51598ms till timeout)
2022-03-28 12:58:52 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:58:52 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 11 polls
2022-03-28 12:58:52 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-0b062573-connect will have 0 replicas not ready, will try again in 1000 ms (168679ms till timeout)
2022-03-28 12:58:52 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (80603ms till timeout)
2022-03-28 12:58:52 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (191512ms till timeout)
2022-03-28 12:58:53 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (35948ms till timeout)
2022-03-28 12:58:53 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Command: oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --bootstrap-server localhost:9092 --alter --topic my-topic-664105056-981624555 --partitions 5
2022-03-28 12:58:53 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 12:58:53 [ForkJoinPool-1-worker-3] DEBUG [TopicST:124] Topic my-topic-664105056-981624555 updated from 3 to 5 partitions
2022-03-28 12:58:53 [ForkJoinPool-1-worker-3] INFO  [KafkaTopicUtils:124] Waiting for KafkaTopic change my-topic-664105056-981624555
2022-03-28 12:58:53 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for KafkaTopic change my-topic-664105056-981624555
2022-03-28 12:58:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (50481ms till timeout)
2022-03-28 12:58:53 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Describing topic my-topic-664105056-981624555 using pod CLI
2022-03-28 12:58:53 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic my-topic-664105056-981624555
2022-03-28 12:58:53 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 10 polls
2022-03-28 12:58:53 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-0b062573-connect will have 0 replicas not ready, will try again in 1000 ms (167463ms till timeout)
2022-03-28 12:58:53 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (79359ms till timeout)
2022-03-28 12:58:53 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:58:54 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (190265ms till timeout)
2022-03-28 12:58:54 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (34948ms till timeout)
2022-03-28 12:58:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (49369ms till timeout)
2022-03-28 12:58:55 [ForkJoinPool-1-worker-13] TRACE [SuiteThreadController:210] testCreateTopicAfterUnsupportedOperation is waiting to proceed with execution but current thread exceed maximum of allowed test cases in parallel. (7/6)
2022-03-28 12:58:55 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (78132ms till timeout)
2022-03-28 12:58:55 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:58:55 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 9 polls
2022-03-28 12:58:55 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-0b062573-connect will have 0 replicas not ready, will try again in 1000 ms (166132ms till timeout)
2022-03-28 12:58:55 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (33947ms till timeout)
2022-03-28 12:58:55 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (189040ms till timeout)
2022-03-28 12:58:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (48258ms till timeout)
2022-03-28 12:58:56 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (32947ms till timeout)
2022-03-28 12:58:56 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (76856ms till timeout)
2022-03-28 12:58:56 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 8 polls
2022-03-28 12:58:56 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-0b062573-connect will have 0 replicas not ready, will try again in 1000 ms (164651ms till timeout)
2022-03-28 12:58:56 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:58:56 [ForkJoinPool-1-worker-7] INFO  [PodUtils:189] Message org.apache.kafka.common.errors.ThrottlingQuotaExceededException: The throttling quota has been exceeded. found in create-admin-my-cluster-d4dad0de-kafka-clients-d552k log
2022-03-28 12:58:56 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (187459ms till timeout)
2022-03-28 12:58:56 [ForkJoinPool-1-worker-7] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment create-admin-my-cluster-d4dad0de-kafka-clients deletion
2022-03-28 12:58:56 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for ReplicaSet create-admin-my-cluster-d4dad0de-kafka-clients to be deleted
2022-03-28 12:58:57 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Command: oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic my-topic-664105056-981624555
2022-03-28 12:58:57 [ForkJoinPool-1-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 12:58:57 [ForkJoinPool-1-worker-3] INFO  [TopicST:470] Checking topic my-topic-664105056-981624555 in Kafka topic-cluster-name
2022-03-28 12:58:57 [ForkJoinPool-1-worker-3] DEBUG [TopicST:471] Topic my-topic-664105056-981624555 info: [Topic:my-topic-664105056-981624555, TopicId:aXlPxvZKQiGvih07-kflGw, PartitionCount:5, ReplicationFactor:3, Configs:min.insync.replicas=2,message.format.version=3.0-IV1, Topic:my-topic-664105056-981624555, Partition:0, Leader:2, Replicas:2,1,0, Isr:2,1,0, Topic:my-topic-664105056-981624555, Partition:1, Leader:1, Replicas:1,0,2, Isr:1,0,2, Topic:my-topic-664105056-981624555, Partition:2, Leader:0, Replicas:0,2,1, Isr:0,2,1, Topic:my-topic-664105056-981624555, Partition:3, Leader:2, Replicas:2,1,0, Isr:2,1,0, Topic:my-topic-664105056-981624555, Partition:4, Leader:0, Replicas:0,2,1, Isr:0,2,1]
2022-03-28 12:58:57 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:674] ============================================================================
2022-03-28 12:58:57 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:675] [operators.topic.TopicST - After Each] - Clean up after test
2022-03-28 12:58:57 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:344] ############################################################################
2022-03-28 12:58:57 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:346] In context testCreateTopicViaKafka is everything deleted.
2022-03-28 12:58:57 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:369] ############################################################################
2022-03-28 12:58:57 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:267] testCreateTopicViaKafka - Notifies waiting test cases:[testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testTlsExternalUserWithQuotas, testUserTemplate, testUpdateUser, testTlsExternalUser, testReceiveSimpleMessageTlsScramSha, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testSendSimpleMessageTlsScramSha, testUserWithNameMoreThan64Chars, testTlsUserWithQuotas, testConfigurationFileIsCreated, testScramUserWithQuotas, testDeployAndUnDeployCruiseControl, testConfigurationPerformanceOptions, testCreateTopicAfterUnsupportedOperation] to and randomly select one to start execution
2022-03-28 12:58:57 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:93] [operators.topic.TopicST] - Removing parallel test: testCreateTopicViaKafka
2022-03-28 12:58:57 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:97] [operators.topic.TopicST] - Parallel test count: 6
2022-03-28 12:58:57 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.topic.TopicST.testCreateTopicViaKafka-FINISHED
2022-03-28 12:58:57 [ForkJoinPool-1-worker-3] INFO  [TestSeparator:30] ############################################################################
2022-03-28 12:58:57 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] ReplicaSet create-admin-my-cluster-d4dad0de-kafka-clients to be deleted not ready, will try again in 5000 ms (179891ms till timeout)
2022-03-28 12:58:57 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (31946ms till timeout)
2022-03-28 12:58:57 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (75743ms till timeout)
2022-03-28 12:58:57 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 7 polls
2022-03-28 12:58:57 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-0b062573-connect will have 0 replicas not ready, will try again in 1000 ms (163432ms till timeout)
2022-03-28 12:58:57 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:58:58 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (186242ms till timeout)
2022-03-28 12:58:58 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (30945ms till timeout)
2022-03-28 12:58:58 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (74631ms till timeout)
2022-03-28 12:58:59 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 6 polls
2022-03-28 12:58:59 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-0b062573-connect will have 0 replicas not ready, will try again in 1000 ms (162215ms till timeout)
2022-03-28 12:58:59 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (29945ms till timeout)
2022-03-28 12:58:59 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:58:59 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (185021ms till timeout)
2022-03-28 12:58:59 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (73519ms till timeout)
2022-03-28 12:59:00 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:230] testCreateTopicAfterUnsupportedOperation test now can proceed its execution
2022-03-28 12:59:00 [ForkJoinPool-1-worker-13] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 12:59:00 [ForkJoinPool-1-worker-13] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-455915b9, testThrottlingQuotasCreateAlterPartitions=my-cluster-d4dad0de, testSendSimpleMessageTls=my-cluster-cfce2f5b, testConfigurationReflection=my-cluster-aa0ecd2f, testUserWithNameMoreThan64Chars=my-cluster-ed39a269, testDeployAndUnDeployCruiseControl=my-cluster-534609d0, testCapacityFile=my-cluster-39ee3f62, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-0b062573, testTlsExternalUser=my-cluster-965e9d1e, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-31feb41b, testSendSimpleMessageTlsScramSha=my-cluster-27773859, testCreateTopicAfterUnsupportedOperation=my-cluster-2938df05, testScramUserWithQuotas=my-cluster-90c82d82, testTopicModificationOfReplicationFactor=my-cluster-421b59dd, testKafkaAdminTopicOperations=my-cluster-6acb723c, testConfigurationFileIsCreated=my-cluster-020ceaf8, testCreateTopicViaKafka=my-cluster-43bd601f, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5, testUpdateUser=my-cluster-e16ccf16, testThrottlingQuotasDeleteTopic=my-cluster-b6bd312e, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d, testTlsExternalUserWithQuotas=my-cluster-3e2e11de, testTlsUserWithQuotas=my-cluster-3b191485, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2, testSendingMessagesToNonExistingTopic=my-cluster-64488b2e, testConfigurationPerformanceOptions=my-cluster-bc0ed00a, testReceiveSimpleMessageTls=my-cluster-362cd511, testUserTemplate=my-cluster-157df94e, testMoreReplicasThanAvailableBrokers=my-cluster-1f5cbe38}
2022-03-28 12:59:00 [ForkJoinPool-1-worker-13] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-1329598231-1183867861, testThrottlingQuotasCreateAlterPartitions=my-user-1809876063-64793399, testSendSimpleMessageTls=my-user-111586425-1868899021, testConfigurationReflection=my-user-1580949953-976509997, testUserWithNameMoreThan64Chars=my-user-1048816470-1103470031, testDeployAndUnDeployCruiseControl=my-user-475346616-1293712526, testCapacityFile=my-user-597365340-661017603, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-1153908156-1369197188, testTlsExternalUser=my-user-1295334081-1602801897, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2100837352-1014792902, testSendSimpleMessageTlsScramSha=my-user-507582460-1003875464, testCreateTopicAfterUnsupportedOperation=my-user-271926232-1154944814, testScramUserWithQuotas=my-user-1857980152-2145707698, testTopicModificationOfReplicationFactor=my-user-380015619-613911473, testKafkaAdminTopicOperations=my-user-2014252818-589270412, testConfigurationFileIsCreated=my-user-1609977058-1196012647, testCreateTopicViaKafka=my-user-1379798639-1428271857, testCreatingUsersWithSecretPrefix=my-user-1928899390-1444689439, testUpdateUser=my-user-1694542041-806176635, testThrottlingQuotasDeleteTopic=my-user-284582998-729631500, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-467839358-1504804463, testTlsExternalUserWithQuotas=my-user-1786174319-1274634874, testTlsUserWithQuotas=my-user-420001054-1296284129, testThrottlingQuotasCreateTopic=my-user-896662438-260898887, testSendingMessagesToNonExistingTopic=my-user-248322508-1660982370, testConfigurationPerformanceOptions=my-user-722474509-712960391, testReceiveSimpleMessageTls=my-user-2012197250-2103354173, testUserTemplate=my-user-1253881534-190261188, testMoreReplicasThanAvailableBrokers=my-user-1381751556-1357722017}
2022-03-28 12:59:00 [ForkJoinPool-1-worker-13] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-170116874-1830928683, testThrottlingQuotasCreateAlterPartitions=my-topic-1581297458-635252970, testSendSimpleMessageTls=my-topic-100192180-936335618, testConfigurationReflection=my-topic-1437230713-1827996725, testUserWithNameMoreThan64Chars=my-topic-1603054043-1686316417, testDeployAndUnDeployCruiseControl=my-topic-1815491150-540070233, testCapacityFile=my-topic-1390944843-775067696, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-613737897-291522718, testTlsExternalUser=my-topic-476529121-306489993, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1084162749-1576061083, testSendSimpleMessageTlsScramSha=my-topic-62182003-1963040205, testCreateTopicAfterUnsupportedOperation=my-topic-1502518950-974162326, testScramUserWithQuotas=my-topic-315112689-1043284277, testTopicModificationOfReplicationFactor=my-topic-1016572292-592627742, testKafkaAdminTopicOperations=my-topic-1915664839-460245273, testConfigurationFileIsCreated=my-topic-155202447-1921897899, testCreateTopicViaKafka=my-topic-664105056-981624555, testCreatingUsersWithSecretPrefix=my-topic-2033928996-1209892355, testUpdateUser=my-topic-1239518257-2056139809, testThrottlingQuotasDeleteTopic=my-topic-1036812834-1980126190, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-712754859-1910580676, testTlsExternalUserWithQuotas=my-topic-593679216-1309035072, testTlsUserWithQuotas=my-topic-1752299353-1315959033, testThrottlingQuotasCreateTopic=my-topic-1532506723-1472611013, testSendingMessagesToNonExistingTopic=my-topic-1513508632-1348735863, testConfigurationPerformanceOptions=my-topic-1891296149-770116276, testReceiveSimpleMessageTls=my-topic-1264858218-642520562, testUserTemplate=my-topic-515111334-943904347, testMoreReplicasThanAvailableBrokers=my-topic-2107805499-2107307630}
2022-03-28 12:59:00 [ForkJoinPool-1-worker-13] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-455915b9-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-d4dad0de-kafka-clients, testSendSimpleMessageTls=my-cluster-cfce2f5b-kafka-clients, testConfigurationReflection=my-cluster-aa0ecd2f-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-ed39a269-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-534609d0-kafka-clients, testCapacityFile=my-cluster-39ee3f62-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-0b062573-kafka-clients, testTlsExternalUser=my-cluster-965e9d1e-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-31feb41b-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-27773859-kafka-clients, testCreateTopicAfterUnsupportedOperation=my-cluster-2938df05-kafka-clients, testScramUserWithQuotas=my-cluster-90c82d82-kafka-clients, testTopicModificationOfReplicationFactor=my-cluster-421b59dd-kafka-clients, testKafkaAdminTopicOperations=my-cluster-6acb723c-kafka-clients, testConfigurationFileIsCreated=my-cluster-020ceaf8-kafka-clients, testCreateTopicViaKafka=my-cluster-43bd601f-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5-kafka-clients, testUpdateUser=my-cluster-e16ccf16-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-b6bd312e-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-3e2e11de-kafka-clients, testTlsUserWithQuotas=my-cluster-3b191485-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2-kafka-clients, testSendingMessagesToNonExistingTopic=my-cluster-64488b2e-kafka-clients, testConfigurationPerformanceOptions=my-cluster-bc0ed00a-kafka-clients, testReceiveSimpleMessageTls=my-cluster-362cd511-kafka-clients, testUserTemplate=my-cluster-157df94e-kafka-clients, testMoreReplicasThanAvailableBrokers=my-cluster-1f5cbe38-kafka-clients}
2022-03-28 12:59:00 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update KafkaTopic topic-with-replication-to-change in namespace topic-st
2022-03-28 12:59:00 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (28944ms till timeout)
2022-03-28 12:59:00 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:topic-with-replication-to-change
2022-03-28 12:59:00 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:433] Wait for KafkaTopic: topic-with-replication-to-change will have desired state: Ready
2022-03-28 12:59:00 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaTopic: topic-with-replication-to-change will have desired state: Ready
2022-03-28 12:59:00 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 5 polls
2022-03-28 12:59:00 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-0b062573-connect will have 0 replicas not ready, will try again in 1000 ms (160796ms till timeout)
2022-03-28 12:59:00 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:00 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:444] KafkaTopic: topic-with-replication-to-change is in desired state: Ready
2022-03-28 12:59:00 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (183597ms till timeout)
2022-03-28 12:59:00 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (72362ms till timeout)
2022-03-28 12:59:01 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:433] Wait for KafkaTopic: topic-with-replication-to-change will have desired state: NotReady
2022-03-28 12:59:01 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaTopic: topic-with-replication-to-change will have desired state: NotReady
2022-03-28 12:59:01 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] KafkaTopic: topic-with-replication-to-change will have desired state: NotReady not ready, will try again in 1000 ms (179891ms till timeout)
2022-03-28 12:59:01 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (27944ms till timeout)
2022-03-28 12:59:01 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 4 polls
2022-03-28 12:59:01 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-0b062573-connect will have 0 replicas not ready, will try again in 1000 ms (159579ms till timeout)
2022-03-28 12:59:01 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:01 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (182373ms till timeout)
2022-03-28 12:59:02 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (71246ms till timeout)
2022-03-28 12:59:02 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (26943ms till timeout)
2022-03-28 12:59:02 [ForkJoinPool-1-worker-7] DEBUG [JobUtils:40] Job create-admin-my-cluster-d4dad0de-kafka-clients was deleted
2022-03-28 12:59:02 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:444] KafkaTopic: topic-with-replication-to-change is in desired state: NotReady
2022-03-28 12:59:02 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-30
2022-03-28 12:59:02 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:155] Create/Update KafkaTopic another-topic in namespace topic-st
2022-03-28 12:59:02 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:another-topic
2022-03-28 12:59:02 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 3 polls
2022-03-28 12:59:02 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-0b062573-connect will have 0 replicas not ready, will try again in 1000 ms (158323ms till timeout)
2022-03-28 12:59:02 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:433] Wait for KafkaTopic: another-topic will have desired state: Ready
2022-03-28 12:59:02 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaTopic: another-topic will have desired state: Ready
2022-03-28 12:59:03 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-30
2022-03-28 12:59:03 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 12:59:03 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-31
2022-03-28 12:59:03 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:03 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (25943ms till timeout)
2022-03-28 12:59:03 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] KafkaTopic: another-topic will have desired state: Ready not ready, will try again in 1000 ms (179792ms till timeout)
2022-03-28 12:59:03 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (70103ms till timeout)
2022-03-28 12:59:03 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (181021ms till timeout)
2022-03-28 12:59:03 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-31
2022-03-28 12:59:03 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 12:59:03 [ForkJoinPool-1-worker-7] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-32
2022-03-28 12:59:04 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (24942ms till timeout)
2022-03-28 12:59:04 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-32
2022-03-28 12:59:04 [ForkJoinPool-1-worker-7] DEBUG [Exec:419] Return code: 0
2022-03-28 12:59:04 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update Job create-admin-my-cluster-d4dad0de-kafka-clients in namespace throttling-quota-st
2022-03-28 12:59:04 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:444] KafkaTopic: another-topic is in desired state: Ready
2022-03-28 12:59:04 [ForkJoinPool-1-worker-13] INFO  [TopicST:456] Checking topic topic-with-replication-to-change in Kafka
2022-03-28 12:59:04 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 12:59:04 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 2 polls
2022-03-28 12:59:04 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-0b062573-connect will have 0 replicas not ready, will try again in 1000 ms (157001ms till timeout)
2022-03-28 12:59:04 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (68983ms till timeout)
2022-03-28 12:59:04 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:04 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:create-admin-my-cluster-d4dad0de-kafka-clients
2022-03-28 12:59:04 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (179803ms till timeout)
2022-03-28 12:59:04 [ForkJoinPool-1-worker-7] INFO  [JobUtils:81] Waiting for job: create-admin-my-cluster-d4dad0de-kafka-clients will be in active state
2022-03-28 12:59:04 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 12:59:04 [ForkJoinPool-1-worker-7] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 12:59:04 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 12:59:04 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (299888ms till timeout)
2022-03-28 12:59:05 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (23941ms till timeout)
2022-03-28 12:59:05 [ForkJoinPool-1-worker-15] INFO  [PodUtils:225] Pod replicas gonna be stable in 1 polls
2022-03-28 12:59:05 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176]  Podmy-cluster-0b062573-connect will have 0 replicas not ready, will try again in 1000 ms (155783ms till timeout)
2022-03-28 12:59:05 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (67775ms till timeout)
2022-03-28 12:59:05 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:05 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (178583ms till timeout)
2022-03-28 12:59:06 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (298777ms till timeout)
2022-03-28 12:59:06 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (22941ms till timeout)
2022-03-28 12:59:06 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (66463ms till timeout)
2022-03-28 12:59:06 [ForkJoinPool-1-worker-15] INFO  [PodUtils:217] Pod replicas are stable for 20 polls intervals
2022-03-28 12:59:06 [ForkJoinPool-1-worker-15] INFO  [PodUtils:228] Pod my-cluster-0b062573-connect has 0 replicas
2022-03-28 12:59:06 [ForkJoinPool-1-worker-15] INFO  [ReconciliationST:108] Setting annotation to "false" and creating KafkaConnector
2022-03-28 12:59:06 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:07 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (177255ms till timeout)
2022-03-28 12:59:07 [ForkJoinPool-1-worker-15] INFO  [DeploymentUtils:161] Wait for Deployment: my-cluster-0b062573-connect will be ready
2022-03-28 12:59:07 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Wait for Deployment: my-cluster-0b062573-connect will be ready
2022-03-28 12:59:07 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (21940ms till timeout)
2022-03-28 12:59:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (297560ms till timeout)
2022-03-28 12:59:07 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (479787ms till timeout)
2022-03-28 12:59:07 [ForkJoinPool-1-worker-13] INFO  [Exec:417] Command: oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 12:59:07 [ForkJoinPool-1-worker-13] INFO  [Exec:417] Return code: 0
2022-03-28 12:59:07 [ForkJoinPool-1-worker-13] INFO  [TopicST:461] Checking in KafkaTopic CR that topic topic-with-replication-to-change exists
2022-03-28 12:59:07 [ForkJoinPool-1-worker-13] INFO  [TopicST:456] Checking topic another-topic in Kafka
2022-03-28 12:59:07 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 12:59:07 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (65351ms till timeout)
2022-03-28 12:59:08 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:08 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (20940ms till timeout)
2022-03-28 12:59:08 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (176035ms till timeout)
2022-03-28 12:59:08 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (296448ms till timeout)
2022-03-28 12:59:08 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (478676ms till timeout)
2022-03-28 12:59:09 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (64239ms till timeout)
2022-03-28 12:59:09 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (19939ms till timeout)
2022-03-28 12:59:09 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (295337ms till timeout)
2022-03-28 12:59:09 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (174706ms till timeout)
2022-03-28 12:59:09 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (477564ms till timeout)
2022-03-28 12:59:10 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (18939ms till timeout)
2022-03-28 12:59:10 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (63023ms till timeout)
2022-03-28 12:59:10 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (294004ms till timeout)
2022-03-28 12:59:10 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:10 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (476340ms till timeout)
2022-03-28 12:59:10 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (173371ms till timeout)
2022-03-28 12:59:11 [ForkJoinPool-1-worker-13] INFO  [Exec:417] Command: oc --namespace topic-st exec topic-cluster-name-kafka-0 -- /bin/bash -c bin/kafka-topics.sh --list --bootstrap-server localhost:9092
2022-03-28 12:59:11 [ForkJoinPool-1-worker-13] INFO  [Exec:417] Return code: 0
2022-03-28 12:59:11 [ForkJoinPool-1-worker-13] INFO  [TopicST:461] Checking in KafkaTopic CR that topic another-topic exists
2022-03-28 12:59:11 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace topic-st delete kafkatopic topic-with-replication-to-change
2022-03-28 12:59:11 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (17938ms till timeout)
2022-03-28 12:59:11 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (61809ms till timeout)
2022-03-28 12:59:11 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace topic-st delete kafkatopic topic-with-replication-to-change
2022-03-28 12:59:11 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 12:59:11 [ForkJoinPool-1-worker-13] INFO  [KafkaTopicUtils:104] Waiting for KafkaTopic topic-with-replication-to-change deletion
2022-03-28 12:59:11 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaTopic deletion topic-with-replication-to-change
2022-03-28 12:59:11 [ForkJoinPool-1-worker-13] TRACE [Exec:248] Running command - oc --namespace topic-st delete kafkatopic another-topic
2022-03-28 12:59:11 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (292891ms till timeout)
2022-03-28 12:59:12 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (475121ms till timeout)
2022-03-28 12:59:12 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:12 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (16938ms till timeout)
2022-03-28 12:59:12 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Command: oc --namespace topic-st delete kafkatopic another-topic
2022-03-28 12:59:12 [ForkJoinPool-1-worker-13] DEBUG [Exec:419] Return code: 0
2022-03-28 12:59:12 [ForkJoinPool-1-worker-13] INFO  [KafkaTopicUtils:104] Waiting for KafkaTopic another-topic deletion
2022-03-28 12:59:12 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for KafkaTopic deletion another-topic
2022-03-28 12:59:12 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (172047ms till timeout)
2022-03-28 12:59:12 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:674] ============================================================================
2022-03-28 12:59:12 [ForkJoinPool-1-worker-13] DEBUG [AbstractST:675] [operators.topic.TopicST - After Each] - Clean up after test
2022-03-28 12:59:12 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:344] ############################################################################
2022-03-28 12:59:12 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:348] Delete all resources for testCreateTopicAfterUnsupportedOperation
2022-03-28 12:59:12 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of KafkaTopic another-topic in namespace topic-st
2022-03-28 12:59:12 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:another-topic
2022-03-28 12:59:12 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of KafkaTopic topic-with-replication-to-change in namespace topic-st
2022-03-28 12:59:12 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (60489ms till timeout)
2022-03-28 12:59:12 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:topic-with-replication-to-change
2022-03-28 12:59:12 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:369] ############################################################################
2022-03-28 12:59:12 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:267] testCreateTopicAfterUnsupportedOperation - Notifies waiting test cases:[testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testTlsExternalUserWithQuotas, testUserTemplate, testUpdateUser, testTlsExternalUser, testReceiveSimpleMessageTlsScramSha, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testSendSimpleMessageTlsScramSha, testUserWithNameMoreThan64Chars, testTlsUserWithQuotas, testConfigurationFileIsCreated, testScramUserWithQuotas, testDeployAndUnDeployCruiseControl, testConfigurationPerformanceOptions] to and randomly select one to start execution
2022-03-28 12:59:12 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:93] [operators.topic.TopicST] - Removing parallel test: testCreateTopicAfterUnsupportedOperation
2022-03-28 12:59:12 [ForkJoinPool-1-worker-13] DEBUG [SuiteThreadController:97] [operators.topic.TopicST] - Parallel test count: 5
2022-03-28 12:59:12 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.topic.TopicST.testCreateTopicAfterUnsupportedOperation-FINISHED
2022-03-28 12:59:12 [ForkJoinPool-1-worker-13] INFO  [TestSeparator:30] ############################################################################
2022-03-28 12:59:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (291779ms till timeout)
2022-03-28 12:59:13 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (474007ms till timeout)
2022-03-28 12:59:13 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (15936ms till timeout)
2022-03-28 12:59:13 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:13 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (170830ms till timeout)
2022-03-28 12:59:14 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (59274ms till timeout)
2022-03-28 12:59:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (290668ms till timeout)
2022-03-28 12:59:14 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (14936ms till timeout)
2022-03-28 12:59:14 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (472895ms till timeout)
2022-03-28 12:59:14 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:14 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (169613ms till timeout)
2022-03-28 12:59:15 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (13935ms till timeout)
2022-03-28 12:59:15 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (58055ms till timeout)
2022-03-28 12:59:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (289493ms till timeout)
2022-03-28 12:59:15 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (471783ms till timeout)
2022-03-28 12:59:15 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:15 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (168395ms till timeout)
2022-03-28 12:59:16 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (12934ms till timeout)
2022-03-28 12:59:16 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (56841ms till timeout)
2022-03-28 12:59:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (288279ms till timeout)
2022-03-28 12:59:16 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (470607ms till timeout)
2022-03-28 12:59:17 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:17 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (167175ms till timeout)
2022-03-28 12:59:17 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (11934ms till timeout)
2022-03-28 12:59:17 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (55621ms till timeout)
2022-03-28 12:59:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (287161ms till timeout)
2022-03-28 12:59:17 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (469394ms till timeout)
2022-03-28 12:59:18 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (10933ms till timeout)
2022-03-28 12:59:18 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:18 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (165955ms till timeout)
2022-03-28 12:59:18 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (54406ms till timeout)
2022-03-28 12:59:18 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (468278ms till timeout)
2022-03-28 12:59:19 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (285840ms till timeout)
2022-03-28 12:59:19 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (9933ms till timeout)
2022-03-28 12:59:19 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:19 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (164724ms till timeout)
2022-03-28 12:59:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (53191ms till timeout)
2022-03-28 12:59:20 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (467063ms till timeout)
2022-03-28 12:59:20 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (8932ms till timeout)
2022-03-28 12:59:20 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (284628ms till timeout)
2022-03-28 12:59:20 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:20 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (163504ms till timeout)
2022-03-28 12:59:21 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (7932ms till timeout)
2022-03-28 12:59:21 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (51974ms till timeout)
2022-03-28 12:59:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (465848ms till timeout)
2022-03-28 12:59:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (283411ms till timeout)
2022-03-28 12:59:21 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:22 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (162282ms till timeout)
2022-03-28 12:59:22 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (6931ms till timeout)
2022-03-28 12:59:22 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (50756ms till timeout)
2022-03-28 12:59:22 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (464631ms till timeout)
2022-03-28 12:59:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (282192ms till timeout)
2022-03-28 12:59:23 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:23 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (5931ms till timeout)
2022-03-28 12:59:23 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (161064ms till timeout)
2022-03-28 12:59:23 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (49541ms till timeout)
2022-03-28 12:59:23 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (463413ms till timeout)
2022-03-28 12:59:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (280979ms till timeout)
2022-03-28 12:59:24 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (4930ms till timeout)
2022-03-28 12:59:24 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:24 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (159843ms till timeout)
2022-03-28 12:59:24 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (48325ms till timeout)
2022-03-28 12:59:24 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (462197ms till timeout)
2022-03-28 12:59:25 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (279762ms till timeout)
2022-03-28 12:59:25 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (3929ms till timeout)
2022-03-28 12:59:25 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:25 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (158626ms till timeout)
2022-03-28 12:59:26 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (2929ms till timeout)
2022-03-28 12:59:26 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (47108ms till timeout)
2022-03-28 12:59:26 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (460981ms till timeout)
2022-03-28 12:59:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (278524ms till timeout)
2022-03-28 12:59:26 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:26 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (157406ms till timeout)
2022-03-28 12:59:27 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 1000 ms (1928ms till timeout)
2022-03-28 12:59:27 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (45892ms till timeout)
2022-03-28 12:59:27 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (459762ms till timeout)
2022-03-28 12:59:27 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (277323ms till timeout)
2022-03-28 12:59:28 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:28 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (156183ms till timeout)
2022-03-28 12:59:28 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties not ready, will try again in 928 ms (928ms till timeout)
2022-03-28 12:59:28 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (44673ms till timeout)
2022-03-28 12:59:28 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (458548ms till timeout)
2022-03-28 12:59:28 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (276110ms till timeout)
2022-03-28 12:59:29 [ForkJoinPool-1-worker-1] ERROR [TestUtils:162] Exception waiting for Verify that kafka configuration {cluster-name=my-cluster-534609d0} has correct cruise control metric reporter properties, null
2022-03-28 12:59:29 [ForkJoinPool-1-worker-1] INFO  [CruiseControlConfigurationST:126] Cruise Control topics will not be deleted and will stay in the Kafka cluster
2022-03-28 12:59:29 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Verify that kafka contains cruise control topics with related configuration.
2022-03-28 12:59:29 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:29 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Verify that kafka contains cruise control topics with related configuration.
2022-03-28 12:59:29 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (154854ms till timeout)
2022-03-28 12:59:29 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (457392ms till timeout)
2022-03-28 12:59:29 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (43412ms till timeout)
2022-03-28 12:59:29 [ForkJoinPool-1-worker-1] INFO  [CruiseControlConfigurationST:130] Adding Cruise Control to the classic Kafka.
2022-03-28 12:59:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (274848ms till timeout)
2022-03-28 12:59:30 [ForkJoinPool-1-worker-1] INFO  [RollingUpdateUtils:73] Waiting for component with name: my-cluster-534609d0-kafka rolling update
2022-03-28 12:59:30 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:74] Waiting for rolling update of component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})
2022-03-28 12:59:30 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for component with name my-cluster-534609d0-kafka rolling update
2022-03-28 12:59:30 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=2fe26899-cd27-4864-948c-40c8f3bbb961}
2022-03-28 12:59:30 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=2fe26899-cd27-4864-948c-40c8f3bbb961}
2022-03-28 12:59:30 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=2fe26899-cd27-4864-948c-40c8f3bbb961}
2022-03-28 12:59:30 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-534609d0-kafka-0 hasn't rolled
2022-03-28 12:59:30 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-534609d0-kafka rolling update not ready, will try again in 5000 ms (1799785ms till timeout)
2022-03-28 12:59:30 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:30 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (153637ms till timeout)
2022-03-28 12:59:30 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (456279ms till timeout)
2022-03-28 12:59:31 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (42292ms till timeout)
2022-03-28 12:59:31 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (273722ms till timeout)
2022-03-28 12:59:31 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:31 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (152418ms till timeout)
2022-03-28 12:59:32 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (455169ms till timeout)
2022-03-28 12:59:32 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (41180ms till timeout)
2022-03-28 12:59:32 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (272613ms till timeout)
2022-03-28 12:59:33 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:33 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (454059ms till timeout)
2022-03-28 12:59:33 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (151094ms till timeout)
2022-03-28 12:59:33 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (39932ms till timeout)
2022-03-28 12:59:33 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (271400ms till timeout)
2022-03-28 12:59:34 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (452948ms till timeout)
2022-03-28 12:59:34 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:34 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (149871ms till timeout)
2022-03-28 12:59:34 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (270284ms till timeout)
2022-03-28 12:59:34 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (38636ms till timeout)
2022-03-28 12:59:35 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=2fe26899-cd27-4864-948c-40c8f3bbb961}
2022-03-28 12:59:35 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (451836ms till timeout)
2022-03-28 12:59:35 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=2fe26899-cd27-4864-948c-40c8f3bbb961}
2022-03-28 12:59:35 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=2fe26899-cd27-4864-948c-40c8f3bbb961}
2022-03-28 12:59:35 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-534609d0-kafka-0 hasn't rolled
2022-03-28 12:59:35 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-534609d0-kafka rolling update not ready, will try again in 5000 ms (1794564ms till timeout)
2022-03-28 12:59:35 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:35 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (148655ms till timeout)
2022-03-28 12:59:35 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (269070ms till timeout)
2022-03-28 12:59:35 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (37417ms till timeout)
2022-03-28 12:59:36 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (450727ms till timeout)
2022-03-28 12:59:36 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:36 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (267959ms till timeout)
2022-03-28 12:59:36 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (147327ms till timeout)
2022-03-28 12:59:37 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (36193ms till timeout)
2022-03-28 12:59:37 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (449617ms till timeout)
2022-03-28 12:59:37 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (266847ms till timeout)
2022-03-28 12:59:38 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:38 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (146108ms till timeout)
2022-03-28 12:59:38 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (34981ms till timeout)
2022-03-28 12:59:38 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (448507ms till timeout)
2022-03-28 12:59:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (265735ms till timeout)
2022-03-28 12:59:39 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:39 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (33867ms till timeout)
2022-03-28 12:59:39 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (144778ms till timeout)
2022-03-28 12:59:39 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (447396ms till timeout)
2022-03-28 12:59:40 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (264624ms till timeout)
2022-03-28 12:59:40 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=2fe26899-cd27-4864-948c-40c8f3bbb961}
2022-03-28 12:59:40 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (32651ms till timeout)
2022-03-28 12:59:40 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:40 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-534609d0-kafka-0=db6cdceb-78d5-4f60-8449-13b272fa0d7c, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=2fe26899-cd27-4864-948c-40c8f3bbb961}
2022-03-28 12:59:40 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-534609d0-kafka-0=db6cdceb-78d5-4f60-8449-13b272fa0d7c, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=2fe26899-cd27-4864-948c-40c8f3bbb961}
2022-03-28 12:59:40 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-534609d0-kafka-1 hasn't rolled
2022-03-28 12:59:40 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-534609d0-kafka rolling update not ready, will try again in 5000 ms (1789359ms till timeout)
2022-03-28 12:59:40 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (143455ms till timeout)
2022-03-28 12:59:40 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (446207ms till timeout)
2022-03-28 12:59:41 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (263512ms till timeout)
2022-03-28 12:59:41 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (31436ms till timeout)
2022-03-28 12:59:41 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:42 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (445095ms till timeout)
2022-03-28 12:59:42 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (142129ms till timeout)
2022-03-28 12:59:42 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (262400ms till timeout)
2022-03-28 12:59:43 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (30221ms till timeout)
2022-03-28 12:59:43 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (443984ms till timeout)
2022-03-28 12:59:43 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:43 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (140911ms till timeout)
2022-03-28 12:59:43 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (261289ms till timeout)
2022-03-28 12:59:44 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (29008ms till timeout)
2022-03-28 12:59:44 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (442871ms till timeout)
2022-03-28 12:59:44 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:44 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (139693ms till timeout)
2022-03-28 12:59:44 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (260105ms till timeout)
2022-03-28 12:59:45 [ForkJoinPool-1-worker-9] INFO  [PodUtils:189] Message Successfully removed all 100 found in delete-admin-my-cluster-6acb723c-kafka-clients-qhjwg log
2022-03-28 12:59:45 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (441663ms till timeout)
2022-03-28 12:59:45 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment delete-admin-my-cluster-6acb723c-kafka-clients deletion
2022-03-28 12:59:45 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for ReplicaSet delete-admin-my-cluster-6acb723c-kafka-clients to be deleted
2022-03-28 12:59:45 [ForkJoinPool-1-worker-9] DEBUG [JobUtils:40] Job delete-admin-my-cluster-6acb723c-kafka-clients was deleted
2022-03-28 12:59:45 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:155] Create/Update Job list-admin-my-cluster-6acb723c-kafka-clients in namespace throttling-quota-st
2022-03-28 12:59:45 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=2fe26899-cd27-4864-948c-40c8f3bbb961}
2022-03-28 12:59:45 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:45 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-534609d0-kafka-0=db6cdceb-78d5-4f60-8449-13b272fa0d7c, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=2fe26899-cd27-4864-948c-40c8f3bbb961}
2022-03-28 12:59:45 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-534609d0-kafka-0=db6cdceb-78d5-4f60-8449-13b272fa0d7c, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=2fe26899-cd27-4864-948c-40c8f3bbb961}
2022-03-28 12:59:45 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-534609d0-kafka-1 hasn't rolled
2022-03-28 12:59:45 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-534609d0-kafka rolling update not ready, will try again in 5000 ms (1784172ms till timeout)
2022-03-28 12:59:46 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (258794ms till timeout)
2022-03-28 12:59:46 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:list-admin-my-cluster-6acb723c-kafka-clients
2022-03-28 12:59:46 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (138270ms till timeout)
2022-03-28 12:59:46 [ForkJoinPool-1-worker-9] INFO  [JobUtils:81] Waiting for job: list-admin-my-cluster-6acb723c-kafka-clients will be in active state
2022-03-28 12:59:46 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 12:59:46 [ForkJoinPool-1-worker-9] INFO  [ClientUtils:76] Waiting for producer/consumer:list-admin-my-cluster-6acb723c-kafka-clients to finished
2022-03-28 12:59:46 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 12:59:46 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job list-admin-my-cluster-6acb723c-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:46 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (119782ms till timeout)
2022-03-28 12:59:46 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (440554ms till timeout)
2022-03-28 12:59:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (257683ms till timeout)
2022-03-28 12:59:47 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:47 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (136943ms till timeout)
2022-03-28 12:59:47 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job list-admin-my-cluster-6acb723c-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:47 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (118565ms till timeout)
2022-03-28 12:59:47 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (439370ms till timeout)
2022-03-28 12:59:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (256566ms till timeout)
2022-03-28 12:59:48 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:48 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (135725ms till timeout)
2022-03-28 12:59:48 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job list-admin-my-cluster-6acb723c-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:48 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (438260ms till timeout)
2022-03-28 12:59:49 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (117240ms till timeout)
2022-03-28 12:59:49 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (255452ms till timeout)
2022-03-28 12:59:49 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:49 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (134507ms till timeout)
2022-03-28 12:59:50 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (437149ms till timeout)
2022-03-28 12:59:50 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job list-admin-my-cluster-6acb723c-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:41Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:50 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (116018ms till timeout)
2022-03-28 12:59:50 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (254342ms till timeout)
2022-03-28 12:59:50 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:50 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=2fe26899-cd27-4864-948c-40c8f3bbb961}
2022-03-28 12:59:51 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (133286ms till timeout)
2022-03-28 12:59:51 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-534609d0-kafka-0=db6cdceb-78d5-4f60-8449-13b272fa0d7c, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=2fe26899-cd27-4864-948c-40c8f3bbb961}
2022-03-28 12:59:51 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-534609d0-kafka-0=db6cdceb-78d5-4f60-8449-13b272fa0d7c, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=2fe26899-cd27-4864-948c-40c8f3bbb961}
2022-03-28 12:59:51 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-534609d0-kafka-1 hasn't rolled
2022-03-28 12:59:51 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-534609d0-kafka rolling update not ready, will try again in 5000 ms (1778867ms till timeout)
2022-03-28 12:59:51 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (435923ms till timeout)
2022-03-28 12:59:51 [ForkJoinPool-1-worker-9] DEBUG [ClientUtils:79] Job list-admin-my-cluster-6acb723c-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T12:59:46Z, conditions=[JobCondition(lastProbeTime=2022-03-28T12:59:46Z, lastTransitionTime=2022-03-28T12:59:46Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T12:59:41Z, succeeded=1, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:51 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (253159ms till timeout)
2022-03-28 12:59:51 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:674] ============================================================================
2022-03-28 12:59:51 [ForkJoinPool-1-worker-9] DEBUG [AbstractST:675] [operators.topic.ThrottlingQuotaST - After Each] - Clean up after test
2022-03-28 12:59:51 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:344] ############################################################################
2022-03-28 12:59:51 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:348] Delete all resources for testKafkaAdminTopicOperations
2022-03-28 12:59:51 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:241] Delete of Job list-admin-my-cluster-6acb723c-kafka-clients in namespace throttling-quota-st
2022-03-28 12:59:51 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of Job create-admin-my-cluster-6acb723c-kafka-clients in namespace throttling-quota-st
2022-03-28 12:59:51 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:list-admin-my-cluster-6acb723c-kafka-clients
2022-03-28 12:59:52 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:create-admin-my-cluster-6acb723c-kafka-clients
2022-03-28 12:59:52 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:241] Delete of Job list-admin-my-cluster-6acb723c-kafka-clients in namespace throttling-quota-st
2022-03-28 12:59:52 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:52 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of KafkaUser my-user-2014252818-589270412 in namespace throttling-quota-st
2022-03-28 12:59:52 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:list-admin-my-cluster-6acb723c-kafka-clients
2022-03-28 12:59:52 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (131885ms till timeout)
2022-03-28 12:59:52 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (434739ms till timeout)
2022-03-28 12:59:52 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-2014252818-589270412
2022-03-28 12:59:52 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:241] Delete of Job delete-admin-my-cluster-6acb723c-kafka-clients in namespace throttling-quota-st
2022-03-28 12:59:52 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:delete-admin-my-cluster-6acb723c-kafka-clients
2022-03-28 12:59:52 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (251981ms till timeout)
2022-03-28 12:59:52 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:369] ############################################################################
2022-03-28 12:59:52 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:267] testKafkaAdminTopicOperations - Notifies waiting test cases:[testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testTlsExternalUserWithQuotas, testUserTemplate, testUpdateUser, testTlsExternalUser, testReceiveSimpleMessageTlsScramSha, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testSendSimpleMessageTlsScramSha, testUserWithNameMoreThan64Chars, testTlsUserWithQuotas, testConfigurationFileIsCreated, testScramUserWithQuotas, testDeployAndUnDeployCruiseControl, testConfigurationPerformanceOptions] to and randomly select one to start execution
2022-03-28 12:59:52 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:93] [operators.topic.ThrottlingQuotaST] - Removing parallel test: testKafkaAdminTopicOperations
2022-03-28 12:59:52 [ForkJoinPool-1-worker-9] DEBUG [SuiteThreadController:97] [operators.topic.ThrottlingQuotaST] - Parallel test count: 4
2022-03-28 12:59:52 [ForkJoinPool-1-worker-9] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testKafkaAdminTopicOperations-FINISHED
2022-03-28 12:59:52 [ForkJoinPool-1-worker-9] INFO  [TestSeparator:30] ############################################################################
2022-03-28 12:59:53 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:53 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (433527ms till timeout)
2022-03-28 12:59:53 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (130562ms till timeout)
2022-03-28 12:59:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (250870ms till timeout)
2022-03-28 12:59:54 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (432417ms till timeout)
2022-03-28 12:59:54 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:54 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129341ms till timeout)
2022-03-28 12:59:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (249754ms till timeout)
2022-03-28 12:59:55 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (431305ms till timeout)
2022-03-28 12:59:56 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:56 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (128124ms till timeout)
2022-03-28 12:59:56 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=2fe26899-cd27-4864-948c-40c8f3bbb961}
2022-03-28 12:59:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (248535ms till timeout)
2022-03-28 12:59:56 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-534609d0-kafka-0=db6cdceb-78d5-4f60-8449-13b272fa0d7c, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=2fe26899-cd27-4864-948c-40c8f3bbb961}
2022-03-28 12:59:56 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-534609d0-kafka-0=db6cdceb-78d5-4f60-8449-13b272fa0d7c, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=2fe26899-cd27-4864-948c-40c8f3bbb961}
2022-03-28 12:59:56 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-534609d0-kafka-1 hasn't rolled
2022-03-28 12:59:56 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-534609d0-kafka rolling update not ready, will try again in 5000 ms (1773705ms till timeout)
2022-03-28 12:59:56 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (430191ms till timeout)
2022-03-28 12:59:57 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:57 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (126902ms till timeout)
2022-03-28 12:59:57 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (247314ms till timeout)
2022-03-28 12:59:58 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (429082ms till timeout)
2022-03-28 12:59:58 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:58 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (246176ms till timeout)
2022-03-28 12:59:58 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (125540ms till timeout)
2022-03-28 12:59:59 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (427971ms till timeout)
2022-03-28 12:59:59 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (245064ms till timeout)
2022-03-28 12:59:59 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 12:59:59 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (124322ms till timeout)
2022-03-28 13:00:00 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (426862ms till timeout)
2022-03-28 13:00:00 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (243946ms till timeout)
2022-03-28 13:00:01 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:00:01 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (123104ms till timeout)
2022-03-28 13:00:01 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=2fe26899-cd27-4864-948c-40c8f3bbb961}
2022-03-28 13:00:01 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (425750ms till timeout)
2022-03-28 13:00:01 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-534609d0-kafka-0=db6cdceb-78d5-4f60-8449-13b272fa0d7c, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=2fe26899-cd27-4864-948c-40c8f3bbb961}
2022-03-28 13:00:01 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-534609d0-kafka-0=db6cdceb-78d5-4f60-8449-13b272fa0d7c, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=2fe26899-cd27-4864-948c-40c8f3bbb961}
2022-03-28 13:00:01 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-534609d0-kafka-1 hasn't rolled
2022-03-28 13:00:01 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-534609d0-kafka rolling update not ready, will try again in 5000 ms (1768480ms till timeout)
2022-03-28 13:00:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (242832ms till timeout)
2022-03-28 13:00:02 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T12:59:57Z, conditions=[JobCondition(lastProbeTime=2022-03-28T12:59:57Z, lastTransitionTime=2022-03-28T12:59:57Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T12:58:19Z, succeeded=1, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:00:02 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (424638ms till timeout)
2022-03-28 13:00:02 [ForkJoinPool-1-worker-11] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 13:00:02 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 13:00:02 [ForkJoinPool-1-worker-11] INFO  [PodUtils:189] Message All topics created found in create-admin-my-cluster-b6bd312e-kafka-clients-g5df8 log
2022-03-28 13:00:02 [ForkJoinPool-1-worker-11] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment create-admin-my-cluster-b6bd312e-kafka-clients deletion
2022-03-28 13:00:02 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for ReplicaSet create-admin-my-cluster-b6bd312e-kafka-clients to be deleted
2022-03-28 13:00:02 [ForkJoinPool-1-worker-11] DEBUG [JobUtils:40] Job create-admin-my-cluster-b6bd312e-kafka-clients was deleted
2022-03-28 13:00:02 [ForkJoinPool-1-worker-11] INFO  [ThrottlingQuotaST:112] Executing 3/5 iteration.
2022-03-28 13:00:02 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st
2022-03-28 13:00:03 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:create-admin-my-cluster-b6bd312e-kafka-clients
2022-03-28 13:00:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (241633ms till timeout)
2022-03-28 13:00:03 [ForkJoinPool-1-worker-11] INFO  [JobUtils:81] Waiting for job: create-admin-my-cluster-b6bd312e-kafka-clients will be in active state
2022-03-28 13:00:03 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 13:00:03 [ForkJoinPool-1-worker-11] INFO  [ClientUtils:76] Waiting for producer/consumer:create-admin-my-cluster-b6bd312e-kafka-clients to finished
2022-03-28 13:00:03 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 13:00:03 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:00:03 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (219782ms till timeout)
2022-03-28 13:00:03 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (423424ms till timeout)
2022-03-28 13:00:04 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (240516ms till timeout)
2022-03-28 13:00:04 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:00:04 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (422315ms till timeout)
2022-03-28 13:00:04 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (218459ms till timeout)
2022-03-28 13:00:05 [ForkJoinPool-1-worker-7] INFO  [PodUtils:189] Message All topics created found in create-admin-my-cluster-d4dad0de-kafka-clients-ldlk5 log
2022-03-28 13:00:05 [ForkJoinPool-1-worker-7] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment create-admin-my-cluster-d4dad0de-kafka-clients deletion
2022-03-28 13:00:05 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for ReplicaSet create-admin-my-cluster-d4dad0de-kafka-clients to be deleted
2022-03-28 13:00:05 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] ReplicaSet create-admin-my-cluster-d4dad0de-kafka-clients to be deleted not ready, will try again in 5000 ms (179889ms till timeout)
2022-03-28 13:00:05 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (421197ms till timeout)
2022-03-28 13:00:06 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:00:06 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (217232ms till timeout)
2022-03-28 13:00:06 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=2fe26899-cd27-4864-948c-40c8f3bbb961}
2022-03-28 13:00:06 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-534609d0-kafka-0=db6cdceb-78d5-4f60-8449-13b272fa0d7c, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=2fe26899-cd27-4864-948c-40c8f3bbb961}
2022-03-28 13:00:06 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-534609d0-kafka-0=db6cdceb-78d5-4f60-8449-13b272fa0d7c, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=2fe26899-cd27-4864-948c-40c8f3bbb961}
2022-03-28 13:00:06 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-534609d0-kafka-1 hasn't rolled
2022-03-28 13:00:06 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-534609d0-kafka rolling update not ready, will try again in 5000 ms (1763264ms till timeout)
2022-03-28 13:00:07 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (420082ms till timeout)
2022-03-28 13:00:07 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:00:07 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (216005ms till timeout)
2022-03-28 13:00:08 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (418967ms till timeout)
2022-03-28 13:00:08 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:00:08 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (214788ms till timeout)
2022-03-28 13:00:09 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (417856ms till timeout)
2022-03-28 13:00:09 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:00:09 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (213570ms till timeout)
2022-03-28 13:00:10 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (416745ms till timeout)
2022-03-28 13:00:10 [ForkJoinPool-1-worker-7] DEBUG [JobUtils:40] Job create-admin-my-cluster-d4dad0de-kafka-clients was deleted
2022-03-28 13:00:10 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update Job alter-admin-my-cluster-d4dad0de-kafka-clients in namespace throttling-quota-st
2022-03-28 13:00:10 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:00:11 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:alter-admin-my-cluster-d4dad0de-kafka-clients
2022-03-28 13:00:11 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (212234ms till timeout)
2022-03-28 13:00:11 [ForkJoinPool-1-worker-7] INFO  [JobUtils:81] Waiting for job: alter-admin-my-cluster-d4dad0de-kafka-clients will be in active state
2022-03-28 13:00:11 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 13:00:11 [ForkJoinPool-1-worker-7] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 13:00:11 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 13:00:11 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (415552ms till timeout)
2022-03-28 13:00:11 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (299781ms till timeout)
2022-03-28 13:00:11 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=2fe26899-cd27-4864-948c-40c8f3bbb961}
2022-03-28 13:00:12 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-534609d0-kafka-0=db6cdceb-78d5-4f60-8449-13b272fa0d7c, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=2fe26899-cd27-4864-948c-40c8f3bbb961}
2022-03-28 13:00:12 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-534609d0-kafka-0=db6cdceb-78d5-4f60-8449-13b272fa0d7c, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=2fe26899-cd27-4864-948c-40c8f3bbb961}
2022-03-28 13:00:12 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-534609d0-kafka-1 hasn't rolled
2022-03-28 13:00:12 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-534609d0-kafka rolling update not ready, will try again in 5000 ms (1758046ms till timeout)
2022-03-28 13:00:12 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:00:12 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (211013ms till timeout)
2022-03-28 13:00:12 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (414443ms till timeout)
2022-03-28 13:00:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (298667ms till timeout)
2022-03-28 13:00:13 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:00:13 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (209793ms till timeout)
2022-03-28 13:00:13 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (413301ms till timeout)
2022-03-28 13:00:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (297532ms till timeout)
2022-03-28 13:00:14 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:00:14 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (208567ms till timeout)
2022-03-28 13:00:14 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (412186ms till timeout)
2022-03-28 13:00:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (296419ms till timeout)
2022-03-28 13:00:15 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:00:16 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (207348ms till timeout)
2022-03-28 13:00:16 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (410991ms till timeout)
2022-03-28 13:00:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (295223ms till timeout)
2022-03-28 13:00:17 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=2fe26899-cd27-4864-948c-40c8f3bbb961}
2022-03-28 13:00:17 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-534609d0-kafka-0=db6cdceb-78d5-4f60-8449-13b272fa0d7c, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=2fe26899-cd27-4864-948c-40c8f3bbb961}
2022-03-28 13:00:17 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-534609d0-kafka-0=db6cdceb-78d5-4f60-8449-13b272fa0d7c, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=2fe26899-cd27-4864-948c-40c8f3bbb961}
2022-03-28 13:00:17 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-534609d0-kafka-1 hasn't rolled
2022-03-28 13:00:17 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-534609d0-kafka rolling update not ready, will try again in 5000 ms (1752829ms till timeout)
2022-03-28 13:00:17 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:00:17 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Wait for Deployment: my-cluster-0b062573-connect will be ready not ready, will try again in 1000 ms (409784ms till timeout)
2022-03-28 13:00:17 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (205927ms till timeout)
2022-03-28 13:00:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (294013ms till timeout)
2022-03-28 13:00:18 [ForkJoinPool-1-worker-15] INFO  [DeploymentUtils:168] Deployment: my-cluster-0b062573-connect is ready
2022-03-28 13:00:18 [ForkJoinPool-1-worker-15] INFO  [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment my-cluster-0b062573-connect to be ready
2022-03-28 13:00:18 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:00:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (292898ms till timeout)
2022-03-28 13:00:18 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-0b062573-connect}, additionalProperties={})to be ready
2022-03-28 13:00:18 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (204596ms till timeout)
2022-03-28 13:00:18 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-connect-675dfbf678-bq9fm not ready: my-cluster-0b062573-connect)
2022-03-28 13:00:18 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-0b062573-connect-675dfbf678-bq9fm are ready
2022-03-28 13:00:18 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-0b062573-connect}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599785ms till timeout)
2022-03-28 13:00:19 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (291786ms till timeout)
2022-03-28 13:00:19 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:00:20 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-connect-675dfbf678-bq9fm not ready: my-cluster-0b062573-connect)
2022-03-28 13:00:20 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-0b062573-connect-675dfbf678-bq9fm are ready
2022-03-28 13:00:20 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-0b062573-connect}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598674ms till timeout)
2022-03-28 13:00:20 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (203271ms till timeout)
2022-03-28 13:00:20 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (290674ms till timeout)
2022-03-28 13:00:21 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-connect-675dfbf678-bq9fm not ready: my-cluster-0b062573-connect)
2022-03-28 13:00:21 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-0b062573-connect-675dfbf678-bq9fm are ready
2022-03-28 13:00:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-0b062573-connect}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597564ms till timeout)
2022-03-28 13:00:21 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:00:21 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (202048ms till timeout)
2022-03-28 13:00:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (289561ms till timeout)
2022-03-28 13:00:22 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-connect-675dfbf678-bq9fm not ready: my-cluster-0b062573-connect)
2022-03-28 13:00:22 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-0b062573-connect-675dfbf678-bq9fm are ready
2022-03-28 13:00:22 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-0b062573-connect}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596447ms till timeout)
2022-03-28 13:00:22 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=2fe26899-cd27-4864-948c-40c8f3bbb961}
2022-03-28 13:00:22 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-534609d0-kafka-0=db6cdceb-78d5-4f60-8449-13b272fa0d7c, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=2fe26899-cd27-4864-948c-40c8f3bbb961}
2022-03-28 13:00:22 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-534609d0-kafka-0=db6cdceb-78d5-4f60-8449-13b272fa0d7c, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=2fe26899-cd27-4864-948c-40c8f3bbb961}
2022-03-28 13:00:22 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-534609d0-kafka-1 hasn't rolled
2022-03-28 13:00:22 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-534609d0-kafka rolling update not ready, will try again in 5000 ms (1747611ms till timeout)
2022-03-28 13:00:22 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:00:22 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (200811ms till timeout)
2022-03-28 13:00:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (288448ms till timeout)
2022-03-28 13:00:23 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-connect-675dfbf678-bq9fm not ready: my-cluster-0b062573-connect)
2022-03-28 13:00:23 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-0b062573-connect-675dfbf678-bq9fm are ready
2022-03-28 13:00:23 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-0b062573-connect}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595334ms till timeout)
2022-03-28 13:00:23 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:00:23 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (199591ms till timeout)
2022-03-28 13:00:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (287332ms till timeout)
2022-03-28 13:00:24 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-connect-675dfbf678-bq9fm not ready: my-cluster-0b062573-connect)
2022-03-28 13:00:24 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-0b062573-connect-675dfbf678-bq9fm are ready
2022-03-28 13:00:24 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-0b062573-connect}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594224ms till timeout)
2022-03-28 13:00:24 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:00:25 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (198375ms till timeout)
2022-03-28 13:00:25 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (286219ms till timeout)
2022-03-28 13:00:25 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-connect-675dfbf678-bq9fm not ready: my-cluster-0b062573-connect)
2022-03-28 13:00:25 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-0b062573-connect-675dfbf678-bq9fm are ready
2022-03-28 13:00:25 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-0b062573-connect}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593107ms till timeout)
2022-03-28 13:00:26 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:00:26 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (197048ms till timeout)
2022-03-28 13:00:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (285025ms till timeout)
2022-03-28 13:00:26 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-connect-675dfbf678-bq9fm not ready: my-cluster-0b062573-connect)
2022-03-28 13:00:26 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-0b062573-connect-675dfbf678-bq9fm are ready
2022-03-28 13:00:26 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-0b062573-connect}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591996ms till timeout)
2022-03-28 13:00:27 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:00:27 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=2fe26899-cd27-4864-948c-40c8f3bbb961}
2022-03-28 13:00:27 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (195827ms till timeout)
2022-03-28 13:00:27 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-534609d0-kafka-0=db6cdceb-78d5-4f60-8449-13b272fa0d7c, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=2fe26899-cd27-4864-948c-40c8f3bbb961}
2022-03-28 13:00:27 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-534609d0-kafka-0=db6cdceb-78d5-4f60-8449-13b272fa0d7c, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=2fe26899-cd27-4864-948c-40c8f3bbb961}
2022-03-28 13:00:27 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-534609d0-kafka-1 hasn't rolled
2022-03-28 13:00:27 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-534609d0-kafka rolling update not ready, will try again in 5000 ms (1742304ms till timeout)
2022-03-28 13:00:27 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (283603ms till timeout)
2022-03-28 13:00:27 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-connect-675dfbf678-bq9fm not ready: my-cluster-0b062573-connect)
2022-03-28 13:00:27 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-0b062573-connect-675dfbf678-bq9fm are ready
2022-03-28 13:00:27 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-0b062573-connect}, additionalProperties={})to be ready not ready, will try again in 1000 ms (590808ms till timeout)
2022-03-28 13:00:28 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:00:28 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (194602ms till timeout)
2022-03-28 13:00:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (282492ms till timeout)
2022-03-28 13:00:29 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-connect-675dfbf678-bq9fm not ready: my-cluster-0b062573-connect)
2022-03-28 13:00:29 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-0b062573-connect-675dfbf678-bq9fm are ready
2022-03-28 13:00:29 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/cluster=my-cluster-0b062573, strimzi.io/kind=KafkaConnect, strimzi.io/name=my-cluster-0b062573-connect}, additionalProperties={})to be ready not ready, will try again in 1000 ms (589588ms till timeout)
2022-03-28 13:00:29 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:00:30 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (193384ms till timeout)
2022-03-28 13:00:30 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (281363ms till timeout)
2022-03-28 13:00:30 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-0b062573-connect-675dfbf678-bq9fm not ready: my-cluster-0b062573-connect)
2022-03-28 13:00:30 [ForkJoinPool-1-worker-15] DEBUG [PodUtils:106] Pods my-cluster-0b062573-connect-675dfbf678-bq9fm are ready
2022-03-28 13:00:30 [ForkJoinPool-1-worker-15] INFO  [DeploymentUtils:197] Deployment my-cluster-0b062573-connect is ready
2022-03-28 13:00:30 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:155] Create/Update KafkaConnector my-cluster-0b062573 in namespace namespace-9
2022-03-28 13:00:30 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:164] Using Namespace: namespace-8
2022-03-28 13:00:30 [ForkJoinPool-1-worker-15] WARN  [VersionUsageUtils:60] The client is using resource type 'kafkaconnectors' with unstable version 'v1beta2'
2022-03-28 13:00:30 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaConnector:my-cluster-0b062573
2022-03-28 13:00:30 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:433] Wait for KafkaConnector: my-cluster-0b062573 will have desired state: Ready
2022-03-28 13:00:30 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for KafkaConnector: my-cluster-0b062573 will have desired state: Ready
2022-03-28 13:00:30 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] KafkaConnector: my-cluster-0b062573 will have desired state: Ready not ready, will try again in 1000 ms (239891ms till timeout)
2022-03-28 13:00:31 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:00:31 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (192166ms till timeout)
2022-03-28 13:00:31 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (280146ms till timeout)
2022-03-28 13:00:31 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:444] KafkaConnector: my-cluster-0b062573 is in desired state: Ready
2022-03-28 13:00:31 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 exec my-cluster-0b062573-connect-675dfbf678-bq9fm -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-0b062573
2022-03-28 13:00:32 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:00:32 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (190949ms till timeout)
2022-03-28 13:00:32 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (278928ms till timeout)
2022-03-28 13:00:32 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=2fe26899-cd27-4864-948c-40c8f3bbb961}
2022-03-28 13:00:33 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-534609d0-kafka-0=db6cdceb-78d5-4f60-8449-13b272fa0d7c, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=12477a35-d5dc-4593-ada1-b7a44f9089ca}
2022-03-28 13:00:33 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-534609d0-kafka-0=db6cdceb-78d5-4f60-8449-13b272fa0d7c, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=12477a35-d5dc-4593-ada1-b7a44f9089ca}
2022-03-28 13:00:33 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-534609d0-kafka-1 hasn't rolled
2022-03-28 13:00:33 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-534609d0-kafka rolling update not ready, will try again in 5000 ms (1737089ms till timeout)
2022-03-28 13:00:33 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-8 exec my-cluster-0b062573-connect-675dfbf678-bq9fm -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-0b062573
2022-03-28 13:00:33 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 13:00:33 [ForkJoinPool-1-worker-15] INFO  [ReconciliationST:118] Adding pause annotation into the KafkaConnector and scaling taskMax to 4
2022-03-28 13:00:33 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:433] Wait for KafkaConnector: my-cluster-0b062573 will have desired state: ReconciliationPaused
2022-03-28 13:00:33 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for KafkaConnector: my-cluster-0b062573 will have desired state: ReconciliationPaused
2022-03-28 13:00:33 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:00:33 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:444] KafkaConnector: my-cluster-0b062573 is in desired state: ReconciliationPaused
2022-03-28 13:00:33 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (277734ms till timeout)
2022-03-28 13:00:33 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Connector's spec will be stable
2022-03-28 13:00:33 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 exec my-cluster-0b062573-connect-675dfbf678-bq9fm -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-0b062573
2022-03-28 13:00:33 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (189536ms till timeout)
2022-03-28 13:00:34 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (276622ms till timeout)
2022-03-28 13:00:35 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:00:35 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-8 exec my-cluster-0b062573-connect-675dfbf678-bq9fm -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-0b062573
2022-03-28 13:00:35 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 13:00:35 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 19 polls
2022-03-28 13:00:35 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (178740ms till timeout)
2022-03-28 13:00:35 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (188315ms till timeout)
2022-03-28 13:00:36 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (275509ms till timeout)
2022-03-28 13:00:36 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 exec my-cluster-0b062573-connect-675dfbf678-bq9fm -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-0b062573
2022-03-28 13:00:36 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:00:36 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (187095ms till timeout)
2022-03-28 13:00:37 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (274396ms till timeout)
2022-03-28 13:00:37 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-8 exec my-cluster-0b062573-connect-675dfbf678-bq9fm -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-0b062573
2022-03-28 13:00:37 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 13:00:37 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 18 polls
2022-03-28 13:00:37 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (176530ms till timeout)
2022-03-28 13:00:37 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:00:37 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (185876ms till timeout)
2022-03-28 13:00:38 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=2fe26899-cd27-4864-948c-40c8f3bbb961}
2022-03-28 13:00:38 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-534609d0-kafka-0=db6cdceb-78d5-4f60-8449-13b272fa0d7c, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=12477a35-d5dc-4593-ada1-b7a44f9089ca}
2022-03-28 13:00:38 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-534609d0-kafka-0=db6cdceb-78d5-4f60-8449-13b272fa0d7c, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=12477a35-d5dc-4593-ada1-b7a44f9089ca}
2022-03-28 13:00:38 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-534609d0-kafka-1 hasn't rolled
2022-03-28 13:00:38 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-534609d0-kafka rolling update not ready, will try again in 5000 ms (1731871ms till timeout)
2022-03-28 13:00:38 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 exec my-cluster-0b062573-connect-675dfbf678-bq9fm -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-0b062573
2022-03-28 13:00:38 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (273170ms till timeout)
2022-03-28 13:00:38 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:00:38 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (184653ms till timeout)
2022-03-28 13:00:39 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-8 exec my-cluster-0b062573-connect-675dfbf678-bq9fm -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-0b062573
2022-03-28 13:00:39 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 13:00:39 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 17 polls
2022-03-28 13:00:39 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (174338ms till timeout)
2022-03-28 13:00:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (272059ms till timeout)
2022-03-28 13:00:39 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:00:39 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (183434ms till timeout)
2022-03-28 13:00:40 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 exec my-cluster-0b062573-connect-675dfbf678-bq9fm -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-0b062573
2022-03-28 13:00:40 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (270948ms till timeout)
2022-03-28 13:00:41 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:00:41 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (182214ms till timeout)
2022-03-28 13:00:41 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (269837ms till timeout)
2022-03-28 13:00:41 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-8 exec my-cluster-0b062573-connect-675dfbf678-bq9fm -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-0b062573
2022-03-28 13:00:41 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 13:00:41 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 16 polls
2022-03-28 13:00:41 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (172061ms till timeout)
2022-03-28 13:00:42 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:00:42 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (180994ms till timeout)
2022-03-28 13:00:42 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 exec my-cluster-0b062573-connect-675dfbf678-bq9fm -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-0b062573
2022-03-28 13:00:42 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (268718ms till timeout)
2022-03-28 13:00:43 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=2fe26899-cd27-4864-948c-40c8f3bbb961}
2022-03-28 13:00:43 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-534609d0-kafka-0=db6cdceb-78d5-4f60-8449-13b272fa0d7c, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=12477a35-d5dc-4593-ada1-b7a44f9089ca}
2022-03-28 13:00:43 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-534609d0-kafka-0=db6cdceb-78d5-4f60-8449-13b272fa0d7c, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=12477a35-d5dc-4593-ada1-b7a44f9089ca}
2022-03-28 13:00:43 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-534609d0-kafka-1 hasn't rolled
2022-03-28 13:00:43 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-534609d0-kafka rolling update not ready, will try again in 5000 ms (1726655ms till timeout)
2022-03-28 13:00:43 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:00:43 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (179777ms till timeout)
2022-03-28 13:00:43 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-8 exec my-cluster-0b062573-connect-675dfbf678-bq9fm -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-0b062573
2022-03-28 13:00:43 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 13:00:43 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 15 polls
2022-03-28 13:00:43 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (169882ms till timeout)
2022-03-28 13:00:43 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (267606ms till timeout)
2022-03-28 13:00:44 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:00:44 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (178557ms till timeout)
2022-03-28 13:00:44 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 exec my-cluster-0b062573-connect-675dfbf678-bq9fm -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-0b062573
2022-03-28 13:00:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (266493ms till timeout)
2022-03-28 13:00:45 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:00:46 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (177338ms till timeout)
2022-03-28 13:00:46 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-8 exec my-cluster-0b062573-connect-675dfbf678-bq9fm -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-0b062573
2022-03-28 13:00:46 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 13:00:46 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 14 polls
2022-03-28 13:00:46 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (167679ms till timeout)
2022-03-28 13:00:46 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (265318ms till timeout)
2022-03-28 13:00:47 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 exec my-cluster-0b062573-connect-675dfbf678-bq9fm -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-0b062573
2022-03-28 13:00:47 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:00:47 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (176118ms till timeout)
2022-03-28 13:00:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (264099ms till timeout)
2022-03-28 13:00:48 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-8 exec my-cluster-0b062573-connect-675dfbf678-bq9fm -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-0b062573
2022-03-28 13:00:48 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 13:00:48 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 13 polls
2022-03-28 13:00:48 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (165480ms till timeout)
2022-03-28 13:00:48 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:00:48 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=2fe26899-cd27-4864-948c-40c8f3bbb961}
2022-03-28 13:00:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (262986ms till timeout)
2022-03-28 13:00:48 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (174684ms till timeout)
2022-03-28 13:00:48 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-534609d0-kafka-0=db6cdceb-78d5-4f60-8449-13b272fa0d7c, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=12477a35-d5dc-4593-ada1-b7a44f9089ca}
2022-03-28 13:00:48 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-534609d0-kafka-0=db6cdceb-78d5-4f60-8449-13b272fa0d7c, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=12477a35-d5dc-4593-ada1-b7a44f9089ca}
2022-03-28 13:00:48 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-534609d0-kafka-1 hasn't rolled
2022-03-28 13:00:48 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-534609d0-kafka rolling update not ready, will try again in 5000 ms (1721373ms till timeout)
2022-03-28 13:00:49 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 exec my-cluster-0b062573-connect-675dfbf678-bq9fm -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-0b062573
2022-03-28 13:00:49 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (261869ms till timeout)
2022-03-28 13:00:49 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:00:49 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (173465ms till timeout)
2022-03-28 13:00:50 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-8 exec my-cluster-0b062573-connect-675dfbf678-bq9fm -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-0b062573
2022-03-28 13:00:50 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 13:00:50 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 12 polls
2022-03-28 13:00:50 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (163274ms till timeout)
2022-03-28 13:00:50 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (260758ms till timeout)
2022-03-28 13:00:51 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:00:51 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (172244ms till timeout)
2022-03-28 13:00:51 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 exec my-cluster-0b062573-connect-675dfbf678-bq9fm -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-0b062573
2022-03-28 13:00:51 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (259646ms till timeout)
2022-03-28 13:00:52 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:00:52 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (171024ms till timeout)
2022-03-28 13:00:52 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-8 exec my-cluster-0b062573-connect-675dfbf678-bq9fm -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-0b062573
2022-03-28 13:00:52 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 13:00:52 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 11 polls
2022-03-28 13:00:52 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (161019ms till timeout)
2022-03-28 13:00:52 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (258536ms till timeout)
2022-03-28 13:00:53 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:00:53 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (169805ms till timeout)
2022-03-28 13:00:53 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=2fe26899-cd27-4864-948c-40c8f3bbb961}
2022-03-28 13:00:53 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 exec my-cluster-0b062573-connect-675dfbf678-bq9fm -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-0b062573
2022-03-28 13:00:53 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-534609d0-kafka-0=db6cdceb-78d5-4f60-8449-13b272fa0d7c, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=12477a35-d5dc-4593-ada1-b7a44f9089ca}
2022-03-28 13:00:53 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-534609d0-kafka-0=db6cdceb-78d5-4f60-8449-13b272fa0d7c, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=12477a35-d5dc-4593-ada1-b7a44f9089ca}
2022-03-28 13:00:53 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:50] At least my-cluster-534609d0-kafka-1 hasn't rolled
2022-03-28 13:00:53 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] component with name my-cluster-534609d0-kafka rolling update not ready, will try again in 5000 ms (1716156ms till timeout)
2022-03-28 13:00:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (257424ms till timeout)
2022-03-28 13:00:54 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:00:54 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (168580ms till timeout)
2022-03-28 13:00:54 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-8 exec my-cluster-0b062573-connect-675dfbf678-bq9fm -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-0b062573
2022-03-28 13:00:54 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 13:00:54 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 10 polls
2022-03-28 13:00:54 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (158820ms till timeout)
2022-03-28 13:00:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (256312ms till timeout)
2022-03-28 13:00:55 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:00:55 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 exec my-cluster-0b062573-connect-675dfbf678-bq9fm -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-0b062573
2022-03-28 13:00:56 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (167361ms till timeout)
2022-03-28 13:00:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (255176ms till timeout)
2022-03-28 13:00:57 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:00:57 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-8 exec my-cluster-0b062573-connect-675dfbf678-bq9fm -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-0b062573
2022-03-28 13:00:57 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 13:00:57 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 9 polls
2022-03-28 13:00:57 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (156565ms till timeout)
2022-03-28 13:00:57 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (166140ms till timeout)
2022-03-28 13:00:57 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (254063ms till timeout)
2022-03-28 13:00:58 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 exec my-cluster-0b062573-connect-675dfbf678-bq9fm -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-0b062573
2022-03-28 13:00:58 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:00:58 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (164920ms till timeout)
2022-03-28 13:00:58 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (252900ms till timeout)
2022-03-28 13:00:58 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:35] Existing snapshot: {my-cluster-534609d0-kafka-0=bf390750-960f-49c8-8c2e-3020194a4152, my-cluster-534609d0-kafka-1=d42410a1-9b0d-4652-a7a5-46ef13cdabc0, my-cluster-534609d0-kafka-2=2fe26899-cd27-4864-948c-40c8f3bbb961}
2022-03-28 13:00:59 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:39] Current snapshot: {my-cluster-534609d0-kafka-0=db6cdceb-78d5-4f60-8449-13b272fa0d7c, my-cluster-534609d0-kafka-1=3b8ed1bf-c330-41cc-8044-326f973e5536, my-cluster-534609d0-kafka-2=12477a35-d5dc-4593-ada1-b7a44f9089ca}
2022-03-28 13:00:59 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:44] Pods in common: {my-cluster-534609d0-kafka-0=db6cdceb-78d5-4f60-8449-13b272fa0d7c, my-cluster-534609d0-kafka-1=3b8ed1bf-c330-41cc-8044-326f973e5536, my-cluster-534609d0-kafka-2=12477a35-d5dc-4593-ada1-b7a44f9089ca}
2022-03-28 13:00:59 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:55] All pods seem to have rolled
2022-03-28 13:00:59 [ForkJoinPool-1-worker-1] INFO  [RollingUpdateUtils:86] Component with name: my-cluster-534609d0-kafka has been successfully rolled
2022-03-28 13:00:59 [ForkJoinPool-1-worker-1] DEBUG [RollingUpdateUtils:87] Component matching LabelSelector: LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={}) successfully rolled
2022-03-28 13:00:59 [ForkJoinPool-1-worker-1] INFO  [RollingUpdateUtils:127] Waiting for 3 Pod(s) of my-cluster-534609d0-kafka to be ready
2022-03-28 13:00:59 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})to be ready
2022-03-28 13:00:59 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-0 not ready: kafka)
2022-03-28 13:00:59 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-534609d0-kafka-1)
2022-03-28 13:00:59 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1799890ms till timeout)
2022-03-28 13:00:59 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-8 exec my-cluster-0b062573-connect-675dfbf678-bq9fm -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-0b062573
2022-03-28 13:00:59 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 13:00:59 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 8 polls
2022-03-28 13:00:59 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (154325ms till timeout)
2022-03-28 13:00:59 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:00:59 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (163704ms till timeout)
2022-03-28 13:00:59 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (251681ms till timeout)
2022-03-28 13:01:00 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 exec my-cluster-0b062573-connect-675dfbf678-bq9fm -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-0b062573
2022-03-28 13:01:00 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-0 not ready: kafka)
2022-03-28 13:01:00 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-534609d0-kafka-1)
2022-03-28 13:01:00 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1798671ms till timeout)
2022-03-28 13:01:00 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:01:00 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (162486ms till timeout)
2022-03-28 13:01:01 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (250467ms till timeout)
2022-03-28 13:01:01 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-8 exec my-cluster-0b062573-connect-675dfbf678-bq9fm -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-0b062573
2022-03-28 13:01:01 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 13:01:01 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 7 polls
2022-03-28 13:01:01 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (152055ms till timeout)
2022-03-28 13:01:01 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-0 not ready: kafka)
2022-03-28 13:01:01 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-534609d0-kafka-1)
2022-03-28 13:01:01 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1797456ms till timeout)
2022-03-28 13:01:02 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:01:02 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (161269ms till timeout)
2022-03-28 13:01:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (249249ms till timeout)
2022-03-28 13:01:02 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 exec my-cluster-0b062573-connect-675dfbf678-bq9fm -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-0b062573
2022-03-28 13:01:03 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-0 not ready: kafka)
2022-03-28 13:01:03 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-534609d0-kafka-1)
2022-03-28 13:01:03 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1796240ms till timeout)
2022-03-28 13:01:03 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:01:03 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (160050ms till timeout)
2022-03-28 13:01:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (248031ms till timeout)
2022-03-28 13:01:04 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-8 exec my-cluster-0b062573-connect-675dfbf678-bq9fm -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-0b062573
2022-03-28 13:01:04 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 13:01:04 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 6 polls
2022-03-28 13:01:04 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (149767ms till timeout)
2022-03-28 13:01:04 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-0 not ready: kafka)
2022-03-28 13:01:04 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-534609d0-kafka-1)
2022-03-28 13:01:04 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1795023ms till timeout)
2022-03-28 13:01:04 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:01:04 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (158831ms till timeout)
2022-03-28 13:01:04 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (246813ms till timeout)
2022-03-28 13:01:05 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 exec my-cluster-0b062573-connect-675dfbf678-bq9fm -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-0b062573
2022-03-28 13:01:05 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-0 not ready: kafka)
2022-03-28 13:01:05 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-534609d0-kafka-1)
2022-03-28 13:01:05 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1793806ms till timeout)
2022-03-28 13:01:05 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:01:05 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (157614ms till timeout)
2022-03-28 13:01:05 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (245594ms till timeout)
2022-03-28 13:01:06 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-8 exec my-cluster-0b062573-connect-675dfbf678-bq9fm -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-0b062573
2022-03-28 13:01:06 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 13:01:06 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 5 polls
2022-03-28 13:01:06 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (147536ms till timeout)
2022-03-28 13:01:06 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-0 not ready: kafka)
2022-03-28 13:01:06 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-534609d0-kafka-1)
2022-03-28 13:01:06 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1792591ms till timeout)
2022-03-28 13:01:06 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:01:07 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (156397ms till timeout)
2022-03-28 13:01:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (244378ms till timeout)
2022-03-28 13:01:07 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 exec my-cluster-0b062573-connect-675dfbf678-bq9fm -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-0b062573
2022-03-28 13:01:07 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-0 not ready: kafka)
2022-03-28 13:01:07 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-534609d0-kafka-1)
2022-03-28 13:01:07 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1791375ms till timeout)
2022-03-28 13:01:08 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:01:08 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (155181ms till timeout)
2022-03-28 13:01:08 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (243161ms till timeout)
2022-03-28 13:01:08 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-8 exec my-cluster-0b062573-connect-675dfbf678-bq9fm -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-0b062573
2022-03-28 13:01:08 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 13:01:08 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 4 polls
2022-03-28 13:01:08 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (145344ms till timeout)
2022-03-28 13:01:09 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-0 not ready: kafka)
2022-03-28 13:01:09 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-534609d0-kafka-1)
2022-03-28 13:01:09 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1790156ms till timeout)
2022-03-28 13:01:09 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:01:09 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 exec my-cluster-0b062573-connect-675dfbf678-bq9fm -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-0b062573
2022-03-28 13:01:09 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (153963ms till timeout)
2022-03-28 13:01:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (241944ms till timeout)
2022-03-28 13:01:10 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-0 not ready: kafka)
2022-03-28 13:01:10 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-534609d0-kafka-1)
2022-03-28 13:01:10 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1788941ms till timeout)
2022-03-28 13:01:10 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:01:10 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (152745ms till timeout)
2022-03-28 13:01:10 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-8 exec my-cluster-0b062573-connect-675dfbf678-bq9fm -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-0b062573
2022-03-28 13:01:10 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 13:01:10 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 3 polls
2022-03-28 13:01:10 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (143063ms till timeout)
2022-03-28 13:01:10 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (240726ms till timeout)
2022-03-28 13:01:11 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-0 not ready: kafka)
2022-03-28 13:01:11 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-534609d0-kafka-1)
2022-03-28 13:01:11 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1787727ms till timeout)
2022-03-28 13:01:11 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 exec my-cluster-0b062573-connect-675dfbf678-bq9fm -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-0b062573
2022-03-28 13:01:11 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:01:11 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (151528ms till timeout)
2022-03-28 13:01:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (239507ms till timeout)
2022-03-28 13:01:12 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-0 not ready: kafka)
2022-03-28 13:01:12 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-534609d0-kafka-1)
2022-03-28 13:01:12 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1786512ms till timeout)
2022-03-28 13:01:12 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-8 exec my-cluster-0b062573-connect-675dfbf678-bq9fm -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-0b062573
2022-03-28 13:01:12 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 13:01:12 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 2 polls
2022-03-28 13:01:12 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (140873ms till timeout)
2022-03-28 13:01:13 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:01:13 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (150307ms till timeout)
2022-03-28 13:01:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (238290ms till timeout)
2022-03-28 13:01:13 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 exec my-cluster-0b062573-connect-675dfbf678-bq9fm -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-0b062573
2022-03-28 13:01:13 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-0 not ready: kafka)
2022-03-28 13:01:13 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:92] Not ready (at least 1 pod not ready: my-cluster-534609d0-kafka-1)
2022-03-28 13:01:13 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1785292ms till timeout)
2022-03-28 13:01:14 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:01:14 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (149089ms till timeout)
2022-03-28 13:01:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (237070ms till timeout)
2022-03-28 13:01:15 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-8 exec my-cluster-0b062573-connect-675dfbf678-bq9fm -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-0b062573
2022-03-28 13:01:15 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 13:01:15 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:162] Connector's spec gonna be stable in 1 polls
2022-03-28 13:01:15 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Connector's spec will be stable not ready, will try again in 1000 ms (138622ms till timeout)
2022-03-28 13:01:15 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-0 not ready: kafka)
2022-03-28 13:01:15 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-1 not ready: kafka)
2022-03-28 13:01:15 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-2 not ready: kafka)
2022-03-28 13:01:15 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:106] Pods my-cluster-534609d0-kafka-0, my-cluster-534609d0-kafka-1, my-cluster-534609d0-kafka-2 are ready
2022-03-28 13:01:15 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1784075ms till timeout)
2022-03-28 13:01:15 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:01:15 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (147871ms till timeout)
2022-03-28 13:01:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (235843ms till timeout)
2022-03-28 13:01:16 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 exec my-cluster-0b062573-connect-675dfbf678-bq9fm -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-0b062573
2022-03-28 13:01:16 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-0 not ready: kafka)
2022-03-28 13:01:16 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-1 not ready: kafka)
2022-03-28 13:01:16 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-2 not ready: kafka)
2022-03-28 13:01:16 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:106] Pods my-cluster-534609d0-kafka-0, my-cluster-534609d0-kafka-1, my-cluster-534609d0-kafka-2 are ready
2022-03-28 13:01:16 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1782858ms till timeout)
2022-03-28 13:01:16 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:01:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (234574ms till timeout)
2022-03-28 13:01:16 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (146482ms till timeout)
2022-03-28 13:01:17 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-8 exec my-cluster-0b062573-connect-675dfbf678-bq9fm -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-0b062573
2022-03-28 13:01:17 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 13:01:17 [ForkJoinPool-1-worker-15] INFO  [KafkaConnectorUtils:154] Connector's spec is stable for 20 polls intervals
2022-03-28 13:01:17 [ForkJoinPool-1-worker-15] INFO  [ReconciliationST:127] Setting annotation to "false", taskMax should be increased to 4
2022-03-28 13:01:17 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-0 not ready: kafka)
2022-03-28 13:01:17 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-1 not ready: kafka)
2022-03-28 13:01:17 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-2 not ready: kafka)
2022-03-28 13:01:17 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:106] Pods my-cluster-534609d0-kafka-0, my-cluster-534609d0-kafka-1, my-cluster-534609d0-kafka-2 are ready
2022-03-28 13:01:17 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1781558ms till timeout)
2022-03-28 13:01:17 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Wait for KafkaConnector config will contain desired config
2022-03-28 13:01:17 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 exec my-cluster-0b062573-connect-675dfbf678-bq9fm -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-0b062573/config
2022-03-28 13:01:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (233464ms till timeout)
2022-03-28 13:01:18 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:01:18 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (145157ms till timeout)
2022-03-28 13:01:18 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-0 not ready: kafka)
2022-03-28 13:01:18 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-1 not ready: kafka)
2022-03-28 13:01:18 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-2 not ready: kafka)
2022-03-28 13:01:18 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:106] Pods my-cluster-534609d0-kafka-0, my-cluster-534609d0-kafka-1, my-cluster-534609d0-kafka-2 are ready
2022-03-28 13:01:18 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1780341ms till timeout)
2022-03-28 13:01:18 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-8 exec my-cluster-0b062573-connect-675dfbf678-bq9fm -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-0b062573/config
2022-03-28 13:01:18 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 13:01:18 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-8 exec my-cluster-0b062573-connect-675dfbf678-bq9fm -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-0b062573/config
2022-03-28 13:01:19 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (232351ms till timeout)
2022-03-28 13:01:19 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:01:19 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (143939ms till timeout)
2022-03-28 13:01:20 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-0 not ready: kafka)
2022-03-28 13:01:20 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-1 not ready: kafka)
2022-03-28 13:01:20 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-2 not ready: kafka)
2022-03-28 13:01:20 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:106] Pods my-cluster-534609d0-kafka-0, my-cluster-534609d0-kafka-1, my-cluster-534609d0-kafka-2 are ready
2022-03-28 13:01:20 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1779124ms till timeout)
2022-03-28 13:01:20 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Command: oc --namespace namespace-8 exec my-cluster-0b062573-connect-675dfbf678-bq9fm -- /bin/bash -c curl http://localhost:8083/connectors/my-cluster-0b062573/config
2022-03-28 13:01:20 [ForkJoinPool-1-worker-15] INFO  [Exec:417] Return code: 0
2022-03-28 13:01:20 [ForkJoinPool-1-worker-15] DEBUG [AbstractST:674] ============================================================================
2022-03-28 13:01:20 [ForkJoinPool-1-worker-15] DEBUG [AbstractST:675] [operators.ReconciliationST - After Each] - Clean up after test
2022-03-28 13:01:20 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:344] ############################################################################
2022-03-28 13:01:20 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:348] Delete all resources for testPauseReconciliationInKafkaAndKafkaConnectWithConnector
2022-03-28 13:01:20 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:241] Delete of NetworkPolicy my-cluster-0b062573-allow in namespace namespace-8
2022-03-28 13:01:20 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of Deployment my-cluster-0b062573-kafka-clients in namespace namespace-8
2022-03-28 13:01:20 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:241] Delete of Kafka my-cluster-0b062573 in namespace namespace-8
2022-03-28 13:01:20 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (231238ms till timeout)
2022-03-28 13:01:20 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-0b062573-kafka-clients
2022-03-28 13:01:20 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource NetworkPolicy:my-cluster-0b062573-allow
2022-03-28 13:01:20 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-0b062573
2022-03-28 13:01:20 [ForkJoinPool-1-worker-9] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:my-cluster-0b062573 not ready, will try again in 10000 ms (839755ms till timeout)
2022-03-28 13:01:20 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:01:20 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (142570ms till timeout)
2022-03-28 13:01:20 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:241] Delete of KafkaConnector my-cluster-0b062573 in namespace namespace-8
2022-03-28 13:01:20 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-0b062573-kafka-clients not ready, will try again in 10000 ms (479417ms till timeout)
2022-03-28 13:01:21 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaConnector:my-cluster-0b062573
2022-03-28 13:01:21 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:241] Delete of KafkaConnect my-cluster-0b062573 in namespace namespace-8
2022-03-28 13:01:21 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-0 not ready: kafka)
2022-03-28 13:01:21 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-1 not ready: kafka)
2022-03-28 13:01:21 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-2 not ready: kafka)
2022-03-28 13:01:21 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:106] Pods my-cluster-534609d0-kafka-0, my-cluster-534609d0-kafka-1, my-cluster-534609d0-kafka-2 are ready
2022-03-28 13:01:21 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1777991ms till timeout)
2022-03-28 13:01:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (230112ms till timeout)
2022-03-28 13:01:21 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaConnect:my-cluster-0b062573
2022-03-28 13:01:21 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:241] Delete of Deployment my-cluster-0b062573-scraper in namespace namespace-8
2022-03-28 13:01:21 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-0b062573-scraper
2022-03-28 13:01:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-0b062573-scraper not ready, will try again in 10000 ms (479779ms till timeout)
2022-03-28 13:01:21 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:01:22 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (141352ms till timeout)
2022-03-28 13:01:22 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-0 not ready: kafka)
2022-03-28 13:01:22 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-1 not ready: kafka)
2022-03-28 13:01:22 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-2 not ready: kafka)
2022-03-28 13:01:22 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:106] Pods my-cluster-534609d0-kafka-0, my-cluster-534609d0-kafka-1, my-cluster-534609d0-kafka-2 are ready
2022-03-28 13:01:22 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1776772ms till timeout)
2022-03-28 13:01:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (228998ms till timeout)
2022-03-28 13:01:23 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:01:23 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (140134ms till timeout)
2022-03-28 13:01:23 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-0 not ready: kafka)
2022-03-28 13:01:23 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-1 not ready: kafka)
2022-03-28 13:01:23 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-2 not ready: kafka)
2022-03-28 13:01:23 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:106] Pods my-cluster-534609d0-kafka-0, my-cluster-534609d0-kafka-1, my-cluster-534609d0-kafka-2 are ready
2022-03-28 13:01:23 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1775556ms till timeout)
2022-03-28 13:01:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (227684ms till timeout)
2022-03-28 13:01:24 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:01:24 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (138890ms till timeout)
2022-03-28 13:01:24 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-0 not ready: kafka)
2022-03-28 13:01:24 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-1 not ready: kafka)
2022-03-28 13:01:24 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-2 not ready: kafka)
2022-03-28 13:01:24 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:106] Pods my-cluster-534609d0-kafka-0, my-cluster-534609d0-kafka-1, my-cluster-534609d0-kafka-2 are ready
2022-03-28 13:01:24 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1774340ms till timeout)
2022-03-28 13:01:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (226566ms till timeout)
2022-03-28 13:01:25 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:01:25 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (137673ms till timeout)
2022-03-28 13:01:26 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-0 not ready: kafka)
2022-03-28 13:01:26 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-1 not ready: kafka)
2022-03-28 13:01:26 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-2 not ready: kafka)
2022-03-28 13:01:26 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:106] Pods my-cluster-534609d0-kafka-0, my-cluster-534609d0-kafka-1, my-cluster-534609d0-kafka-2 are ready
2022-03-28 13:01:26 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={strimzi.io/name=my-cluster-534609d0-kafka, strimzi.io/cluster=my-cluster-534609d0, strimzi.io/kind=Kafka}, additionalProperties={})to be ready not ready, will try again in 1000 ms (1773125ms till timeout)
2022-03-28 13:01:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (225253ms till timeout)
2022-03-28 13:01:26 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:01:26 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (136455ms till timeout)
2022-03-28 13:01:27 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-0 not ready: kafka)
2022-03-28 13:01:27 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-1 not ready: kafka)
2022-03-28 13:01:27 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:98] Not ready (at least 1 container of pod my-cluster-534609d0-kafka-2 not ready: kafka)
2022-03-28 13:01:27 [ForkJoinPool-1-worker-1] DEBUG [PodUtils:106] Pods my-cluster-534609d0-kafka-0, my-cluster-534609d0-kafka-1, my-cluster-534609d0-kafka-2 are ready
2022-03-28 13:01:27 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (224126ms till timeout)
2022-03-28 13:01:27 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-534609d0 will have desired state: Ready
2022-03-28 13:01:27 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-534609d0 will have desired state: Ready
2022-03-28 13:01:27 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:444] Kafka: my-cluster-534609d0 is in desired state: Ready
2022-03-28 13:01:27 [ForkJoinPool-1-worker-1] INFO  [RollingUpdateUtils:132] Kafka: my-cluster-534609d0 is ready
2022-03-28 13:01:27 [ForkJoinPool-1-worker-1] INFO  [CruiseControlConfigurationST:136] Verifying that in Kafka config map there is configuration to cruise control metric reporter
2022-03-28 13:01:27 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Verify that kafka configuration {cruise.control.metrics.reporter.ssl.keystore.location=/tmp/kafka/cluster.keystore.p12, cruise.control.metrics.reporter.ssl.endpoint.identification.algorithm=HTTPS, cruise.control.metrics.reporter.ssl.truststore.password=${CERTS_STORE_PASSWORD}, cruise.control.metrics.reporter.ssl.truststore.type=PKCS12, cruise.control.metrics.reporter.ssl.keystore.password=${CERTS_STORE_PASSWORD}, cruise.control.metrics.topic.replication.factor=3, cluster-name=my-cluster-534609d0, cruise.control.metrics.reporter.security.protocol=SSL, cruise.control.metrics.reporter.ssl.keystore.type=PKCS12, cruise.control.metrics.topic.num.partitions=1, cruise.control.metrics.reporter.ssl.truststore.location=/tmp/kafka/cluster.truststore.p12, cruise.control.metrics.topic=strimzi.cruisecontrol.metrics, cruise.control.metrics.topic.min.insync.replicas=1, cruise.control.metrics.reporter.bootstrap.servers=my-cluster-534609d0-kafka-brokers:9091, cruise.control.metrics.topic.auto.create=true} has correct cruise control metric reporter properties
2022-03-28 13:01:27 [ForkJoinPool-1-worker-1] INFO  [CruiseControlConfigurationST:139] Verifying that Cruise Control topics are created after CC is instantiated.
2022-03-28 13:01:27 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Verify that kafka contains cruise control topics with related configuration.
2022-03-28 13:01:27 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Verify that kafka contains cruise control topics with related configuration.
2022-03-28 13:01:28 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:01:28 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:674] ============================================================================
2022-03-28 13:01:28 [ForkJoinPool-1-worker-1] DEBUG [AbstractST:675] [cruisecontrol.CruiseControlConfigurationST - After Each] - Clean up after test
2022-03-28 13:01:28 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:344] ############################################################################
2022-03-28 13:01:28 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:348] Delete all resources for testDeployAndUnDeployCruiseControl
2022-03-28 13:01:28 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:241] Delete of Kafka my-cluster-534609d0 in namespace namespace-6
2022-03-28 13:01:28 [ForkJoinPool-1-worker-1] INFO  [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-6, for cruise control Kafka cluster my-cluster-534609d0
2022-03-28 13:01:28 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (135062ms till timeout)
2022-03-28 13:01:28 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (222933ms till timeout)
2022-03-28 13:01:29 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-534609d0
2022-03-28 13:01:29 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:01:29 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:369] ############################################################################
2022-03-28 13:01:29 [ForkJoinPool-1-worker-1] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-6 for test case:testDeployAndUnDeployCruiseControl
2022-03-28 13:01:29 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (133697ms till timeout)
2022-03-28 13:01:29 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Namespace namespace-6 removal
2022-03-28 13:01:29 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 13:01:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (221661ms till timeout)
2022-03-28 13:01:30 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 13:01:30 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 13:01:30 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (479480ms till timeout)
2022-03-28 13:01:30 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:01:30 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (220548ms till timeout)
2022-03-28 13:01:31 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (132350ms till timeout)
2022-03-28 13:01:31 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 13:01:31 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-0b062573-kafka-clients not ready, will try again in 10000 ms (468987ms till timeout)
2022-03-28 13:01:32 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (219432ms till timeout)
2022-03-28 13:01:32 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:01:32 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-0b062573-scraper not ready, will try again in 10000 ms (469448ms till timeout)
2022-03-28 13:01:32 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (131126ms till timeout)
2022-03-28 13:01:32 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 13:01:32 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 13:01:32 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (477512ms till timeout)
2022-03-28 13:01:33 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (218318ms till timeout)
2022-03-28 13:01:33 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 13:01:33 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:01:33 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129908ms till timeout)
2022-03-28 13:01:33 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 13:01:33 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 13:01:33 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (475982ms till timeout)
2022-03-28 13:01:34 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (217206ms till timeout)
2022-03-28 13:01:34 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:01:34 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (128689ms till timeout)
2022-03-28 13:01:34 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 13:01:35 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (216084ms till timeout)
2022-03-28 13:01:35 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 13:01:35 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 13:01:35 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (474021ms till timeout)
2022-03-28 13:01:35 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:01:35 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (127472ms till timeout)
2022-03-28 13:01:36 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (214972ms till timeout)
2022-03-28 13:01:36 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 13:01:37 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:01:37 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (126254ms till timeout)
2022-03-28 13:01:37 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 13:01:37 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 13:01:37 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (472456ms till timeout)
2022-03-28 13:01:37 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (213862ms till timeout)
2022-03-28 13:01:38 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:01:38 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (125036ms till timeout)
2022-03-28 13:01:38 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 13:01:38 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (212751ms till timeout)
2022-03-28 13:01:39 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:01:39 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (123818ms till timeout)
2022-03-28 13:01:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (211638ms till timeout)
2022-03-28 13:01:40 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:01:40 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (122599ms till timeout)
2022-03-28 13:01:40 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (210525ms till timeout)
2022-03-28 13:01:41 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-0b062573-kafka-clients not ready, will try again in 10000 ms (458658ms till timeout)
2022-03-28 13:01:41 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T12:59:58Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:01:42 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (209366ms till timeout)
2022-03-28 13:01:42 [ForkJoinPool-1-worker-11] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 13:01:42 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 13:01:42 [ForkJoinPool-1-worker-11] INFO  [PodUtils:189] Message All topics created found in create-admin-my-cluster-b6bd312e-kafka-clients-5lrj2 log
2022-03-28 13:01:42 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-0b062573-scraper not ready, will try again in 10000 ms (459058ms till timeout)
2022-03-28 13:01:42 [ForkJoinPool-1-worker-11] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment create-admin-my-cluster-b6bd312e-kafka-clients deletion
2022-03-28 13:01:42 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for ReplicaSet create-admin-my-cluster-b6bd312e-kafka-clients to be deleted
2022-03-28 13:01:42 [ForkJoinPool-1-worker-11] DEBUG [JobUtils:40] Job create-admin-my-cluster-b6bd312e-kafka-clients was deleted
2022-03-28 13:01:42 [ForkJoinPool-1-worker-11] INFO  [ThrottlingQuotaST:112] Executing 4/5 iteration.
2022-03-28 13:01:42 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st
2022-03-28 13:01:42 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:create-admin-my-cluster-b6bd312e-kafka-clients
2022-03-28 13:01:43 [ForkJoinPool-1-worker-11] INFO  [JobUtils:81] Waiting for job: create-admin-my-cluster-b6bd312e-kafka-clients will be in active state
2022-03-28 13:01:43 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 13:01:43 [ForkJoinPool-1-worker-11] INFO  [ClientUtils:76] Waiting for producer/consumer:create-admin-my-cluster-b6bd312e-kafka-clients to finished
2022-03-28 13:01:43 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 13:01:43 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:01:43 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (208169ms till timeout)
2022-03-28 13:01:43 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (219676ms till timeout)
2022-03-28 13:01:43 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 13:01:43 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 13:01:43 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (465897ms till timeout)
2022-03-28 13:01:44 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (207055ms till timeout)
2022-03-28 13:01:44 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:01:44 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (218443ms till timeout)
2022-03-28 13:01:44 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 13:01:45 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 13:01:45 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 13:01:45 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (464336ms till timeout)
2022-03-28 13:01:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (205942ms till timeout)
2022-03-28 13:01:45 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:01:45 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (217222ms till timeout)
2022-03-28 13:01:46 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 13:01:46 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (204832ms till timeout)
2022-03-28 13:01:47 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:01:47 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 13:01:47 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 13:01:47 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (462772ms till timeout)
2022-03-28 13:01:47 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (216002ms till timeout)
2022-03-28 13:01:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (203721ms till timeout)
2022-03-28 13:01:48 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 13:01:48 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:01:48 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (214783ms till timeout)
2022-03-28 13:01:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (202608ms till timeout)
2022-03-28 13:01:49 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:01:49 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (213561ms till timeout)
2022-03-28 13:01:50 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (201495ms till timeout)
2022-03-28 13:01:50 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:01:50 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (212338ms till timeout)
2022-03-28 13:01:51 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (200382ms till timeout)
2022-03-28 13:01:51 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:01:52 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (210954ms till timeout)
2022-03-28 13:01:52 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (199230ms till timeout)
2022-03-28 13:01:52 [ForkJoinPool-1-worker-13] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-0b062573-kafka-clients not ready, will try again in 10000 ms (447964ms till timeout)
2022-03-28 13:01:52 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-0b062573-scraper not ready, will try again in 10000 ms (448839ms till timeout)
2022-03-28 13:01:53 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:01:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (198114ms till timeout)
2022-03-28 13:01:53 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (209620ms till timeout)
2022-03-28 13:01:53 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 13:01:53 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 13:01:53 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (456276ms till timeout)
2022-03-28 13:01:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (197003ms till timeout)
2022-03-28 13:01:54 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 13:01:54 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:01:54 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (208395ms till timeout)
2022-03-28 13:01:55 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 13:01:55 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 13:01:55 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (454752ms till timeout)
2022-03-28 13:01:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (195891ms till timeout)
2022-03-28 13:01:55 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:01:55 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (207177ms till timeout)
2022-03-28 13:01:56 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 13:01:56 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 13:01:56 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 13:01:56 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (453208ms till timeout)
2022-03-28 13:01:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (194779ms till timeout)
2022-03-28 13:01:57 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:01:57 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (205956ms till timeout)
2022-03-28 13:01:57 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 13:01:57 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (193668ms till timeout)
2022-03-28 13:01:58 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:01:58 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (204733ms till timeout)
2022-03-28 13:01:58 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (192555ms till timeout)
2022-03-28 13:01:59 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:01:59 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (203514ms till timeout)
2022-03-28 13:02:00 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (191443ms till timeout)
2022-03-28 13:02:00 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:02:00 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (202297ms till timeout)
2022-03-28 13:02:01 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (190333ms till timeout)
2022-03-28 13:02:01 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:02:02 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (201075ms till timeout)
2022-03-28 13:02:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (189218ms till timeout)
2022-03-28 13:02:03 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:369] ############################################################################
2022-03-28 13:02:03 [ForkJoinPool-1-worker-15] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-8 for test case:testPauseReconciliationInKafkaAndKafkaConnectWithConnector
2022-03-28 13:02:03 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Namespace namespace-8 removal
2022-03-28 13:02:03 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 13:02:03 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 13:02:03 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 13:02:03 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (446708ms till timeout)
2022-03-28 13:02:03 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:02:03 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (199792ms till timeout)
2022-03-28 13:02:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (188066ms till timeout)
2022-03-28 13:02:03 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 13:02:03 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 13:02:03 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (479553ms till timeout)
2022-03-28 13:02:04 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 13:02:04 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:02:04 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (198569ms till timeout)
2022-03-28 13:02:04 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 13:02:04 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 13:02:04 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 0
2022-03-28 13:02:04 [ForkJoinPool-1-worker-1] TRACE [TestUtils:176] Namespace namespace-6 removal not ready, will try again in 1000 ms (445230ms till timeout)
2022-03-28 13:02:04 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (186812ms till timeout)
2022-03-28 13:02:05 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 13:02:05 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 13:02:05 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (478096ms till timeout)
2022-03-28 13:02:05 [ForkJoinPool-1-worker-1] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 13:02:05 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:02:05 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (197352ms till timeout)
2022-03-28 13:02:05 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (185629ms till timeout)
2022-03-28 13:02:06 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 13:02:06 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-9 get Namespace namespace-6 -o yaml
2022-03-28 13:02:06 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Return code: 1
2022-03-28 13:02:06 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 13:02:06 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-6" not found
2022-03-28 13:02:06 [ForkJoinPool-1-worker-1] DEBUG [Exec:419] ======STDERR END======
2022-03-28 13:02:06 [ForkJoinPool-1-worker-1] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[namespace-8], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[cruise-control-configuration-st], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[infra-namespace, reconciliation-st], io.strimzi.test.logs.CollectorElement@f4405b3b=[], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 13:02:06 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:267] testDeployAndUnDeployCruiseControl - Notifies waiting test cases:[testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testTlsExternalUserWithQuotas, testUserTemplate, testUpdateUser, testTlsExternalUser, testReceiveSimpleMessageTlsScramSha, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testSendSimpleMessageTlsScramSha, testUserWithNameMoreThan64Chars, testTlsUserWithQuotas, testConfigurationFileIsCreated, testScramUserWithQuotas, testDeployAndUnDeployCruiseControl, testConfigurationPerformanceOptions] to and randomly select one to start execution
2022-03-28 13:02:06 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:93] [cruisecontrol.CruiseControlConfigurationST] - Removing parallel test: testDeployAndUnDeployCruiseControl
2022-03-28 13:02:06 [ForkJoinPool-1-worker-1] DEBUG [SuiteThreadController:97] [cruisecontrol.CruiseControlConfigurationST] - Parallel test count: 3
2022-03-28 13:02:06 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:29] io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST.testDeployAndUnDeployCruiseControl-FINISHED
2022-03-28 13:02:06 [ForkJoinPool-1-worker-1] INFO  [TestSeparator:30] ############################################################################
2022-03-28 13:02:06 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:689] ============================================================================
2022-03-28 13:02:06 [ForkJoinPool-1-worker-3] DEBUG [AbstractST:690] [cruisecontrol.CruiseControlConfigurationST - After All] - Clean up after test suite
2022-03-28 13:02:06 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:344] ############################################################################
2022-03-28 13:02:06 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:346] In context CruiseControlConfigurationST is everything deleted.
2022-03-28 13:02:06 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:369] ############################################################################
2022-03-28 13:02:06 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Namespace cruise-control-configuration-st removal
2022-03-28 13:02:06 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 13:02:06 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 13:02:06 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 13:02:06 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (476659ms till timeout)
2022-03-28 13:02:06 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 13:02:06 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:02:06 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace cruise-control-configuration-st removal not ready, will try again in 1000 ms (479579ms till timeout)
2022-03-28 13:02:06 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:02:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (184516ms till timeout)
2022-03-28 13:02:07 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (196020ms till timeout)
2022-03-28 13:02:07 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 13:02:07 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 13:02:07 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 13:02:07 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 13:02:07 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (475200ms till timeout)
2022-03-28 13:02:08 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 13:02:08 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:02:08 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace cruise-control-configuration-st removal not ready, will try again in 1000 ms (478128ms till timeout)
2022-03-28 13:02:08 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (183400ms till timeout)
2022-03-28 13:02:08 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:02:08 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (194798ms till timeout)
2022-03-28 13:02:08 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 13:02:09 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 13:02:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (182289ms till timeout)
2022-03-28 13:02:09 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 13:02:09 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 13:02:09 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (473711ms till timeout)
2022-03-28 13:02:09 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:02:09 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 13:02:09 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:02:09 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace cruise-control-configuration-st removal not ready, will try again in 1000 ms (476651ms till timeout)
2022-03-28 13:02:09 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (193579ms till timeout)
2022-03-28 13:02:10 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (181179ms till timeout)
2022-03-28 13:02:10 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 13:02:10 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 13:02:10 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:02:10 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (192362ms till timeout)
2022-03-28 13:02:10 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 13:02:10 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 13:02:10 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (472250ms till timeout)
2022-03-28 13:02:10 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 13:02:10 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:02:10 [ForkJoinPool-1-worker-3] TRACE [TestUtils:176] Namespace cruise-control-configuration-st removal not ready, will try again in 1000 ms (475199ms till timeout)
2022-03-28 13:02:11 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (180067ms till timeout)
2022-03-28 13:02:11 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:02:11 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 13:02:11 [ForkJoinPool-1-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 13:02:11 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (191142ms till timeout)
2022-03-28 13:02:12 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 13:02:12 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 13:02:12 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (470757ms till timeout)
2022-03-28 13:02:12 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-9 get Namespace cruise-control-configuration-st -o yaml
2022-03-28 13:02:12 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Return code: 1
2022-03-28 13:02:12 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 13:02:12 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] Error from server (NotFound): namespaces "cruise-control-configuration-st" not found
2022-03-28 13:02:12 [ForkJoinPool-1-worker-3] DEBUG [Exec:419] ======STDERR END======
2022-03-28 13:02:12 [ForkJoinPool-1-worker-3] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[namespace-8], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[infra-namespace, reconciliation-st], io.strimzi.test.logs.CollectorElement@f4405b3b=[], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 13:02:12 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:254] CruiseControlConfigurationST - Notifies waiting test suites:[UserST, TopicST, ReconciliationST, CruiseControlConfigurationST, HttpBridgeScramShaST, ThrottlingQuotaST] to and randomly select one to start execution
2022-03-28 13:02:12 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:85] [cruisecontrol.CruiseControlConfigurationST] - Removing parallel suite: CruiseControlConfigurationST
2022-03-28 13:02:12 [ForkJoinPool-1-worker-3] DEBUG [SuiteThreadController:89] [cruisecontrol.CruiseControlConfigurationST] - Parallel suites count: 3
[INFO] Tests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 982.592 s - in io.strimzi.systemtest.cruisecontrol.CruiseControlConfigurationST
2022-03-28 13:02:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (178955ms till timeout)
2022-03-28 13:02:13 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:02:13 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (189924ms till timeout)
2022-03-28 13:02:13 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 13:02:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (177839ms till timeout)
2022-03-28 13:02:13 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 13:02:13 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 13:02:13 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (469309ms till timeout)
2022-03-28 13:02:14 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:02:14 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (188707ms till timeout)
2022-03-28 13:02:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (176727ms till timeout)
2022-03-28 13:02:14 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 13:02:15 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 13:02:15 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 13:02:15 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace namespace-8 removal not ready, will try again in 1000 ms (467852ms till timeout)
2022-03-28 13:02:15 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:02:15 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (187488ms till timeout)
2022-03-28 13:02:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (175612ms till timeout)
2022-03-28 13:02:16 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 13:02:16 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-9 get Namespace namespace-8 -o yaml
2022-03-28 13:02:16 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 1
2022-03-28 13:02:16 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 13:02:16 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-8" not found
2022-03-28 13:02:16 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] ======STDERR END======
2022-03-28 13:02:16 [ForkJoinPool-1-worker-15] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[infra-namespace, reconciliation-st], io.strimzi.test.logs.CollectorElement@f4405b3b=[], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 13:02:16 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:267] testPauseReconciliationInKafkaAndKafkaConnectWithConnector - Notifies waiting test cases:[testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testTlsExternalUserWithQuotas, testUserTemplate, testUpdateUser, testTlsExternalUser, testReceiveSimpleMessageTlsScramSha, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testSendSimpleMessageTlsScramSha, testUserWithNameMoreThan64Chars, testTlsUserWithQuotas, testConfigurationFileIsCreated, testScramUserWithQuotas, testDeployAndUnDeployCruiseControl, testConfigurationPerformanceOptions] to and randomly select one to start execution
2022-03-28 13:02:16 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:93] [operators.ReconciliationST] - Removing parallel test: testPauseReconciliationInKafkaAndKafkaConnectWithConnector
2022-03-28 13:02:16 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:97] [operators.ReconciliationST] - Parallel test count: 2
2022-03-28 13:02:16 [ForkJoinPool-1-worker-15] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.ReconciliationST.testPauseReconciliationInKafkaAndKafkaConnectWithConnector-FINISHED
2022-03-28 13:02:16 [ForkJoinPool-1-worker-15] INFO  [TestSeparator:30] ############################################################################
2022-03-28 13:02:16 [ForkJoinPool-1-worker-15] DEBUG [AbstractST:689] ============================================================================
2022-03-28 13:02:16 [ForkJoinPool-1-worker-15] DEBUG [AbstractST:690] [operators.ReconciliationST - After All] - Clean up after test suite
2022-03-28 13:02:16 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:344] ############################################################################
2022-03-28 13:02:16 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:346] In context ReconciliationST is everything deleted.
2022-03-28 13:02:16 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:369] ############################################################################
2022-03-28 13:02:16 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:02:16 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Namespace reconciliation-st removal
2022-03-28 13:02:16 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 13:02:17 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (186131ms till timeout)
2022-03-28 13:02:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (174405ms till timeout)
2022-03-28 13:02:17 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 13:02:17 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 13:02:17 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace reconciliation-st removal not ready, will try again in 1000 ms (479524ms till timeout)
2022-03-28 13:02:18 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:02:18 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (184912ms till timeout)
2022-03-28 13:02:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (173191ms till timeout)
2022-03-28 13:02:18 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 13:02:18 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 13:02:18 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 13:02:18 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace reconciliation-st removal not ready, will try again in 1000 ms (478059ms till timeout)
2022-03-28 13:02:19 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:02:19 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (172081ms till timeout)
2022-03-28 13:02:19 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (183587ms till timeout)
2022-03-28 13:02:19 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 13:02:20 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 13:02:20 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 13:02:20 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace reconciliation-st removal not ready, will try again in 1000 ms (476594ms till timeout)
2022-03-28 13:02:20 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (170969ms till timeout)
2022-03-28 13:02:20 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:02:20 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (182365ms till timeout)
2022-03-28 13:02:21 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 13:02:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (169856ms till timeout)
2022-03-28 13:02:21 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 13:02:21 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 0
2022-03-28 13:02:21 [ForkJoinPool-1-worker-15] TRACE [TestUtils:176] Namespace reconciliation-st removal not ready, will try again in 1000 ms (475125ms till timeout)
2022-03-28 13:02:21 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:02:21 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (181147ms till timeout)
2022-03-28 13:02:22 [ForkJoinPool-1-worker-15] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 13:02:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (168742ms till timeout)
2022-03-28 13:02:23 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:02:23 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-9 get Namespace reconciliation-st -o yaml
2022-03-28 13:02:23 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Return code: 1
2022-03-28 13:02:23 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 13:02:23 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] Error from server (NotFound): namespaces "reconciliation-st" not found
2022-03-28 13:02:23 [ForkJoinPool-1-worker-15] DEBUG [Exec:419] ======STDERR END======
2022-03-28 13:02:23 [ForkJoinPool-1-worker-15] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@34093228=[throttling-quota-st], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[], io.strimzi.test.logs.CollectorElement@b850ef2a=[infra-namespace, reconciliation-st], io.strimzi.test.logs.CollectorElement@f4405b3b=[], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 13:02:23 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:254] ReconciliationST - Notifies waiting test suites:[UserST, TopicST, ReconciliationST, CruiseControlConfigurationST, HttpBridgeScramShaST, ThrottlingQuotaST] to and randomly select one to start execution
2022-03-28 13:02:23 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:85] [operators.ReconciliationST] - Removing parallel suite: ReconciliationST
2022-03-28 13:02:23 [ForkJoinPool-1-worker-15] DEBUG [SuiteThreadController:89] [operators.ReconciliationST] - Parallel suites count: 2
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 989.176 s - in io.strimzi.systemtest.operators.ReconciliationST
2022-03-28 13:02:23 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (179930ms till timeout)
2022-03-28 13:02:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (167631ms till timeout)
2022-03-28 13:02:24 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:02:24 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (178710ms till timeout)
2022-03-28 13:02:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (166519ms till timeout)
2022-03-28 13:02:25 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:02:25 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (177491ms till timeout)
2022-03-28 13:02:26 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (165406ms till timeout)
2022-03-28 13:02:26 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:02:26 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (176272ms till timeout)
2022-03-28 13:02:27 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (164295ms till timeout)
2022-03-28 13:02:27 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:02:28 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (175050ms till timeout)
2022-03-28 13:02:28 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (163185ms till timeout)
2022-03-28 13:02:29 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:02:29 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (173826ms till timeout)
2022-03-28 13:02:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (162074ms till timeout)
2022-03-28 13:02:30 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:02:30 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (172607ms till timeout)
2022-03-28 13:02:30 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (160874ms till timeout)
2022-03-28 13:02:31 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:02:31 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (171384ms till timeout)
2022-03-28 13:02:31 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (159662ms till timeout)
2022-03-28 13:02:32 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:02:32 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (158551ms till timeout)
2022-03-28 13:02:33 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (170057ms till timeout)
2022-03-28 13:02:34 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (157439ms till timeout)
2022-03-28 13:02:34 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:02:34 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (168834ms till timeout)
2022-03-28 13:02:35 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (156327ms till timeout)
2022-03-28 13:02:35 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:02:35 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (167613ms till timeout)
2022-03-28 13:02:36 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (155217ms till timeout)
2022-03-28 13:02:36 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:02:36 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (166394ms till timeout)
2022-03-28 13:02:37 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (154104ms till timeout)
2022-03-28 13:02:37 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:02:37 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (165175ms till timeout)
2022-03-28 13:02:38 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (152992ms till timeout)
2022-03-28 13:02:39 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:02:39 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (163953ms till timeout)
2022-03-28 13:02:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (151874ms till timeout)
2022-03-28 13:02:40 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:02:40 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (162734ms till timeout)
2022-03-28 13:02:40 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (150764ms till timeout)
2022-03-28 13:02:41 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:02:41 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (161515ms till timeout)
2022-03-28 13:02:41 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (149654ms till timeout)
2022-03-28 13:02:42 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:02:42 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (160293ms till timeout)
2022-03-28 13:02:42 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (148541ms till timeout)
2022-03-28 13:02:43 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:02:44 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (159073ms till timeout)
2022-03-28 13:02:44 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (147349ms till timeout)
2022-03-28 13:02:45 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:02:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (146239ms till timeout)
2022-03-28 13:02:45 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (157744ms till timeout)
2022-03-28 13:02:46 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (145127ms till timeout)
2022-03-28 13:02:46 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:02:46 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (156524ms till timeout)
2022-03-28 13:02:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (144015ms till timeout)
2022-03-28 13:02:47 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:02:47 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (155306ms till timeout)
2022-03-28 13:02:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (142903ms till timeout)
2022-03-28 13:02:48 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:02:49 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (154080ms till timeout)
2022-03-28 13:02:49 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (141792ms till timeout)
2022-03-28 13:02:50 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:02:50 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (152861ms till timeout)
2022-03-28 13:02:50 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (140680ms till timeout)
2022-03-28 13:02:51 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:02:51 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (151640ms till timeout)
2022-03-28 13:02:51 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (139564ms till timeout)
2022-03-28 13:02:52 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:02:52 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (150423ms till timeout)
2022-03-28 13:02:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (138451ms till timeout)
2022-03-28 13:02:53 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:02:53 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (149204ms till timeout)
2022-03-28 13:02:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (137340ms till timeout)
2022-03-28 13:02:55 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:02:55 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (147984ms till timeout)
2022-03-28 13:02:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (136229ms till timeout)
2022-03-28 13:02:56 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:02:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (135069ms till timeout)
2022-03-28 13:02:56 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (146575ms till timeout)
2022-03-28 13:02:57 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (133957ms till timeout)
2022-03-28 13:02:57 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:02:57 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (145354ms till timeout)
2022-03-28 13:02:58 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (132847ms till timeout)
2022-03-28 13:02:58 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:02:58 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (144136ms till timeout)
2022-03-28 13:02:59 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (131735ms till timeout)
2022-03-28 13:03:00 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:03:00 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (142917ms till timeout)
2022-03-28 13:03:00 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (130622ms till timeout)
2022-03-28 13:03:01 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:03:01 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (141698ms till timeout)
2022-03-28 13:03:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (129510ms till timeout)
2022-03-28 13:03:02 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:03:02 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (140480ms till timeout)
2022-03-28 13:03:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (128400ms till timeout)
2022-03-28 13:03:03 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:03:03 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (139259ms till timeout)
2022-03-28 13:03:04 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (127288ms till timeout)
2022-03-28 13:03:04 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:03:05 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (138040ms till timeout)
2022-03-28 13:03:05 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (126174ms till timeout)
2022-03-28 13:03:06 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:03:06 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (136822ms till timeout)
2022-03-28 13:03:06 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (125063ms till timeout)
2022-03-28 13:03:07 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:03:07 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (135601ms till timeout)
2022-03-28 13:03:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (123844ms till timeout)
2022-03-28 13:03:08 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:03:08 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (134381ms till timeout)
2022-03-28 13:03:08 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (122658ms till timeout)
2022-03-28 13:03:09 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:03:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (121546ms till timeout)
2022-03-28 13:03:10 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (133052ms till timeout)
2022-03-28 13:03:11 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (120432ms till timeout)
2022-03-28 13:03:11 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:03:11 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (131829ms till timeout)
2022-03-28 13:03:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (119322ms till timeout)
2022-03-28 13:03:12 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:03:12 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (130609ms till timeout)
2022-03-28 13:03:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (118211ms till timeout)
2022-03-28 13:03:13 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:03:13 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129391ms till timeout)
2022-03-28 13:03:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (117098ms till timeout)
2022-03-28 13:03:14 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:03:14 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (128165ms till timeout)
2022-03-28 13:03:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (115986ms till timeout)
2022-03-28 13:03:16 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:03:16 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (126950ms till timeout)
2022-03-28 13:03:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (114868ms till timeout)
2022-03-28 13:03:17 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:03:17 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (125728ms till timeout)
2022-03-28 13:03:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (113755ms till timeout)
2022-03-28 13:03:18 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:03:18 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (124507ms till timeout)
2022-03-28 13:03:18 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (112641ms till timeout)
2022-03-28 13:03:19 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:03:19 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (123288ms till timeout)
2022-03-28 13:03:19 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (111530ms till timeout)
2022-03-28 13:03:20 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:03:21 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (122070ms till timeout)
2022-03-28 13:03:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (110349ms till timeout)
2022-03-28 13:03:22 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:01:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:03:22 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (109237ms till timeout)
2022-03-28 13:03:22 [ForkJoinPool-1-worker-11] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 13:03:22 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 13:03:22 [ForkJoinPool-1-worker-11] INFO  [PodUtils:189] Message All topics created found in create-admin-my-cluster-b6bd312e-kafka-clients-x8k2v log
2022-03-28 13:03:22 [ForkJoinPool-1-worker-11] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment create-admin-my-cluster-b6bd312e-kafka-clients deletion
2022-03-28 13:03:22 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for ReplicaSet create-admin-my-cluster-b6bd312e-kafka-clients to be deleted
2022-03-28 13:03:22 [ForkJoinPool-1-worker-11] DEBUG [JobUtils:40] Job create-admin-my-cluster-b6bd312e-kafka-clients was deleted
2022-03-28 13:03:22 [ForkJoinPool-1-worker-11] INFO  [ThrottlingQuotaST:112] Executing 5/5 iteration.
2022-03-28 13:03:22 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st
2022-03-28 13:03:22 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:create-admin-my-cluster-b6bd312e-kafka-clients
2022-03-28 13:03:23 [ForkJoinPool-1-worker-11] INFO  [JobUtils:81] Waiting for job: create-admin-my-cluster-b6bd312e-kafka-clients will be in active state
2022-03-28 13:03:23 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 13:03:23 [ForkJoinPool-1-worker-11] INFO  [ClientUtils:76] Waiting for producer/consumer:create-admin-my-cluster-b6bd312e-kafka-clients to finished
2022-03-28 13:03:23 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 13:03:23 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:03:23 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (219783ms till timeout)
2022-03-28 13:03:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (108027ms till timeout)
2022-03-28 13:03:24 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:03:24 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (218565ms till timeout)
2022-03-28 13:03:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (106813ms till timeout)
2022-03-28 13:03:25 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:03:25 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (217349ms till timeout)
2022-03-28 13:03:25 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (105595ms till timeout)
2022-03-28 13:03:26 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:03:27 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (104484ms till timeout)
2022-03-28 13:03:27 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (216021ms till timeout)
2022-03-28 13:03:28 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (103370ms till timeout)
2022-03-28 13:03:28 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:03:28 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (214798ms till timeout)
2022-03-28 13:03:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (102259ms till timeout)
2022-03-28 13:03:29 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:03:29 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (213557ms till timeout)
2022-03-28 13:03:30 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (101149ms till timeout)
2022-03-28 13:03:30 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:03:30 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (212340ms till timeout)
2022-03-28 13:03:31 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (100029ms till timeout)
2022-03-28 13:03:31 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:03:32 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (211123ms till timeout)
2022-03-28 13:03:32 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (98917ms till timeout)
2022-03-28 13:03:33 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:03:33 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (209904ms till timeout)
2022-03-28 13:03:33 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (97805ms till timeout)
2022-03-28 13:03:34 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:03:34 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (208684ms till timeout)
2022-03-28 13:03:34 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (96694ms till timeout)
2022-03-28 13:03:35 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:03:35 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (207465ms till timeout)
2022-03-28 13:03:35 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (95583ms till timeout)
2022-03-28 13:03:36 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:03:36 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (206239ms till timeout)
2022-03-28 13:03:37 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (94471ms till timeout)
2022-03-28 13:03:38 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:03:38 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (205021ms till timeout)
2022-03-28 13:03:38 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (93263ms till timeout)
2022-03-28 13:03:39 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:03:39 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (203802ms till timeout)
2022-03-28 13:03:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (92049ms till timeout)
2022-03-28 13:03:40 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:03:40 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (202584ms till timeout)
2022-03-28 13:03:40 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (90830ms till timeout)
2022-03-28 13:03:41 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:03:41 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (89718ms till timeout)
2022-03-28 13:03:41 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (201255ms till timeout)
2022-03-28 13:03:42 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (88604ms till timeout)
2022-03-28 13:03:43 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:03:43 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (200033ms till timeout)
2022-03-28 13:03:44 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (87492ms till timeout)
2022-03-28 13:03:44 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:03:44 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (198814ms till timeout)
2022-03-28 13:03:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (86381ms till timeout)
2022-03-28 13:03:45 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:03:45 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (197595ms till timeout)
2022-03-28 13:03:46 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (85268ms till timeout)
2022-03-28 13:03:46 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:03:46 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (196375ms till timeout)
2022-03-28 13:03:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (84156ms till timeout)
2022-03-28 13:03:47 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:03:48 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (195157ms till timeout)
2022-03-28 13:03:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (83043ms till timeout)
2022-03-28 13:03:49 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:03:49 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (193936ms till timeout)
2022-03-28 13:03:49 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (81932ms till timeout)
2022-03-28 13:03:50 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:03:50 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (192708ms till timeout)
2022-03-28 13:03:50 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (80820ms till timeout)
2022-03-28 13:03:51 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:03:51 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (191488ms till timeout)
2022-03-28 13:03:51 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (79709ms till timeout)
2022-03-28 13:03:52 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:03:52 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (190271ms till timeout)
2022-03-28 13:03:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (78483ms till timeout)
2022-03-28 13:03:54 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:03:54 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (189047ms till timeout)
2022-03-28 13:03:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (77293ms till timeout)
2022-03-28 13:03:55 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:03:55 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (76182ms till timeout)
2022-03-28 13:03:55 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (187718ms till timeout)
2022-03-28 13:03:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (75070ms till timeout)
2022-03-28 13:03:56 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:03:56 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (186497ms till timeout)
2022-03-28 13:03:57 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (73957ms till timeout)
2022-03-28 13:03:57 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:03:57 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (185278ms till timeout)
2022-03-28 13:03:58 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (72845ms till timeout)
2022-03-28 13:03:58 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:03:59 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (184058ms till timeout)
2022-03-28 13:03:59 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (71734ms till timeout)
2022-03-28 13:04:00 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:00 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (182839ms till timeout)
2022-03-28 13:04:00 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (70620ms till timeout)
2022-03-28 13:04:01 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:01 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (181619ms till timeout)
2022-03-28 13:04:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (69500ms till timeout)
2022-03-28 13:04:02 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:02 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (180401ms till timeout)
2022-03-28 13:04:03 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (68388ms till timeout)
2022-03-28 13:04:03 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:03 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (179182ms till timeout)
2022-03-28 13:04:04 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (67276ms till timeout)
2022-03-28 13:04:05 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:05 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (177964ms till timeout)
2022-03-28 13:04:05 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (66165ms till timeout)
2022-03-28 13:04:06 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:06 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (176744ms till timeout)
2022-03-28 13:04:06 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (64990ms till timeout)
2022-03-28 13:04:07 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (63879ms till timeout)
2022-03-28 13:04:07 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (175416ms till timeout)
2022-03-28 13:04:08 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (62768ms till timeout)
2022-03-28 13:04:08 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:08 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (174187ms till timeout)
2022-03-28 13:04:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (61656ms till timeout)
2022-03-28 13:04:10 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:10 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (172968ms till timeout)
2022-03-28 13:04:10 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (60544ms till timeout)
2022-03-28 13:04:11 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:11 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (171750ms till timeout)
2022-03-28 13:04:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (59433ms till timeout)
2022-03-28 13:04:12 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:12 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (170532ms till timeout)
2022-03-28 13:04:13 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (58320ms till timeout)
2022-03-28 13:04:13 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:13 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (169313ms till timeout)
2022-03-28 13:04:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (57209ms till timeout)
2022-03-28 13:04:14 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:15 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (168093ms till timeout)
2022-03-28 13:04:15 [ForkJoinPool-1-worker-7] INFO  [PodUtils:189] Message org.apache.kafka.common.errors.ThrottlingQuotaExceededException: The throttling quota has been exceeded. found in alter-admin-my-cluster-d4dad0de-kafka-clients-zql5r log
2022-03-28 13:04:15 [ForkJoinPool-1-worker-7] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment alter-admin-my-cluster-d4dad0de-kafka-clients deletion
2022-03-28 13:04:15 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for ReplicaSet alter-admin-my-cluster-d4dad0de-kafka-clients to be deleted
2022-03-28 13:04:15 [ForkJoinPool-1-worker-7] DEBUG [JobUtils:40] Job alter-admin-my-cluster-d4dad0de-kafka-clients was deleted
2022-03-28 13:04:15 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:155] Create/Update Job teardown-delete in namespace throttling-quota-st
2022-03-28 13:04:15 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:teardown-delete
2022-03-28 13:04:15 [ForkJoinPool-1-worker-7] INFO  [JobUtils:81] Waiting for job: teardown-delete will be in active state
2022-03-28 13:04:15 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 13:04:16 [ForkJoinPool-1-worker-7] INFO  [ClientUtils:76] Waiting for producer/consumer:teardown-delete to finished
2022-03-28 13:04:16 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 13:04:16 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:16 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:16 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (166765ms till timeout)
2022-03-28 13:04:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129571ms till timeout)
2022-03-28 13:04:17 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:17 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:17 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (165425ms till timeout)
2022-03-28 13:04:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (128232ms till timeout)
2022-03-28 13:04:18 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:18 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:19 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (164067ms till timeout)
2022-03-28 13:04:19 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (126876ms till timeout)
2022-03-28 13:04:20 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:20 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:20 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (162741ms till timeout)
2022-03-28 13:04:20 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (125550ms till timeout)
2022-03-28 13:04:21 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:21 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:21 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (161414ms till timeout)
2022-03-28 13:04:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (124222ms till timeout)
2022-03-28 13:04:22 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:22 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:23 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (160087ms till timeout)
2022-03-28 13:04:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (122895ms till timeout)
2022-03-28 13:04:24 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:24 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:24 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (158760ms till timeout)
2022-03-28 13:04:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (121566ms till timeout)
2022-03-28 13:04:25 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:25 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:25 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (157435ms till timeout)
2022-03-28 13:04:25 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (120243ms till timeout)
2022-03-28 13:04:26 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:26 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:27 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (156108ms till timeout)
2022-03-28 13:04:27 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (118917ms till timeout)
2022-03-28 13:04:28 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:28 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:28 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (154781ms till timeout)
2022-03-28 13:04:28 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (117590ms till timeout)
2022-03-28 13:04:29 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:29 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:29 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (153455ms till timeout)
2022-03-28 13:04:29 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (116264ms till timeout)
2022-03-28 13:04:30 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:30 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:31 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (152131ms till timeout)
2022-03-28 13:04:31 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (114940ms till timeout)
2022-03-28 13:04:32 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:32 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:32 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (150805ms till timeout)
2022-03-28 13:04:32 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (113612ms till timeout)
2022-03-28 13:04:33 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:33 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:33 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (149478ms till timeout)
2022-03-28 13:04:33 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (112284ms till timeout)
2022-03-28 13:04:34 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:34 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:35 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (148152ms till timeout)
2022-03-28 13:04:35 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (110960ms till timeout)
2022-03-28 13:04:36 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:36 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:36 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (146826ms till timeout)
2022-03-28 13:04:36 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (109634ms till timeout)
2022-03-28 13:04:37 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:37 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:37 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (145493ms till timeout)
2022-03-28 13:04:37 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (108302ms till timeout)
2022-03-28 13:04:38 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:38 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:38 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (144169ms till timeout)
2022-03-28 13:04:39 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (106977ms till timeout)
2022-03-28 13:04:40 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:40 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:40 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (142845ms till timeout)
2022-03-28 13:04:40 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (105653ms till timeout)
2022-03-28 13:04:41 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:41 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:41 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (141521ms till timeout)
2022-03-28 13:04:41 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (104330ms till timeout)
2022-03-28 13:04:42 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:42 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:42 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (140197ms till timeout)
2022-03-28 13:04:43 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (103006ms till timeout)
2022-03-28 13:04:44 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:44 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:44 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (138873ms till timeout)
2022-03-28 13:04:44 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (101682ms till timeout)
2022-03-28 13:04:45 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:45 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:45 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (137549ms till timeout)
2022-03-28 13:04:45 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (100358ms till timeout)
2022-03-28 13:04:46 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:46 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:46 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (136222ms till timeout)
2022-03-28 13:04:47 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (99031ms till timeout)
2022-03-28 13:04:48 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:48 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:48 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (134899ms till timeout)
2022-03-28 13:04:48 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (97708ms till timeout)
2022-03-28 13:04:49 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:49 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:49 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (133574ms till timeout)
2022-03-28 13:04:49 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (96383ms till timeout)
2022-03-28 13:04:50 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:50 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:50 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (132249ms till timeout)
2022-03-28 13:04:51 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (95058ms till timeout)
2022-03-28 13:04:52 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:52 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:52 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (130922ms till timeout)
2022-03-28 13:04:52 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (93730ms till timeout)
2022-03-28 13:04:53 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:53 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:53 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129595ms till timeout)
2022-03-28 13:04:53 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (92403ms till timeout)
2022-03-28 13:04:54 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:54 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:54 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (128271ms till timeout)
2022-03-28 13:04:54 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (91080ms till timeout)
2022-03-28 13:04:55 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:56 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:56 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (126948ms till timeout)
2022-03-28 13:04:56 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (89757ms till timeout)
2022-03-28 13:04:57 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:57 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:57 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (125622ms till timeout)
2022-03-28 13:04:57 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (88431ms till timeout)
2022-03-28 13:04:58 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:58 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:04:58 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (124297ms till timeout)
2022-03-28 13:04:58 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (87106ms till timeout)
2022-03-28 13:04:59 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:05:00 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:05:00 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (122973ms till timeout)
2022-03-28 13:05:00 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (85781ms till timeout)
2022-03-28 13:05:01 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:03:18Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:05:01 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:05:01 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (84429ms till timeout)
2022-03-28 13:05:01 [ForkJoinPool-1-worker-11] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 13:05:01 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 13:05:01 [ForkJoinPool-1-worker-11] INFO  [PodUtils:189] Message All topics created found in create-admin-my-cluster-b6bd312e-kafka-clients-7t7zr log
2022-03-28 13:05:02 [ForkJoinPool-1-worker-11] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment create-admin-my-cluster-b6bd312e-kafka-clients deletion
2022-03-28 13:05:02 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for ReplicaSet create-admin-my-cluster-b6bd312e-kafka-clients to be deleted
2022-03-28 13:05:02 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] ReplicaSet create-admin-my-cluster-b6bd312e-kafka-clients to be deleted not ready, will try again in 5000 ms (179891ms till timeout)
2022-03-28 13:05:02 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:05:02 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (83212ms till timeout)
2022-03-28 13:05:03 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:05:04 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (81993ms till timeout)
2022-03-28 13:05:05 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:05:05 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (80775ms till timeout)
2022-03-28 13:05:06 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:05:06 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (79556ms till timeout)
2022-03-28 13:05:07 [ForkJoinPool-1-worker-11] DEBUG [JobUtils:40] Job create-admin-my-cluster-b6bd312e-kafka-clients was deleted
2022-03-28 13:05:07 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st
2022-03-28 13:05:07 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:delete-admin-my-cluster-b6bd312e-kafka-clients
2022-03-28 13:05:07 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:05:07 [ForkJoinPool-1-worker-11] INFO  [JobUtils:81] Waiting for job: delete-admin-my-cluster-b6bd312e-kafka-clients will be in active state
2022-03-28 13:05:07 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 13:05:07 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (78230ms till timeout)
2022-03-28 13:05:08 [ForkJoinPool-1-worker-11] INFO  [PodUtils:186] Waiting for message will be in the log
2022-03-28 13:05:08 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Waiting for message will be in the log
2022-03-28 13:05:08 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (299890ms till timeout)
2022-03-28 13:05:08 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:05:09 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (77011ms till timeout)
2022-03-28 13:05:09 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (298778ms till timeout)
2022-03-28 13:05:10 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:05:10 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (75792ms till timeout)
2022-03-28 13:05:10 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (297664ms till timeout)
2022-03-28 13:05:11 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:05:11 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (74574ms till timeout)
2022-03-28 13:05:11 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (296451ms till timeout)
2022-03-28 13:05:12 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:05:12 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (73356ms till timeout)
2022-03-28 13:05:12 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (295233ms till timeout)
2022-03-28 13:05:13 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:05:13 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (294122ms till timeout)
2022-03-28 13:05:14 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (72029ms till timeout)
2022-03-28 13:05:15 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (293009ms till timeout)
2022-03-28 13:05:15 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:05:15 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (70804ms till timeout)
2022-03-28 13:05:16 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (291686ms till timeout)
2022-03-28 13:05:16 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:05:16 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (69586ms till timeout)
2022-03-28 13:05:17 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (290366ms till timeout)
2022-03-28 13:05:17 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:05:17 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (68270ms till timeout)
2022-03-28 13:05:19 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (289047ms till timeout)
2022-03-28 13:05:19 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:05:19 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (66953ms till timeout)
2022-03-28 13:05:20 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (287727ms till timeout)
2022-03-28 13:05:20 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:05:20 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (65631ms till timeout)
2022-03-28 13:05:21 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (286408ms till timeout)
2022-03-28 13:05:21 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:05:21 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (64314ms till timeout)
2022-03-28 13:05:22 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (285090ms till timeout)
2022-03-28 13:05:22 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:05:23 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (62994ms till timeout)
2022-03-28 13:05:24 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (283770ms till timeout)
2022-03-28 13:05:24 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:05:24 [ForkJoinPool-1-worker-7] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (61675ms till timeout)
2022-03-28 13:05:25 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (282452ms till timeout)
2022-03-28 13:05:25 [ForkJoinPool-1-worker-7] DEBUG [ClientUtils:79] Job teardown-delete in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:04:11Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:05:25 [ForkJoinPool-1-worker-7] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment teardown-delete deletion
2022-03-28 13:05:25 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for ReplicaSet teardown-delete to be deleted
2022-03-28 13:05:25 [ForkJoinPool-1-worker-7] DEBUG [JobUtils:40] Job teardown-delete was deleted
2022-03-28 13:05:25 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:674] ============================================================================
2022-03-28 13:05:25 [ForkJoinPool-1-worker-7] DEBUG [AbstractST:675] [operators.topic.ThrottlingQuotaST - After Each] - Clean up after test
2022-03-28 13:05:25 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:344] ############################################################################
2022-03-28 13:05:25 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:348] Delete all resources for testThrottlingQuotasCreateAlterPartitions
2022-03-28 13:05:25 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of Job create-admin-my-cluster-d4dad0de-kafka-clients in namespace throttling-quota-st
2022-03-28 13:05:25 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:241] Delete of KafkaUser my-user-1809876063-64793399 in namespace throttling-quota-st
2022-03-28 13:05:25 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:241] Delete of Job create-admin-my-cluster-d4dad0de-kafka-clients in namespace throttling-quota-st
2022-03-28 13:05:25 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of Job teardown-delete in namespace throttling-quota-st
2022-03-28 13:05:25 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:241] Delete of Job alter-admin-my-cluster-d4dad0de-kafka-clients in namespace throttling-quota-st
2022-03-28 13:05:26 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-1809876063-64793399
2022-03-28 13:05:26 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:alter-admin-my-cluster-d4dad0de-kafka-clients
2022-03-28 13:05:26 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:create-admin-my-cluster-d4dad0de-kafka-clients
2022-03-28 13:05:26 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:create-admin-my-cluster-d4dad0de-kafka-clients
2022-03-28 13:05:26 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:teardown-delete
2022-03-28 13:05:26 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:369] ############################################################################
2022-03-28 13:05:26 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:267] testThrottlingQuotasCreateAlterPartitions - Notifies waiting test cases:[testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testTlsExternalUserWithQuotas, testUserTemplate, testUpdateUser, testTlsExternalUser, testReceiveSimpleMessageTlsScramSha, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testSendSimpleMessageTlsScramSha, testUserWithNameMoreThan64Chars, testTlsUserWithQuotas, testConfigurationFileIsCreated, testScramUserWithQuotas, testDeployAndUnDeployCruiseControl, testConfigurationPerformanceOptions] to and randomly select one to start execution
2022-03-28 13:05:26 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:93] [operators.topic.ThrottlingQuotaST] - Removing parallel test: testThrottlingQuotasCreateAlterPartitions
2022-03-28 13:05:26 [ForkJoinPool-1-worker-7] DEBUG [SuiteThreadController:97] [operators.topic.ThrottlingQuotaST] - Parallel test count: 1
2022-03-28 13:05:26 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testThrottlingQuotasCreateAlterPartitions-FINISHED
2022-03-28 13:05:26 [ForkJoinPool-1-worker-7] INFO  [TestSeparator:30] ############################################################################
2022-03-28 13:05:26 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (281237ms till timeout)
2022-03-28 13:05:28 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (279919ms till timeout)
2022-03-28 13:05:29 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (278598ms till timeout)
2022-03-28 13:05:30 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (277280ms till timeout)
2022-03-28 13:05:32 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (275956ms till timeout)
2022-03-28 13:05:33 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (274636ms till timeout)
2022-03-28 13:05:34 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (273315ms till timeout)
2022-03-28 13:05:36 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (271997ms till timeout)
2022-03-28 13:05:37 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (270678ms till timeout)
2022-03-28 13:05:38 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (269356ms till timeout)
2022-03-28 13:05:40 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (268036ms till timeout)
2022-03-28 13:05:41 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (266717ms till timeout)
2022-03-28 13:05:42 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (265398ms till timeout)
2022-03-28 13:05:43 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (264078ms till timeout)
2022-03-28 13:05:45 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (262756ms till timeout)
2022-03-28 13:05:46 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (261438ms till timeout)
2022-03-28 13:05:47 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (260119ms till timeout)
2022-03-28 13:05:49 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (258800ms till timeout)
2022-03-28 13:05:50 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (257478ms till timeout)
2022-03-28 13:05:51 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (256161ms till timeout)
2022-03-28 13:05:53 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (254841ms till timeout)
2022-03-28 13:05:54 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (253507ms till timeout)
2022-03-28 13:05:55 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (252187ms till timeout)
2022-03-28 13:05:57 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (250868ms till timeout)
2022-03-28 13:05:58 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (249546ms till timeout)
2022-03-28 13:05:59 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (248228ms till timeout)
2022-03-28 13:06:01 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (246910ms till timeout)
2022-03-28 13:06:02 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (245590ms till timeout)
2022-03-28 13:06:03 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (244267ms till timeout)
2022-03-28 13:06:05 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (242946ms till timeout)
2022-03-28 13:06:06 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (241627ms till timeout)
2022-03-28 13:06:07 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (240307ms till timeout)
2022-03-28 13:06:09 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (238984ms till timeout)
2022-03-28 13:06:10 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (237664ms till timeout)
2022-03-28 13:06:11 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (236344ms till timeout)
2022-03-28 13:06:13 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (235027ms till timeout)
2022-03-28 13:06:14 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (233707ms till timeout)
2022-03-28 13:06:15 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (232387ms till timeout)
2022-03-28 13:06:16 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (231068ms till timeout)
2022-03-28 13:06:18 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (229750ms till timeout)
2022-03-28 13:06:19 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (228431ms till timeout)
2022-03-28 13:06:20 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (227111ms till timeout)
2022-03-28 13:06:22 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (225791ms till timeout)
2022-03-28 13:06:23 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (224471ms till timeout)
2022-03-28 13:06:24 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (223114ms till timeout)
2022-03-28 13:06:26 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (221794ms till timeout)
2022-03-28 13:06:27 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (220476ms till timeout)
2022-03-28 13:06:28 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (219157ms till timeout)
2022-03-28 13:06:30 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (217838ms till timeout)
2022-03-28 13:06:31 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (216518ms till timeout)
2022-03-28 13:06:32 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (215196ms till timeout)
2022-03-28 13:06:34 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (213877ms till timeout)
2022-03-28 13:06:35 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (212559ms till timeout)
2022-03-28 13:06:36 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (211237ms till timeout)
2022-03-28 13:06:38 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (209918ms till timeout)
2022-03-28 13:06:39 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (208600ms till timeout)
2022-03-28 13:06:40 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (207281ms till timeout)
2022-03-28 13:06:42 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (205962ms till timeout)
2022-03-28 13:06:43 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (204644ms till timeout)
2022-03-28 13:06:44 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (203326ms till timeout)
2022-03-28 13:06:46 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (202007ms till timeout)
2022-03-28 13:06:47 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (200686ms till timeout)
2022-03-28 13:06:48 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (199367ms till timeout)
2022-03-28 13:06:50 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (198043ms till timeout)
2022-03-28 13:06:51 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (196723ms till timeout)
2022-03-28 13:06:52 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (195404ms till timeout)
2022-03-28 13:06:53 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (194083ms till timeout)
2022-03-28 13:06:55 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (192764ms till timeout)
2022-03-28 13:06:56 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (191446ms till timeout)
2022-03-28 13:06:57 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (190127ms till timeout)
2022-03-28 13:06:59 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (188806ms till timeout)
2022-03-28 13:07:00 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (187486ms till timeout)
2022-03-28 13:07:01 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (186166ms till timeout)
2022-03-28 13:07:03 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (184847ms till timeout)
2022-03-28 13:07:04 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (183527ms till timeout)
2022-03-28 13:07:05 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (182208ms till timeout)
2022-03-28 13:07:07 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (180890ms till timeout)
2022-03-28 13:07:08 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (179571ms till timeout)
2022-03-28 13:07:09 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (178247ms till timeout)
2022-03-28 13:07:11 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (176929ms till timeout)
2022-03-28 13:07:12 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (175609ms till timeout)
2022-03-28 13:07:13 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (174290ms till timeout)
2022-03-28 13:07:15 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (172970ms till timeout)
2022-03-28 13:07:16 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (171650ms till timeout)
2022-03-28 13:07:17 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (170331ms till timeout)
2022-03-28 13:07:19 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (169011ms till timeout)
2022-03-28 13:07:20 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (167691ms till timeout)
2022-03-28 13:07:21 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (166374ms till timeout)
2022-03-28 13:07:23 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (165055ms till timeout)
2022-03-28 13:07:24 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (163735ms till timeout)
2022-03-28 13:07:25 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (162415ms till timeout)
2022-03-28 13:07:26 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (161096ms till timeout)
2022-03-28 13:07:28 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (159779ms till timeout)
2022-03-28 13:07:29 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (158460ms till timeout)
2022-03-28 13:07:30 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (157134ms till timeout)
2022-03-28 13:07:32 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (155817ms till timeout)
2022-03-28 13:07:33 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (154499ms till timeout)
2022-03-28 13:07:34 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (153179ms till timeout)
2022-03-28 13:07:36 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (151859ms till timeout)
2022-03-28 13:07:37 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (150541ms till timeout)
2022-03-28 13:07:38 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (149224ms till timeout)
2022-03-28 13:07:40 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (147891ms till timeout)
2022-03-28 13:07:41 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (146572ms till timeout)
2022-03-28 13:07:42 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (145253ms till timeout)
2022-03-28 13:07:44 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (143936ms till timeout)
2022-03-28 13:07:45 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (142616ms till timeout)
2022-03-28 13:07:46 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (141295ms till timeout)
2022-03-28 13:07:48 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (139974ms till timeout)
2022-03-28 13:07:49 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (138655ms till timeout)
2022-03-28 13:07:50 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (137336ms till timeout)
2022-03-28 13:07:52 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (136014ms till timeout)
2022-03-28 13:07:53 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (134693ms till timeout)
2022-03-28 13:07:54 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (133373ms till timeout)
2022-03-28 13:07:56 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (132054ms till timeout)
2022-03-28 13:07:57 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (130733ms till timeout)
2022-03-28 13:07:58 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (129413ms till timeout)
2022-03-28 13:07:59 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (128094ms till timeout)
2022-03-28 13:08:01 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (126774ms till timeout)
2022-03-28 13:08:02 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (125448ms till timeout)
2022-03-28 13:08:03 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (124129ms till timeout)
2022-03-28 13:08:05 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (122707ms till timeout)
2022-03-28 13:08:06 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (121388ms till timeout)
2022-03-28 13:08:07 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (120068ms till timeout)
2022-03-28 13:08:09 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (118748ms till timeout)
2022-03-28 13:08:10 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (117427ms till timeout)
2022-03-28 13:08:11 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (116108ms till timeout)
2022-03-28 13:08:13 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (114788ms till timeout)
2022-03-28 13:08:14 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (113468ms till timeout)
2022-03-28 13:08:15 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (112151ms till timeout)
2022-03-28 13:08:17 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (110832ms till timeout)
2022-03-28 13:08:18 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (109502ms till timeout)
2022-03-28 13:08:19 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (108179ms till timeout)
2022-03-28 13:08:21 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (106860ms till timeout)
2022-03-28 13:08:22 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (105540ms till timeout)
2022-03-28 13:08:23 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (104220ms till timeout)
2022-03-28 13:08:25 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (102877ms till timeout)
2022-03-28 13:08:26 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (101517ms till timeout)
2022-03-28 13:08:27 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (100198ms till timeout)
2022-03-28 13:08:29 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (98875ms till timeout)
2022-03-28 13:08:30 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (97556ms till timeout)
2022-03-28 13:08:31 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (96196ms till timeout)
2022-03-28 13:08:33 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (94878ms till timeout)
2022-03-28 13:08:34 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (93559ms till timeout)
2022-03-28 13:08:35 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (92240ms till timeout)
2022-03-28 13:08:37 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (90921ms till timeout)
2022-03-28 13:08:38 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (89603ms till timeout)
2022-03-28 13:08:39 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (88284ms till timeout)
2022-03-28 13:08:41 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (86965ms till timeout)
2022-03-28 13:08:42 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (85647ms till timeout)
2022-03-28 13:08:43 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (84328ms till timeout)
2022-03-28 13:08:45 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (83010ms till timeout)
2022-03-28 13:08:46 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (81688ms till timeout)
2022-03-28 13:08:47 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (80367ms till timeout)
2022-03-28 13:08:49 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (79047ms till timeout)
2022-03-28 13:08:50 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (77727ms till timeout)
2022-03-28 13:08:51 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (76407ms till timeout)
2022-03-28 13:08:52 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (75084ms till timeout)
2022-03-28 13:08:54 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (73764ms till timeout)
2022-03-28 13:08:55 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (72445ms till timeout)
2022-03-28 13:08:56 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (71125ms till timeout)
2022-03-28 13:08:58 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (69807ms till timeout)
2022-03-28 13:08:59 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (68488ms till timeout)
2022-03-28 13:09:00 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (67167ms till timeout)
2022-03-28 13:09:02 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (65847ms till timeout)
2022-03-28 13:09:03 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (64526ms till timeout)
2022-03-28 13:09:04 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (63207ms till timeout)
2022-03-28 13:09:06 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (61888ms till timeout)
2022-03-28 13:09:07 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (60567ms till timeout)
2022-03-28 13:09:08 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (59246ms till timeout)
2022-03-28 13:09:10 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (57927ms till timeout)
2022-03-28 13:09:11 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (56608ms till timeout)
2022-03-28 13:09:12 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (55289ms till timeout)
2022-03-28 13:09:14 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (53970ms till timeout)
2022-03-28 13:09:15 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Waiting for message will be in the log not ready, will try again in 1000 ms (52651ms till timeout)
2022-03-28 13:09:16 [ForkJoinPool-1-worker-11] INFO  [PodUtils:189] Message org.apache.kafka.common.errors.ThrottlingQuotaExceededException: The throttling quota has been exceeded. found in delete-admin-my-cluster-b6bd312e-kafka-clients-ct629 log
2022-03-28 13:09:16 [ForkJoinPool-1-worker-11] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment delete-admin-my-cluster-b6bd312e-kafka-clients deletion
2022-03-28 13:09:16 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for ReplicaSet delete-admin-my-cluster-b6bd312e-kafka-clients to be deleted
2022-03-28 13:09:16 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] ReplicaSet delete-admin-my-cluster-b6bd312e-kafka-clients to be deleted not ready, will try again in 5000 ms (179889ms till timeout)
2022-03-28 13:09:22 [ForkJoinPool-1-worker-11] DEBUG [JobUtils:40] Job delete-admin-my-cluster-b6bd312e-kafka-clients was deleted
2022-03-28 13:09:22 [ForkJoinPool-1-worker-11] INFO  [ThrottlingQuotaST:144] Executing 1/5 iteration for delete-admin-my-cluster-b6bd312e-kafka-clients.
2022-03-28 13:09:22 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st
2022-03-28 13:09:22 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:delete-admin-my-cluster-b6bd312e-kafka-clients
2022-03-28 13:09:22 [ForkJoinPool-1-worker-11] INFO  [JobUtils:81] Waiting for job: delete-admin-my-cluster-b6bd312e-kafka-clients will be in active state
2022-03-28 13:09:22 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 13:09:22 [ForkJoinPool-1-worker-11] INFO  [ClientUtils:76] Waiting for producer/consumer:delete-admin-my-cluster-b6bd312e-kafka-clients to finished
2022-03-28 13:09:22 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 13:09:22 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:09:22 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129784ms till timeout)
2022-03-28 13:09:23 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:09:23 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (128565ms till timeout)
2022-03-28 13:09:24 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:09:25 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (127348ms till timeout)
2022-03-28 13:09:26 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:09:26 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (126027ms till timeout)
2022-03-28 13:09:27 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:09:27 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (124807ms till timeout)
2022-03-28 13:09:28 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:09:28 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (123588ms till timeout)
2022-03-28 13:09:29 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:09:30 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (122370ms till timeout)
2022-03-28 13:09:31 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:09:31 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (121151ms till timeout)
2022-03-28 13:09:32 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:09:32 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (119932ms till timeout)
2022-03-28 13:09:33 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:09:33 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (118714ms till timeout)
2022-03-28 13:09:34 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:09:34 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (117493ms till timeout)
2022-03-28 13:09:36 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:09:36 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (116275ms till timeout)
2022-03-28 13:09:37 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:09:37 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (115055ms till timeout)
2022-03-28 13:09:38 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:09:38 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (113836ms till timeout)
2022-03-28 13:09:39 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:09:39 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (112618ms till timeout)
2022-03-28 13:09:40 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:09:41 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (111399ms till timeout)
2022-03-28 13:09:42 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:09:42 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (110178ms till timeout)
2022-03-28 13:09:43 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:09:43 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (108960ms till timeout)
2022-03-28 13:09:44 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:09:44 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (107741ms till timeout)
2022-03-28 13:09:45 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:09:45 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (106524ms till timeout)
2022-03-28 13:09:46 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:09:47 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (105303ms till timeout)
2022-03-28 13:09:48 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:09:48 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (104085ms till timeout)
2022-03-28 13:09:49 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:09:49 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (102864ms till timeout)
2022-03-28 13:09:50 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:09:50 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (101646ms till timeout)
2022-03-28 13:09:51 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:09:51 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (100426ms till timeout)
2022-03-28 13:09:53 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:09:53 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (99209ms till timeout)
2022-03-28 13:09:54 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:09:54 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (97991ms till timeout)
2022-03-28 13:09:55 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:09:55 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (96773ms till timeout)
2022-03-28 13:09:56 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:09:56 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (95554ms till timeout)
2022-03-28 13:09:57 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:09:58 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (94336ms till timeout)
2022-03-28 13:09:59 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:09:59 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (93117ms till timeout)
2022-03-28 13:10:00 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:10:00 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (91897ms till timeout)
2022-03-28 13:10:01 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:10:01 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (90676ms till timeout)
2022-03-28 13:10:02 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:10:02 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (89458ms till timeout)
2022-03-28 13:10:04 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:10:04 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (88237ms till timeout)
2022-03-28 13:10:05 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:10:05 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (87019ms till timeout)
2022-03-28 13:10:06 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:10:06 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (85797ms till timeout)
2022-03-28 13:10:07 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:10:07 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (84576ms till timeout)
2022-03-28 13:10:08 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:10:09 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (83355ms till timeout)
2022-03-28 13:10:10 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:10:10 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (82138ms till timeout)
2022-03-28 13:10:11 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:10:11 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (80918ms till timeout)
2022-03-28 13:10:12 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:10:12 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (79700ms till timeout)
2022-03-28 13:10:13 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:10:13 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (78479ms till timeout)
2022-03-28 13:10:15 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:10:15 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (77264ms till timeout)
2022-03-28 13:10:16 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:10:16 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (76040ms till timeout)
2022-03-28 13:10:17 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:10:17 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (74821ms till timeout)
2022-03-28 13:10:18 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:10:18 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (73601ms till timeout)
2022-03-28 13:10:19 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:10:20 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (72375ms till timeout)
2022-03-28 13:10:21 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:10:21 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (71155ms till timeout)
2022-03-28 13:10:22 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:10:22 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (69936ms till timeout)
2022-03-28 13:10:23 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:10:23 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (68715ms till timeout)
2022-03-28 13:10:24 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:10:24 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (67498ms till timeout)
2022-03-28 13:10:26 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:10:26 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (66275ms till timeout)
2022-03-28 13:10:27 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T13:10:21Z, conditions=[JobCondition(lastProbeTime=2022-03-28T13:10:21Z, lastTransitionTime=2022-03-28T13:10:21Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T13:09:17Z, succeeded=1, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:10:27 [ForkJoinPool-1-worker-11] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment delete-admin-my-cluster-b6bd312e-kafka-clients deletion
2022-03-28 13:10:27 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for ReplicaSet delete-admin-my-cluster-b6bd312e-kafka-clients to be deleted
2022-03-28 13:10:27 [ForkJoinPool-1-worker-11] DEBUG [JobUtils:40] Job delete-admin-my-cluster-b6bd312e-kafka-clients was deleted
2022-03-28 13:10:27 [ForkJoinPool-1-worker-11] INFO  [ThrottlingQuotaST:144] Executing 2/5 iteration for delete-admin-my-cluster-b6bd312e-kafka-clients.
2022-03-28 13:10:27 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st
2022-03-28 13:10:27 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:delete-admin-my-cluster-b6bd312e-kafka-clients
2022-03-28 13:10:27 [ForkJoinPool-1-worker-11] INFO  [JobUtils:81] Waiting for job: delete-admin-my-cluster-b6bd312e-kafka-clients will be in active state
2022-03-28 13:10:27 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 13:10:27 [ForkJoinPool-1-worker-11] INFO  [ClientUtils:76] Waiting for producer/consumer:delete-admin-my-cluster-b6bd312e-kafka-clients to finished
2022-03-28 13:10:27 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 13:10:28 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:10:28 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129785ms till timeout)
2022-03-28 13:10:29 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:10:29 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (128564ms till timeout)
2022-03-28 13:10:30 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:10:30 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (127343ms till timeout)
2022-03-28 13:10:31 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:10:31 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (126125ms till timeout)
2022-03-28 13:10:32 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:10:33 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (124905ms till timeout)
2022-03-28 13:10:34 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:10:34 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (123687ms till timeout)
2022-03-28 13:10:35 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:10:35 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (122469ms till timeout)
2022-03-28 13:10:36 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:10:36 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (121249ms till timeout)
2022-03-28 13:10:37 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:10:37 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (120029ms till timeout)
2022-03-28 13:10:38 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:10:39 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (118810ms till timeout)
2022-03-28 13:10:40 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:10:40 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (117590ms till timeout)
2022-03-28 13:10:41 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:10:41 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (116337ms till timeout)
2022-03-28 13:10:42 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:10:42 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (115118ms till timeout)
2022-03-28 13:10:43 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:10:44 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (113899ms till timeout)
2022-03-28 13:10:45 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:10:45 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (112680ms till timeout)
2022-03-28 13:10:46 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:10:46 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (111459ms till timeout)
2022-03-28 13:10:47 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:10:47 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (110239ms till timeout)
2022-03-28 13:10:48 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:10:48 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (109019ms till timeout)
2022-03-28 13:10:50 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:10:50 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (107800ms till timeout)
2022-03-28 13:10:51 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:10:51 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (106582ms till timeout)
2022-03-28 13:10:52 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:10:52 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (105360ms till timeout)
2022-03-28 13:10:53 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:10:53 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (104138ms till timeout)
2022-03-28 13:10:54 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:10:54 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (102920ms till timeout)
2022-03-28 13:10:56 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:10:56 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (101701ms till timeout)
2022-03-28 13:10:57 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:10:57 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (100481ms till timeout)
2022-03-28 13:10:58 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:10:58 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (99262ms till timeout)
2022-03-28 13:10:59 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:10:59 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (98041ms till timeout)
2022-03-28 13:11:00 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:11:01 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (96822ms till timeout)
2022-03-28 13:11:02 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:11:02 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (95602ms till timeout)
2022-03-28 13:11:03 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:11:03 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (94383ms till timeout)
2022-03-28 13:11:04 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:11:04 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (93163ms till timeout)
2022-03-28 13:11:05 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:11:05 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (91943ms till timeout)
2022-03-28 13:11:07 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:11:07 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (90725ms till timeout)
2022-03-28 13:11:08 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:11:08 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (89506ms till timeout)
2022-03-28 13:11:09 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:11:09 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (88287ms till timeout)
2022-03-28 13:11:10 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:11:10 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (87068ms till timeout)
2022-03-28 13:11:11 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:11:12 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (85849ms till timeout)
2022-03-28 13:11:13 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:11:13 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (84624ms till timeout)
2022-03-28 13:11:14 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:11:14 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (83406ms till timeout)
2022-03-28 13:11:15 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:11:15 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (82187ms till timeout)
2022-03-28 13:11:16 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:11:16 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (80966ms till timeout)
2022-03-28 13:11:18 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:11:18 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (79746ms till timeout)
2022-03-28 13:11:19 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:11:19 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (78528ms till timeout)
2022-03-28 13:11:20 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:11:20 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (77306ms till timeout)
2022-03-28 13:11:21 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:11:21 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (76079ms till timeout)
2022-03-28 13:11:22 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:11:23 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (74858ms till timeout)
2022-03-28 13:11:24 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:11:24 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (73640ms till timeout)
2022-03-28 13:11:25 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:11:25 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (72421ms till timeout)
2022-03-28 13:11:26 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:11:26 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (71157ms till timeout)
2022-03-28 13:11:27 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:11:27 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (69938ms till timeout)
2022-03-28 13:11:29 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:11:29 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (68722ms till timeout)
2022-03-28 13:11:30 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:11:30 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (67500ms till timeout)
2022-03-28 13:11:31 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:11:31 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (66284ms till timeout)
2022-03-28 13:11:32 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T13:11:27Z, conditions=[JobCondition(lastProbeTime=2022-03-28T13:11:27Z, lastTransitionTime=2022-03-28T13:11:27Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T13:10:23Z, succeeded=1, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:11:32 [ForkJoinPool-1-worker-11] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment delete-admin-my-cluster-b6bd312e-kafka-clients deletion
2022-03-28 13:11:32 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for ReplicaSet delete-admin-my-cluster-b6bd312e-kafka-clients to be deleted
2022-03-28 13:11:33 [ForkJoinPool-1-worker-11] DEBUG [JobUtils:40] Job delete-admin-my-cluster-b6bd312e-kafka-clients was deleted
2022-03-28 13:11:33 [ForkJoinPool-1-worker-11] INFO  [ThrottlingQuotaST:144] Executing 3/5 iteration for delete-admin-my-cluster-b6bd312e-kafka-clients.
2022-03-28 13:11:33 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st
2022-03-28 13:11:33 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:delete-admin-my-cluster-b6bd312e-kafka-clients
2022-03-28 13:11:33 [ForkJoinPool-1-worker-11] INFO  [JobUtils:81] Waiting for job: delete-admin-my-cluster-b6bd312e-kafka-clients will be in active state
2022-03-28 13:11:33 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 13:11:33 [ForkJoinPool-1-worker-11] INFO  [ClientUtils:76] Waiting for producer/consumer:delete-admin-my-cluster-b6bd312e-kafka-clients to finished
2022-03-28 13:11:33 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 13:11:33 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:11:33 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129784ms till timeout)
2022-03-28 13:11:34 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:11:34 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (128558ms till timeout)
2022-03-28 13:11:35 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:11:36 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (127339ms till timeout)
2022-03-28 13:11:37 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:11:37 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (126121ms till timeout)
2022-03-28 13:11:38 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:11:38 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (124902ms till timeout)
2022-03-28 13:11:39 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:11:39 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (123684ms till timeout)
2022-03-28 13:11:40 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:11:40 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (122462ms till timeout)
2022-03-28 13:11:42 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:11:42 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (121244ms till timeout)
2022-03-28 13:11:43 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:11:43 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (120025ms till timeout)
2022-03-28 13:11:44 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:11:44 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (118807ms till timeout)
2022-03-28 13:11:45 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:11:45 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (117587ms till timeout)
2022-03-28 13:11:46 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:11:47 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (116366ms till timeout)
2022-03-28 13:11:48 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:11:48 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (115146ms till timeout)
2022-03-28 13:11:49 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:11:49 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (113927ms till timeout)
2022-03-28 13:11:50 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:11:50 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (112707ms till timeout)
2022-03-28 13:11:51 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:11:51 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (111484ms till timeout)
2022-03-28 13:11:53 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:11:53 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (110265ms till timeout)
2022-03-28 13:11:54 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:11:54 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (109044ms till timeout)
2022-03-28 13:11:55 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:11:55 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (107825ms till timeout)
2022-03-28 13:11:56 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:11:56 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (106605ms till timeout)
2022-03-28 13:11:57 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:11:58 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (105386ms till timeout)
2022-03-28 13:11:59 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:11:59 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (104166ms till timeout)
2022-03-28 13:12:00 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:12:00 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (102948ms till timeout)
2022-03-28 13:12:01 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:12:01 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (101727ms till timeout)
2022-03-28 13:12:02 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:12:02 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (100511ms till timeout)
2022-03-28 13:12:04 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:12:04 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (99290ms till timeout)
2022-03-28 13:12:05 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:12:05 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (98073ms till timeout)
2022-03-28 13:12:06 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:12:06 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (96852ms till timeout)
2022-03-28 13:12:07 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:12:07 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (95633ms till timeout)
2022-03-28 13:12:08 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:12:08 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (94414ms till timeout)
2022-03-28 13:12:10 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:12:10 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (93195ms till timeout)
2022-03-28 13:12:11 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:12:11 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (91976ms till timeout)
2022-03-28 13:12:12 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:12:12 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (90753ms till timeout)
2022-03-28 13:12:13 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:12:13 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (89533ms till timeout)
2022-03-28 13:12:14 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:12:15 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (88314ms till timeout)
2022-03-28 13:12:16 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:12:16 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (87094ms till timeout)
2022-03-28 13:12:17 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:12:17 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (85875ms till timeout)
2022-03-28 13:12:18 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:12:18 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (84656ms till timeout)
2022-03-28 13:12:19 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:12:19 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (83435ms till timeout)
2022-03-28 13:12:21 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:12:21 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (82214ms till timeout)
2022-03-28 13:12:22 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:12:22 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (80993ms till timeout)
2022-03-28 13:12:23 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:12:23 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (79772ms till timeout)
2022-03-28 13:12:24 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:12:24 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (78553ms till timeout)
2022-03-28 13:12:25 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:12:26 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (77335ms till timeout)
2022-03-28 13:12:27 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:12:27 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (76114ms till timeout)
2022-03-28 13:12:28 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:12:28 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (74896ms till timeout)
2022-03-28 13:12:29 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:12:29 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (73678ms till timeout)
2022-03-28 13:12:30 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:12:30 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (72460ms till timeout)
2022-03-28 13:12:32 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:12:32 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (71240ms till timeout)
2022-03-28 13:12:33 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:12:33 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (70022ms till timeout)
2022-03-28 13:12:34 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:12:34 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (68803ms till timeout)
2022-03-28 13:12:35 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:12:35 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (67585ms till timeout)
2022-03-28 13:12:36 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T13:12:32Z, conditions=[JobCondition(lastProbeTime=2022-03-28T13:12:32Z, lastTransitionTime=2022-03-28T13:12:32Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T13:11:28Z, succeeded=1, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:12:37 [ForkJoinPool-1-worker-11] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment delete-admin-my-cluster-b6bd312e-kafka-clients deletion
2022-03-28 13:12:37 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for ReplicaSet delete-admin-my-cluster-b6bd312e-kafka-clients to be deleted
2022-03-28 13:12:37 [ForkJoinPool-1-worker-11] DEBUG [JobUtils:40] Job delete-admin-my-cluster-b6bd312e-kafka-clients was deleted
2022-03-28 13:12:37 [ForkJoinPool-1-worker-11] INFO  [ThrottlingQuotaST:144] Executing 4/5 iteration for delete-admin-my-cluster-b6bd312e-kafka-clients.
2022-03-28 13:12:37 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st
2022-03-28 13:12:37 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:delete-admin-my-cluster-b6bd312e-kafka-clients
2022-03-28 13:12:37 [ForkJoinPool-1-worker-11] INFO  [JobUtils:81] Waiting for job: delete-admin-my-cluster-b6bd312e-kafka-clients will be in active state
2022-03-28 13:12:37 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 13:12:37 [ForkJoinPool-1-worker-11] INFO  [ClientUtils:76] Waiting for producer/consumer:delete-admin-my-cluster-b6bd312e-kafka-clients to finished
2022-03-28 13:12:37 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 13:12:37 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:12:37 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129780ms till timeout)
2022-03-28 13:12:38 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:12:39 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (128559ms till timeout)
2022-03-28 13:12:40 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:12:40 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (127338ms till timeout)
2022-03-28 13:12:41 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:12:41 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (126117ms till timeout)
2022-03-28 13:12:42 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:12:42 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (124895ms till timeout)
2022-03-28 13:12:43 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:12:43 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (123675ms till timeout)
2022-03-28 13:12:45 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:12:45 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (122456ms till timeout)
2022-03-28 13:12:46 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:12:46 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (121236ms till timeout)
2022-03-28 13:12:47 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:12:47 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (120019ms till timeout)
2022-03-28 13:12:48 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:12:48 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (118799ms till timeout)
2022-03-28 13:12:49 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:12:50 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (117578ms till timeout)
2022-03-28 13:12:51 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:12:51 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (116356ms till timeout)
2022-03-28 13:12:52 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:12:52 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (115097ms till timeout)
2022-03-28 13:12:53 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:12:53 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (113878ms till timeout)
2022-03-28 13:12:54 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:12:54 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (112659ms till timeout)
2022-03-28 13:12:56 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:12:56 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (111440ms till timeout)
2022-03-28 13:12:57 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:12:57 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (110218ms till timeout)
2022-03-28 13:12:58 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:12:58 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (108995ms till timeout)
2022-03-28 13:12:59 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:12:59 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (107775ms till timeout)
2022-03-28 13:13:00 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:13:01 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (106549ms till timeout)
2022-03-28 13:13:02 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:13:02 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (105330ms till timeout)
2022-03-28 13:13:03 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:13:03 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (104107ms till timeout)
2022-03-28 13:13:04 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:13:04 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (102888ms till timeout)
2022-03-28 13:13:05 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:13:05 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (101668ms till timeout)
2022-03-28 13:13:07 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:13:07 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (100450ms till timeout)
2022-03-28 13:13:08 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:13:08 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (99231ms till timeout)
2022-03-28 13:13:09 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:13:09 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (98013ms till timeout)
2022-03-28 13:13:10 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:13:10 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (96792ms till timeout)
2022-03-28 13:13:11 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:13:12 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (95570ms till timeout)
2022-03-28 13:13:13 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:13:13 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (94344ms till timeout)
2022-03-28 13:13:14 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:13:14 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (93125ms till timeout)
2022-03-28 13:13:15 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:13:15 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (91907ms till timeout)
2022-03-28 13:13:16 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:13:16 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (90688ms till timeout)
2022-03-28 13:13:18 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:13:18 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (89470ms till timeout)
2022-03-28 13:13:19 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:13:19 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (88250ms till timeout)
2022-03-28 13:13:20 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:13:20 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (87030ms till timeout)
2022-03-28 13:13:21 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:13:21 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (85811ms till timeout)
2022-03-28 13:13:22 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:13:22 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (84591ms till timeout)
2022-03-28 13:13:24 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:13:24 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (83370ms till timeout)
2022-03-28 13:13:25 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:13:25 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (82150ms till timeout)
2022-03-28 13:13:26 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:13:26 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (80931ms till timeout)
2022-03-28 13:13:27 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:13:27 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (79712ms till timeout)
2022-03-28 13:13:28 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:13:29 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (78493ms till timeout)
2022-03-28 13:13:30 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:13:30 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (77275ms till timeout)
2022-03-28 13:13:31 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:13:31 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (76057ms till timeout)
2022-03-28 13:13:32 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:13:32 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (74837ms till timeout)
2022-03-28 13:13:33 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:13:33 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (73616ms till timeout)
2022-03-28 13:13:35 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:13:35 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (72397ms till timeout)
2022-03-28 13:13:36 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:13:36 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (71177ms till timeout)
2022-03-28 13:13:37 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:13:37 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (69959ms till timeout)
2022-03-28 13:13:38 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:13:38 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (68740ms till timeout)
2022-03-28 13:13:39 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:13:40 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (67522ms till timeout)
2022-03-28 13:13:41 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:13:41 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (66304ms till timeout)
2022-03-28 13:13:42 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T13:13:37Z, conditions=[JobCondition(lastProbeTime=2022-03-28T13:13:37Z, lastTransitionTime=2022-03-28T13:13:37Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T13:12:32Z, succeeded=1, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:13:42 [ForkJoinPool-1-worker-11] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment delete-admin-my-cluster-b6bd312e-kafka-clients deletion
2022-03-28 13:13:42 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for ReplicaSet delete-admin-my-cluster-b6bd312e-kafka-clients to be deleted
2022-03-28 13:13:42 [ForkJoinPool-1-worker-11] DEBUG [JobUtils:40] Job delete-admin-my-cluster-b6bd312e-kafka-clients was deleted
2022-03-28 13:13:42 [ForkJoinPool-1-worker-11] INFO  [ThrottlingQuotaST:144] Executing 5/5 iteration for delete-admin-my-cluster-b6bd312e-kafka-clients.
2022-03-28 13:13:42 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:155] Create/Update Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st
2022-03-28 13:13:42 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Job:delete-admin-my-cluster-b6bd312e-kafka-clients
2022-03-28 13:13:42 [ForkJoinPool-1-worker-11] INFO  [JobUtils:81] Waiting for job: delete-admin-my-cluster-b6bd312e-kafka-clients will be in active state
2022-03-28 13:13:42 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for job active
2022-03-28 13:13:43 [ForkJoinPool-1-worker-11] INFO  [ClientUtils:76] Waiting for producer/consumer:delete-admin-my-cluster-b6bd312e-kafka-clients to finished
2022-03-28 13:13:43 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for job finished
2022-03-28 13:13:43 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:13:43 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (129783ms till timeout)
2022-03-28 13:13:44 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:13:44 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (128565ms till timeout)
2022-03-28 13:13:45 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:13:45 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (127346ms till timeout)
2022-03-28 13:13:46 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:13:46 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (126126ms till timeout)
2022-03-28 13:13:48 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:13:48 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (124907ms till timeout)
2022-03-28 13:13:49 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:13:49 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (123686ms till timeout)
2022-03-28 13:13:50 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:13:50 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (122467ms till timeout)
2022-03-28 13:13:51 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:13:51 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (121249ms till timeout)
2022-03-28 13:13:52 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:13:53 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (120023ms till timeout)
2022-03-28 13:13:54 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:13:54 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (118806ms till timeout)
2022-03-28 13:13:55 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:13:55 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (117579ms till timeout)
2022-03-28 13:13:56 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:13:56 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (116362ms till timeout)
2022-03-28 13:13:57 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:13:57 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (115143ms till timeout)
2022-03-28 13:13:59 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:13:59 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (113925ms till timeout)
2022-03-28 13:14:00 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:14:00 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (112705ms till timeout)
2022-03-28 13:14:01 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:14:01 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (111486ms till timeout)
2022-03-28 13:14:02 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:14:02 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (110267ms till timeout)
2022-03-28 13:14:03 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:14:04 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (109049ms till timeout)
2022-03-28 13:14:05 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:14:05 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (107828ms till timeout)
2022-03-28 13:14:06 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:14:06 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (106611ms till timeout)
2022-03-28 13:14:07 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:14:07 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (105390ms till timeout)
2022-03-28 13:14:08 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:14:08 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (104173ms till timeout)
2022-03-28 13:14:10 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:14:10 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (102951ms till timeout)
2022-03-28 13:14:11 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:14:11 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (101730ms till timeout)
2022-03-28 13:14:12 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:14:12 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (100509ms till timeout)
2022-03-28 13:14:13 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:14:13 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (99290ms till timeout)
2022-03-28 13:14:14 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:14:14 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (98070ms till timeout)
2022-03-28 13:14:16 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:14:16 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (96853ms till timeout)
2022-03-28 13:14:17 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:14:17 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (95630ms till timeout)
2022-03-28 13:14:18 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:14:18 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (94414ms till timeout)
2022-03-28 13:14:19 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:14:19 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (93190ms till timeout)
2022-03-28 13:14:20 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:14:21 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (91971ms till timeout)
2022-03-28 13:14:22 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:14:22 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (90753ms till timeout)
2022-03-28 13:14:23 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:14:23 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (89534ms till timeout)
2022-03-28 13:14:24 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:14:24 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (88310ms till timeout)
2022-03-28 13:14:25 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:14:25 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (87092ms till timeout)
2022-03-28 13:14:27 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:14:27 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (85866ms till timeout)
2022-03-28 13:14:28 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:14:28 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (84646ms till timeout)
2022-03-28 13:14:29 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:14:29 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (83428ms till timeout)
2022-03-28 13:14:30 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:14:30 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (82210ms till timeout)
2022-03-28 13:14:31 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:14:32 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (80992ms till timeout)
2022-03-28 13:14:33 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:14:33 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (79774ms till timeout)
2022-03-28 13:14:34 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:14:34 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (78554ms till timeout)
2022-03-28 13:14:35 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:14:35 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (77337ms till timeout)
2022-03-28 13:14:36 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:14:36 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (76117ms till timeout)
2022-03-28 13:14:38 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:14:38 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (74899ms till timeout)
2022-03-28 13:14:39 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:14:39 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (73679ms till timeout)
2022-03-28 13:14:40 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:14:40 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (72460ms till timeout)
2022-03-28 13:14:41 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:14:41 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (71239ms till timeout)
2022-03-28 13:14:42 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:14:43 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (70021ms till timeout)
2022-03-28 13:14:44 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:14:44 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (68798ms till timeout)
2022-03-28 13:14:45 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=1, completedIndexes=null, completionTime=null, conditions=[], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=null, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:14:45 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] job finished not ready, will try again in 1000 ms (67581ms till timeout)
2022-03-28 13:14:46 [ForkJoinPool-1-worker-11] DEBUG [ClientUtils:79] Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st, has status JobStatus(active=null, completedIndexes=null, completionTime=2022-03-28T13:14:41Z, conditions=[JobCondition(lastProbeTime=2022-03-28T13:14:41Z, lastTransitionTime=2022-03-28T13:14:41Z, message=null, reason=null, status=True, type=Complete, additionalProperties={})], failed=null, ready=null, startTime=2022-03-28T13:13:38Z, succeeded=1, uncountedTerminatedPods=UncountedTerminatedPods(failed=[], succeeded=[], additionalProperties={}), additionalProperties={})
2022-03-28 13:14:46 [ForkJoinPool-1-worker-11] DEBUG [JobUtils:37] Waiting for ReplicaSet of Deployment delete-admin-my-cluster-b6bd312e-kafka-clients deletion
2022-03-28 13:14:46 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for ReplicaSet delete-admin-my-cluster-b6bd312e-kafka-clients to be deleted
2022-03-28 13:14:46 [ForkJoinPool-1-worker-11] DEBUG [JobUtils:40] Job delete-admin-my-cluster-b6bd312e-kafka-clients was deleted
2022-03-28 13:14:46 [ForkJoinPool-1-worker-11] DEBUG [AbstractST:674] ============================================================================
2022-03-28 13:14:46 [ForkJoinPool-1-worker-11] DEBUG [AbstractST:675] [operators.topic.ThrottlingQuotaST - After Each] - Clean up after test
2022-03-28 13:14:46 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:344] ############################################################################
2022-03-28 13:14:46 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:348] Delete all resources for testThrottlingQuotasDeleteTopic
2022-03-28 13:14:46 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st
2022-03-28 13:14:46 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:241] Delete of Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st
2022-03-28 13:14:46 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:241] Delete of KafkaUser my-user-284582998-729631500 in namespace throttling-quota-st
2022-03-28 13:14:46 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:241] Delete of Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st
2022-03-28 13:14:46 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:241] Delete of Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st
2022-03-28 13:14:46 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st
2022-03-28 13:14:47 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:create-admin-my-cluster-b6bd312e-kafka-clients
2022-03-28 13:14:47 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:create-admin-my-cluster-b6bd312e-kafka-clients
2022-03-28 13:14:47 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:delete-admin-my-cluster-b6bd312e-kafka-clients
2022-03-28 13:14:47 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:delete-admin-my-cluster-b6bd312e-kafka-clients
2022-03-28 13:14:47 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaUser:my-user-284582998-729631500
2022-03-28 13:14:47 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:create-admin-my-cluster-b6bd312e-kafka-clients
2022-03-28 13:14:47 [ForkJoinPool-1-worker-7] INFO  [ResourceManager:241] Delete of Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st
2022-03-28 13:14:47 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:241] Delete of Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st
2022-03-28 13:14:47 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:241] Delete of Job create-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st
2022-03-28 13:14:47 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:241] Delete of Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st
2022-03-28 13:14:47 [ForkJoinPool-1-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:delete-admin-my-cluster-b6bd312e-kafka-clients
2022-03-28 13:14:47 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:241] Delete of Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st
2022-03-28 13:14:47 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of Job delete-admin-my-cluster-b6bd312e-kafka-clients in namespace throttling-quota-st
2022-03-28 13:14:47 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:create-admin-my-cluster-b6bd312e-kafka-clients
2022-03-28 13:14:47 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:delete-admin-my-cluster-b6bd312e-kafka-clients
2022-03-28 13:14:47 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:create-admin-my-cluster-b6bd312e-kafka-clients
2022-03-28 13:14:47 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:delete-admin-my-cluster-b6bd312e-kafka-clients
2022-03-28 13:14:47 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Job:delete-admin-my-cluster-b6bd312e-kafka-clients
2022-03-28 13:14:47 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:369] ############################################################################
2022-03-28 13:14:47 [ForkJoinPool-1-worker-11] DEBUG [SuiteThreadController:267] testThrottlingQuotasDeleteTopic - Notifies waiting test cases:[testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testTlsExternalUserWithQuotas, testUserTemplate, testUpdateUser, testTlsExternalUser, testReceiveSimpleMessageTlsScramSha, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testSendSimpleMessageTlsScramSha, testUserWithNameMoreThan64Chars, testTlsUserWithQuotas, testConfigurationFileIsCreated, testScramUserWithQuotas, testDeployAndUnDeployCruiseControl, testConfigurationPerformanceOptions] to and randomly select one to start execution
2022-03-28 13:14:47 [ForkJoinPool-1-worker-11] DEBUG [SuiteThreadController:93] [operators.topic.ThrottlingQuotaST] - Removing parallel test: testThrottlingQuotasDeleteTopic
2022-03-28 13:14:47 [ForkJoinPool-1-worker-11] DEBUG [SuiteThreadController:97] [operators.topic.ThrottlingQuotaST] - Parallel test count: 0
2022-03-28 13:14:47 [ForkJoinPool-1-worker-11] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.topic.ThrottlingQuotaST.testThrottlingQuotasDeleteTopic-FINISHED
2022-03-28 13:14:47 [ForkJoinPool-1-worker-11] INFO  [TestSeparator:30] ############################################################################
2022-03-28 13:14:47 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:23] ############################################################################
2022-03-28 13:14:47 [ForkJoinPool-1-worker-11] INFO  [ThrottlingQuotaST:353] Tearing down resources after all test
2022-03-28 13:14:47 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.topic.TopicST.testDeleteTopicEnableFalse-STARTED
2022-03-28 13:14:47 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:658] ============================================================================
2022-03-28 13:14:47 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:659] [operators.topic.TopicST - Before Each] - Setup test case environment
2022-03-28 13:14:47 [ForkJoinPool-1-worker-5] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 13:14:47 [ForkJoinPool-1-worker-5] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-455915b9, testThrottlingQuotasCreateAlterPartitions=my-cluster-d4dad0de, testSendSimpleMessageTls=my-cluster-cfce2f5b, testConfigurationReflection=my-cluster-aa0ecd2f, testUserWithNameMoreThan64Chars=my-cluster-ed39a269, testDeployAndUnDeployCruiseControl=my-cluster-534609d0, testCapacityFile=my-cluster-39ee3f62, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-0b062573, testTlsExternalUser=my-cluster-965e9d1e, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-31feb41b, testSendSimpleMessageTlsScramSha=my-cluster-27773859, testCreateTopicAfterUnsupportedOperation=my-cluster-2938df05, testScramUserWithQuotas=my-cluster-90c82d82, testTopicModificationOfReplicationFactor=my-cluster-421b59dd, testKafkaAdminTopicOperations=my-cluster-6acb723c, testConfigurationFileIsCreated=my-cluster-020ceaf8, testCreateTopicViaKafka=my-cluster-43bd601f, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5, testUpdateUser=my-cluster-e16ccf16, testThrottlingQuotasDeleteTopic=my-cluster-b6bd312e, testDeleteTopicEnableFalse=my-cluster-8c17a5fe, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d, testTlsExternalUserWithQuotas=my-cluster-3e2e11de, testTlsUserWithQuotas=my-cluster-3b191485, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2, testSendingMessagesToNonExistingTopic=my-cluster-64488b2e, testConfigurationPerformanceOptions=my-cluster-bc0ed00a, testReceiveSimpleMessageTls=my-cluster-362cd511, testUserTemplate=my-cluster-157df94e, testMoreReplicasThanAvailableBrokers=my-cluster-1f5cbe38}
2022-03-28 13:14:47 [ForkJoinPool-1-worker-5] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-1329598231-1183867861, testThrottlingQuotasCreateAlterPartitions=my-user-1809876063-64793399, testSendSimpleMessageTls=my-user-111586425-1868899021, testConfigurationReflection=my-user-1580949953-976509997, testUserWithNameMoreThan64Chars=my-user-1048816470-1103470031, testDeployAndUnDeployCruiseControl=my-user-475346616-1293712526, testCapacityFile=my-user-597365340-661017603, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-1153908156-1369197188, testTlsExternalUser=my-user-1295334081-1602801897, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2100837352-1014792902, testSendSimpleMessageTlsScramSha=my-user-507582460-1003875464, testCreateTopicAfterUnsupportedOperation=my-user-271926232-1154944814, testScramUserWithQuotas=my-user-1857980152-2145707698, testTopicModificationOfReplicationFactor=my-user-380015619-613911473, testKafkaAdminTopicOperations=my-user-2014252818-589270412, testConfigurationFileIsCreated=my-user-1609977058-1196012647, testCreateTopicViaKafka=my-user-1379798639-1428271857, testCreatingUsersWithSecretPrefix=my-user-1928899390-1444689439, testUpdateUser=my-user-1694542041-806176635, testThrottlingQuotasDeleteTopic=my-user-284582998-729631500, testDeleteTopicEnableFalse=my-user-1491155200-894698476, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-467839358-1504804463, testTlsExternalUserWithQuotas=my-user-1786174319-1274634874, testTlsUserWithQuotas=my-user-420001054-1296284129, testThrottlingQuotasCreateTopic=my-user-896662438-260898887, testSendingMessagesToNonExistingTopic=my-user-248322508-1660982370, testConfigurationPerformanceOptions=my-user-722474509-712960391, testReceiveSimpleMessageTls=my-user-2012197250-2103354173, testUserTemplate=my-user-1253881534-190261188, testMoreReplicasThanAvailableBrokers=my-user-1381751556-1357722017}
2022-03-28 13:14:47 [ForkJoinPool-1-worker-5] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-170116874-1830928683, testThrottlingQuotasCreateAlterPartitions=my-topic-1581297458-635252970, testSendSimpleMessageTls=my-topic-100192180-936335618, testConfigurationReflection=my-topic-1437230713-1827996725, testUserWithNameMoreThan64Chars=my-topic-1603054043-1686316417, testDeployAndUnDeployCruiseControl=my-topic-1815491150-540070233, testCapacityFile=my-topic-1390944843-775067696, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-613737897-291522718, testTlsExternalUser=my-topic-476529121-306489993, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1084162749-1576061083, testSendSimpleMessageTlsScramSha=my-topic-62182003-1963040205, testCreateTopicAfterUnsupportedOperation=my-topic-1502518950-974162326, testScramUserWithQuotas=my-topic-315112689-1043284277, testTopicModificationOfReplicationFactor=my-topic-1016572292-592627742, testKafkaAdminTopicOperations=my-topic-1915664839-460245273, testConfigurationFileIsCreated=my-topic-155202447-1921897899, testCreateTopicViaKafka=my-topic-664105056-981624555, testCreatingUsersWithSecretPrefix=my-topic-2033928996-1209892355, testUpdateUser=my-topic-1239518257-2056139809, testThrottlingQuotasDeleteTopic=my-topic-1036812834-1980126190, testDeleteTopicEnableFalse=my-topic-1837190581-1217032035, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-712754859-1910580676, testTlsExternalUserWithQuotas=my-topic-593679216-1309035072, testTlsUserWithQuotas=my-topic-1752299353-1315959033, testThrottlingQuotasCreateTopic=my-topic-1532506723-1472611013, testSendingMessagesToNonExistingTopic=my-topic-1513508632-1348735863, testConfigurationPerformanceOptions=my-topic-1891296149-770116276, testReceiveSimpleMessageTls=my-topic-1264858218-642520562, testUserTemplate=my-topic-515111334-943904347, testMoreReplicasThanAvailableBrokers=my-topic-2107805499-2107307630}
2022-03-28 13:14:47 [ForkJoinPool-1-worker-5] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-455915b9-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-d4dad0de-kafka-clients, testSendSimpleMessageTls=my-cluster-cfce2f5b-kafka-clients, testConfigurationReflection=my-cluster-aa0ecd2f-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-ed39a269-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-534609d0-kafka-clients, testCapacityFile=my-cluster-39ee3f62-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-0b062573-kafka-clients, testTlsExternalUser=my-cluster-965e9d1e-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-31feb41b-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-27773859-kafka-clients, testCreateTopicAfterUnsupportedOperation=my-cluster-2938df05-kafka-clients, testScramUserWithQuotas=my-cluster-90c82d82-kafka-clients, testTopicModificationOfReplicationFactor=my-cluster-421b59dd-kafka-clients, testKafkaAdminTopicOperations=my-cluster-6acb723c-kafka-clients, testConfigurationFileIsCreated=my-cluster-020ceaf8-kafka-clients, testCreateTopicViaKafka=my-cluster-43bd601f-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5-kafka-clients, testUpdateUser=my-cluster-e16ccf16-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-b6bd312e-kafka-clients, testDeleteTopicEnableFalse=my-cluster-8c17a5fe-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-30bb976d-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-3e2e11de-kafka-clients, testTlsUserWithQuotas=my-cluster-3b191485-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2-kafka-clients, testSendingMessagesToNonExistingTopic=my-cluster-64488b2e-kafka-clients, testConfigurationPerformanceOptions=my-cluster-bc0ed00a-kafka-clients, testReceiveSimpleMessageTls=my-cluster-362cd511-kafka-clients, testUserTemplate=my-cluster-157df94e-kafka-clients, testMoreReplicasThanAvailableBrokers=my-cluster-1f5cbe38-kafka-clients}
2022-03-28 13:14:48 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-8c17a5fe-isolated in namespace topic-st
2022-03-28 13:14:48 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-8c17a5fe-isolated
2022-03-28 13:14:48 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-365
2022-03-28 13:14:48 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready
2022-03-28 13:14:48 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready
2022-03-28 13:14:48 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (839881ms till timeout)
2022-03-28 13:14:49 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-365
2022-03-28 13:14:49 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:14:49 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-45
2022-03-28 13:14:49 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (838770ms till timeout)
2022-03-28 13:14:50 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-create-45
2022-03-28 13:14:50 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:14:50 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-103
2022-03-28 13:14:50 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-103
2022-03-28 13:14:50 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:14:50 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-104
2022-03-28 13:14:50 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (837659ms till timeout)
2022-03-28 13:14:51 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-104
2022-03-28 13:14:51 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:14:51 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-130
2022-03-28 13:14:52 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-130
2022-03-28 13:14:52 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:14:52 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-131
2022-03-28 13:14:52 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (836549ms till timeout)
2022-03-28 13:14:52 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-131
2022-03-28 13:14:52 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:14:52 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-132
2022-03-28 13:14:53 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-132
2022-03-28 13:14:53 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:14:53 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-133
2022-03-28 13:14:53 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (835439ms till timeout)
2022-03-28 13:14:53 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-133
2022-03-28 13:14:53 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:14:53 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-134
2022-03-28 13:14:54 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-134
2022-03-28 13:14:54 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:14:54 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-135
2022-03-28 13:14:54 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (834329ms till timeout)
2022-03-28 13:14:54 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-135
2022-03-28 13:14:54 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:14:54 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-136
2022-03-28 13:14:55 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-136
2022-03-28 13:14:55 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:14:55 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-137
2022-03-28 13:14:55 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (833220ms till timeout)
2022-03-28 13:14:55 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-137
2022-03-28 13:14:55 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:14:55 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-138
2022-03-28 13:14:56 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-138
2022-03-28 13:14:56 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:14:56 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-139
2022-03-28 13:14:56 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (832111ms till timeout)
2022-03-28 13:14:56 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-139
2022-03-28 13:14:56 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:14:56 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-140
2022-03-28 13:14:57 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-140
2022-03-28 13:14:57 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:14:57 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-141
2022-03-28 13:14:57 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (831001ms till timeout)
2022-03-28 13:14:58 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-141
2022-03-28 13:14:58 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:14:58 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-142
2022-03-28 13:14:58 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-142
2022-03-28 13:14:58 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:14:58 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-143
2022-03-28 13:14:58 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (829890ms till timeout)
2022-03-28 13:14:59 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-143
2022-03-28 13:14:59 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:14:59 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-144
2022-03-28 13:14:59 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-144
2022-03-28 13:14:59 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:14:59 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-145
2022-03-28 13:14:59 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (828780ms till timeout)
2022-03-28 13:15:00 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-145
2022-03-28 13:15:00 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:00 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-146
2022-03-28 13:15:00 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-146
2022-03-28 13:15:00 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:00 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-147
2022-03-28 13:15:00 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (827669ms till timeout)
2022-03-28 13:15:01 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-147
2022-03-28 13:15:01 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:01 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-148
2022-03-28 13:15:01 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-148
2022-03-28 13:15:01 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:01 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-149
2022-03-28 13:15:02 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (826557ms till timeout)
2022-03-28 13:15:02 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-149
2022-03-28 13:15:02 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:02 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-150
2022-03-28 13:15:02 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-150
2022-03-28 13:15:02 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:02 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-151
2022-03-28 13:15:03 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (825409ms till timeout)
2022-03-28 13:15:03 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-151
2022-03-28 13:15:03 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:03 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-152
2022-03-28 13:15:03 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-152
2022-03-28 13:15:03 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:03 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-153
2022-03-28 13:15:04 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (824296ms till timeout)
2022-03-28 13:15:04 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-153
2022-03-28 13:15:04 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:04 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-154
2022-03-28 13:15:05 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-154
2022-03-28 13:15:05 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:05 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-155
2022-03-28 13:15:05 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (823180ms till timeout)
2022-03-28 13:15:05 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-155
2022-03-28 13:15:05 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:05 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-156
2022-03-28 13:15:06 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-156
2022-03-28 13:15:06 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:06 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-157
2022-03-28 13:15:06 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (822069ms till timeout)
2022-03-28 13:15:06 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-157
2022-03-28 13:15:06 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:06 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-158
2022-03-28 13:15:07 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-158
2022-03-28 13:15:07 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:07 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-159
2022-03-28 13:15:07 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (820957ms till timeout)
2022-03-28 13:15:07 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-159
2022-03-28 13:15:07 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:07 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-160
2022-03-28 13:15:08 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-160
2022-03-28 13:15:08 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:08 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-161
2022-03-28 13:15:08 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (819849ms till timeout)
2022-03-28 13:15:09 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-161
2022-03-28 13:15:09 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:09 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-162
2022-03-28 13:15:09 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-162
2022-03-28 13:15:09 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:09 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-163
2022-03-28 13:15:09 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (818736ms till timeout)
2022-03-28 13:15:10 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-163
2022-03-28 13:15:10 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:10 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-164
2022-03-28 13:15:10 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-164
2022-03-28 13:15:10 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:10 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-165
2022-03-28 13:15:10 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (817625ms till timeout)
2022-03-28 13:15:11 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-165
2022-03-28 13:15:11 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:11 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-166
2022-03-28 13:15:11 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-166
2022-03-28 13:15:11 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:11 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-167
2022-03-28 13:15:12 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (816514ms till timeout)
2022-03-28 13:15:12 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-167
2022-03-28 13:15:12 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:12 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-168
2022-03-28 13:15:12 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-168
2022-03-28 13:15:12 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:12 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-169
2022-03-28 13:15:13 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (815402ms till timeout)
2022-03-28 13:15:13 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-169
2022-03-28 13:15:13 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:13 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-170
2022-03-28 13:15:14 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-170
2022-03-28 13:15:14 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:14 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-171
2022-03-28 13:15:14 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (814292ms till timeout)
2022-03-28 13:15:14 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-171
2022-03-28 13:15:14 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:14 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-172
2022-03-28 13:15:15 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-172
2022-03-28 13:15:15 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:15 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-173
2022-03-28 13:15:15 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (813180ms till timeout)
2022-03-28 13:15:15 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-173
2022-03-28 13:15:15 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:15 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-174
2022-03-28 13:15:16 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-174
2022-03-28 13:15:16 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:16 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-175
2022-03-28 13:15:16 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (812072ms till timeout)
2022-03-28 13:15:16 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-175
2022-03-28 13:15:16 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:16 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-176
2022-03-28 13:15:17 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-176
2022-03-28 13:15:17 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:17 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-177
2022-03-28 13:15:17 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (810963ms till timeout)
2022-03-28 13:15:17 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-177
2022-03-28 13:15:17 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:17 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-178
2022-03-28 13:15:18 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-178
2022-03-28 13:15:18 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:18 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-179
2022-03-28 13:15:18 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (809852ms till timeout)
2022-03-28 13:15:19 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-179
2022-03-28 13:15:19 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:19 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-180
2022-03-28 13:15:19 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-180
2022-03-28 13:15:19 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:19 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-181
2022-03-28 13:15:19 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (808737ms till timeout)
2022-03-28 13:15:20 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-181
2022-03-28 13:15:20 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:20 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-182
2022-03-28 13:15:20 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-182
2022-03-28 13:15:20 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:20 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-183
2022-03-28 13:15:20 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (807628ms till timeout)
2022-03-28 13:15:21 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-183
2022-03-28 13:15:21 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:21 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-184
2022-03-28 13:15:21 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-184
2022-03-28 13:15:21 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:21 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-185
2022-03-28 13:15:22 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (806519ms till timeout)
2022-03-28 13:15:22 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-185
2022-03-28 13:15:22 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:22 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-186
2022-03-28 13:15:22 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-186
2022-03-28 13:15:22 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:22 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-187
2022-03-28 13:15:23 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (805408ms till timeout)
2022-03-28 13:15:23 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-187
2022-03-28 13:15:23 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:23 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-188
2022-03-28 13:15:23 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-188
2022-03-28 13:15:23 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:23 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-189
2022-03-28 13:15:24 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (804296ms till timeout)
2022-03-28 13:15:24 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-189
2022-03-28 13:15:24 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:24 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-190
2022-03-28 13:15:25 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-190
2022-03-28 13:15:25 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:25 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-191
2022-03-28 13:15:25 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (803187ms till timeout)
2022-03-28 13:15:25 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-191
2022-03-28 13:15:25 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:25 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-192
2022-03-28 13:15:26 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-192
2022-03-28 13:15:26 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:26 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-193
2022-03-28 13:15:26 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (802078ms till timeout)
2022-03-28 13:15:26 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-193
2022-03-28 13:15:26 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:26 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-194
2022-03-28 13:15:27 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-194
2022-03-28 13:15:27 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:27 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-195
2022-03-28 13:15:27 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (800968ms till timeout)
2022-03-28 13:15:27 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-195
2022-03-28 13:15:27 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:27 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-196
2022-03-28 13:15:28 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-196
2022-03-28 13:15:28 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:28 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-197
2022-03-28 13:15:28 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (799856ms till timeout)
2022-03-28 13:15:28 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-197
2022-03-28 13:15:28 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:28 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-198
2022-03-28 13:15:29 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-198
2022-03-28 13:15:29 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:29 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-199
2022-03-28 13:15:29 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (798745ms till timeout)
2022-03-28 13:15:29 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-199
2022-03-28 13:15:29 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:29 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-200
2022-03-28 13:15:30 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-200
2022-03-28 13:15:30 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:30 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-201
2022-03-28 13:15:30 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (797635ms till timeout)
2022-03-28 13:15:31 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-201
2022-03-28 13:15:31 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:31 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-202
2022-03-28 13:15:31 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-202
2022-03-28 13:15:31 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:31 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-203
2022-03-28 13:15:32 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (796523ms till timeout)
2022-03-28 13:15:32 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-203
2022-03-28 13:15:32 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:32 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-204
2022-03-28 13:15:32 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-204
2022-03-28 13:15:32 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:32 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-205
2022-03-28 13:15:33 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (795414ms till timeout)
2022-03-28 13:15:33 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-205
2022-03-28 13:15:33 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:33 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-206
2022-03-28 13:15:34 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-206
2022-03-28 13:15:34 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:34 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-207
2022-03-28 13:15:34 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (794299ms till timeout)
2022-03-28 13:15:34 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-207
2022-03-28 13:15:34 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:34 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-219
2022-03-28 13:15:35 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-219
2022-03-28 13:15:35 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:35 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-225
2022-03-28 13:15:35 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (793190ms till timeout)
2022-03-28 13:15:35 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-225
2022-03-28 13:15:35 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:35 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-250
2022-03-28 13:15:36 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-250
2022-03-28 13:15:36 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:36 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-251
2022-03-28 13:15:36 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (792080ms till timeout)
2022-03-28 13:15:37 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-251
2022-03-28 13:15:37 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:37 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-252
2022-03-28 13:15:37 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-252
2022-03-28 13:15:37 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:37 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-253
2022-03-28 13:15:37 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (790968ms till timeout)
2022-03-28 13:15:38 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-253
2022-03-28 13:15:38 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:38 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-254
2022-03-28 13:15:38 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-254
2022-03-28 13:15:38 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:38 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-255
2022-03-28 13:15:38 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (789859ms till timeout)
2022-03-28 13:15:39 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-255
2022-03-28 13:15:39 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:39 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-256
2022-03-28 13:15:39 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-256
2022-03-28 13:15:39 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:39 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-257
2022-03-28 13:15:39 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (788750ms till timeout)
2022-03-28 13:15:40 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-257
2022-03-28 13:15:40 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:40 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-258
2022-03-28 13:15:40 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-258
2022-03-28 13:15:40 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:40 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-259
2022-03-28 13:15:40 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (787640ms till timeout)
2022-03-28 13:15:41 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-259
2022-03-28 13:15:41 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:41 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-260
2022-03-28 13:15:41 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-260
2022-03-28 13:15:41 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:41 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-261
2022-03-28 13:15:42 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (786532ms till timeout)
2022-03-28 13:15:42 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-261
2022-03-28 13:15:42 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:42 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-262
2022-03-28 13:15:43 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-262
2022-03-28 13:15:43 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:43 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-263
2022-03-28 13:15:43 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (785422ms till timeout)
2022-03-28 13:15:43 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-263
2022-03-28 13:15:43 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:43 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-264
2022-03-28 13:15:44 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-264
2022-03-28 13:15:44 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:44 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-265
2022-03-28 13:15:44 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (784313ms till timeout)
2022-03-28 13:15:44 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-265
2022-03-28 13:15:44 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:44 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-266
2022-03-28 13:15:45 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-266
2022-03-28 13:15:45 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:45 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-267
2022-03-28 13:15:45 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (783201ms till timeout)
2022-03-28 13:15:45 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-267
2022-03-28 13:15:45 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:45 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-268
2022-03-28 13:15:46 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-268
2022-03-28 13:15:46 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:46 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-269
2022-03-28 13:15:46 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (782090ms till timeout)
2022-03-28 13:15:47 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-269
2022-03-28 13:15:47 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:47 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-270
2022-03-28 13:15:47 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-270
2022-03-28 13:15:47 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:47 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-271
2022-03-28 13:15:47 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (780982ms till timeout)
2022-03-28 13:15:48 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-271
2022-03-28 13:15:48 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:48 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-272
2022-03-28 13:15:48 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-272
2022-03-28 13:15:48 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:48 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-273
2022-03-28 13:15:48 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (779871ms till timeout)
2022-03-28 13:15:49 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-273
2022-03-28 13:15:49 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:49 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-274
2022-03-28 13:15:49 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-274
2022-03-28 13:15:49 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:49 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-275
2022-03-28 13:15:49 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (778759ms till timeout)
2022-03-28 13:15:50 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-275
2022-03-28 13:15:50 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:50 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-276
2022-03-28 13:15:50 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-276
2022-03-28 13:15:50 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:50 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-277
2022-03-28 13:15:50 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (777650ms till timeout)
2022-03-28 13:15:51 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-277
2022-03-28 13:15:51 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:51 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-278
2022-03-28 13:15:52 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-278
2022-03-28 13:15:52 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:52 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-279
2022-03-28 13:15:52 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (776538ms till timeout)
2022-03-28 13:15:52 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-279
2022-03-28 13:15:52 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:52 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-280
2022-03-28 13:15:53 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-280
2022-03-28 13:15:53 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:53 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-281
2022-03-28 13:15:53 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (775428ms till timeout)
2022-03-28 13:15:53 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-281
2022-03-28 13:15:53 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:53 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-282
2022-03-28 13:15:54 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-282
2022-03-28 13:15:54 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:54 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-283
2022-03-28 13:15:54 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Kafka: my-cluster-8c17a5fe-isolated will have desired state: Ready not ready, will try again in 1000 ms (774317ms till timeout)
2022-03-28 13:15:54 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-283
2022-03-28 13:15:54 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:54 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-284
2022-03-28 13:15:55 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-284
2022-03-28 13:15:55 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:55 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-285
2022-03-28 13:15:55 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:444] Kafka: my-cluster-8c17a5fe-isolated is in desired state: Ready
2022-03-28 13:15:55 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update Deployment my-cluster-8c17a5fe-isolated-kafka-clients in namespace topic-st
2022-03-28 13:15:55 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:my-cluster-8c17a5fe-isolated-kafka-clients
2022-03-28 13:15:55 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-285
2022-03-28 13:15:55 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:55 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-286
2022-03-28 13:15:55 [ForkJoinPool-1-worker-5] INFO  [DeploymentUtils:161] Wait for Deployment: my-cluster-8c17a5fe-isolated-kafka-clients will be ready
2022-03-28 13:15:55 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Wait for Deployment: my-cluster-8c17a5fe-isolated-kafka-clients will be ready
2022-03-28 13:15:56 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Wait for Deployment: my-cluster-8c17a5fe-isolated-kafka-clients will be ready not ready, will try again in 1000 ms (479893ms till timeout)
2022-03-28 13:15:56 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-286
2022-03-28 13:15:56 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:56 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-287
2022-03-28 13:15:56 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-287
2022-03-28 13:15:56 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:56 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-288
2022-03-28 13:15:57 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Wait for Deployment: my-cluster-8c17a5fe-isolated-kafka-clients will be ready not ready, will try again in 1000 ms (478782ms till timeout)
2022-03-28 13:15:57 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-288
2022-03-28 13:15:57 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:57 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-289
2022-03-28 13:15:58 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-289
2022-03-28 13:15:58 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:58 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-290
2022-03-28 13:15:58 [ForkJoinPool-1-worker-5] INFO  [DeploymentUtils:168] Deployment: my-cluster-8c17a5fe-isolated-kafka-clients is ready
2022-03-28 13:15:58 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:155] Create/Update KafkaTopic my-topic-1837190581-1217032035 in namespace topic-st
2022-03-28 13:15:58 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-1837190581-1217032035
2022-03-28 13:15:58 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-1837190581-1217032035 will have desired state: Ready
2022-03-28 13:15:58 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-1837190581-1217032035 will have desired state: Ready
2022-03-28 13:15:58 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] KafkaTopic: my-topic-1837190581-1217032035 will have desired state: Ready not ready, will try again in 1000 ms (179891ms till timeout)
2022-03-28 13:15:58 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-290
2022-03-28 13:15:58 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:58 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-291
2022-03-28 13:15:59 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-291
2022-03-28 13:15:59 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:59 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-292
2022-03-28 13:15:59 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:444] KafkaTopic: my-topic-1837190581-1217032035 is in desired state: Ready
2022-03-28 13:15:59 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-292
2022-03-28 13:15:59 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:15:59 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-293
2022-03-28 13:16:00 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-293
2022-03-28 13:16:00 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:00 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-294
2022-03-28 13:16:00 [ForkJoinPool-1-worker-5] INFO  [AbstractKafkaClient:188] Consumer group were not specified going to create the random one.
2022-03-28 13:16:00 [ForkJoinPool-1-worker-5] DEBUG [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@45f9692f, which are set.
2022-03-28 13:16:00 [ForkJoinPool-1-worker-5] INFO  [InternalKafkaClient:93] Starting verifiableClient plain producer with the following configuration: VerifiableClient{allowedArguments=[TOPIC, BOOTSTRAP_SERVER, BROKER_LIST, MAX_MESSAGES, THROUGHPUT, ACKS, PRODUCER_CONFIG, MESSAGE_CREATE_TIME, VALUE_PREFIX, REPEATING_KEYS, USER], lock=java.lang.Object@4f79d3b9, messages=[], arguments=[--topic, my-topic-1837190581-1217032035, --max-messages, 100, --bootstrap-server, my-cluster-8c17a5fe-isolated-kafka-bootstrap.topic-st.svc:9092], executable='/opt/kafka/producer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_PRODUCER, podName='my-cluster-8c17a5fe-isolated-kafka-clients-64dc8cbc89-bb6j9', podNamespace='topic-st', bootstrapServer='my-cluster-8c17a5fe-isolated-kafka-bootstrap.topic-st.svc:9092', topicName='my-topic-1837190581-1217032035', maxMessages=100, kafkaUsername='null', consumerGroupName='null', consumerInstanceId='null', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@45f9692f}
2022-03-28 13:16:00 [ForkJoinPool-1-worker-5] INFO  [InternalKafkaClient:94] Producing 100 messages to my-cluster-8c17a5fe-isolated-kafka-bootstrap.topic-st.svc:9092:my-topic-1837190581-1217032035 from pod my-cluster-8c17a5fe-isolated-kafka-clients-64dc8cbc89-bb6j9
2022-03-28 13:16:00 [ForkJoinPool-1-worker-5] INFO  [VerifiableClient:192] Client command: oc exec my-cluster-8c17a5fe-isolated-kafka-clients-64dc8cbc89-bb6j9 -n topic-st -- /opt/kafka/producer.sh --topic my-topic-1837190581-1217032035 --max-messages 100 --bootstrap-server my-cluster-8c17a5fe-isolated-kafka-bootstrap.topic-st.svc:9092
2022-03-28 13:16:00 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc exec my-cluster-8c17a5fe-isolated-kafka-clients-64dc8cbc89-bb6j9 -n topic-st -- /opt/kafka/producer.sh --topic my-topic-1837190581-1217032035 --max-messages 100 --bootstrap-server my-cluster-8c17a5fe-isolated-kafka-bootstrap.topic-st.svc:9092
2022-03-28 13:16:00 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-294
2022-03-28 13:16:00 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:00 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-295
2022-03-28 13:16:01 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-295
2022-03-28 13:16:01 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:01 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-296
2022-03-28 13:16:01 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-296
2022-03-28 13:16:01 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:01 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-297
2022-03-28 13:16:02 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-297
2022-03-28 13:16:02 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:02 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-298
2022-03-28 13:16:03 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-298
2022-03-28 13:16:03 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:03 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-299
2022-03-28 13:16:03 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-299
2022-03-28 13:16:03 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:03 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-300
2022-03-28 13:16:03 [ForkJoinPool-1-worker-5] INFO  [InternalKafkaClient:97] Producer finished correctly: true
2022-03-28 13:16:03 [ForkJoinPool-1-worker-5] INFO  [InternalKafkaClient:101] Producer produced 100 messages
2022-03-28 13:16:03 [ForkJoinPool-1-worker-5] INFO  [TopicST:395] Deleting KafkaTopic: my-topic-1837190581-1217032035
2022-03-28 13:16:03 [ForkJoinPool-1-worker-5] INFO  [TopicST:397] KafkaTopic my-topic-1837190581-1217032035 deleted
2022-03-28 13:16:03 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Topic my-topic-1837190581-1217032035 has rolled
2022-03-28 13:16:03 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (299890ms till timeout)
2022-03-28 13:16:04 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-300
2022-03-28 13:16:04 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:04 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-301
2022-03-28 13:16:04 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-301
2022-03-28 13:16:04 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:04 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-302
2022-03-28 13:16:05 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (298780ms till timeout)
2022-03-28 13:16:05 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-302
2022-03-28 13:16:05 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:05 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-303
2022-03-28 13:16:05 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-303
2022-03-28 13:16:05 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:05 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-304
2022-03-28 13:16:06 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (297672ms till timeout)
2022-03-28 13:16:06 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-304
2022-03-28 13:16:06 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:06 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-305
2022-03-28 13:16:06 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-305
2022-03-28 13:16:06 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:06 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-306
2022-03-28 13:16:07 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (296563ms till timeout)
2022-03-28 13:16:07 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-306
2022-03-28 13:16:07 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:07 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-307
2022-03-28 13:16:07 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-307
2022-03-28 13:16:07 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:07 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-308
2022-03-28 13:16:08 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (295455ms till timeout)
2022-03-28 13:16:08 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-308
2022-03-28 13:16:08 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:08 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-309
2022-03-28 13:16:08 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-309
2022-03-28 13:16:08 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:08 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-310
2022-03-28 13:16:09 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (294345ms till timeout)
2022-03-28 13:16:09 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-310
2022-03-28 13:16:09 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:09 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-311
2022-03-28 13:16:10 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-311
2022-03-28 13:16:10 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:10 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-312
2022-03-28 13:16:10 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (293238ms till timeout)
2022-03-28 13:16:10 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-312
2022-03-28 13:16:10 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:10 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-313
2022-03-28 13:16:11 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-313
2022-03-28 13:16:11 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:11 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-314
2022-03-28 13:16:11 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-314
2022-03-28 13:16:11 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:11 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-315
2022-03-28 13:16:11 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (292129ms till timeout)
2022-03-28 13:16:12 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-315
2022-03-28 13:16:12 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:12 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-316
2022-03-28 13:16:12 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-316
2022-03-28 13:16:12 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:12 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-317
2022-03-28 13:16:12 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (291017ms till timeout)
2022-03-28 13:16:13 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-317
2022-03-28 13:16:13 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:13 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-318
2022-03-28 13:16:13 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-318
2022-03-28 13:16:13 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:13 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-319
2022-03-28 13:16:13 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (289906ms till timeout)
2022-03-28 13:16:14 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-319
2022-03-28 13:16:14 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:14 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-320
2022-03-28 13:16:14 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-320
2022-03-28 13:16:14 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:14 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-321
2022-03-28 13:16:15 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (288797ms till timeout)
2022-03-28 13:16:15 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-321
2022-03-28 13:16:15 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:15 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-322
2022-03-28 13:16:16 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-322
2022-03-28 13:16:16 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:16 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-323
2022-03-28 13:16:16 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (287688ms till timeout)
2022-03-28 13:16:16 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-323
2022-03-28 13:16:16 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:16 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-324
2022-03-28 13:16:17 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-324
2022-03-28 13:16:17 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:17 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-325
2022-03-28 13:16:17 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (286576ms till timeout)
2022-03-28 13:16:17 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-325
2022-03-28 13:16:17 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:17 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-326
2022-03-28 13:16:18 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-326
2022-03-28 13:16:18 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:18 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-327
2022-03-28 13:16:18 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (285467ms till timeout)
2022-03-28 13:16:18 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-327
2022-03-28 13:16:18 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:18 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-328
2022-03-28 13:16:19 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (284358ms till timeout)
2022-03-28 13:16:19 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-328
2022-03-28 13:16:19 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:19 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-345
2022-03-28 13:16:20 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-345
2022-03-28 13:16:20 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:20 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-346
2022-03-28 13:16:20 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (283247ms till timeout)
2022-03-28 13:16:20 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-346
2022-03-28 13:16:20 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:20 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-370
2022-03-28 13:16:21 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-370
2022-03-28 13:16:21 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:21 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-371
2022-03-28 13:16:21 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (282139ms till timeout)
2022-03-28 13:16:21 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-371
2022-03-28 13:16:21 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:21 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-372
2022-03-28 13:16:22 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-372
2022-03-28 13:16:22 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:22 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-373
2022-03-28 13:16:22 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (281030ms till timeout)
2022-03-28 13:16:22 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-373
2022-03-28 13:16:22 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:22 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-374
2022-03-28 13:16:23 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-374
2022-03-28 13:16:23 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:23 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-375
2022-03-28 13:16:23 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (279921ms till timeout)
2022-03-28 13:16:24 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-375
2022-03-28 13:16:24 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:24 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-376
2022-03-28 13:16:24 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-376
2022-03-28 13:16:24 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:24 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-377
2022-03-28 13:16:24 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (278811ms till timeout)
2022-03-28 13:16:25 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-377
2022-03-28 13:16:25 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:25 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-378
2022-03-28 13:16:25 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-378
2022-03-28 13:16:25 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:25 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-379
2022-03-28 13:16:26 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (277702ms till timeout)
2022-03-28 13:16:26 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-379
2022-03-28 13:16:26 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:26 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-380
2022-03-28 13:16:26 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-380
2022-03-28 13:16:26 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:26 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-381
2022-03-28 13:16:27 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (276591ms till timeout)
2022-03-28 13:16:27 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-381
2022-03-28 13:16:27 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:27 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-382
2022-03-28 13:16:28 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-382
2022-03-28 13:16:28 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:28 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-383
2022-03-28 13:16:28 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (275483ms till timeout)
2022-03-28 13:16:28 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-383
2022-03-28 13:16:28 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:28 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-384
2022-03-28 13:16:29 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-384
2022-03-28 13:16:29 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:29 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-385
2022-03-28 13:16:29 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (274375ms till timeout)
2022-03-28 13:16:30 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (273264ms till timeout)
2022-03-28 13:16:31 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (272154ms till timeout)
2022-03-28 13:16:32 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (271045ms till timeout)
2022-03-28 13:16:33 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (269931ms till timeout)
2022-03-28 13:16:34 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (268819ms till timeout)
2022-03-28 13:16:35 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-385
2022-03-28 13:16:35 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:35 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-386
2022-03-28 13:16:36 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (267711ms till timeout)
2022-03-28 13:16:37 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (266600ms till timeout)
2022-03-28 13:16:38 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (265490ms till timeout)
2022-03-28 13:16:39 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (264380ms till timeout)
2022-03-28 13:16:40 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (263271ms till timeout)
2022-03-28 13:16:41 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-386
2022-03-28 13:16:41 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:41 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-387
2022-03-28 13:16:41 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (262159ms till timeout)
2022-03-28 13:16:42 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (261049ms till timeout)
2022-03-28 13:16:43 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (259938ms till timeout)
2022-03-28 13:16:44 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (258827ms till timeout)
2022-03-28 13:16:46 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (257717ms till timeout)
2022-03-28 13:16:46 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-387
2022-03-28 13:16:46 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:46 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-388
2022-03-28 13:16:47 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (256608ms till timeout)
2022-03-28 13:16:47 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-388
2022-03-28 13:16:47 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:47 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-389
2022-03-28 13:16:47 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-389
2022-03-28 13:16:47 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:47 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-390
2022-03-28 13:16:48 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (255498ms till timeout)
2022-03-28 13:16:48 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-390
2022-03-28 13:16:48 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:48 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-391
2022-03-28 13:16:49 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (254387ms till timeout)
2022-03-28 13:16:49 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-391
2022-03-28 13:16:49 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:49 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-392
2022-03-28 13:16:50 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (253278ms till timeout)
2022-03-28 13:16:51 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (252168ms till timeout)
2022-03-28 13:16:52 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (251057ms till timeout)
2022-03-28 13:16:53 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (249947ms till timeout)
2022-03-28 13:16:54 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (248837ms till timeout)
2022-03-28 13:16:55 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-392
2022-03-28 13:16:55 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:55 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-393
2022-03-28 13:16:56 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-393
2022-03-28 13:16:56 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:16:56 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-394
2022-03-28 13:16:56 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (247726ms till timeout)
2022-03-28 13:16:57 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (246616ms till timeout)
2022-03-28 13:16:58 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (245504ms till timeout)
2022-03-28 13:16:59 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (244390ms till timeout)
2022-03-28 13:17:00 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (243280ms till timeout)
2022-03-28 13:17:01 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-394
2022-03-28 13:17:01 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:01 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-395
2022-03-28 13:17:01 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (242173ms till timeout)
2022-03-28 13:17:02 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-395
2022-03-28 13:17:02 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:02 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-396
2022-03-28 13:17:02 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (241063ms till timeout)
2022-03-28 13:17:02 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-396
2022-03-28 13:17:02 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:02 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-397
2022-03-28 13:17:03 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-397
2022-03-28 13:17:03 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:03 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-398
2022-03-28 13:17:03 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (239953ms till timeout)
2022-03-28 13:17:03 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-398
2022-03-28 13:17:03 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:03 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-399
2022-03-28 13:17:04 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-399
2022-03-28 13:17:04 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:04 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-406
2022-03-28 13:17:04 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (238841ms till timeout)
2022-03-28 13:17:04 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-406
2022-03-28 13:17:04 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:04 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-407
2022-03-28 13:17:05 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-407
2022-03-28 13:17:05 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:05 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-408
2022-03-28 13:17:06 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-408
2022-03-28 13:17:06 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:06 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-409
2022-03-28 13:17:06 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (237731ms till timeout)
2022-03-28 13:17:06 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-409
2022-03-28 13:17:06 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:06 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-410
2022-03-28 13:17:07 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-410
2022-03-28 13:17:07 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:07 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-411
2022-03-28 13:17:07 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (236622ms till timeout)
2022-03-28 13:17:07 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-411
2022-03-28 13:17:07 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:07 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-412
2022-03-28 13:17:08 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-412
2022-03-28 13:17:08 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:08 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-413
2022-03-28 13:17:08 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (235511ms till timeout)
2022-03-28 13:17:08 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-413
2022-03-28 13:17:08 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:08 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-414
2022-03-28 13:17:09 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-414
2022-03-28 13:17:09 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:09 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-415
2022-03-28 13:17:09 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (234402ms till timeout)
2022-03-28 13:17:09 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-415
2022-03-28 13:17:09 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:09 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-416
2022-03-28 13:17:10 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-416
2022-03-28 13:17:10 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:10 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-417
2022-03-28 13:17:10 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (233293ms till timeout)
2022-03-28 13:17:10 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-417
2022-03-28 13:17:10 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:10 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-418
2022-03-28 13:17:11 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-418
2022-03-28 13:17:11 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:11 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-419
2022-03-28 13:17:11 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (232183ms till timeout)
2022-03-28 13:17:12 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-419
2022-03-28 13:17:12 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:12 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-420
2022-03-28 13:17:12 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-420
2022-03-28 13:17:12 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:12 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-421
2022-03-28 13:17:12 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (231075ms till timeout)
2022-03-28 13:17:13 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-421
2022-03-28 13:17:13 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:13 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-422
2022-03-28 13:17:13 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-422
2022-03-28 13:17:13 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:13 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-423
2022-03-28 13:17:13 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (229966ms till timeout)
2022-03-28 13:17:14 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-423
2022-03-28 13:17:14 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:14 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-424
2022-03-28 13:17:14 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-424
2022-03-28 13:17:14 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:14 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-425
2022-03-28 13:17:14 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (228857ms till timeout)
2022-03-28 13:17:15 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-425
2022-03-28 13:17:15 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:15 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-426
2022-03-28 13:17:15 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-426
2022-03-28 13:17:15 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:15 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-427
2022-03-28 13:17:16 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (227746ms till timeout)
2022-03-28 13:17:16 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-427
2022-03-28 13:17:16 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:16 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-428
2022-03-28 13:17:17 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-428
2022-03-28 13:17:17 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:17 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-429
2022-03-28 13:17:17 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (226637ms till timeout)
2022-03-28 13:17:17 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-429
2022-03-28 13:17:17 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:17 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-430
2022-03-28 13:17:18 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-430
2022-03-28 13:17:18 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:18 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-431
2022-03-28 13:17:18 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (225524ms till timeout)
2022-03-28 13:17:18 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-431
2022-03-28 13:17:18 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:18 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-432
2022-03-28 13:17:19 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-432
2022-03-28 13:17:19 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:19 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-433
2022-03-28 13:17:19 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (224414ms till timeout)
2022-03-28 13:17:19 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-433
2022-03-28 13:17:19 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:19 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-434
2022-03-28 13:17:20 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-434
2022-03-28 13:17:20 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:20 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-435
2022-03-28 13:17:20 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (223304ms till timeout)
2022-03-28 13:17:20 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-435
2022-03-28 13:17:20 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:20 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-436
2022-03-28 13:17:21 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-436
2022-03-28 13:17:21 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:21 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-437
2022-03-28 13:17:21 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (222196ms till timeout)
2022-03-28 13:17:22 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-437
2022-03-28 13:17:22 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:22 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-438
2022-03-28 13:17:22 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-438
2022-03-28 13:17:22 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:22 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-439
2022-03-28 13:17:22 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (221088ms till timeout)
2022-03-28 13:17:23 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-439
2022-03-28 13:17:23 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:23 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-440
2022-03-28 13:17:23 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-440
2022-03-28 13:17:23 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:23 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-441
2022-03-28 13:17:23 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (219979ms till timeout)
2022-03-28 13:17:24 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-441
2022-03-28 13:17:24 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:24 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-442
2022-03-28 13:17:24 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-442
2022-03-28 13:17:24 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:24 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-443
2022-03-28 13:17:24 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (218868ms till timeout)
2022-03-28 13:17:25 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-443
2022-03-28 13:17:25 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:25 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-444
2022-03-28 13:17:25 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-444
2022-03-28 13:17:25 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:25 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-445
2022-03-28 13:17:26 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (217759ms till timeout)
2022-03-28 13:17:26 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-445
2022-03-28 13:17:26 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:26 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-446
2022-03-28 13:17:27 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-446
2022-03-28 13:17:27 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:27 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-447
2022-03-28 13:17:27 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (216650ms till timeout)
2022-03-28 13:17:27 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-447
2022-03-28 13:17:27 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:27 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-448
2022-03-28 13:17:28 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-448
2022-03-28 13:17:28 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:28 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-449
2022-03-28 13:17:28 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (215540ms till timeout)
2022-03-28 13:17:28 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-449
2022-03-28 13:17:28 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:28 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-466
2022-03-28 13:17:29 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-466
2022-03-28 13:17:29 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:29 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-467
2022-03-28 13:17:29 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (214429ms till timeout)
2022-03-28 13:17:29 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-467
2022-03-28 13:17:29 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:29 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-490
2022-03-28 13:17:30 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-490
2022-03-28 13:17:30 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:30 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-491
2022-03-28 13:17:30 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (213319ms till timeout)
2022-03-28 13:17:30 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-491
2022-03-28 13:17:30 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:30 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-492
2022-03-28 13:17:31 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-492
2022-03-28 13:17:31 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:31 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-493
2022-03-28 13:17:31 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (212209ms till timeout)
2022-03-28 13:17:32 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-493
2022-03-28 13:17:32 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:32 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-494
2022-03-28 13:17:32 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-494
2022-03-28 13:17:32 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:32 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-495
2022-03-28 13:17:32 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (211100ms till timeout)
2022-03-28 13:17:33 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-495
2022-03-28 13:17:33 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:33 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-496
2022-03-28 13:17:33 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-496
2022-03-28 13:17:33 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:33 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-497
2022-03-28 13:17:33 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (209991ms till timeout)
2022-03-28 13:17:34 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-497
2022-03-28 13:17:34 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:34 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-498
2022-03-28 13:17:34 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-498
2022-03-28 13:17:34 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:34 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-499
2022-03-28 13:17:34 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (208880ms till timeout)
2022-03-28 13:17:35 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-delete-499
2022-03-28 13:17:35 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:35 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-0
2022-03-28 13:17:35 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-0
2022-03-28 13:17:35 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:35 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-1
2022-03-28 13:17:36 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (207768ms till timeout)
2022-03-28 13:17:36 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-1
2022-03-28 13:17:36 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:36 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-10
2022-03-28 13:17:37 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-10
2022-03-28 13:17:37 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:37 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-11
2022-03-28 13:17:37 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (206657ms till timeout)
2022-03-28 13:17:37 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-11
2022-03-28 13:17:37 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:37 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-12
2022-03-28 13:17:38 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-12
2022-03-28 13:17:38 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:38 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-13
2022-03-28 13:17:38 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (205547ms till timeout)
2022-03-28 13:17:38 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-13
2022-03-28 13:17:38 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:38 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-14
2022-03-28 13:17:39 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-14
2022-03-28 13:17:39 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:39 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-15
2022-03-28 13:17:39 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (204438ms till timeout)
2022-03-28 13:17:39 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-15
2022-03-28 13:17:39 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:39 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-16
2022-03-28 13:17:40 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-16
2022-03-28 13:17:40 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:40 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-17
2022-03-28 13:17:40 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (203328ms till timeout)
2022-03-28 13:17:40 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-17
2022-03-28 13:17:40 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:40 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-18
2022-03-28 13:17:41 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-18
2022-03-28 13:17:41 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:41 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-19
2022-03-28 13:17:41 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (202217ms till timeout)
2022-03-28 13:17:41 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-19
2022-03-28 13:17:41 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:41 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-2
2022-03-28 13:17:42 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-2
2022-03-28 13:17:42 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:42 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-3
2022-03-28 13:17:42 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (201106ms till timeout)
2022-03-28 13:17:43 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-3
2022-03-28 13:17:43 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:43 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-4
2022-03-28 13:17:43 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-4
2022-03-28 13:17:43 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:43 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-5
2022-03-28 13:17:43 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (199994ms till timeout)
2022-03-28 13:17:44 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-5
2022-03-28 13:17:44 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:44 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-6
2022-03-28 13:17:44 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-6
2022-03-28 13:17:44 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:44 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-7
2022-03-28 13:17:44 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (198882ms till timeout)
2022-03-28 13:17:45 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-7
2022-03-28 13:17:45 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:45 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-8
2022-03-28 13:17:45 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-8
2022-03-28 13:17:45 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:45 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-9
2022-03-28 13:17:46 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (197767ms till timeout)
2022-03-28 13:17:46 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace throttling-quota-st delete kafkatopic quota-topic-test-partitions-9
2022-03-28 13:17:46 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:46 [ForkJoinPool-1-worker-11] DEBUG [AbstractST:689] ============================================================================
2022-03-28 13:17:46 [ForkJoinPool-1-worker-11] DEBUG [AbstractST:690] [operators.topic.ThrottlingQuotaST - After All] - Clean up after test suite
2022-03-28 13:17:46 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:344] ############################################################################
2022-03-28 13:17:46 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:348] Delete all resources for ThrottlingQuotaST
2022-03-28 13:17:46 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:241] Delete of Kafka quota-cluster in namespace throttling-quota-st
2022-03-28 13:17:46 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:quota-cluster
2022-03-28 13:17:46 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:quota-cluster not ready, will try again in 10000 ms (839877ms till timeout)
2022-03-28 13:17:47 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Topic my-topic-1837190581-1217032035 has rolled not ready, will try again in 1000 ms (196658ms till timeout)
2022-03-28 13:17:48 [ForkJoinPool-1-worker-5] INFO  [TopicST:401] Wait KafkaTopic my-topic-1837190581-1217032035 recreation
2022-03-28 13:17:48 [ForkJoinPool-1-worker-5] INFO  [KafkaTopicUtils:78] Waiting for KafkaTopic my-topic-1837190581-1217032035 creation 
2022-03-28 13:17:48 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for KafkaTopic creation my-topic-1837190581-1217032035
2022-03-28 13:17:48 [ForkJoinPool-1-worker-5] INFO  [TopicST:403] KafkaTopic my-topic-1837190581-1217032035 recreated
2022-03-28 13:17:48 [ForkJoinPool-1-worker-5] DEBUG [VerifiableClient:137] This is all args io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@2665f2b9, which are set.
2022-03-28 13:17:48 [ForkJoinPool-1-worker-5] INFO  [InternalKafkaClient:156] Starting verifiableClient plain consumer with the following configuration: VerifiableClient{allowedArguments=[BOOTSTRAP_SERVER, BROKER_LIST, TOPIC, GROUP_ID, MAX_MESSAGES, SESSION_TIMEOUT, VERBOSE, ENABLE_AUTOCOMMIT, RESET_POLICY, ASSIGMENT_STRATEGY, CONSUMER_CONFIG, USER, GROUP_INSTANCE_ID], lock=java.lang.Object@278019d7, messages=[], arguments=[--topic, my-topic-1837190581-1217032035, --max-messages, 100, --group-id, my-consumer-group-195092808, --group-instance-id, instance704771958, --bootstrap-server, my-cluster-8c17a5fe-isolated-kafka-bootstrap.topic-st.svc:9092], executable='/opt/kafka/consumer.sh', executor=null, clientType=CLI_KAFKA_VERIFIABLE_CONSUMER, podName='my-cluster-8c17a5fe-isolated-kafka-clients-64dc8cbc89-bb6j9', podNamespace='topic-st', bootstrapServer='my-cluster-8c17a5fe-isolated-kafka-bootstrap.topic-st.svc:9092', topicName='my-topic-1837190581-1217032035', maxMessages=100, kafkaUsername='null', consumerGroupName='my-consumer-group-195092808', consumerInstanceId='instance704771958', clientArgumentMap=io.strimzi.systemtest.kafkaclients.clients.ClientArgumentMap@2665f2b9}
2022-03-28 13:17:48 [ForkJoinPool-1-worker-5] INFO  [InternalKafkaClient:157] Consuming 100 messages from my-cluster-8c17a5fe-isolated-kafka-bootstrap.topic-st.svc:9092#my-topic-1837190581-1217032035 from pod my-cluster-8c17a5fe-isolated-kafka-clients-64dc8cbc89-bb6j9
2022-03-28 13:17:48 [ForkJoinPool-1-worker-5] INFO  [VerifiableClient:192] Client command: oc exec my-cluster-8c17a5fe-isolated-kafka-clients-64dc8cbc89-bb6j9 -n topic-st -- /opt/kafka/consumer.sh --topic my-topic-1837190581-1217032035 --max-messages 100 --group-id my-consumer-group-195092808 --group-instance-id instance704771958 --bootstrap-server my-cluster-8c17a5fe-isolated-kafka-bootstrap.topic-st.svc:9092
2022-03-28 13:17:48 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc exec my-cluster-8c17a5fe-isolated-kafka-clients-64dc8cbc89-bb6j9 -n topic-st -- /opt/kafka/consumer.sh --topic my-topic-1837190581-1217032035 --max-messages 100 --group-id my-consumer-group-195092808 --group-instance-id instance704771958 --bootstrap-server my-cluster-8c17a5fe-isolated-kafka-bootstrap.topic-st.svc:9092
2022-03-28 13:17:54 [ForkJoinPool-1-worker-5] INFO  [InternalKafkaClient:160] Consumer finished correctly: true
2022-03-28 13:17:54 [ForkJoinPool-1-worker-5] INFO  [InternalKafkaClient:163] Consumer consumed 100 messages
2022-03-28 13:17:54 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:674] ============================================================================
2022-03-28 13:17:54 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:675] [operators.topic.TopicST - After Each] - Clean up after test
2022-03-28 13:17:54 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:344] ############################################################################
2022-03-28 13:17:54 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:348] Delete all resources for testDeleteTopicEnableFalse
2022-03-28 13:17:54 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of Deployment my-cluster-8c17a5fe-isolated-kafka-clients in namespace topic-st
2022-03-28 13:17:54 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of Kafka my-cluster-8c17a5fe-isolated in namespace topic-st
2022-03-28 13:17:54 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:241] Delete of KafkaTopic my-topic-1837190581-1217032035 in namespace topic-st
2022-03-28 13:17:54 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:my-cluster-8c17a5fe-isolated-kafka-clients
2022-03-28 13:17:55 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-8c17a5fe-isolated
2022-03-28 13:17:55 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-1837190581-1217032035
2022-03-28 13:17:55 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-8c17a5fe-isolated-kafka-clients not ready, will try again in 10000 ms (479351ms till timeout)
2022-03-28 13:17:56 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:369] ############################################################################
2022-03-28 13:17:56 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Namespace throttling-quota-st removal
2022-03-28 13:17:56 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 13:17:57 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 13:17:57 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:57 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace throttling-quota-st removal not ready, will try again in 1000 ms (479554ms till timeout)
2022-03-28 13:17:58 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 13:17:58 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 13:17:58 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:17:58 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace throttling-quota-st removal not ready, will try again in 1000 ms (478122ms till timeout)
2022-03-28 13:17:59 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 13:18:00 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 13:18:00 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:18:00 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace throttling-quota-st removal not ready, will try again in 1000 ms (476609ms till timeout)
2022-03-28 13:18:01 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 13:18:01 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 13:18:01 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 0
2022-03-28 13:18:01 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Namespace throttling-quota-st removal not ready, will try again in 1000 ms (475124ms till timeout)
2022-03-28 13:18:02 [ForkJoinPool-1-worker-11] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 13:18:06 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-8c17a5fe-isolated-kafka-clients not ready, will try again in 10000 ms (468904ms till timeout)
2022-03-28 13:18:08 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-9 get Namespace throttling-quota-st -o yaml
2022-03-28 13:18:08 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Return code: 1
2022-03-28 13:18:08 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 13:18:08 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] Error from server (NotFound): namespaces "throttling-quota-st" not found
2022-03-28 13:18:08 [ForkJoinPool-1-worker-11] DEBUG [Exec:419] ======STDERR END======
2022-03-28 13:18:08 [ForkJoinPool-1-worker-11] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@34093228=[], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[topic-st], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[], io.strimzi.test.logs.CollectorElement@b850ef2a=[infra-namespace, reconciliation-st], io.strimzi.test.logs.CollectorElement@f4405b3b=[], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 13:18:08 [ForkJoinPool-1-worker-11] DEBUG [SuiteThreadController:254] ThrottlingQuotaST - Notifies waiting test suites:[UserST, TopicST, ReconciliationST, CruiseControlConfigurationST, HttpBridgeScramShaST, ThrottlingQuotaST] to and randomly select one to start execution
2022-03-28 13:18:08 [ForkJoinPool-1-worker-11] DEBUG [SuiteThreadController:85] [operators.topic.ThrottlingQuotaST] - Removing parallel suite: ThrottlingQuotaST
2022-03-28 13:18:08 [ForkJoinPool-1-worker-11] DEBUG [SuiteThreadController:89] [operators.topic.ThrottlingQuotaST] - Parallel suites count: 1
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1,938.537 s - in io.strimzi.systemtest.operators.topic.ThrottlingQuotaST
2022-03-28 13:18:16 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Deployment:my-cluster-8c17a5fe-isolated-kafka-clients not ready, will try again in 10000 ms (458457ms till timeout)
2022-03-28 13:18:26 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:369] ############################################################################
2022-03-28 13:18:26 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.topic.TopicST.testDeleteTopicEnableFalse-FINISHED
2022-03-28 13:18:26 [ForkJoinPool-1-worker-5] INFO  [TestSeparator:30] ############################################################################
2022-03-28 13:18:26 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:689] ============================================================================
2022-03-28 13:18:26 [ForkJoinPool-1-worker-5] DEBUG [AbstractST:690] [operators.topic.TopicST - After All] - Clean up after test suite
2022-03-28 13:18:26 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:344] ############################################################################
2022-03-28 13:18:26 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:348] Delete all resources for TopicST
2022-03-28 13:18:26 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of Kafka topic-cluster-name in namespace topic-st
2022-03-28 13:18:26 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:topic-cluster-name
2022-03-28 13:18:26 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource Kafka:topic-cluster-name not ready, will try again in 10000 ms (839886ms till timeout)
2022-03-28 13:18:37 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:369] ############################################################################
2022-03-28 13:18:37 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Namespace topic-st removal
2022-03-28 13:18:37 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 13:18:37 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 13:18:37 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 13:18:37 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (479470ms till timeout)
2022-03-28 13:18:38 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 13:18:44 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 13:18:44 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 13:18:44 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (472990ms till timeout)
2022-03-28 13:18:45 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 13:18:45 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 13:18:45 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 13:18:45 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (471408ms till timeout)
2022-03-28 13:18:46 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 13:18:47 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 13:18:47 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 13:18:47 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (469445ms till timeout)
2022-03-28 13:18:48 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 13:18:49 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 13:18:49 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 0
2022-03-28 13:18:49 [ForkJoinPool-1-worker-5] TRACE [TestUtils:176] Namespace topic-st removal not ready, will try again in 1000 ms (467899ms till timeout)
2022-03-28 13:18:50 [ForkJoinPool-1-worker-5] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 13:18:50 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-9 get Namespace topic-st -o yaml
2022-03-28 13:18:50 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Return code: 1
2022-03-28 13:18:50 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 13:18:50 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] Error from server (NotFound): namespaces "topic-st" not found
2022-03-28 13:18:50 [ForkJoinPool-1-worker-5] DEBUG [Exec:419] ======STDERR END======
2022-03-28 13:18:50 [ForkJoinPool-1-worker-5] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@89ada1ec=[], io.strimzi.test.logs.CollectorElement@34093228=[], io.strimzi.test.logs.CollectorElement@dcc1a6e7=[], io.strimzi.test.logs.CollectorElement@c8c27cca=[], io.strimzi.test.logs.CollectorElement@c45aa4b1=[], io.strimzi.test.logs.CollectorElement@c4c0ea0=[], io.strimzi.test.logs.CollectorElement@aec142ef=[], io.strimzi.test.logs.CollectorElement@24b97ba9=[], io.strimzi.test.logs.CollectorElement@f851b6c3=[], io.strimzi.test.logs.CollectorElement@3881d5f2=[], io.strimzi.test.logs.CollectorElement@e3067dd1=[], io.strimzi.test.logs.CollectorElement@d2d70f60=[], io.strimzi.test.logs.CollectorElement@5c7379cb=[], io.strimzi.test.logs.CollectorElement@62b9e483=[], io.strimzi.test.logs.CollectorElement@b850ef2a=[infra-namespace, reconciliation-st], io.strimzi.test.logs.CollectorElement@f4405b3b=[], io.strimzi.test.logs.CollectorElement@3e255cd9=[]}
2022-03-28 13:18:50 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:254] TopicST - Notifies waiting test suites:[UserST, TopicST, ReconciliationST, CruiseControlConfigurationST, HttpBridgeScramShaST, ThrottlingQuotaST] to and randomly select one to start execution
2022-03-28 13:18:50 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:85] [operators.topic.TopicST] - Removing parallel suite: TopicST
2022-03-28 13:18:50 [ForkJoinPool-1-worker-5] DEBUG [SuiteThreadController:89] [operators.topic.TopicST] - Parallel suites count: 0
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1,980.94 s - in io.strimzi.systemtest.operators.topic.TopicST
2022-03-28 13:18:50 [ForkJoinPool-1-worker-3] INFO  [SetupClusterOperator:618] ============================================================================
2022-03-28 13:18:50 [ForkJoinPool-1-worker-3] INFO  [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-03-28 13:18:50 [ForkJoinPool-1-worker-3] INFO  [SetupClusterOperator:620] ============================================================================
2022-03-28 13:18:50 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:344] ############################################################################
2022-03-28 13:18:50 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-03-28 13:18:50 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-28 13:18:50 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-28 13:18:50 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-28 13:18:50 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-28 13:18:50 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-28 13:18:50 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-28 13:18:50 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-28 13:18:51 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-28 13:18:51 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-28 13:18:51 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-28 13:18:51 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-28 13:18:51 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-28 13:18:51 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-28 13:18:51 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-28 13:18:51 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-28 13:18:51 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-28 13:18:51 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-28 13:18:51 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-28 13:18:51 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-28 13:18:51 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-28 13:18:51 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-28 13:18:51 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-28 13:18:51 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-28 13:18:51 [ForkJoinPool-1-worker-11] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-28 13:18:51 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-28 13:18:51 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-28 13:18:51 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-28 13:18:52 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-28 13:18:52 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-28 13:18:52 [ForkJoinPool-1-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-28 13:18:52 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-28 13:18:52 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-28 13:18:52 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-28 13:18:52 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-28 13:18:52 [ForkJoinPool-1-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-28 13:18:52 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-28 13:18:52 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-28 13:18:52 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-28 13:18:52 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-28 13:18:53 [ForkJoinPool-1-worker-11] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io not ready, will try again in 10000 ms (179244ms till timeout)
2022-03-28 13:18:53 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-28 13:18:53 [ForkJoinPool-1-worker-1] INFO  [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-28 13:18:53 [ForkJoinPool-1-worker-13] INFO  [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-28 13:18:53 [ForkJoinPool-1-worker-9] INFO  [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-28 13:18:53 [ForkJoinPool-1-worker-1] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-28 13:18:53 [ForkJoinPool-1-worker-15] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-28 13:18:53 [ForkJoinPool-1-worker-5] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-28 13:18:53 [ForkJoinPool-1-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-28 13:18:53 [ForkJoinPool-1-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-28 13:18:53 [ForkJoinPool-1-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-28 13:18:53 [ForkJoinPool-1-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-28 13:19:03 [ForkJoinPool-1-worker-3] INFO  [ResourceManager:369] ############################################################################
2022-03-28 13:19:03 [ForkJoinPool-1-worker-3] DEBUG [KubeClusterResource:216] Deleting Namespace: infra-namespace
2022-03-28 13:19:03 [ForkJoinPool-1-worker-3] DEBUG [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-28 13:19:03 [ForkJoinPool-1-worker-3] DEBUG [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-28 13:19:03 [ForkJoinPool-1-worker-3] DEBUG [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v70475
2022-03-28 13:19:03 [ForkJoinPool-1-worker-3] DEBUG [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v70475
2022-03-28 13:19:03 [ForkJoinPool-1-worker-3] DEBUG [AbstractWatchManager:222] Watching https://api.morsak-410.strimzi.app-services-dev.net:6443/api/v1/namespaces?fieldSelector=metadata.name%3Dinfra-namespace&resourceVersion=70475&allowWatchBookmarks=true&watch=true...
2022-03-28 13:19:09 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-28 13:19:09 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [Reflector:139] Event received MODIFIED Namespace resourceVersion 70544
2022-03-28 13:19:14 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [Reflector:139] Event received MODIFIED Namespace resourceVersion 70581
2022-03-28 13:19:15 [ForkJoinPool-1-worker-3] DEBUG [KubeClusterResource:216] Deleting Namespace: reconciliation-st
2022-03-28 13:19:15 [ForkJoinPool-1-worker-11] DEBUG [KubeClusterResource:216] Deleting Namespace: infra-namespace
2022-03-28 13:19:15 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v70544 in namespace default
2022-03-28 13:19:15 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@6b4f77ea
2022-03-28 13:19:15 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [Reflector:181] Watch gracefully closed
2022-03-28 13:19:15 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@170760be
2022-03-28 13:19:15 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@170760be
2022-03-28 13:19:15 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@170760be
2022-03-28 13:19:15 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [Reflector:139] Event received DELETED Namespace resourceVersion 70582
2022-03-28 13:19:15 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-28 13:19:15 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [AbstractWatchManager:140] Ignoring error for already closed/closing connection
2022-03-28 13:19:15 [ForkJoinPool-1-worker-11] DEBUG [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-28 13:19:15 [ForkJoinPool-1-worker-11] DEBUG [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-28 13:19:15 [ForkJoinPool-1-worker-3] DEBUG [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-28 13:19:15 [ForkJoinPool-1-worker-3] DEBUG [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-28 13:19:15 [ForkJoinPool-1-worker-11] DEBUG [Reflector:95] Listing items (0) for resource class io.fabric8.kubernetes.api.model.Namespace v70585
2022-03-28 13:19:15 [ForkJoinPool-1-worker-3] DEBUG [Reflector:95] Listing items (0) for resource class io.fabric8.kubernetes.api.model.Namespace v70586
2022-03-28 13:19:15 [main] INFO  [TestExecutionListener:40] =======================================================================
2022-03-28 13:19:15 [main] INFO  [TestExecutionListener:41] =======================================================================
2022-03-28 13:19:15 [main] INFO  [TestExecutionListener:42]                         Test run finished
2022-03-28 13:19:15 [main] INFO  [TestExecutionListener:43] =======================================================================
2022-03-28 13:19:15 [main] INFO  [TestExecutionListener:44] =======================================================================
2022-03-28 13:19:15 [main] INFO  [TestExecutionListener:29] =======================================================================
2022-03-28 13:19:15 [main] INFO  [TestExecutionListener:30] =======================================================================
2022-03-28 13:19:15 [main] INFO  [TestExecutionListener:31]                         Test run started
2022-03-28 13:19:15 [main] INFO  [TestExecutionListener:32] =======================================================================
2022-03-28 13:19:15 [main] INFO  [TestExecutionListener:33] =======================================================================
2022-03-28 13:19:15 [main] INFO  [TestExecutionListener:48] Following testclasses are selected for run:
2022-03-28 13:19:15 [main] INFO  [TestExecutionListener:51] -> io.strimzi.systemtest.operators.ReconciliationST
2022-03-28 13:19:15 [main] INFO  [TestExecutionListener:52] =======================================================================
2022-03-28 13:19:15 [main] INFO  [TestExecutionListener:53] =======================================================================
[INFO] Running io.strimzi.systemtest.operators.ReconciliationST
2022-03-28 13:19:15 [ForkJoinPool-2-worker-3] DEBUG [BeforeAllOnce:51] ============================================================================
2022-03-28 13:19:15 [ForkJoinPool-2-worker-3] DEBUG [BeforeAllOnce:52] [io.strimzi.systemtest.operators.ReconciliationST - Before Suite] - Setup Suite environment
2022-03-28 13:19:15 [ForkJoinPool-2-worker-3] INFO  [SetupClusterOperator:197] Cluster operator installation configuration:
extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@5abe87ac
clusterOperatorName=strimzi-cluster-operator
namespaceInstallTo=infra-namespace
namespaceToWatch=*
bindingsNamespaces=[infra-namespace]
operationTimeout=300000
reconciliationInterval=30000
clusterOperatorRBACType=CLUSTER
2022-03-28 13:19:15 [ForkJoinPool-2-worker-3] DEBUG [SetupClusterOperator:198] Cluster operator installation configuration:
SetupClusterOperator{cluster=io.strimzi.test.k8s.KubeClusterResource@12a2b997, extensionContext=org.junit.jupiter.engine.descriptor.JupiterEngineExtensionContext@5abe87ac, clusterOperatorName='strimzi-cluster-operator', namespaceInstallTo='infra-namespace', namespaceToWatch='*', bindingsNamespaces=[infra-namespace], operationTimeout=300000, reconciliationInterval=30000, extraEnvVars=[], extraLabels={}, clusterOperatorRBACType=CLUSTER, testClassName='null', testMethodName='null'}
2022-03-28 13:19:15 [ForkJoinPool-2-worker-3] INFO  [SetupClusterOperator:248] Install ClusterOperator via Yaml bundle
2022-03-28 13:19:15 [ForkJoinPool-2-worker-3] INFO  [KubeClusterResource:156] Creating Namespace: infra-namespace
2022-03-28 13:19:15 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Namespace infra-namespace
2022-03-28 13:19:15 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-9 get Namespace infra-namespace -o json
2022-03-28 13:19:16 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-9 get Namespace infra-namespace -o json
2022-03-28 13:19:16 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:19:16 [ForkJoinPool-2-worker-3] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c29,c9",
            "openshift.io/sa.scc.supplemental-groups": "1000830000/10000",
            "openshift.io/sa.scc.uid-range": "1000830000/10000"
        },
        "creationTimestamp": "2022-03-28T13:19:11Z",
        "labels": {
            "kubernetes.io/metadata.name": "infra-namespace"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T13:19:11Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T13:19:11Z"
            }
        ],
        "name": "infra-namespace",
        "resourceVersion": "70599",
        "uid": "ec9db1b0-add6-423a-b6a1-e5f8842a8f09"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 13:19:16 [ForkJoinPool-2-worker-3] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace]}
2022-03-28 13:19:16 [ForkJoinPool-2-worker-3] INFO  [KubeClusterResource:82] Client use Namespace: infra-namespace
2022-03-28 13:19:16 [ForkJoinPool-2-worker-3] DEBUG [SetupClusterOperator:478] Installation resource type: ServiceAccount
2022-03-28 13:19:16 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-28 13:19:16 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-28 13:19:16 [ForkJoinPool-2-worker-3] DEBUG [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-28 13:19:16 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-28 13:19:16 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-28 13:19:16 [ForkJoinPool-2-worker-3] DEBUG [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-28 13:19:16 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-28 13:19:16 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-28 13:19:16 [ForkJoinPool-2-worker-3] DEBUG [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-28 13:19:16 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-28 13:19:17 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-28 13:19:17 [ForkJoinPool-2-worker-3] DEBUG [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-28 13:19:17 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-28 13:19:17 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-28 13:19:17 [ForkJoinPool-2-worker-3] DEBUG [SetupClusterOperator:478] Installation resource type: ClusterRole
2022-03-28 13:19:17 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-28 13:19:17 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-28 13:19:17 [ForkJoinPool-2-worker-3] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 13:19:17 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-28 13:19:18 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-28 13:19:18 [ForkJoinPool-2-worker-3] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 13:19:18 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-28 13:19:18 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-28 13:19:19 [ForkJoinPool-2-worker-3] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 13:19:19 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-28 13:19:19 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-28 13:19:19 [ForkJoinPool-2-worker-3] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 13:19:19 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-28 13:19:19 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-28 13:19:19 [ForkJoinPool-2-worker-3] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 13:19:19 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-28 13:19:20 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-28 13:19:20 [ForkJoinPool-2-worker-3] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 13:19:20 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-28 13:19:20 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-28 13:19:20 [ForkJoinPool-2-worker-3] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 13:19:20 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-28 13:19:20 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-28 13:19:21 [ForkJoinPool-2-worker-3] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 13:19:21 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-28 13:19:21 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-28 13:19:21 [ForkJoinPool-2-worker-3] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 13:19:21 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-28 13:19:21 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-28 13:19:21 [ForkJoinPool-2-worker-3] DEBUG [SetupClusterOperator:478] Installation resource type: Crd
2022-03-28 13:19:21 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-28 13:19:22 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-28 13:19:22 [ForkJoinPool-2-worker-3] DEBUG [SetupClusterOperator:478] Installation resource type: ConfigMap
2022-03-28 13:19:22 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-28 13:19:22 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-28 13:19:22 [ForkJoinPool-2-worker-3] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=infra-namespace, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 13:19:22 [ForkJoinPool-2-worker-3] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: infra-namespace
2022-03-28 13:19:22 [ForkJoinPool-2-worker-3] INFO  [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/cloud-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/021-ClusterRoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-28 13:19:22 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-28 13:19:22 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-28 13:19:22 [ForkJoinPool-2-worker-3] INFO  [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/cloud-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/030-ClusterRoleBinding-strimzi-cluster-operator-kafka-broker-delegation.yaml in namespace infra-namespace
2022-03-28 13:19:22 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-28 13:19:22 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-28 13:19:22 [ForkJoinPool-2-worker-3] INFO  [ClusterRoleBindingResource:48] Creating ClusterRoleBinding in test case JUnit Jupiter from /home/cloud-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/033-ClusterRoleBinding-strimzi-cluster-operator-kafka-client-delegation.yaml in namespace infra-namespace
2022-03-28 13:19:22 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-28 13:19:23 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-28 13:19:23 [ForkJoinPool-2-worker-3] INFO  [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/cloud-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/020-RoleBinding-strimzi-cluster-operator.yaml in namespace infra-namespace
2022-03-28 13:19:23 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-28 13:19:23 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-28 13:19:23 [ForkJoinPool-2-worker-3] INFO  [RoleBindingResource:43] Creating RoleBinding in test case JUnit Jupiter from /home/cloud-user/strimzi-kafka-operator/systemtest/../packaging/install/cluster-operator/031-RoleBinding-strimzi-cluster-operator-entity-operator-delegation.yaml in namespace infra-namespace
2022-03-28 13:19:23 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-28 13:19:23 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-28 13:19:23 [ForkJoinPool-2-worker-3] INFO  [ClusterRoleBindingTemplates:21] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-28 13:19:23 [ForkJoinPool-2-worker-3] INFO  [ClusterRoleBindingTemplates:26] Creating ClusterRoleBinding that grant cluster-wide access to all OpenShift projects
2022-03-28 13:19:23 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-28 13:19:23 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-28 13:19:23 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-28 13:19:24 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-28 13:19:24 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-28 13:19:24 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-28 13:19:24 [ForkJoinPool-2-worker-3] INFO  [DeploymentUtils:161] Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-28 13:19:24 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Wait for Deployment: strimzi-cluster-operator will be ready
2022-03-28 13:19:24 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (479891ms till timeout)
2022-03-28 13:19:25 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (478782ms till timeout)
2022-03-28 13:19:26 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (477665ms till timeout)
2022-03-28 13:19:27 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (476555ms till timeout)
2022-03-28 13:19:29 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (475443ms till timeout)
2022-03-28 13:19:30 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (474333ms till timeout)
2022-03-28 13:19:31 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (473222ms till timeout)
2022-03-28 13:19:32 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (472111ms till timeout)
2022-03-28 13:19:33 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (471000ms till timeout)
2022-03-28 13:19:34 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (469889ms till timeout)
2022-03-28 13:19:35 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (468778ms till timeout)
2022-03-28 13:19:36 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (467668ms till timeout)
2022-03-28 13:19:37 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (466559ms till timeout)
2022-03-28 13:19:39 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (465449ms till timeout)
2022-03-28 13:19:40 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (464338ms till timeout)
2022-03-28 13:19:41 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (463226ms till timeout)
2022-03-28 13:19:42 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (462116ms till timeout)
2022-03-28 13:19:43 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (461004ms till timeout)
2022-03-28 13:19:44 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (459894ms till timeout)
2022-03-28 13:19:45 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (458781ms till timeout)
2022-03-28 13:19:46 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (457669ms till timeout)
2022-03-28 13:19:47 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (456560ms till timeout)
2022-03-28 13:19:49 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (455449ms till timeout)
2022-03-28 13:19:50 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (454338ms till timeout)
2022-03-28 13:19:51 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (453228ms till timeout)
2022-03-28 13:19:52 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (452119ms till timeout)
2022-03-28 13:19:53 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Wait for Deployment: strimzi-cluster-operator will be ready not ready, will try again in 1000 ms (451007ms till timeout)
2022-03-28 13:19:54 [ForkJoinPool-2-worker-3] INFO  [DeploymentUtils:168] Deployment: strimzi-cluster-operator is ready
2022-03-28 13:19:54 [ForkJoinPool-2-worker-3] INFO  [DeploymentUtils:194] Waiting for 1 Pod(s) of Deployment strimzi-cluster-operator to be ready
2022-03-28 13:19:54 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready
2022-03-28 13:19:54 [ForkJoinPool-2-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-qrzvs not ready: strimzi-cluster-operator)
2022-03-28 13:19:54 [ForkJoinPool-2-worker-3] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-qrzvs are ready
2022-03-28 13:19:54 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (599892ms till timeout)
2022-03-28 13:19:55 [ForkJoinPool-2-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-qrzvs not ready: strimzi-cluster-operator)
2022-03-28 13:19:55 [ForkJoinPool-2-worker-3] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-qrzvs are ready
2022-03-28 13:19:55 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (598783ms till timeout)
2022-03-28 13:19:57 [ForkJoinPool-2-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-qrzvs not ready: strimzi-cluster-operator)
2022-03-28 13:19:57 [ForkJoinPool-2-worker-3] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-qrzvs are ready
2022-03-28 13:19:57 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (597671ms till timeout)
2022-03-28 13:19:58 [ForkJoinPool-2-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-qrzvs not ready: strimzi-cluster-operator)
2022-03-28 13:19:58 [ForkJoinPool-2-worker-3] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-qrzvs are ready
2022-03-28 13:19:58 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (596560ms till timeout)
2022-03-28 13:19:59 [ForkJoinPool-2-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-qrzvs not ready: strimzi-cluster-operator)
2022-03-28 13:19:59 [ForkJoinPool-2-worker-3] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-qrzvs are ready
2022-03-28 13:19:59 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (595448ms till timeout)
2022-03-28 13:20:00 [ForkJoinPool-2-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-qrzvs not ready: strimzi-cluster-operator)
2022-03-28 13:20:00 [ForkJoinPool-2-worker-3] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-qrzvs are ready
2022-03-28 13:20:00 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (594337ms till timeout)
2022-03-28 13:20:01 [ForkJoinPool-2-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-qrzvs not ready: strimzi-cluster-operator)
2022-03-28 13:20:01 [ForkJoinPool-2-worker-3] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-qrzvs are ready
2022-03-28 13:20:01 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (593224ms till timeout)
2022-03-28 13:20:02 [ForkJoinPool-2-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-qrzvs not ready: strimzi-cluster-operator)
2022-03-28 13:20:02 [ForkJoinPool-2-worker-3] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-qrzvs are ready
2022-03-28 13:20:02 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (592113ms till timeout)
2022-03-28 13:20:03 [ForkJoinPool-2-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-qrzvs not ready: strimzi-cluster-operator)
2022-03-28 13:20:03 [ForkJoinPool-2-worker-3] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-qrzvs are ready
2022-03-28 13:20:03 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (591001ms till timeout)
2022-03-28 13:20:04 [ForkJoinPool-2-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-qrzvs not ready: strimzi-cluster-operator)
2022-03-28 13:20:04 [ForkJoinPool-2-worker-3] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-qrzvs are ready
2022-03-28 13:20:04 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] All pods matching LabelSelector(matchExpressions=[], matchLabels={name=strimzi-cluster-operator}, additionalProperties={})to be ready not ready, will try again in 1000 ms (589890ms till timeout)
2022-03-28 13:20:05 [ForkJoinPool-2-worker-3] DEBUG [PodUtils:98] Not ready (at least 1 container of pod strimzi-cluster-operator-5dc4cd9447-qrzvs not ready: strimzi-cluster-operator)
2022-03-28 13:20:05 [ForkJoinPool-2-worker-3] DEBUG [PodUtils:106] Pods strimzi-cluster-operator-5dc4cd9447-qrzvs are ready
2022-03-28 13:20:05 [ForkJoinPool-2-worker-3] INFO  [DeploymentUtils:197] Deployment strimzi-cluster-operator is ready
2022-03-28 13:20:05 [ForkJoinPool-2-worker-3] DEBUG [AbstractST:666] ============================================================================
2022-03-28 13:20:05 [ForkJoinPool-2-worker-3] DEBUG [AbstractST:667] [operators.ReconciliationST - Before All] - Setup test suite environment
2022-03-28 13:20:05 [ForkJoinPool-2-worker-3] DEBUG [SuiteThreadController:69] [operators.ReconciliationST] - Adding parallel suite: ReconciliationST
2022-03-28 13:20:05 [ForkJoinPool-2-worker-3] DEBUG [SuiteThreadController:73] [operators.ReconciliationST] - Parallel suites count: 1
2022-03-28 13:20:05 [ForkJoinPool-2-worker-3] DEBUG [SuiteThreadController:184] ReconciliationST suite now can proceed its execution
2022-03-28 13:20:05 [ForkJoinPool-2-worker-3] DEBUG [TestSuiteNamespaceManager:128] Content of the test suite namespaces map:
{HttpBridgeTlsST=[http-bridge-tls-st], CruiseControlST=[cruise-control-st], AbstractST=[abstract-st], OpaIntegrationST=[opa-integration-st], CruiseControlApiST=[cruise-control-api-st], HttpBridgeScramShaST=[http-bridge-scram-sha-st], ReconciliationST=[reconciliation-st], DynamicConfST=[dynamic-conf-st], AbstractUpgradeST=[abstract-upgrade-st], QuotasST=[quotas-st], DynamicConfSharedST=[dynamic-conf-shared-st], RollingUpdateST=[rolling-update-st], OlmAbstractST=[olm-abstract-st], CustomAuthorizerST=[custom-authorizer-st], AlternativeReconcileTriggersST=[alternative-reconcile-triggers-st], LogSettingST=[log-setting-st], ListenersST=[listeners-st], UserST=[user-st], TopicST=[topic-st], SecurityST=[security-st], TracingST=[tracing-st], AbstractNamespaceST=[abstract-namespace-st], CruiseControlConfigurationST=[cruise-control-configuration-st], HttpBridgeCorsST=[http-bridge-cors-st], ConfigProviderST=[config-provider-st], KafkaST=[kafka-st], LoggingChangeST=[logging-change-st], HttpBridgeKafkaExternalListenersST=[http-bridge-kafka-external-listeners-st], OauthAbstractST=[oauth-abstract-st], ThrottlingQuotaST=[throttling-quota-st], MultipleListenersST=[multiple-listeners-st]}
2022-03-28 13:20:05 [ForkJoinPool-2-worker-3] DEBUG [TestSuiteNamespaceManager:129] Test suite `ReconciliationST` creates these additional namespaces:[reconciliation-st]
2022-03-28 13:20:06 [ForkJoinPool-2-worker-3] INFO  [KubeClusterResource:156] Creating Namespace: reconciliation-st
2022-03-28 13:20:06 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Namespace reconciliation-st
2022-03-28 13:20:06 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace infra-namespace get Namespace reconciliation-st -o json
2022-03-28 13:20:06 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace infra-namespace get Namespace reconciliation-st -o json
2022-03-28 13:20:06 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:20:06 [ForkJoinPool-2-worker-3] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c29,c14",
            "openshift.io/sa.scc.supplemental-groups": "1000840000/10000",
            "openshift.io/sa.scc.uid-range": "1000840000/10000"
        },
        "creationTimestamp": "2022-03-28T13:20:01Z",
        "labels": {
            "kubernetes.io/metadata.name": "reconciliation-st"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T13:20:01Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T13:20:01Z"
            }
        ],
        "name": "reconciliation-st",
        "resourceVersion": "71001",
        "uid": "d0525223-d758-4969-a066-3496f821b97d"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 13:20:06 [ForkJoinPool-2-worker-3] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st]}
2022-03-28 13:20:06 [ForkJoinPool-2-worker-3] INFO  [KubeClusterResource:82] Client use Namespace: reconciliation-st
2022-03-28 13:20:06 [ForkJoinPool-2-worker-3] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=reconciliation-st, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 13:20:06 [ForkJoinPool-2-worker-3] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: reconciliation-st
2022-03-28 13:20:06 [ForkJoinPool-2-worker-3] INFO  [TestSeparator:23] ############################################################################
2022-03-28 13:20:06 [ForkJoinPool-2-worker-3] INFO  [TestSeparator:24] io.strimzi.systemtest.operators.ReconciliationST.testPauseReconciliationInKafkaRebalanceAndTopic-STARTED
2022-03-28 13:20:06 [ForkJoinPool-2-worker-3] DEBUG [AbstractST:658] ============================================================================
2022-03-28 13:20:06 [ForkJoinPool-2-worker-3] DEBUG [AbstractST:659] [operators.ReconciliationST - Before Each] - Setup test case environment
2022-03-28 13:20:06 [ForkJoinPool-2-worker-3] DEBUG [SuiteThreadController:77] [operators.ReconciliationST] - Adding parallel test: testPauseReconciliationInKafkaRebalanceAndTopic
2022-03-28 13:20:06 [ForkJoinPool-2-worker-3] DEBUG [SuiteThreadController:81] [operators.ReconciliationST] - Parallel test count: 1
2022-03-28 13:20:06 [ForkJoinPool-2-worker-3] DEBUG [SuiteThreadController:230] testPauseReconciliationInKafkaRebalanceAndTopic test now can proceed its execution
2022-03-28 13:20:06 [ForkJoinPool-2-worker-3] INFO  [AbstractST:597] Not first test we are gonna generate cluster name
2022-03-28 13:20:06 [ForkJoinPool-2-worker-3] TRACE [AbstractST:606] CLUSTER_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-455915b9, testThrottlingQuotasCreateAlterPartitions=my-cluster-d4dad0de, testSendSimpleMessageTls=my-cluster-cfce2f5b, testConfigurationReflection=my-cluster-aa0ecd2f, testUserWithNameMoreThan64Chars=my-cluster-ed39a269, testDeployAndUnDeployCruiseControl=my-cluster-534609d0, testCapacityFile=my-cluster-39ee3f62, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-0b062573, testTlsExternalUser=my-cluster-965e9d1e, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-31feb41b, testSendSimpleMessageTlsScramSha=my-cluster-27773859, testCreateTopicAfterUnsupportedOperation=my-cluster-2938df05, testScramUserWithQuotas=my-cluster-90c82d82, testTopicModificationOfReplicationFactor=my-cluster-421b59dd, testKafkaAdminTopicOperations=my-cluster-6acb723c, testConfigurationFileIsCreated=my-cluster-020ceaf8, testCreateTopicViaKafka=my-cluster-43bd601f, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5, testUpdateUser=my-cluster-e16ccf16, testThrottlingQuotasDeleteTopic=my-cluster-b6bd312e, testDeleteTopicEnableFalse=my-cluster-8c17a5fe, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-c249d04b, testTlsExternalUserWithQuotas=my-cluster-3e2e11de, testTlsUserWithQuotas=my-cluster-3b191485, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2, testSendingMessagesToNonExistingTopic=my-cluster-64488b2e, testConfigurationPerformanceOptions=my-cluster-bc0ed00a, testReceiveSimpleMessageTls=my-cluster-362cd511, testUserTemplate=my-cluster-157df94e, testMoreReplicasThanAvailableBrokers=my-cluster-1f5cbe38}
2022-03-28 13:20:06 [ForkJoinPool-2-worker-3] TRACE [AbstractST:607] USERS_NAME_MAP: {testReceiveSimpleMessageTlsScramSha=my-user-1329598231-1183867861, testThrottlingQuotasCreateAlterPartitions=my-user-1809876063-64793399, testSendSimpleMessageTls=my-user-111586425-1868899021, testConfigurationReflection=my-user-1580949953-976509997, testUserWithNameMoreThan64Chars=my-user-1048816470-1103470031, testDeployAndUnDeployCruiseControl=my-user-475346616-1293712526, testCapacityFile=my-user-597365340-661017603, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-user-1153908156-1369197188, testTlsExternalUser=my-user-1295334081-1602801897, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-user-2100837352-1014792902, testSendSimpleMessageTlsScramSha=my-user-507582460-1003875464, testCreateTopicAfterUnsupportedOperation=my-user-271926232-1154944814, testScramUserWithQuotas=my-user-1857980152-2145707698, testTopicModificationOfReplicationFactor=my-user-380015619-613911473, testKafkaAdminTopicOperations=my-user-2014252818-589270412, testConfigurationFileIsCreated=my-user-1609977058-1196012647, testCreateTopicViaKafka=my-user-1379798639-1428271857, testCreatingUsersWithSecretPrefix=my-user-1928899390-1444689439, testUpdateUser=my-user-1694542041-806176635, testThrottlingQuotasDeleteTopic=my-user-284582998-729631500, testDeleteTopicEnableFalse=my-user-1491155200-894698476, testPauseReconciliationInKafkaRebalanceAndTopic=my-user-1717427059-821941866, testTlsExternalUserWithQuotas=my-user-1786174319-1274634874, testTlsUserWithQuotas=my-user-420001054-1296284129, testThrottlingQuotasCreateTopic=my-user-896662438-260898887, testSendingMessagesToNonExistingTopic=my-user-248322508-1660982370, testConfigurationPerformanceOptions=my-user-722474509-712960391, testReceiveSimpleMessageTls=my-user-2012197250-2103354173, testUserTemplate=my-user-1253881534-190261188, testMoreReplicasThanAvailableBrokers=my-user-1381751556-1357722017}
2022-03-28 13:20:06 [ForkJoinPool-2-worker-3] TRACE [AbstractST:608] TOPIC_NAMES_MAP: {testReceiveSimpleMessageTlsScramSha=my-topic-170116874-1830928683, testThrottlingQuotasCreateAlterPartitions=my-topic-1581297458-635252970, testSendSimpleMessageTls=my-topic-100192180-936335618, testConfigurationReflection=my-topic-1437230713-1827996725, testUserWithNameMoreThan64Chars=my-topic-1603054043-1686316417, testDeployAndUnDeployCruiseControl=my-topic-1815491150-540070233, testCapacityFile=my-topic-1390944843-775067696, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-topic-613737897-291522718, testTlsExternalUser=my-topic-476529121-306489993, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-topic-1084162749-1576061083, testSendSimpleMessageTlsScramSha=my-topic-62182003-1963040205, testCreateTopicAfterUnsupportedOperation=my-topic-1502518950-974162326, testScramUserWithQuotas=my-topic-315112689-1043284277, testTopicModificationOfReplicationFactor=my-topic-1016572292-592627742, testKafkaAdminTopicOperations=my-topic-1915664839-460245273, testConfigurationFileIsCreated=my-topic-155202447-1921897899, testCreateTopicViaKafka=my-topic-664105056-981624555, testCreatingUsersWithSecretPrefix=my-topic-2033928996-1209892355, testUpdateUser=my-topic-1239518257-2056139809, testThrottlingQuotasDeleteTopic=my-topic-1036812834-1980126190, testDeleteTopicEnableFalse=my-topic-1837190581-1217032035, testPauseReconciliationInKafkaRebalanceAndTopic=my-topic-352474572-722982643, testTlsExternalUserWithQuotas=my-topic-593679216-1309035072, testTlsUserWithQuotas=my-topic-1752299353-1315959033, testThrottlingQuotasCreateTopic=my-topic-1532506723-1472611013, testSendingMessagesToNonExistingTopic=my-topic-1513508632-1348735863, testConfigurationPerformanceOptions=my-topic-1891296149-770116276, testReceiveSimpleMessageTls=my-topic-1264858218-642520562, testUserTemplate=my-topic-515111334-943904347, testMoreReplicasThanAvailableBrokers=my-topic-2107805499-2107307630}
2022-03-28 13:20:06 [ForkJoinPool-2-worker-3] TRACE [AbstractST:609] THIS IS CLIENTS MAP: {testReceiveSimpleMessageTlsScramSha=my-cluster-455915b9-kafka-clients, testThrottlingQuotasCreateAlterPartitions=my-cluster-d4dad0de-kafka-clients, testSendSimpleMessageTls=my-cluster-cfce2f5b-kafka-clients, testConfigurationReflection=my-cluster-aa0ecd2f-kafka-clients, testUserWithNameMoreThan64Chars=my-cluster-ed39a269-kafka-clients, testDeployAndUnDeployCruiseControl=my-cluster-534609d0-kafka-clients, testCapacityFile=my-cluster-39ee3f62-kafka-clients, testPauseReconciliationInKafkaAndKafkaConnectWithConnector=my-cluster-0b062573-kafka-clients, testTlsExternalUser=my-cluster-965e9d1e-kafka-clients, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods=my-cluster-31feb41b-kafka-clients, testSendSimpleMessageTlsScramSha=my-cluster-27773859-kafka-clients, testCreateTopicAfterUnsupportedOperation=my-cluster-2938df05-kafka-clients, testScramUserWithQuotas=my-cluster-90c82d82-kafka-clients, testTopicModificationOfReplicationFactor=my-cluster-421b59dd-kafka-clients, testKafkaAdminTopicOperations=my-cluster-6acb723c-kafka-clients, testConfigurationFileIsCreated=my-cluster-020ceaf8-kafka-clients, testCreateTopicViaKafka=my-cluster-43bd601f-kafka-clients, testCreatingUsersWithSecretPrefix=my-cluster-c525f5e5-kafka-clients, testUpdateUser=my-cluster-e16ccf16-kafka-clients, testThrottlingQuotasDeleteTopic=my-cluster-b6bd312e-kafka-clients, testDeleteTopicEnableFalse=my-cluster-8c17a5fe-kafka-clients, testPauseReconciliationInKafkaRebalanceAndTopic=my-cluster-c249d04b-kafka-clients, testTlsExternalUserWithQuotas=my-cluster-3e2e11de-kafka-clients, testTlsUserWithQuotas=my-cluster-3b191485-kafka-clients, testThrottlingQuotasCreateTopic=my-cluster-0690c7b2-kafka-clients, testSendingMessagesToNonExistingTopic=my-cluster-64488b2e-kafka-clients, testConfigurationPerformanceOptions=my-cluster-bc0ed00a-kafka-clients, testReceiveSimpleMessageTls=my-cluster-362cd511-kafka-clients, testUserTemplate=my-cluster-157df94e-kafka-clients, testMoreReplicasThanAvailableBrokers=my-cluster-1f5cbe38-kafka-clients}
2022-03-28 13:20:06 [ForkJoinPool-2-worker-3] INFO  [TestSuiteNamespaceManager:163] Creating namespace:namespace-10 for test case:testPauseReconciliationInKafkaRebalanceAndTopic
2022-03-28 13:20:06 [ForkJoinPool-2-worker-3] INFO  [KubeClusterResource:156] Creating Namespace: namespace-10
2022-03-28 13:20:06 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Namespace namespace-10
2022-03-28 13:20:06 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace reconciliation-st get Namespace namespace-10 -o json
2022-03-28 13:20:07 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace reconciliation-st get Namespace namespace-10 -o json
2022-03-28 13:20:07 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:20:07 [ForkJoinPool-2-worker-3] TRACE [BaseCmdKubeClient:353] {
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "annotations": {
            "openshift.io/sa.scc.mcs": "s0:c29,c19",
            "openshift.io/sa.scc.supplemental-groups": "1000850000/10000",
            "openshift.io/sa.scc.uid-range": "1000850000/10000"
        },
        "creationTimestamp": "2022-03-28T13:20:02Z",
        "labels": {
            "kubernetes.io/metadata.name": "namespace-10"
        },
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:annotations": {
                            ".": {},
                            "f:openshift.io/sa.scc.mcs": {},
                            "f:openshift.io/sa.scc.supplemental-groups": {},
                            "f:openshift.io/sa.scc.uid-range": {}
                        }
                    }
                },
                "manager": "cluster-policy-controller",
                "operation": "Update",
                "time": "2022-03-28T13:20:02Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:metadata": {
                        "f:labels": {
                            ".": {},
                            "f:kubernetes.io/metadata.name": {}
                        }
                    }
                },
                "manager": "okhttp",
                "operation": "Update",
                "time": "2022-03-28T13:20:02Z"
            }
        ],
        "name": "namespace-10",
        "resourceVersion": "71044",
        "uid": "2354df66-9c0b-404d-8238-06c1cfe8c0e5"
    },
    "spec": {
        "finalizers": [
            "kubernetes"
        ]
    },
    "status": {
        "phase": "Active"
    }
}

2022-03-28 13:20:07 [ForkJoinPool-2-worker-3] TRACE [KubeClusterResource:389] SUITE_NAMESPACE_MAP: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[namespace-10]}
2022-03-28 13:20:07 [ForkJoinPool-2-worker-3] INFO  [KubeClusterResource:82] Client use Namespace: namespace-10
2022-03-28 13:20:07 [ForkJoinPool-2-worker-3] DEBUG [NetworkPolicyTemplates:62] Creating NetworkPolicy: NetworkPolicy(apiVersion=networking.k8s.io/v1, kind=NetworkPolicy, metadata=ObjectMeta(annotations=null, clusterName=null, creationTimestamp=null, deletionGracePeriodSeconds=null, deletionTimestamp=null, finalizers=[], generateName=null, generation=null, labels=null, managedFields=[], name=global-network-policy, namespace=namespace-10, ownerReferences=[], resourceVersion=null, selfLink=null, uid=null, additionalProperties={}), spec=NetworkPolicySpec(egress=[], ingress=[], podSelector=LabelSelector(matchExpressions=[], matchLabels=null, additionalProperties={}), policyTypes=[Ingress], additionalProperties={}), additionalProperties={})
2022-03-28 13:20:07 [ForkJoinPool-2-worker-3] INFO  [NetworkPolicyResource:239] NetworkPolicy successfully set to: true for namespace: namespace-10
2022-03-28 13:20:07 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update Kafka my-cluster-c249d04b in namespace namespace-10
2022-03-28 13:20:07 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:164] Using Namespace: namespace-10
2022-03-28 13:20:07 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource Kafka:my-cluster-c249d04b
2022-03-28 13:20:07 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:433] Wait for Kafka: my-cluster-c249d04b will have desired state: Ready
2022-03-28 13:20:07 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Kafka: my-cluster-c249d04b will have desired state: Ready
2022-03-28 13:20:07 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1319892ms till timeout)
2022-03-28 13:20:08 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1318781ms till timeout)
2022-03-28 13:20:10 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1317669ms till timeout)
2022-03-28 13:20:11 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1316555ms till timeout)
2022-03-28 13:20:12 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1315441ms till timeout)
2022-03-28 13:20:13 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1314330ms till timeout)
2022-03-28 13:20:14 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1313217ms till timeout)
2022-03-28 13:20:15 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1312105ms till timeout)
2022-03-28 13:20:16 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1310994ms till timeout)
2022-03-28 13:20:17 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1309884ms till timeout)
2022-03-28 13:20:18 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1308772ms till timeout)
2022-03-28 13:20:20 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1307659ms till timeout)
2022-03-28 13:20:21 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1306550ms till timeout)
2022-03-28 13:20:22 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1305439ms till timeout)
2022-03-28 13:20:23 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1304326ms till timeout)
2022-03-28 13:20:24 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1303213ms till timeout)
2022-03-28 13:20:25 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1302105ms till timeout)
2022-03-28 13:20:26 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1300994ms till timeout)
2022-03-28 13:20:27 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1299884ms till timeout)
2022-03-28 13:20:28 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1298772ms till timeout)
2022-03-28 13:20:30 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1297662ms till timeout)
2022-03-28 13:20:31 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1296551ms till timeout)
2022-03-28 13:20:32 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1295439ms till timeout)
2022-03-28 13:20:33 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1293901ms till timeout)
2022-03-28 13:20:34 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1292787ms till timeout)
2022-03-28 13:20:36 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1291677ms till timeout)
2022-03-28 13:20:37 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1290564ms till timeout)
2022-03-28 13:20:38 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1289454ms till timeout)
2022-03-28 13:20:39 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1288342ms till timeout)
2022-03-28 13:20:40 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1287231ms till timeout)
2022-03-28 13:20:41 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1286119ms till timeout)
2022-03-28 13:20:42 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1285007ms till timeout)
2022-03-28 13:20:43 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1283896ms till timeout)
2022-03-28 13:20:44 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1282785ms till timeout)
2022-03-28 13:20:46 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1281673ms till timeout)
2022-03-28 13:20:47 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1280561ms till timeout)
2022-03-28 13:20:48 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1279451ms till timeout)
2022-03-28 13:20:49 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1278341ms till timeout)
2022-03-28 13:20:50 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1277230ms till timeout)
2022-03-28 13:20:51 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1276118ms till timeout)
2022-03-28 13:20:52 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1275007ms till timeout)
2022-03-28 13:20:53 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1273896ms till timeout)
2022-03-28 13:20:54 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1272786ms till timeout)
2022-03-28 13:20:56 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1271668ms till timeout)
2022-03-28 13:20:57 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1270556ms till timeout)
2022-03-28 13:20:58 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1269445ms till timeout)
2022-03-28 13:20:59 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1268334ms till timeout)
2022-03-28 13:21:00 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1267223ms till timeout)
2022-03-28 13:21:01 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1266112ms till timeout)
2022-03-28 13:21:02 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1265002ms till timeout)
2022-03-28 13:21:03 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1263888ms till timeout)
2022-03-28 13:21:04 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1262776ms till timeout)
2022-03-28 13:21:06 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1261665ms till timeout)
2022-03-28 13:21:07 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1260553ms till timeout)
2022-03-28 13:21:08 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1259441ms till timeout)
2022-03-28 13:21:09 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1258328ms till timeout)
2022-03-28 13:21:10 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1257218ms till timeout)
2022-03-28 13:21:11 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1256105ms till timeout)
2022-03-28 13:21:12 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1254992ms till timeout)
2022-03-28 13:21:13 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1253881ms till timeout)
2022-03-28 13:21:14 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1252771ms till timeout)
2022-03-28 13:21:16 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1251660ms till timeout)
2022-03-28 13:21:17 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1250548ms till timeout)
2022-03-28 13:21:18 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1249438ms till timeout)
2022-03-28 13:21:19 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1248285ms till timeout)
2022-03-28 13:21:20 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1247174ms till timeout)
2022-03-28 13:21:21 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1246061ms till timeout)
2022-03-28 13:21:22 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1244949ms till timeout)
2022-03-28 13:21:23 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1243837ms till timeout)
2022-03-28 13:21:24 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1242725ms till timeout)
2022-03-28 13:21:26 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1241615ms till timeout)
2022-03-28 13:21:27 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1240503ms till timeout)
2022-03-28 13:21:28 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1239394ms till timeout)
2022-03-28 13:21:29 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1238282ms till timeout)
2022-03-28 13:21:30 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1237171ms till timeout)
2022-03-28 13:21:31 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1236058ms till timeout)
2022-03-28 13:21:32 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1234947ms till timeout)
2022-03-28 13:21:33 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1233835ms till timeout)
2022-03-28 13:21:35 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1232723ms till timeout)
2022-03-28 13:21:36 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1231612ms till timeout)
2022-03-28 13:21:37 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1230502ms till timeout)
2022-03-28 13:21:38 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1229390ms till timeout)
2022-03-28 13:21:39 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1228277ms till timeout)
2022-03-28 13:21:40 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1227165ms till timeout)
2022-03-28 13:21:41 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1226055ms till timeout)
2022-03-28 13:21:42 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1224943ms till timeout)
2022-03-28 13:21:43 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1223830ms till timeout)
2022-03-28 13:21:45 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1222719ms till timeout)
2022-03-28 13:21:46 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1221609ms till timeout)
2022-03-28 13:21:47 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1220497ms till timeout)
2022-03-28 13:21:48 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1219386ms till timeout)
2022-03-28 13:21:49 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1218276ms till timeout)
2022-03-28 13:21:50 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1217165ms till timeout)
2022-03-28 13:21:51 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Kafka: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (1216051ms till timeout)
2022-03-28 13:21:52 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:444] Kafka: my-cluster-c249d04b is in desired state: Ready
2022-03-28 13:21:52 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update KafkaTopic my-topic-352474572-722982643 in namespace namespace-10
2022-03-28 13:21:52 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:164] Using Namespace: namespace-10
2022-03-28 13:21:52 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaTopic:my-topic-352474572-722982643
2022-03-28 13:21:53 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-352474572-722982643 will have desired state: Ready
2022-03-28 13:21:53 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-352474572-722982643 will have desired state: Ready
2022-03-28 13:21:53 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:444] KafkaTopic: my-topic-352474572-722982643 is in desired state: Ready
2022-03-28 13:21:53 [ForkJoinPool-2-worker-3] INFO  [ReconciliationST:147] Adding pause annotation into KafkaTopic resource and changing replication factor
2022-03-28 13:21:53 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:433] Wait for KafkaTopic: my-topic-352474572-722982643 will have desired state: ReconciliationPaused
2022-03-28 13:21:53 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for KafkaTopic: my-topic-352474572-722982643 will have desired state: ReconciliationPaused
2022-03-28 13:21:53 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaTopic: my-topic-352474572-722982643 will have desired state: ReconciliationPaused not ready, will try again in 1000 ms (179893ms till timeout)
2022-03-28 13:21:54 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:444] KafkaTopic: my-topic-352474572-722982643 is in desired state: ReconciliationPaused
2022-03-28 13:21:54 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-c249d04b-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-352474572-722982643 --describe --bootstrap-server my-cluster-c249d04b-kafka-bootstrap:9092
2022-03-28 13:21:57 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-c249d04b-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-352474572-722982643 --describe --bootstrap-server my-cluster-c249d04b-kafka-bootstrap:9092
2022-03-28 13:21:57 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 13:21:57 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for KafkaTopic's spec will be stable
2022-03-28 13:21:57 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-c249d04b-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-352474572-722982643 --describe --bootstrap-server my-cluster-c249d04b-kafka-bootstrap:9092
2022-03-28 13:22:01 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-c249d04b-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-352474572-722982643 --describe --bootstrap-server my-cluster-c249d04b-kafka-bootstrap:9092
2022-03-28 13:22:01 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 13:22:01 [ForkJoinPool-2-worker-3] INFO  [KafkaTopicUtils:209] KafkaTopic's spec gonna be stable in 19 polls
2022-03-28 13:22:01 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (176713ms till timeout)
2022-03-28 13:22:02 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-c249d04b-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-352474572-722982643 --describe --bootstrap-server my-cluster-c249d04b-kafka-bootstrap:9092
2022-03-28 13:22:05 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-c249d04b-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-352474572-722982643 --describe --bootstrap-server my-cluster-c249d04b-kafka-bootstrap:9092
2022-03-28 13:22:05 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 13:22:05 [ForkJoinPool-2-worker-3] INFO  [KafkaTopicUtils:209] KafkaTopic's spec gonna be stable in 18 polls
2022-03-28 13:22:05 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (172452ms till timeout)
2022-03-28 13:22:06 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-c249d04b-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-352474572-722982643 --describe --bootstrap-server my-cluster-c249d04b-kafka-bootstrap:9092
2022-03-28 13:22:10 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-c249d04b-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-352474572-722982643 --describe --bootstrap-server my-cluster-c249d04b-kafka-bootstrap:9092
2022-03-28 13:22:10 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 13:22:10 [ForkJoinPool-2-worker-3] INFO  [KafkaTopicUtils:209] KafkaTopic's spec gonna be stable in 17 polls
2022-03-28 13:22:10 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (167400ms till timeout)
2022-03-28 13:22:11 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-c249d04b-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-352474572-722982643 --describe --bootstrap-server my-cluster-c249d04b-kafka-bootstrap:9092
2022-03-28 13:22:24 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-c249d04b-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-352474572-722982643 --describe --bootstrap-server my-cluster-c249d04b-kafka-bootstrap:9092
2022-03-28 13:22:24 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 13:22:24 [ForkJoinPool-2-worker-3] INFO  [KafkaTopicUtils:209] KafkaTopic's spec gonna be stable in 16 polls
2022-03-28 13:22:24 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (153019ms till timeout)
2022-03-28 13:22:25 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-c249d04b-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-352474572-722982643 --describe --bootstrap-server my-cluster-c249d04b-kafka-bootstrap:9092
2022-03-28 13:22:34 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-c249d04b-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-352474572-722982643 --describe --bootstrap-server my-cluster-c249d04b-kafka-bootstrap:9092
2022-03-28 13:22:34 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 13:22:34 [ForkJoinPool-2-worker-3] INFO  [KafkaTopicUtils:209] KafkaTopic's spec gonna be stable in 15 polls
2022-03-28 13:22:34 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (143673ms till timeout)
2022-03-28 13:22:35 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-c249d04b-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-352474572-722982643 --describe --bootstrap-server my-cluster-c249d04b-kafka-bootstrap:9092
2022-03-28 13:22:38 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-c249d04b-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-352474572-722982643 --describe --bootstrap-server my-cluster-c249d04b-kafka-bootstrap:9092
2022-03-28 13:22:38 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 13:22:38 [ForkJoinPool-2-worker-3] INFO  [KafkaTopicUtils:209] KafkaTopic's spec gonna be stable in 14 polls
2022-03-28 13:22:38 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (139220ms till timeout)
2022-03-28 13:22:39 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-c249d04b-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-352474572-722982643 --describe --bootstrap-server my-cluster-c249d04b-kafka-bootstrap:9092
2022-03-28 13:22:42 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-c249d04b-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-352474572-722982643 --describe --bootstrap-server my-cluster-c249d04b-kafka-bootstrap:9092
2022-03-28 13:22:42 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 13:22:42 [ForkJoinPool-2-worker-3] INFO  [KafkaTopicUtils:209] KafkaTopic's spec gonna be stable in 13 polls
2022-03-28 13:22:42 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (134881ms till timeout)
2022-03-28 13:22:43 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-c249d04b-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-352474572-722982643 --describe --bootstrap-server my-cluster-c249d04b-kafka-bootstrap:9092
2022-03-28 13:22:47 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-c249d04b-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-352474572-722982643 --describe --bootstrap-server my-cluster-c249d04b-kafka-bootstrap:9092
2022-03-28 13:22:47 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 13:22:47 [ForkJoinPool-2-worker-3] INFO  [KafkaTopicUtils:209] KafkaTopic's spec gonna be stable in 12 polls
2022-03-28 13:22:47 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (130678ms till timeout)
2022-03-28 13:22:48 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-c249d04b-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-352474572-722982643 --describe --bootstrap-server my-cluster-c249d04b-kafka-bootstrap:9092
2022-03-28 13:22:51 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-c249d04b-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-352474572-722982643 --describe --bootstrap-server my-cluster-c249d04b-kafka-bootstrap:9092
2022-03-28 13:22:51 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 13:22:51 [ForkJoinPool-2-worker-3] INFO  [KafkaTopicUtils:209] KafkaTopic's spec gonna be stable in 11 polls
2022-03-28 13:22:51 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (126472ms till timeout)
2022-03-28 13:22:52 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-c249d04b-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-352474572-722982643 --describe --bootstrap-server my-cluster-c249d04b-kafka-bootstrap:9092
2022-03-28 13:22:55 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-c249d04b-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-352474572-722982643 --describe --bootstrap-server my-cluster-c249d04b-kafka-bootstrap:9092
2022-03-28 13:22:55 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 13:22:55 [ForkJoinPool-2-worker-3] INFO  [KafkaTopicUtils:209] KafkaTopic's spec gonna be stable in 10 polls
2022-03-28 13:22:55 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (122217ms till timeout)
2022-03-28 13:22:56 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-c249d04b-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-352474572-722982643 --describe --bootstrap-server my-cluster-c249d04b-kafka-bootstrap:9092
2022-03-28 13:22:59 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-c249d04b-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-352474572-722982643 --describe --bootstrap-server my-cluster-c249d04b-kafka-bootstrap:9092
2022-03-28 13:22:59 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 13:22:59 [ForkJoinPool-2-worker-3] INFO  [KafkaTopicUtils:209] KafkaTopic's spec gonna be stable in 9 polls
2022-03-28 13:22:59 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (117965ms till timeout)
2022-03-28 13:23:00 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-c249d04b-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-352474572-722982643 --describe --bootstrap-server my-cluster-c249d04b-kafka-bootstrap:9092
2022-03-28 13:23:04 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-c249d04b-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-352474572-722982643 --describe --bootstrap-server my-cluster-c249d04b-kafka-bootstrap:9092
2022-03-28 13:23:04 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 13:23:04 [ForkJoinPool-2-worker-3] INFO  [KafkaTopicUtils:209] KafkaTopic's spec gonna be stable in 8 polls
2022-03-28 13:23:04 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (113743ms till timeout)
2022-03-28 13:23:05 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-c249d04b-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-352474572-722982643 --describe --bootstrap-server my-cluster-c249d04b-kafka-bootstrap:9092
2022-03-28 13:23:08 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-c249d04b-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-352474572-722982643 --describe --bootstrap-server my-cluster-c249d04b-kafka-bootstrap:9092
2022-03-28 13:23:08 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 13:23:08 [ForkJoinPool-2-worker-3] INFO  [KafkaTopicUtils:209] KafkaTopic's spec gonna be stable in 7 polls
2022-03-28 13:23:08 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (109411ms till timeout)
2022-03-28 13:23:09 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-c249d04b-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-352474572-722982643 --describe --bootstrap-server my-cluster-c249d04b-kafka-bootstrap:9092
2022-03-28 13:23:12 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-c249d04b-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-352474572-722982643 --describe --bootstrap-server my-cluster-c249d04b-kafka-bootstrap:9092
2022-03-28 13:23:12 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 13:23:12 [ForkJoinPool-2-worker-3] INFO  [KafkaTopicUtils:209] KafkaTopic's spec gonna be stable in 6 polls
2022-03-28 13:23:12 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (105115ms till timeout)
2022-03-28 13:23:13 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-c249d04b-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-352474572-722982643 --describe --bootstrap-server my-cluster-c249d04b-kafka-bootstrap:9092
2022-03-28 13:23:17 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-c249d04b-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-352474572-722982643 --describe --bootstrap-server my-cluster-c249d04b-kafka-bootstrap:9092
2022-03-28 13:23:17 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 13:23:17 [ForkJoinPool-2-worker-3] INFO  [KafkaTopicUtils:209] KafkaTopic's spec gonna be stable in 5 polls
2022-03-28 13:23:17 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (100834ms till timeout)
2022-03-28 13:23:18 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-c249d04b-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-352474572-722982643 --describe --bootstrap-server my-cluster-c249d04b-kafka-bootstrap:9092
2022-03-28 13:23:21 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-c249d04b-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-352474572-722982643 --describe --bootstrap-server my-cluster-c249d04b-kafka-bootstrap:9092
2022-03-28 13:23:21 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 13:23:21 [ForkJoinPool-2-worker-3] INFO  [KafkaTopicUtils:209] KafkaTopic's spec gonna be stable in 4 polls
2022-03-28 13:23:21 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (96641ms till timeout)
2022-03-28 13:23:22 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-c249d04b-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-352474572-722982643 --describe --bootstrap-server my-cluster-c249d04b-kafka-bootstrap:9092
2022-03-28 13:23:25 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-c249d04b-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-352474572-722982643 --describe --bootstrap-server my-cluster-c249d04b-kafka-bootstrap:9092
2022-03-28 13:23:25 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 13:23:25 [ForkJoinPool-2-worker-3] INFO  [KafkaTopicUtils:209] KafkaTopic's spec gonna be stable in 3 polls
2022-03-28 13:23:25 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (92433ms till timeout)
2022-03-28 13:23:26 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-c249d04b-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-352474572-722982643 --describe --bootstrap-server my-cluster-c249d04b-kafka-bootstrap:9092
2022-03-28 13:23:29 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-c249d04b-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-352474572-722982643 --describe --bootstrap-server my-cluster-c249d04b-kafka-bootstrap:9092
2022-03-28 13:23:29 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 13:23:29 [ForkJoinPool-2-worker-3] INFO  [KafkaTopicUtils:209] KafkaTopic's spec gonna be stable in 2 polls
2022-03-28 13:23:29 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (88158ms till timeout)
2022-03-28 13:23:30 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-c249d04b-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-352474572-722982643 --describe --bootstrap-server my-cluster-c249d04b-kafka-bootstrap:9092
2022-03-28 13:23:33 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-c249d04b-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-352474572-722982643 --describe --bootstrap-server my-cluster-c249d04b-kafka-bootstrap:9092
2022-03-28 13:23:33 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 13:23:33 [ForkJoinPool-2-worker-3] INFO  [KafkaTopicUtils:209] KafkaTopic's spec gonna be stable in 1 polls
2022-03-28 13:23:33 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaTopic's spec will be stable not ready, will try again in 1000 ms (83875ms till timeout)
2022-03-28 13:23:34 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 exec my-cluster-c249d04b-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-352474572-722982643 --describe --bootstrap-server my-cluster-c249d04b-kafka-bootstrap:9092
2022-03-28 13:23:38 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Command: oc --namespace namespace-10 exec my-cluster-c249d04b-kafka-0 -- /opt/kafka/bin/kafka-topics.sh --topic my-topic-352474572-722982643 --describe --bootstrap-server my-cluster-c249d04b-kafka-bootstrap:9092
2022-03-28 13:23:38 [ForkJoinPool-2-worker-3] INFO  [Exec:417] Return code: 0
2022-03-28 13:23:38 [ForkJoinPool-2-worker-3] INFO  [KafkaTopicUtils:201] KafkaTopic's spec is stable for 20 polls intervals
2022-03-28 13:23:38 [ForkJoinPool-2-worker-3] INFO  [ReconciliationST:156] Setting annotation to "false", partitions should be scaled to 4
2022-03-28 13:23:39 [ForkJoinPool-2-worker-3] INFO  [KafkaTopicUtils:124] Waiting for KafkaTopic change my-topic-352474572-722982643
2022-03-28 13:23:39 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for KafkaTopic change my-topic-352474572-722982643
2022-03-28 13:23:39 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:155] Create/Update KafkaRebalance my-cluster-c249d04b in namespace namespace-10
2022-03-28 13:23:39 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:164] Using Namespace: namespace-10
2022-03-28 13:23:39 [ForkJoinPool-2-worker-3] WARN  [VersionUsageUtils:60] The client is using resource type 'kafkarebalances' with unstable version 'v1beta2'
2022-03-28 13:23:39 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: readiness is fulfilled for resource KafkaRebalance:my-cluster-c249d04b
2022-03-28 13:23:39 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:433] Wait for KafkaRebalance: my-cluster-c249d04b will have desired state: PendingProposal
2022-03-28 13:23:39 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for KafkaRebalance: my-cluster-c249d04b will have desired state: PendingProposal
2022-03-28 13:23:39 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: PendingProposal not ready, will try again in 1000 ms (359902ms till timeout)
2022-03-28 13:23:40 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:444] KafkaRebalance: my-cluster-c249d04b is in desired state: PendingProposal
2022-03-28 13:23:40 [ForkJoinPool-2-worker-3] INFO  [ReconciliationST:164] Waiting for ProposalReady, then add pause and rebalance annotation, rebalancing should not be triggered
2022-03-28 13:23:40 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:433] Wait for KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady
2022-03-28 13:23:40 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady
2022-03-28 13:23:40 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (599904ms till timeout)
2022-03-28 13:23:42 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (598805ms till timeout)
2022-03-28 13:23:43 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (597705ms till timeout)
2022-03-28 13:23:44 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (596607ms till timeout)
2022-03-28 13:23:45 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (595505ms till timeout)
2022-03-28 13:23:46 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (594403ms till timeout)
2022-03-28 13:23:47 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (593304ms till timeout)
2022-03-28 13:23:48 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (592201ms till timeout)
2022-03-28 13:23:49 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (591097ms till timeout)
2022-03-28 13:23:50 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (589999ms till timeout)
2022-03-28 13:23:51 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (588901ms till timeout)
2022-03-28 13:23:53 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (587803ms till timeout)
2022-03-28 13:23:54 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (586701ms till timeout)
2022-03-28 13:23:55 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (585601ms till timeout)
2022-03-28 13:23:56 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (584503ms till timeout)
2022-03-28 13:23:57 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (583404ms till timeout)
2022-03-28 13:23:58 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (582305ms till timeout)
2022-03-28 13:23:59 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (581207ms till timeout)
2022-03-28 13:24:00 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (580106ms till timeout)
2022-03-28 13:24:01 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (579007ms till timeout)
2022-03-28 13:24:02 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (577908ms till timeout)
2022-03-28 13:24:04 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (576808ms till timeout)
2022-03-28 13:24:05 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (575711ms till timeout)
2022-03-28 13:24:06 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (574612ms till timeout)
2022-03-28 13:24:07 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (573514ms till timeout)
2022-03-28 13:24:08 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (572415ms till timeout)
2022-03-28 13:24:09 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (571317ms till timeout)
2022-03-28 13:24:10 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (570219ms till timeout)
2022-03-28 13:24:11 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (569120ms till timeout)
2022-03-28 13:24:12 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (568021ms till timeout)
2022-03-28 13:24:13 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (566922ms till timeout)
2022-03-28 13:24:15 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (565823ms till timeout)
2022-03-28 13:24:16 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (564725ms till timeout)
2022-03-28 13:24:17 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (563626ms till timeout)
2022-03-28 13:24:18 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (562527ms till timeout)
2022-03-28 13:24:19 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (561427ms till timeout)
2022-03-28 13:24:20 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (560329ms till timeout)
2022-03-28 13:24:21 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (559230ms till timeout)
2022-03-28 13:24:22 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (558131ms till timeout)
2022-03-28 13:24:23 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (557033ms till timeout)
2022-03-28 13:24:24 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (555934ms till timeout)
2022-03-28 13:24:26 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (554798ms till timeout)
2022-03-28 13:24:27 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (553699ms till timeout)
2022-03-28 13:24:28 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (552600ms till timeout)
2022-03-28 13:24:29 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (551501ms till timeout)
2022-03-28 13:24:30 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (550403ms till timeout)
2022-03-28 13:24:31 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (549304ms till timeout)
2022-03-28 13:24:32 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (548205ms till timeout)
2022-03-28 13:24:33 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (547107ms till timeout)
2022-03-28 13:24:34 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (546008ms till timeout)
2022-03-28 13:24:35 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (544909ms till timeout)
2022-03-28 13:24:37 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (543810ms till timeout)
2022-03-28 13:24:38 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (542709ms till timeout)
2022-03-28 13:24:39 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (541608ms till timeout)
2022-03-28 13:24:40 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (540511ms till timeout)
2022-03-28 13:24:41 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (539413ms till timeout)
2022-03-28 13:24:42 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (538314ms till timeout)
2022-03-28 13:24:43 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (537215ms till timeout)
2022-03-28 13:24:44 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (536116ms till timeout)
2022-03-28 13:24:45 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (535014ms till timeout)
2022-03-28 13:24:46 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (533913ms till timeout)
2022-03-28 13:24:48 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (532814ms till timeout)
2022-03-28 13:24:49 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (531715ms till timeout)
2022-03-28 13:24:50 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (530618ms till timeout)
2022-03-28 13:24:51 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (529519ms till timeout)
2022-03-28 13:24:52 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (528421ms till timeout)
2022-03-28 13:24:53 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (527323ms till timeout)
2022-03-28 13:24:54 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (526225ms till timeout)
2022-03-28 13:24:55 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (525128ms till timeout)
2022-03-28 13:24:56 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (524029ms till timeout)
2022-03-28 13:24:57 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (522931ms till timeout)
2022-03-28 13:24:58 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (521833ms till timeout)
2022-03-28 13:25:00 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (520734ms till timeout)
2022-03-28 13:25:01 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (519635ms till timeout)
2022-03-28 13:25:02 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (518536ms till timeout)
2022-03-28 13:25:03 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (517438ms till timeout)
2022-03-28 13:25:04 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (516339ms till timeout)
2022-03-28 13:25:05 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (515241ms till timeout)
2022-03-28 13:25:06 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (514142ms till timeout)
2022-03-28 13:25:07 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (513043ms till timeout)
2022-03-28 13:25:08 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (511945ms till timeout)
2022-03-28 13:25:09 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (510846ms till timeout)
2022-03-28 13:25:11 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (509748ms till timeout)
2022-03-28 13:25:12 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (508649ms till timeout)
2022-03-28 13:25:13 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (507550ms till timeout)
2022-03-28 13:25:14 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (506451ms till timeout)
2022-03-28 13:25:15 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (505344ms till timeout)
2022-03-28 13:25:16 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (504245ms till timeout)
2022-03-28 13:25:17 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (503147ms till timeout)
2022-03-28 13:25:18 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (502048ms till timeout)
2022-03-28 13:25:19 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (500949ms till timeout)
2022-03-28 13:25:20 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (499849ms till timeout)
2022-03-28 13:25:22 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (498751ms till timeout)
2022-03-28 13:25:23 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (497652ms till timeout)
2022-03-28 13:25:24 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (496553ms till timeout)
2022-03-28 13:25:25 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (495455ms till timeout)
2022-03-28 13:25:26 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (494356ms till timeout)
2022-03-28 13:25:27 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (493258ms till timeout)
2022-03-28 13:25:28 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (492159ms till timeout)
2022-03-28 13:25:29 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (491060ms till timeout)
2022-03-28 13:25:30 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (489963ms till timeout)
2022-03-28 13:25:31 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (488863ms till timeout)
2022-03-28 13:25:33 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (487764ms till timeout)
2022-03-28 13:25:34 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (486665ms till timeout)
2022-03-28 13:25:35 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (485567ms till timeout)
2022-03-28 13:25:36 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (484468ms till timeout)
2022-03-28 13:25:37 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (483369ms till timeout)
2022-03-28 13:25:38 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (482271ms till timeout)
2022-03-28 13:25:39 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (481172ms till timeout)
2022-03-28 13:25:40 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (480073ms till timeout)
2022-03-28 13:25:41 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (478974ms till timeout)
2022-03-28 13:25:42 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (477876ms till timeout)
2022-03-28 13:25:44 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (476778ms till timeout)
2022-03-28 13:25:45 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (475679ms till timeout)
2022-03-28 13:25:46 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (474581ms till timeout)
2022-03-28 13:25:47 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (473482ms till timeout)
2022-03-28 13:25:48 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (472383ms till timeout)
2022-03-28 13:25:49 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (471284ms till timeout)
2022-03-28 13:25:50 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (470187ms till timeout)
2022-03-28 13:25:51 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (469090ms till timeout)
2022-03-28 13:25:52 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (467991ms till timeout)
2022-03-28 13:25:53 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (466893ms till timeout)
2022-03-28 13:25:55 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (465795ms till timeout)
2022-03-28 13:25:56 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (464697ms till timeout)
2022-03-28 13:25:57 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (463599ms till timeout)
2022-03-28 13:25:58 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (462501ms till timeout)
2022-03-28 13:25:59 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (461401ms till timeout)
2022-03-28 13:26:00 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (460305ms till timeout)
2022-03-28 13:26:01 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (459206ms till timeout)
2022-03-28 13:26:02 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (458108ms till timeout)
2022-03-28 13:26:03 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (457009ms till timeout)
2022-03-28 13:26:04 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (455910ms till timeout)
2022-03-28 13:26:06 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (454811ms till timeout)
2022-03-28 13:26:07 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (453711ms till timeout)
2022-03-28 13:26:08 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (452613ms till timeout)
2022-03-28 13:26:09 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (451514ms till timeout)
2022-03-28 13:26:10 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (450417ms till timeout)
2022-03-28 13:26:11 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (449318ms till timeout)
2022-03-28 13:26:12 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (448219ms till timeout)
2022-03-28 13:26:13 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (447119ms till timeout)
2022-03-28 13:26:14 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (446020ms till timeout)
2022-03-28 13:26:15 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (444921ms till timeout)
2022-03-28 13:26:17 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (443822ms till timeout)
2022-03-28 13:26:18 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (442723ms till timeout)
2022-03-28 13:26:19 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (441624ms till timeout)
2022-03-28 13:26:20 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (440525ms till timeout)
2022-03-28 13:26:21 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (439425ms till timeout)
2022-03-28 13:26:22 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (438327ms till timeout)
2022-03-28 13:26:23 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (437229ms till timeout)
2022-03-28 13:26:24 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (436131ms till timeout)
2022-03-28 13:26:25 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (435033ms till timeout)
2022-03-28 13:26:26 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (433908ms till timeout)
2022-03-28 13:26:28 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (432809ms till timeout)
2022-03-28 13:26:29 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (431710ms till timeout)
2022-03-28 13:26:30 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (430612ms till timeout)
2022-03-28 13:26:31 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (429513ms till timeout)
2022-03-28 13:26:32 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (428413ms till timeout)
2022-03-28 13:26:33 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (427315ms till timeout)
2022-03-28 13:26:34 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (426217ms till timeout)
2022-03-28 13:26:35 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (425118ms till timeout)
2022-03-28 13:26:36 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (424019ms till timeout)
2022-03-28 13:26:37 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (422921ms till timeout)
2022-03-28 13:26:39 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (421735ms till timeout)
2022-03-28 13:26:40 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (420637ms till timeout)
2022-03-28 13:26:41 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (419539ms till timeout)
2022-03-28 13:26:42 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (418441ms till timeout)
2022-03-28 13:26:43 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (417343ms till timeout)
2022-03-28 13:26:44 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady not ready, will try again in 1000 ms (416245ms till timeout)
2022-03-28 13:26:45 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:444] KafkaRebalance: my-cluster-c249d04b is in desired state: ProposalReady
2022-03-28 13:26:45 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:433] Wait for KafkaRebalance: my-cluster-c249d04b will have desired state: ReconciliationPaused
2022-03-28 13:26:45 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for KafkaRebalance: my-cluster-c249d04b will have desired state: ReconciliationPaused
2022-03-28 13:26:46 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:444] KafkaRebalance: my-cluster-c249d04b is in desired state: ReconciliationPaused
2022-03-28 13:26:46 [ForkJoinPool-2-worker-3] INFO  [KafkaRebalanceUtils:63] Reconciliation #1(test) KafkaRebalance(namespace-10/my-cluster-c249d04b): Annotating KafkaRebalance:my-cluster-c249d04b with annotation approve
2022-03-28 13:26:46 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 annotate kafkarebalance my-cluster-c249d04b strimzi.io/rebalance=approve
2022-03-28 13:26:47 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 annotate kafkarebalance my-cluster-c249d04b strimzi.io/rebalance=approve
2022-03-28 13:26:47 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:26:47 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for KafkaRebalance status will be stable
2022-03-28 13:26:47 [ForkJoinPool-2-worker-3] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-c249d04b): KafkaRebalance status gonna be stable in 19 polls
2022-03-28 13:26:47 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (179902ms till timeout)
2022-03-28 13:26:48 [ForkJoinPool-2-worker-3] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-c249d04b): KafkaRebalance status gonna be stable in 18 polls
2022-03-28 13:26:48 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (178802ms till timeout)
2022-03-28 13:26:49 [ForkJoinPool-2-worker-3] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-c249d04b): KafkaRebalance status gonna be stable in 17 polls
2022-03-28 13:26:49 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (177703ms till timeout)
2022-03-28 13:26:50 [ForkJoinPool-2-worker-3] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-c249d04b): KafkaRebalance status gonna be stable in 16 polls
2022-03-28 13:26:50 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (176603ms till timeout)
2022-03-28 13:26:51 [ForkJoinPool-2-worker-3] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-c249d04b): KafkaRebalance status gonna be stable in 15 polls
2022-03-28 13:26:51 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (175505ms till timeout)
2022-03-28 13:26:53 [ForkJoinPool-2-worker-3] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-c249d04b): KafkaRebalance status gonna be stable in 14 polls
2022-03-28 13:26:53 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (174406ms till timeout)
2022-03-28 13:26:54 [ForkJoinPool-2-worker-3] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-c249d04b): KafkaRebalance status gonna be stable in 13 polls
2022-03-28 13:26:54 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (173307ms till timeout)
2022-03-28 13:26:55 [ForkJoinPool-2-worker-3] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-c249d04b): KafkaRebalance status gonna be stable in 12 polls
2022-03-28 13:26:55 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (172208ms till timeout)
2022-03-28 13:26:56 [ForkJoinPool-2-worker-3] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-c249d04b): KafkaRebalance status gonna be stable in 11 polls
2022-03-28 13:26:56 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (171110ms till timeout)
2022-03-28 13:26:57 [ForkJoinPool-2-worker-3] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-c249d04b): KafkaRebalance status gonna be stable in 10 polls
2022-03-28 13:26:57 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (170011ms till timeout)
2022-03-28 13:26:58 [ForkJoinPool-2-worker-3] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-c249d04b): KafkaRebalance status gonna be stable in 9 polls
2022-03-28 13:26:58 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (168911ms till timeout)
2022-03-28 13:26:59 [ForkJoinPool-2-worker-3] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-c249d04b): KafkaRebalance status gonna be stable in 8 polls
2022-03-28 13:26:59 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (167813ms till timeout)
2022-03-28 13:27:00 [ForkJoinPool-2-worker-3] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-c249d04b): KafkaRebalance status gonna be stable in 7 polls
2022-03-28 13:27:00 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (166715ms till timeout)
2022-03-28 13:27:01 [ForkJoinPool-2-worker-3] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-c249d04b): KafkaRebalance status gonna be stable in 6 polls
2022-03-28 13:27:01 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (165616ms till timeout)
2022-03-28 13:27:02 [ForkJoinPool-2-worker-3] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-c249d04b): KafkaRebalance status gonna be stable in 5 polls
2022-03-28 13:27:02 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (164516ms till timeout)
2022-03-28 13:27:04 [ForkJoinPool-2-worker-3] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-c249d04b): KafkaRebalance status gonna be stable in 4 polls
2022-03-28 13:27:04 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (163417ms till timeout)
2022-03-28 13:27:05 [ForkJoinPool-2-worker-3] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-c249d04b): KafkaRebalance status gonna be stable in 3 polls
2022-03-28 13:27:05 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (162319ms till timeout)
2022-03-28 13:27:06 [ForkJoinPool-2-worker-3] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-c249d04b): KafkaRebalance status gonna be stable in 2 polls
2022-03-28 13:27:06 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (161220ms till timeout)
2022-03-28 13:27:07 [ForkJoinPool-2-worker-3] INFO  [KafkaRebalanceUtils:126] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-c249d04b): KafkaRebalance status gonna be stable in 1 polls
2022-03-28 13:27:07 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance status will be stable not ready, will try again in 1000 ms (160120ms till timeout)
2022-03-28 13:27:08 [ForkJoinPool-2-worker-3] INFO  [KafkaRebalanceUtils:118] Reconciliation #2(test) KafkaRebalance(namespace-10/my-cluster-c249d04b): KafkaRebalance status is stable for 20 polls intervals
2022-03-28 13:27:08 [ForkJoinPool-2-worker-3] INFO  [ReconciliationST:178] Setting annotation to "false" and waiting for KafkaRebalance to be in Ready state
2022-03-28 13:27:08 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:433] Wait for KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady
2022-03-28 13:27:08 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for KafkaRebalance: my-cluster-c249d04b will have desired state: ProposalReady
2022-03-28 13:27:08 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:444] KafkaRebalance: my-cluster-c249d04b is in desired state: ProposalReady
2022-03-28 13:27:08 [ForkJoinPool-2-worker-3] INFO  [KafkaRebalanceUtils:63] Reconciliation #3(test) KafkaRebalance(namespace-10/my-cluster-c249d04b): Annotating KafkaRebalance:my-cluster-c249d04b with annotation approve
2022-03-28 13:27:08 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 annotate kafkarebalance my-cluster-c249d04b strimzi.io/rebalance=approve
2022-03-28 13:27:09 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 annotate kafkarebalance my-cluster-c249d04b strimzi.io/rebalance=approve
2022-03-28 13:27:09 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:27:09 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:433] Wait for KafkaRebalance: my-cluster-c249d04b will have desired state: Ready
2022-03-28 13:27:09 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for KafkaRebalance: my-cluster-c249d04b will have desired state: Ready
2022-03-28 13:27:09 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (599904ms till timeout)
2022-03-28 13:27:10 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (598806ms till timeout)
2022-03-28 13:27:11 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (597707ms till timeout)
2022-03-28 13:27:12 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (596608ms till timeout)
2022-03-28 13:27:13 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (595508ms till timeout)
2022-03-28 13:27:15 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (594408ms till timeout)
2022-03-28 13:27:16 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (593310ms till timeout)
2022-03-28 13:27:17 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (592211ms till timeout)
2022-03-28 13:27:18 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (591111ms till timeout)
2022-03-28 13:27:19 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (590009ms till timeout)
2022-03-28 13:27:20 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (588910ms till timeout)
2022-03-28 13:27:21 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (587811ms till timeout)
2022-03-28 13:27:22 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (586712ms till timeout)
2022-03-28 13:27:23 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (585613ms till timeout)
2022-03-28 13:27:24 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (584513ms till timeout)
2022-03-28 13:27:26 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (583416ms till timeout)
2022-03-28 13:27:27 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (582316ms till timeout)
2022-03-28 13:27:28 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (581217ms till timeout)
2022-03-28 13:27:29 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (580119ms till timeout)
2022-03-28 13:27:30 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (579020ms till timeout)
2022-03-28 13:27:31 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (577921ms till timeout)
2022-03-28 13:27:32 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (576822ms till timeout)
2022-03-28 13:27:33 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (575723ms till timeout)
2022-03-28 13:27:34 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (574623ms till timeout)
2022-03-28 13:27:35 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (573525ms till timeout)
2022-03-28 13:27:37 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (572426ms till timeout)
2022-03-28 13:27:38 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (571327ms till timeout)
2022-03-28 13:27:39 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (570229ms till timeout)
2022-03-28 13:27:40 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (569130ms till timeout)
2022-03-28 13:27:41 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (568032ms till timeout)
2022-03-28 13:27:42 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (566933ms till timeout)
2022-03-28 13:27:43 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (565833ms till timeout)
2022-03-28 13:27:44 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (564733ms till timeout)
2022-03-28 13:27:45 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (563629ms till timeout)
2022-03-28 13:27:46 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (562528ms till timeout)
2022-03-28 13:27:48 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (561430ms till timeout)
2022-03-28 13:27:49 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (560330ms till timeout)
2022-03-28 13:27:50 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (559224ms till timeout)
2022-03-28 13:27:51 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (558126ms till timeout)
2022-03-28 13:27:52 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (557027ms till timeout)
2022-03-28 13:27:53 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (555927ms till timeout)
2022-03-28 13:27:54 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (554826ms till timeout)
2022-03-28 13:27:55 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (553729ms till timeout)
2022-03-28 13:27:56 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (552630ms till timeout)
2022-03-28 13:27:57 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (551531ms till timeout)
2022-03-28 13:27:59 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (550432ms till timeout)
2022-03-28 13:28:00 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (549333ms till timeout)
2022-03-28 13:28:01 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (548235ms till timeout)
2022-03-28 13:28:02 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (547136ms till timeout)
2022-03-28 13:28:03 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (546038ms till timeout)
2022-03-28 13:28:04 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (544939ms till timeout)
2022-03-28 13:28:05 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (543838ms till timeout)
2022-03-28 13:28:06 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (542739ms till timeout)
2022-03-28 13:28:07 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (541640ms till timeout)
2022-03-28 13:28:09 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (540448ms till timeout)
2022-03-28 13:28:10 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (539349ms till timeout)
2022-03-28 13:28:11 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (538251ms till timeout)
2022-03-28 13:28:12 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (537152ms till timeout)
2022-03-28 13:28:13 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (536051ms till timeout)
2022-03-28 13:28:14 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (534952ms till timeout)
2022-03-28 13:28:15 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (533852ms till timeout)
2022-03-28 13:28:16 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (532752ms till timeout)
2022-03-28 13:28:17 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (531653ms till timeout)
2022-03-28 13:28:18 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (530553ms till timeout)
2022-03-28 13:28:20 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (529454ms till timeout)
2022-03-28 13:28:21 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (528356ms till timeout)
2022-03-28 13:28:22 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (527258ms till timeout)
2022-03-28 13:28:23 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (526158ms till timeout)
2022-03-28 13:28:24 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (525058ms till timeout)
2022-03-28 13:28:25 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (523958ms till timeout)
2022-03-28 13:28:26 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (522860ms till timeout)
2022-03-28 13:28:27 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (521721ms till timeout)
2022-03-28 13:28:28 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (520622ms till timeout)
2022-03-28 13:28:29 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (519523ms till timeout)
2022-03-28 13:28:31 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (518425ms till timeout)
2022-03-28 13:28:32 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (517326ms till timeout)
2022-03-28 13:28:33 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (516226ms till timeout)
2022-03-28 13:28:34 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (515128ms till timeout)
2022-03-28 13:28:35 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (514031ms till timeout)
2022-03-28 13:28:36 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (512932ms till timeout)
2022-03-28 13:28:37 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (511832ms till timeout)
2022-03-28 13:28:38 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (510733ms till timeout)
2022-03-28 13:28:39 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (509634ms till timeout)
2022-03-28 13:28:40 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (508537ms till timeout)
2022-03-28 13:28:42 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (507438ms till timeout)
2022-03-28 13:28:43 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (506339ms till timeout)
2022-03-28 13:28:44 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] KafkaRebalance: my-cluster-c249d04b will have desired state: Ready not ready, will try again in 1000 ms (505239ms till timeout)
2022-03-28 13:28:45 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:444] KafkaRebalance: my-cluster-c249d04b is in desired state: Ready
2022-03-28 13:28:45 [ForkJoinPool-2-worker-3] DEBUG [AbstractST:674] ============================================================================
2022-03-28 13:28:45 [ForkJoinPool-2-worker-3] DEBUG [AbstractST:675] [operators.ReconciliationST - After Each] - Clean up after test
2022-03-28 13:28:45 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:344] ############################################################################
2022-03-28 13:28:45 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:348] Delete all resources for testPauseReconciliationInKafkaRebalanceAndTopic
2022-03-28 13:28:45 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:241] Delete of KafkaTopic my-topic-352474572-722982643 in namespace namespace-10
2022-03-28 13:28:45 [ForkJoinPool-2-worker-7] INFO  [ResourceManager:241] Delete of KafkaRebalance my-cluster-c249d04b in namespace namespace-10
2022-03-28 13:28:45 [ForkJoinPool-2-worker-5] INFO  [ResourceManager:241] Delete of Kafka my-cluster-c249d04b in namespace namespace-10
2022-03-28 13:28:45 [ForkJoinPool-2-worker-5] INFO  [KafkaResource:64] Explicit deletion of KafkaTopics in namespace namespace-10, for cruise control Kafka cluster my-cluster-c249d04b
2022-03-28 13:28:45 [ForkJoinPool-2-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaRebalance:my-cluster-c249d04b
2022-03-28 13:28:45 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource KafkaTopic:my-topic-352474572-722982643
2022-03-28 13:28:46 [ForkJoinPool-2-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Kafka:my-cluster-c249d04b
2022-03-28 13:28:46 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:369] ############################################################################
2022-03-28 13:28:46 [ForkJoinPool-2-worker-3] INFO  [TestSuiteNamespaceManager:200] Deleting namespace:namespace-10 for test case:testPauseReconciliationInKafkaRebalanceAndTopic
2022-03-28 13:28:46 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Namespace namespace-10 removal
2022-03-28 13:28:46 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 13:28:46 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 13:28:46 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:28:46 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (479501ms till timeout)
2022-03-28 13:28:47 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 13:28:48 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 13:28:48 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:28:48 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (478074ms till timeout)
2022-03-28 13:28:49 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 13:28:49 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 13:28:49 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:28:49 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (476611ms till timeout)
2022-03-28 13:28:50 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 13:28:51 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 13:28:51 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:28:51 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (475167ms till timeout)
2022-03-28 13:28:52 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 13:28:52 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 13:28:52 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:28:52 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (473714ms till timeout)
2022-03-28 13:28:53 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 13:28:54 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 13:28:54 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:28:54 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (472215ms till timeout)
2022-03-28 13:28:55 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 13:28:55 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 13:28:55 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:28:55 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (470738ms till timeout)
2022-03-28 13:28:56 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 13:28:57 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 13:28:57 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:28:57 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (469244ms till timeout)
2022-03-28 13:28:58 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 13:28:58 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 13:28:58 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:28:58 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (467789ms till timeout)
2022-03-28 13:28:59 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 13:29:00 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 13:29:00 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:29:00 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (466337ms till timeout)
2022-03-28 13:29:01 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 13:29:01 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 13:29:01 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:29:01 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (464888ms till timeout)
2022-03-28 13:29:02 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 13:29:02 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 13:29:02 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:29:02 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (463437ms till timeout)
2022-03-28 13:29:03 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 13:29:04 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 13:29:04 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:29:04 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (461963ms till timeout)
2022-03-28 13:29:05 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 13:29:05 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 13:29:05 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:29:05 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (460520ms till timeout)
2022-03-28 13:29:06 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 13:29:07 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 13:29:07 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:29:07 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (459045ms till timeout)
2022-03-28 13:29:08 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 13:29:08 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 13:29:08 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:29:08 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (457532ms till timeout)
2022-03-28 13:29:09 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 13:29:10 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 13:29:10 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:29:10 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (456073ms till timeout)
2022-03-28 13:29:11 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 13:29:11 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 13:29:11 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:29:11 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (454635ms till timeout)
2022-03-28 13:29:12 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 13:29:13 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 13:29:13 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:29:13 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (453215ms till timeout)
2022-03-28 13:29:14 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 13:29:14 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 13:29:14 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:29:14 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace namespace-10 removal not ready, will try again in 1000 ms (451756ms till timeout)
2022-03-28 13:29:15 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 13:29:16 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-10 get Namespace namespace-10 -o yaml
2022-03-28 13:29:16 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 1
2022-03-28 13:29:16 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 13:29:16 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Error from server (NotFound): namespaces "namespace-10" not found
2022-03-28 13:29:16 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] ======STDERR END======
2022-03-28 13:29:16 [ForkJoinPool-2-worker-3] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@62b9e483=[reconciliation-st], io.strimzi.test.logs.CollectorElement@b850ef2a=[]}
2022-03-28 13:29:16 [ForkJoinPool-2-worker-3] DEBUG [SuiteThreadController:267] testPauseReconciliationInKafkaRebalanceAndTopic - Notifies waiting test cases:[testCapacityFile, testPauseReconciliationInKafkaRebalanceAndTopic, testThrottlingQuotasCreateTopic, testCreatingUsersWithSecretPrefix, testTlsExternalUserWithQuotas, testUserTemplate, testUpdateUser, testTlsExternalUser, testReceiveSimpleMessageTlsScramSha, testConfigurationDiskChangeDoNotTriggersRollingUpdateOfKafkaPods, testSendSimpleMessageTlsScramSha, testUserWithNameMoreThan64Chars, testTlsUserWithQuotas, testConfigurationFileIsCreated, testScramUserWithQuotas, testDeployAndUnDeployCruiseControl, testConfigurationPerformanceOptions, testPauseReconciliationInKafkaRebalanceAndTopic] to and randomly select one to start execution
2022-03-28 13:29:16 [ForkJoinPool-2-worker-3] DEBUG [SuiteThreadController:93] [operators.ReconciliationST] - Removing parallel test: testPauseReconciliationInKafkaRebalanceAndTopic
2022-03-28 13:29:16 [ForkJoinPool-2-worker-3] DEBUG [SuiteThreadController:97] [operators.ReconciliationST] - Parallel test count: 0
2022-03-28 13:29:16 [ForkJoinPool-2-worker-3] INFO  [TestSeparator:29] io.strimzi.systemtest.operators.ReconciliationST.testPauseReconciliationInKafkaRebalanceAndTopic-FINISHED
2022-03-28 13:29:16 [ForkJoinPool-2-worker-3] INFO  [TestSeparator:30] ############################################################################
2022-03-28 13:29:16 [ForkJoinPool-2-worker-3] DEBUG [AbstractST:689] ============================================================================
2022-03-28 13:29:16 [ForkJoinPool-2-worker-3] DEBUG [AbstractST:690] [operators.ReconciliationST - After All] - Clean up after test suite
2022-03-28 13:29:16 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:344] ############################################################################
2022-03-28 13:29:16 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:346] In context ReconciliationST is everything deleted.
2022-03-28 13:29:16 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:369] ############################################################################
2022-03-28 13:29:16 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Namespace reconciliation-st removal
2022-03-28 13:29:16 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace reconciliation-st -o yaml
2022-03-28 13:29:16 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace reconciliation-st -o yaml
2022-03-28 13:29:16 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:29:16 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace reconciliation-st removal not ready, will try again in 1000 ms (479513ms till timeout)
2022-03-28 13:29:17 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace reconciliation-st -o yaml
2022-03-28 13:29:18 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace reconciliation-st -o yaml
2022-03-28 13:29:18 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:29:18 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace reconciliation-st removal not ready, will try again in 1000 ms (478041ms till timeout)
2022-03-28 13:29:19 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace reconciliation-st -o yaml
2022-03-28 13:29:19 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace reconciliation-st -o yaml
2022-03-28 13:29:19 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:29:19 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace reconciliation-st removal not ready, will try again in 1000 ms (476570ms till timeout)
2022-03-28 13:29:20 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace reconciliation-st -o yaml
2022-03-28 13:29:21 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Command: oc --namespace namespace-10 get Namespace reconciliation-st -o yaml
2022-03-28 13:29:21 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 0
2022-03-28 13:29:21 [ForkJoinPool-2-worker-3] TRACE [TestUtils:176] Namespace reconciliation-st removal not ready, will try again in 1000 ms (475092ms till timeout)
2022-03-28 13:29:22 [ForkJoinPool-2-worker-3] TRACE [Exec:248] Running command - oc --namespace namespace-10 get Namespace reconciliation-st -o yaml
2022-03-28 13:29:22 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Failed to exec command: oc --namespace namespace-10 get Namespace reconciliation-st -o yaml
2022-03-28 13:29:22 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Return code: 1
2022-03-28 13:29:22 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] ======STDERR START=======
2022-03-28 13:29:22 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] Error from server (NotFound): namespaces "reconciliation-st" not found
2022-03-28 13:29:22 [ForkJoinPool-2-worker-3] DEBUG [Exec:419] ======STDERR END======
2022-03-28 13:29:22 [ForkJoinPool-2-worker-3] TRACE [KubeClusterResource:398] SUITE_NAMESPACE_MAP after deletion: {io.strimzi.test.logs.CollectorElement@3c1=[infra-namespace], io.strimzi.test.logs.CollectorElement@62b9e483=[], io.strimzi.test.logs.CollectorElement@b850ef2a=[]}
2022-03-28 13:29:22 [ForkJoinPool-2-worker-3] DEBUG [SuiteThreadController:254] ReconciliationST - Notifies waiting test suites:[UserST, TopicST, ReconciliationST, CruiseControlConfigurationST, HttpBridgeScramShaST, ThrottlingQuotaST, ReconciliationST] to and randomly select one to start execution
2022-03-28 13:29:22 [ForkJoinPool-2-worker-3] DEBUG [SuiteThreadController:85] [operators.ReconciliationST] - Removing parallel suite: ReconciliationST
2022-03-28 13:29:22 [ForkJoinPool-2-worker-3] DEBUG [SuiteThreadController:89] [operators.ReconciliationST] - Parallel suites count: 0
[INFO] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 606.947 s - in io.strimzi.systemtest.operators.ReconciliationST
2022-03-28 13:29:22 [ForkJoinPool-2-worker-3] INFO  [SetupClusterOperator:618] ============================================================================
2022-03-28 13:29:22 [ForkJoinPool-2-worker-3] INFO  [SetupClusterOperator:619] Un-installing cluster operator from infra-namespace namespace
2022-03-28 13:29:22 [ForkJoinPool-2-worker-3] INFO  [SetupClusterOperator:620] ============================================================================
2022-03-28 13:29:22 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:344] ############################################################################
2022-03-28 13:29:22 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:348] Delete all resources for JUnit Jupiter
2022-03-28 13:29:22 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkarebalances.kafka.strimzi.io in namespace (not set)
2022-03-28 13:29:22 [ForkJoinPool-2-worker-5] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnects.kafka.strimzi.io in namespace (not set)
2022-03-28 13:29:22 [ForkJoinPool-2-worker-7] INFO  [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-client-delegation in namespace (not set)
2022-03-28 13:29:22 [ForkJoinPool-2-worker-15] INFO  [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-28 13:29:22 [ForkJoinPool-2-worker-13] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkaconnectors.kafka.strimzi.io in namespace (not set)
2022-03-28 13:29:22 [ForkJoinPool-2-worker-9] INFO  [ResourceManager:241] Delete of Deployment strimzi-cluster-operator in namespace infra-namespace
2022-03-28 13:29:22 [ForkJoinPool-2-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-client-delegation
2022-03-28 13:29:22 [ForkJoinPool-2-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnects.kafka.strimzi.io
2022-03-28 13:29:22 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkarebalances.kafka.strimzi.io
2022-03-28 13:29:22 [ForkJoinPool-2-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-namespaced
2022-03-28 13:29:22 [ForkJoinPool-2-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkaconnectors.kafka.strimzi.io
2022-03-28 13:29:22 [ForkJoinPool-2-worker-5] INFO  [ResourceManager:241] Delete of CustomResourceDefinition strimzipodsets.core.strimzi.io in namespace (not set)
2022-03-28 13:29:23 [ForkJoinPool-2-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource Deployment:strimzi-cluster-operator
2022-03-28 13:29:23 [ForkJoinPool-2-worker-7] INFO  [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator in namespace infra-namespace
2022-03-28 13:29:23 [ForkJoinPool-2-worker-13] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormaker2s.kafka.strimzi.io in namespace (not set)
2022-03-28 13:29:23 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator in namespace (not set)
2022-03-28 13:29:23 [ForkJoinPool-2-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:strimzipodsets.core.strimzi.io
2022-03-28 13:29:23 [ForkJoinPool-2-worker-15] INFO  [ResourceManager:241] Delete of RoleBinding strimzi-cluster-operator-entity-operator-delegation in namespace infra-namespace
2022-03-28 13:29:23 [ForkJoinPool-2-worker-5] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkas.kafka.strimzi.io in namespace (not set)
2022-03-28 13:29:23 [ForkJoinPool-2-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator
2022-03-28 13:29:23 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator
2022-03-28 13:29:23 [ForkJoinPool-2-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormaker2s.kafka.strimzi.io
2022-03-28 13:29:23 [ForkJoinPool-2-worker-13] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkabridges.kafka.strimzi.io in namespace (not set)
2022-03-28 13:29:23 [ForkJoinPool-2-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource RoleBinding:strimzi-cluster-operator-entity-operator-delegation
2022-03-28 13:29:23 [ForkJoinPool-2-worker-5] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io
2022-03-28 13:29:23 [ForkJoinPool-2-worker-9] INFO  [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-entity-operator in namespace (not set)
2022-03-28 13:29:23 [ForkJoinPool-2-worker-7] INFO  [ResourceManager:241] Delete of ClusterRoleBinding strimzi-cluster-operator-kafka-broker-delegation in namespace (not set)
2022-03-28 13:29:23 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:241] Delete of ConfigMap strimzi-cluster-operator in namespace infra-namespace
2022-03-28 13:29:23 [ForkJoinPool-2-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkabridges.kafka.strimzi.io
2022-03-28 13:29:23 [ForkJoinPool-2-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-entity-operator
2022-03-28 13:29:23 [ForkJoinPool-2-worker-15] INFO  [ResourceManager:241] Delete of ClusterRole strimzi-kafka-broker in namespace (not set)
2022-03-28 13:29:23 [ForkJoinPool-2-worker-13] INFO  [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-namespaced in namespace (not set)
2022-03-28 13:29:23 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ConfigMap:strimzi-cluster-operator
2022-03-28 13:29:23 [ForkJoinPool-2-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRoleBinding:strimzi-cluster-operator-kafka-broker-delegation
2022-03-28 13:29:24 [ForkJoinPool-2-worker-5] TRACE [TestUtils:176] Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkas.kafka.strimzi.io not ready, will try again in 10000 ms (179474ms till timeout)
2022-03-28 13:29:24 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkausers.kafka.strimzi.io in namespace (not set)
2022-03-28 13:29:24 [ForkJoinPool-2-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-namespaced
2022-03-28 13:29:24 [ForkJoinPool-2-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-broker
2022-03-28 13:29:24 [ForkJoinPool-2-worker-9] INFO  [ResourceManager:241] Delete of ClusterRole strimzi-kafka-client in namespace (not set)
2022-03-28 13:29:24 [ForkJoinPool-2-worker-7] INFO  [ResourceManager:241] Delete of ServiceAccount strimzi-cluster-operator in namespace infra-namespace
2022-03-28 13:29:24 [ForkJoinPool-2-worker-3] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkausers.kafka.strimzi.io
2022-03-28 13:29:24 [ForkJoinPool-2-worker-9] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-kafka-client
2022-03-28 13:29:24 [ForkJoinPool-2-worker-13] INFO  [ResourceManager:241] Delete of ClusterRole strimzi-cluster-operator-global in namespace (not set)
2022-03-28 13:29:24 [ForkJoinPool-2-worker-15] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkatopics.kafka.strimzi.io in namespace (not set)
2022-03-28 13:29:24 [ForkJoinPool-2-worker-11] INFO  [ResourceManager:241] Delete of CustomResourceDefinition kafkamirrormakers.kafka.strimzi.io in namespace (not set)
2022-03-28 13:29:24 [ForkJoinPool-2-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ServiceAccount:strimzi-cluster-operator
2022-03-28 13:29:24 [ForkJoinPool-2-worker-7] INFO  [ResourceManager:241] Delete of ClusterRole strimzi-entity-operator in namespace (not set)
2022-03-28 13:29:24 [ForkJoinPool-2-worker-11] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkamirrormakers.kafka.strimzi.io
2022-03-28 13:29:24 [ForkJoinPool-2-worker-13] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-cluster-operator-global
2022-03-28 13:29:24 [ForkJoinPool-2-worker-15] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource CustomResourceDefinition:kafkatopics.kafka.strimzi.io
2022-03-28 13:29:24 [ForkJoinPool-2-worker-7] DEBUG [TestUtils:125] Waiting for Resource condition: deletion is fulfilled for resource ClusterRole:strimzi-entity-operator
2022-03-28 13:29:34 [ForkJoinPool-2-worker-3] INFO  [ResourceManager:369] ############################################################################
2022-03-28 13:29:34 [ForkJoinPool-2-worker-3] DEBUG [KubeClusterResource:216] Deleting Namespace: infra-namespace
2022-03-28 13:29:34 [ForkJoinPool-2-worker-3] DEBUG [DefaultSharedIndexInformer:142] informer: ready to run resync and reflector for class io.fabric8.kubernetes.api.model.Namespace with resync 0
2022-03-28 13:29:34 [ForkJoinPool-2-worker-3] DEBUG [DefaultSharedIndexInformer:212] informer#Controller: resync skipped due to 0 full resync period class io.fabric8.kubernetes.api.model.Namespace
2022-03-28 13:29:34 [ForkJoinPool-2-worker-3] DEBUG [Reflector:95] Listing items (1) for resource class io.fabric8.kubernetes.api.model.Namespace v75266
2022-03-28 13:29:34 [ForkJoinPool-2-worker-3] DEBUG [Reflector:103] Starting watcher for resource class io.fabric8.kubernetes.api.model.Namespace v75266
2022-03-28 13:29:34 [ForkJoinPool-2-worker-3] DEBUG [AbstractWatchManager:222] Watching https://api.morsak-410.strimzi.app-services-dev.net:6443/api/v1/namespaces?fieldSelector=metadata.name%3Dinfra-namespace&resourceVersion=75266&allowWatchBookmarks=true&watch=true...
2022-03-28 13:29:34 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [WatcherWebSocketListener:43] WebSocket successfully opened
2022-03-28 13:29:40 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [Reflector:139] Event received MODIFIED Namespace resourceVersion 75332
2022-03-28 13:29:45 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [Reflector:139] Event received MODIFIED Namespace resourceVersion 75377
2022-03-28 13:29:45 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [Reflector:64] Stopping watcher for resource class io.fabric8.kubernetes.api.model.Namespace v75332 in namespace default
2022-03-28 13:29:45 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [AbstractWatchManager:230] Force closing the watch io.fabric8.kubernetes.client.dsl.internal.WatchConnectionManager@e1a8ef1
2022-03-28 13:29:45 [main] INFO  [TestExecutionListener:40] =======================================================================
2022-03-28 13:29:45 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [Reflector:181] Watch gracefully closed
2022-03-28 13:29:45 [main] INFO  [TestExecutionListener:41] =======================================================================
2022-03-28 13:29:45 [main] INFO  [TestExecutionListener:42]                         Test run finished
2022-03-28 13:29:45 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@a251bea
2022-03-28 13:29:45 [main] INFO  [TestExecutionListener:43] =======================================================================
2022-03-28 13:29:45 [main] INFO  [TestExecutionListener:44] =======================================================================
2022-03-28 13:29:45 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [WatchConnectionManager:60] Closing websocket io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@a251bea
2022-03-28 13:29:45 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [WatchConnectionManager:63] Websocket already closed io.fabric8.kubernetes.client.okhttp.OkHttpWebSocketImpl@a251bea
2022-03-28 13:29:45 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [Reflector:139] Event received DELETED Namespace resourceVersion 75378
[INFO] 
[INFO] Results:
[INFO] 
[WARNING] Flakes: 
[WARNING] io.strimzi.systemtest.operators.ReconciliationST.testPauseReconciliationInKafkaRebalanceAndTopic(ExtensionContext)
[ERROR]   Run 1: ReconciliationST.testPauseReconciliationInKafkaRebalanceAndTopic:154 ? Wait Ti...
[INFO]   Run 2: PASS
[INFO] 
[INFO] 
[WARNING] Tests run: 30, Failures: 0, Errors: 0, Skipped: 0, Flakes: 1
[INFO] 
[INFO] 
[INFO] --- maven-failsafe-plugin:3.0.0-M5:verify (default) @ systemtest ---
[INFO] 
[INFO] --- maven-dependency-plugin:3.1.1:analyze-only (analyze) @ systemtest ---
2022-03-28 13:29:45 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [WatcherWebSocketListener:79] WebSocket close received. code: 1000, reason: 
2022-03-28 13:29:45 [OkHttp https://api.morsak-410.strimzi.app-services-dev.net:6443/...] DEBUG [AbstractWatchManager:140] Ignoring error for already closed/closing connection
[INFO] No dependency problems found
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary for Strimzi - Apache Kafka on Kubernetes and OpenShift 0.29.0-SNAPSHOT:
[INFO] 
[INFO] Strimzi - Apache Kafka on Kubernetes and OpenShift . SUCCESS [  4.037 s]
[INFO] test ............................................... SUCCESS [  5.562 s]
[INFO] crd-annotations .................................... SUCCESS [  3.840 s]
[INFO] crd-generator ...................................... SUCCESS [  6.478 s]
[INFO] api ................................................ SUCCESS [ 27.784 s]
[INFO] mockkube ........................................... SUCCESS [  4.665 s]
[INFO] config-model ....................................... SUCCESS [  3.822 s]
[INFO] certificate-manager ................................ SUCCESS [  4.133 s]
[INFO] operator-common .................................... SUCCESS [  7.398 s]
[INFO] systemtest ......................................... SUCCESS [44:21 min]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  45:30 min
[INFO] Finished at: 2022-03-28T09:29:46-04:00
[INFO] ------------------------------------------------------------------------
